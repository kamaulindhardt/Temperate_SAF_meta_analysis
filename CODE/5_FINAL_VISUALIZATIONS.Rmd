---
title: "5_FINAL_VISUALIZATIONS"
author: "M.K.K. Lindhardt"
date: "2024-11-17"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between


Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

# STEP 0 PREPARING SCRIPT AND READ IN THE DATA

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    patchwork,        # ggplot2 API for sequentially building up a plot
    purrr, 
    ggridges,
    ggbreak,
    tidyr,
    forcats,
    ###################################################################################################################
    # Spatial Data
    tidygeocoder,     # Unified interface for performing both forward and reverse geocoding queries
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    rnaturalearth,    # For world map data
    rnaturalearthdata, 
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})


###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("extract", "raster")
conflict_prefer("intersect", "base")
```


Loading the datasets

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())


# Define your working directory using 'here'
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load datasets
non_imp_dataset <- readRDS(file.path(output_dir, "non_imp_data_rom_tshering.rds"))
imp_dataset <- readRDS(file.path(output_dir, "imp_data_rom_tshering.rds"))
```

```{r}
# Custom colors for each response variable
custom_colors <- c(
  "Biodiversity"             = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)
```


```{r}
imp_dataset |> glimpse()
```



# STEP 1 MAP OF STUDIES' GEOGRAPHIC DISTRIBUTION - SPATIAL DISTRIBUTION 


Publication-ready map that visualizes the ecosystem services (response variables) reported in each study (id_article),
```{r}
# Step 1: Simplify the dataset for visualization
geo_data <- imp_dataset %>%
  group_by(lat = final_lat, lon = final_lon, response_variable) %>%
  summarize(
    n_studies = n_distinct(id_article),
    .groups = "drop"
  ) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

geo_data
```


```{r}
# Step 2: Base world map
world_map <- map_data("world")


# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity"            = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)


# Step 3: Create the enhanced map
geo_distribution_of_studies_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add jittered points for studies
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 2, height = 1.5)
  ) +
  # Apply custom colors
  scale_color_manual(values = custom_colors, 
                     name = "Ecosystem Service",
                     # Enlarges color legend symbols
                     guide = guide_legend(override.aes = list(size = 5))
                     ) +
  scale_size_continuous(
    name = "Number of Studies",
    range = c(4, 8),  # Adjust size range for better visibility
    breaks = c(1, 2, 5),  # Customize breaks based on study count
    labels = c("1", "2", "5+")
  ) +
  # Add labels and enhance the theme
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry Studies",
    subtitle = "Larger Map for Clearer Visualization",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_quickmap() +  # Quick world map projection
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
geo_distribution_of_studies_map
```

```{r}
# Define European bounding box (Longitude and Latitude limits)
europe_bbox <- c(xmin = -15, xmax = 25, ymin = 34, ymax = 58)

# Create the zoomed-in map for Europe
geo_distribution_europe_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add jittered points for studies (Europe only)
  geom_point(
    data = geo_data %>% filter(lon >= europe_bbox["xmin"], lon <= europe_bbox["xmax"],
                               lat >= europe_bbox["ymin"], lat <= europe_bbox["ymax"]),
    aes(x = lon, y = lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 1, height = 0.8)
  ) +
  # Apply custom colors and increase legend dot size
  scale_color_manual(
    values = custom_colors,
    name = "Ecosystem Service",
    guide = guide_legend(override.aes = list(size = 5))  # Enlarges legend symbols
  ) +
  # Keep study size legend unchanged
  scale_size_continuous(
    name = "Number of Studies",
    range = c(4, 8),  
    breaks = c(1, 2, 5),  
    labels = c("1", "2", "5+")
  ) +
  # Add labels and enhance the theme
  labs(
    title = "",
    subtitle = "",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "none",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the zoomed-in Europe map
geo_distribution_europe_map
```

```{r}
# Combining the maps in a nicely designed multiplot


# Global map
geo_distribution_of_studies_map

# Zoomed-in Europe map
geo_distribution_europe_map
```


```{r}
imp_dataset |> glimpse()
```

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(sf)
library(ggnewscale)  # Allows multiple fill scales in ggplot2
library(rnaturalearth)
library(rnaturalearthdata)
library(ggforce)  # For drawing pie charts
```

```{r}
# Step 2: Aggregate Data at Country Level
country_study_counts <- imp_dataset %>%
  group_by(location) %>%
  summarize(n_studies = n_distinct(id_article), .groups = "drop") 

country_study_counts |> glimpse()

# Step 3: Calculate Response Variable Proportions Per Country
response_proportions <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop") %>%
  group_by(location) %>%
  mutate(proportion = n_articles / sum(n_articles)) 

response_proportions |> glimpse()
```

```{r}
# Load world map
world_map <- map_data("world")

world <- ne_countries(scale = "medium", returnclass = "sf")
colnames(world)  # Check available columns

# Step 2: Aggregate number of studies per country
country_study_counts <- imp_dataset %>%
  group_by(location) %>%
  summarize(n_studies = n_distinct(id_article), .groups = "drop")

# Ensure correct numeric format
country_study_counts$n_studies <- as.numeric(country_study_counts$n_studies)

# Step 3: Calculate response variable proportions per country
response_proportions <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop") %>%
  group_by(location) %>%
  mutate(proportion = n_articles / sum(n_articles)) 

# Step 4: Merge study counts with world map
world_data <- world %>%
  left_join(country_study_counts, by = c("name" = "location"))

# Step 5: Compute centroids for pie chart placement
# Convert to MULTIPOINT for countries with multiple polygons
centroids <- world_data %>%
  filter(!is.na(n_studies)) %>%
  st_as_sf() %>%
  st_centroid(of_largest_polygon = TRUE)  # Ensures correct placement

# Standardize country names before merging
country_study_counts <- country_study_counts %>%
  mutate(location = recode(location,
                           "UK" = "United Kingdom",
                           "USA" = "United States of America",
                           "Canada" = "Canada",
                           "Hungary" = "Hungary"))

response_proportions <- response_proportions %>%
  mutate(location = recode(location,
                           "UK" = "United Kingdom",
                           "USA" = "United States of America",
                           "Canada" = "Canada",
                           "Hungary" = "Hungary"))

# Merge study counts with world map
world_data <- world %>%
  left_join(country_study_counts, by = c("name" = "location"))

unique(world_data$name)


# Compute centroids again
centroids <- world_data %>%
  filter(!is.na(n_studies)) %>%
  st_as_sf() %>%
  st_centroid(of_largest_polygon = TRUE)  # Ensures correct placement

# Merge response proportions
pie_chart_data <- centroids %>%
  left_join(response_proportions, by = c("name" = "location")) %>%
  filter(!is.na(proportion))  # Remove missing proportions

# pie_chart_data$sovereignt
# 
# world_data$sovereignt
# 
# centroids$sovereignt
```


```{r}
# Define custom colors for response variables
response_colors <- c(
  "Biodiversity"             = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)

# Step 1: Load world map
world <- ne_countries(scale = "medium", returnclass = "sf")


# Step 6: Generate the final map
final_global_map <- ggplot() +
  # Base map with study counts as color
  geom_sf(data = world_data, aes(fill = n_studies), color = "black", size = 0.3) +
  #scale_fill_viridis_c(name = "Number of Studies", na.value = "gray90", option = "magma") + 
  # Use "Blues" scale from RColorBrewer (monochrome blue gradient)
  scale_fill_distiller(
    name = "Number of Studies",
    palette = "Blues",  # Set to Brewer's "Blues" scale
    direction = 1,  # Light to dark
    na.value = "gray90",
    limits = c(0, max(world_data$n_studies, na.rm = TRUE))
  ) + 

  # Add new fill scale for pie charts (prevents conflicts)
  new_scale_fill() +

  # Add pie charts at country centroids
  geom_arc_bar(
    data = pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],  # X-coord (longitude)
      y0 = st_coordinates(geometry)[,2],  # Y-coord (latitude)
      r0 = 0, r = 3,  # Adjust radius size as needed
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Response Variable", values = response_colors) +  # Use categorical scale

  # Labels & theme adjustments
  labs(
    title = "Geographical Distribution of Ecosystem Services in Agroforestry Studies",
    subtitle = "Pie chart shows proportion of each response variable; color indicates study count",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50")
  )

# Display the final global map
print(final_global_map)
```



```{r}
# Step 1: Define European Bounding Box
europe_bbox <- c(xmin = -10, xmax = 30, ymin = 35, ymax = 65)

# Step 1: Extract all European countries (even those without studies)
europe_all_countries <- world %>%
  filter(continent == "Europe") %>%
  select(name, geometry)  # Keep only necessary columns

# Step 2: Merge `europe_all_countries` with study count data
europe_all_countries <- europe_all_countries %>%
  left_join(country_study_counts, by = c("name" = "location")) %>%
  mutate(n_studies = ifelse(is.na(n_studies), 0, n_studies))  # Fill missing values with 0

# Step 3: Filter Pie Chart Data for Europe Only (Only for Countries with Data)
europe_pie_chart_data <- pie_chart_data %>%
  filter(st_coordinates(geometry)[,1] >= europe_bbox["xmin"],
         st_coordinates(geometry)[,1] <= europe_bbox["xmax"],
         st_coordinates(geometry)[,2] >= europe_bbox["ymin"],
         st_coordinates(geometry)[,2] <= europe_bbox["ymax"])

# Step 4: Filter Pie Chart Data for Europe Only (Only for Countries with Data)
# Step 4: Generate the Zoomed-in Europe Map
geo_distribution_europe_map <- ggplot() +
  # Base map: Show all countries in Europe (with or without data)
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  
  # Ensure `scale_fill_viridis_c()` starts at 1, and gray out countries without data
  # scale_fill_viridis_c(
  #   name = "Number of Studies",
  #   option = "viridis",
  #   na.value = "gray90",
  #   limits = c(1, max(europe_all_countries$n_studies, na.rm = TRUE))
  # ) + 
  # Use "Blues" scale from RColorBrewer (monochrome blue gradient)
  scale_fill_distiller(
    name = "Number of Studies",
    palette = "Blues",  # Set to Brewer's "Blues" scale
    direction = 1,  # Light to dark
    na.value = "gray90",
    limits = c(0, max(europe_all_countries$n_studies, na.rm = TRUE))
  ) + 

  # Fix: Add `new_scale_fill()` before pie charts to separate categorical colors
  new_scale_fill() +

  # Add pie charts at country centroids for response proportions (Only where data exists)
  geom_arc_bar(
    data = europe_pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],  # Longitude
      y0 = st_coordinates(geometry)[,2],  # Latitude
      r0 = 0, r = 2,  # Adjust radius size
      fill = response_variable,  # Ensure this is NOT mapped to `n_studies`
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Response Variable", values = response_colors) +  # Use categorical scale

  # Labels and theme adjustments
  labs(
    title = "Geographical Distribution of Ecosystem Services in Agroforestry Studies (Europe)",
    subtitle = "Pie chart shows proportion of each response variable; color indicates study count",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50")
  )

# Step 5: Display the Zoomed-in Europe Map
print(geo_distribution_europe_map)
```

```{r}
# Generating combined map

# Step 1: Define Bounding Box for Europe
europe_bbox <- c(xmin = -10, xmax = 30, ymin = 35, ymax = 65)

# Step 2: Remove Antarctica and Generate the Global Map
global_map <- ggplot() +
  geom_sf(data = world_data %>% filter(continent != "Antarctica"), aes(fill = n_studies), color = "black", size = 0.3) +
  scale_fill_distiller(
    name = "No. of Studies",  # Renamed legend
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(0, max(world_data$n_studies, na.rm = TRUE)),
    breaks = c(1,3,5,7,9)
    ) + 
  new_scale_fill() +
  geom_arc_bar(
    data = pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 3,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Ecosystem Service", values = response_colors) +  # Updated legend title) +  
  theme_void() +
  theme(legend.position = "top") +
  guides(fill = guide_legend(nrow = 1, ncol = 8,
                             byrow = TRUE)) 

global_map

# Step 3: Generate the Zoomed-in Europe Map using BBOX
europe_map <- ggplot() +
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  scale_fill_distiller(
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(1, 8)
  ) + 
  new_scale_fill() +
  geom_arc_bar(
    data = europe_pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 2,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(values = response_colors) +  
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_void() +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "white"),
    panel.border = element_rect(color = "black", size = 3,
                                fill = NA)
  )

europe_map

# Step 4: Combine the Global and Europe Maps using Patchwork
final_combined_map_of_study_geodistribution <- global_map + 
  inset_element(europe_map, 
                left = 0.10, 
                bottom = 0.18, 
                right = 0.95, 
                top = 0.63, 
                align_to = "panel")

# Step 6: Display the Final Merged Map
final_combined_map_of_study_geodistribution
```



## Adapted Spatial / Geographic Distribution of Studies with Pie Charts

```{r}
unique_countries <- unique(pie_chart_data$admin)
unique_countries
```

```{r}
generate_piechart_for_country <- function(country_name) {
  ggplot() +
    geom_arc_bar(
      data = filter(pie_chart_data, admin == country_name),
      aes(
        x0 = st_coordinates(geometry)[, 1],
        y0 = st_coordinates(geometry)[, 2],
        r0 = 0, r = 3,
        fill = response_variable,
        amount = proportion
      ),
      stat = "pie",
      inherit.aes = FALSE
    ) +
    scale_fill_manual(values = response_colors) +
    coord_fixed() +
    theme_void() +
    theme(
      legend.position = "right",
      plot.title = element_text(hjust = 0.5)
    ) +
    labs(title = paste("Pie Chart –", country_name))
}
```

```{r}
generate_piechart_for_country("United Kingdom")
```
```{r}
for (country in unique_countries) {
  print(generate_piechart_for_country(country))
  readline(prompt = "Press [Enter] to continue...")
}
```

```{r}
global_map_north <- ggplot() +
  geom_sf(
    data = world_data %>% filter(continent != "Antarctica"),
    aes(fill = n_studies), color = "black", size = 0.3
  ) +
  scale_fill_distiller(
    name = "No. of Studies",
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(0, 10),  # Fixed scale from 0 to 10
    breaks = c(0, 2, 4, 6, 8, 10)
  ) +
  new_scale_fill() +
  scale_fill_manual(name = "Ecosystem Service", values = response_colors) +
  coord_sf(ylim = c(0, 90)) +  # Crop to Northern Hemisphere
  theme_void() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.5, "cm"),           # Smaller legend keys
    legend.text = element_text(size = 10),       # Smaller text
    legend.title = element_text(size = 10, face = "bold")  # Smaller, bold title
  ) +
  guides(fill = guide_legend(nrow = 1, byrow = TRUE))

global_map_north

# Step 2: Generate the Zoomed-in Europe Map using BBOX
europe_map <- ggplot() +
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  scale_fill_distiller(
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(1, 8)
  ) + 
  new_scale_fill() +
  scale_fill_manual(values = response_colors) +  
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_void() +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "white"),
    panel.border = element_rect(color = "black", size = 3,
                                fill = NA)
  )


final_combined_map_of_study_geodistribution <- global_map_north +
  inset_element(
    europe_map,
    left = 0.00,    # expand slightly to the left - further to the left
    bottom = 0.28,  # move slightly lower to accommodate extra height - a bit lower to make space
    right = 1.00,   # expand slightly to the right - all the way to the right
    top = 0.92,     # push the top further up - even taller
    align_to = "panel"
  )


# Display it
final_combined_map_of_study_geodistribution
```
















































# STEP 2 TEMPORAL DISTRIBUTION

```{r}
imp_dataset |> glimpse()
```


## Distribution of Studies Over Time

```{r}
# Define a function for generating the chart
create_stacked_bar_chart <- function(data, id_column, custom_colors, title = "Distribution Over Time", y_label = "Count") {
  
  # Prepare the data: Count unique entries by year and response variable
  grouped_data <- data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |>
    distinct(!!sym(id_column), Year, response_variable) |>  # Dynamically use id_column
    count(Year, response_variable)
  
  # Create the stacked bar chart
  ggplot(grouped_data, aes(x = Year, y = n, fill = response_variable)) +
    geom_bar(stat = "identity", position = "stack", color = "black") +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(grouped_data$Year, na.rm = TRUE), max(grouped_data$Year, na.rm = TRUE), by = 5),  # Adjust as needed
      labels = scales::number_format(scale = 1, accuracy = 1)
    ) +
    labs(
      title = title,
      subtitle = "Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette

# Generate chart for id_article (studies)
chart_studies <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_article",  # Switch to "id_obs" for observations
  custom_colors = custom_colors,
  title = "Distribution of Studies Over Time",
  y_label = "Number of Studies"
)

# Generate chart for id_obs (observations)
chart_observations <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_obs",
  custom_colors = custom_colors,
  title = "Distribution of Observations Over Time",
  y_label = "Number of Observations"
)

# Print the chart for studies
print(chart_studies)

# Print the chart for observations
print(chart_observations)
```

## Cumulative Distribution of Studies Over Time

```{r}
# -------------------------
# Function 1: Prepare Cumulative Data
# -------------------------
prepare_cumulative_data <- function(data, id_column, max_year = 2024) {
  data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |>  # Extract Year
    distinct(!!sym(id_column), Year, response_variable) |>                # Unique combo per response
    count(Year, response_variable) |>                                     # Count articles per year x response
    complete(
      Year = seq(min(Year, na.rm = TRUE), max(max_year, max(Year, na.rm = TRUE)), by = 1),
      response_variable,
      fill = list(n = 0)
    ) |>
    group_by(response_variable) |>
    arrange(Year) |>
    mutate(cumulative_n = cumsum(n)) |>
    ungroup()
}

# -------------------------
# Function 2: Create Cumulative Area Chart
# -------------------------
create_cumulative_chart <- function(data, custom_colors,
                                    title = "Cumulative Distribution Over Time",
                                    y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", linewidth = 0.3) +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5),
      expand = expansion(mult = c(0, 0.01))  # Remove whitespace
    ) +
    labs(
      #title = title,
      #subtitle = "Cumulative Count Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Agroecosystem Services"
    ) +
    theme_bw(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      axis.text = element_text(size = 11),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "right"
    )
}

# -------------------------
# Step 1: Prepare Data (extend to 2024)
# -------------------------
cumulative_data_articles <- prepare_cumulative_data(
  data = imp_dataset,
  id_column = "id_article",
  max_year = 2025
)

# -------------------------
# Step 2: Plot Chart
# -------------------------
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# -------------------------
# Step 3: Display Chart
# -------------------------
cumulative_articles_chart

```

```{r}
n_distinct(imp_dataset$id_article)
```

```{r}

# Function to prepare cumulative data for each response_variable
prepare_cumulative_data <- function(data) {
  data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |> # Extract Year
    distinct(id_article, Year, response_variable) |>                     # Ensure unique id_article per Year & response_variable
    group_by(Year, response_variable) |>                                 # Group by Year & response_variable
    summarise(unique_articles = n_distinct(id_article), .groups = "drop") |> # Count unique id_articles per group
    complete(Year = seq(min(Year, na.rm = TRUE), max(Year, na.rm = TRUE), by = 1),  # Add missing years
             response_variable,                                          # Ensure all response_variables exist for all years
             fill = list(unique_articles = 0)) |>                        # Fill missing counts with 0
    group_by(response_variable) |>                                      # Group by response_variable
    arrange(Year) |>                                                    # Ensure correct year order within each group
    mutate(cumulative_n = cumsum(unique_articles)) |>                   # Calculate cumulative sum
    ungroup()                                                           # Remove grouping
}

# Function to prepare cumulative data for the total unique articles
prepare_unique_articles <- function(data) {
  data %>%
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) %>%  # Extract year from date column
    group_by(id_article) %>%  # Group by id_article
    summarise(first_year = min(Year, na.rm = TRUE), .groups = "drop") %>%  # Get the first year each article appears
    count(first_year, name = "unique_articles") %>%  # Count unique articles by their first year
    arrange(first_year) %>%  # Ensure data is sorted by year
    mutate(cumulative_unique = cumsum(unique_articles)) %>%  # Calculate cumulative sum
    rename(Year = first_year)  # Rename column for consistency
}

# Function to create the cumulative chart
create_cumulative_chart <- function(data, unique_articles_data, custom_colors, title = "Cumulative Distribution Over Time", y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", size = 0.3) +  # Stacked area chart
    geom_line(data = unique_articles_data, aes(x = Year, y = cumulative_unique), 
              inherit.aes = FALSE, color = "black", size = 1.2) +  # Black line for cumulative unique articles
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5)
    ) +
    labs(
      title = title,
      subtitle = "Cumulative Count Stacked by Response Variable with Total Unique Articles",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette


# Prepare the data
cumulative_data_articles <- prepare_cumulative_data(imp_dataset)  # Data for the stacked area chart
cumulative_unique_articles <- prepare_unique_articles(imp_dataset)  # Data for the black line

# Generate the cumulative chart
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  unique_articles_data = cumulative_unique_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# Print the chart
print(cumulative_articles_chart)
```
```{r}
n_distinct(imp_dataset$id_article)  # Should match the final value in cumulative_unique
print(cumulative_unique_articles) |> glimpse() # Ensure no year surpasses the total unique articles
```


```{r}
# Prepare the data: Count unique studies by year and response variable
stacked_data <- imp_dataset |>
  mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |>
  distinct(id_article, Year, response_variable) |>  # Ensure unique studies
  count(Year, response_variable)

# Define custom color palette


# Create stacked bar chart
studies_over_time <- stacked_data |> 
  ggplot(aes(x = Year, y = n, fill = response_variable)) +
  geom_bar(stat = "identity", position = "stack", color = "black") +
  scale_fill_manual(values = custom_colors) +
  scale_x_continuous(
    breaks = seq(min(stacked_data$Year), max(stacked_data$Year), by = 5),  # Adjust as needed
    labels = scales::number_format(scale = 1, accuracy = 1)
  ) +
  labs(
    title = "Distribution of Studies Over Time",
    subtitle = "Stacked by Response Variable",
    x = "Year",
    y = "Number of Studies",
    fill = "Response Variable"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    legend.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

studies_over_time
```


```{r}
# Histogram for the distribution of unique studies over time
imp_dataset |> 
  distinct(id_article, experiment_year) |> 
ggplot(aes(x = as.Date(experiment_year))) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Unique Studies Over Time", x = "Year", y = "Number of Studies") +
  theme_minimal()
```

Step 3: Distribution of Studies Across Moderators

```{r}
# Bar plot for crop_type
data_distribution_crop_type <- 
  imp_dataset |> 
  ggplot(aes(x = crop_type)) +
  geom_bar(fill = "cornflowerblue") +
  labs(title = "Distribution of Entries Across Tree-Crop Combinations", x = "Tree-Crop Combination", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_crop_type
```
```{r}
# Bar plot for season
data_distribution_season <- 
  imp_dataset |>
  ggplot(aes(x = season)) +
  geom_bar(fill = "darkorange") +
  labs(title = "Distribution of Entries Across Seasons", x = "Season", y = "Number of Entries") +
  theme_minimal()

data_distribution_season
```

```{r}
# Bar plot for alley width category
```

Step 4: Effect Size Distribution



Step 5: Explore Moderators and Levels

```{r}
# Faceted bar plots for different moderators
data_distribution_moderator_levels_seasen <- 
  imp_dataset |> 
  ggplot(aes(x = response_variable)) +
  geom_bar(aes(fill = season)) +
  facet_wrap(~ crop_type) +
  labs(title = "Number of Entries by Response Variable and Season", x = "Response Variable", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_moderator_levels_seasen
```

Step 6: Distribution in Space (Using generated Categorized Location Column)

```{r}
# Bar plot for the number of studies per sub_region
data_distribution_studies_agroclimregion <- 
  imp_dataset |> 
  ggplot(aes(x = bioclim_sub_regions)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Number of Studies Per Continent", x = "Continent", y = "Number of Studies") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1))

data_distribution_studies_agroclimregion
```






# STEP 3 PROPORTION OF EFFECT SIZE DIRECTION AS NUMBER OF POSITIVE, NEUTRAL, AND NEGATIVE


Based on the minimal_random_effects model

```{r}
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom_tshering.rds"))
```

```{r}
# Step 1: Calculate Z-score, p-value, and categorize effect sizes
imp_data_rom_calc <- imp_data_rom %>%
  mutate(
    Z = yi / sqrt(vi),  # Calculate Z-score
    Pval = 2 * (1 - pnorm(abs(Z))),  # Two-tailed p-value
    category = case_when(
      Pval < 0.05 & yi > 0 ~ "Positive",  # Significant positive effect
      Pval < 0.05 & yi < 0 ~ "Negative",  # Significant negative effect
      TRUE ~ "Neutral"  # Non-significant or neutral effect
    )
  )

# Step 2: Count effect sizes per category for each response variable
summary_data_for_eff_calc <- imp_data_rom_calc %>%
  count(response_variable, category, name = "count") %>%  # More efficient than group_by() + summarise(n())
  group_by(response_variable) %>%
  mutate(
    total_count = sum(count),  # Total effect sizes per response variable
    percentage = (count / total_count) * 100  # Compute proportion
  ) %>%
  ungroup()  # Remove grouping after calculation

# Step 3: Ensure response variables are ordered by total effect sizes
summary_data_for_eff_calc <- summary_data_for_eff_calc %>%
  mutate(response_variable = fct_reorder(response_variable, total_count, .fun = sum))

# Step 4: Create stacked bar plot for effect size proportions
stacked_barplot <- ggplot(summary_data_for_eff_calc, aes(x = response_variable, y = percentage, fill = category)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("Positive" = "#1b9e77", "Negative" = "#d95f02", "Neutral" = "#757575")) +
  labs(
    title = "Percentage of Effect Sizes by Response Variable",
    x = "Response Variable",
    y = "Percentage of Effect Sizes",
    fill = "Effect Size Category"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),  # Keep only y-axis grid lines
    panel.grid.major.x = element_blank(),  # Remove x-axis grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    axis.line = element_line(linewidth = 0.5)  # Keep axis lines visible
  ) +
  scale_y_continuous(
    limits = c(0, 100),  # Y-axis fixed between 0 and 100%
    breaks = seq(0, 100, by = 20)  # Set breaks at 20% intervals
  )

# Step 5: Display the plot
stacked_barplot
```

## Proportion of effect sizes with three bars

```{r}
# Step 1: Calculate Z-score, p-value, and categorize effect sizes
imp_data_rom_calc <- imp_data_rom %>%
  mutate(
    Z = yi / sqrt(vi),  # Calculate Z-score
    Pval = 2 * (1 - pnorm(abs(Z))),  # Two-tailed p-value
    category = case_when(
      Pval < 0.05 & yi > 0 ~ "Positive",  # Significant positive effect
      Pval < 0.05 & yi < 0 ~ "Negative",  # Significant negative effect
      TRUE ~ "Neutral"  # Non-significant or neutral effect
    )
  )

# Step 2: Count effect sizes per category for each response variable
summary_data_for_eff_calc <- imp_data_rom_calc %>%
  count(response_variable, category, name = "count") %>%  # More efficient than group_by() + summarise(n())
  group_by(response_variable) %>%
  mutate(
    total_count = sum(count),  # Total effect sizes per response variable
    percentage = (count / total_count) * 100  # Compute proportion
  ) %>%
  ungroup()  # Remove grouping after calculation

# Step 3: Ensure response variables are ordered by total effect sizes
summary_data_for_eff_calc <- summary_data_for_eff_calc %>%
  mutate(response_variable = fct_reorder(response_variable, total_count, .fun = sum))


# Step 1: Ensure every response_variable × category combination exists
all_combinations <- expand.grid(
  response_variable = unique(summary_data_for_eff_calc$response_variable),
  category = c("Positive", "Negative", "Neutral")
)

summary_data_for_eff_complete <- all_combinations %>%
  left_join(summary_data_for_eff_calc, by = c("response_variable", "category")) %>%
  mutate(
    count = replace_na(count, 0),
    total_count = ave(count, response_variable, FUN = sum),
    percentage = ifelse(total_count == 0, 0, (count / total_count) * 100)
  )

# Step 2: Reorder response variables by total effect size count
summary_data_for_eff_complete <- summary_data_for_eff_complete %>%
  mutate(response_variable = fct_reorder(response_variable, total_count, .fun = sum))

# Step 3: Create properly spaced grouped bar plot
grouped_barplot_fixed_percent_effect_size <- ggplot(summary_data_for_eff_complete, aes(x = response_variable, y = percentage, fill = category)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  scale_fill_manual(values = c("Positive" = "#1b9e77", "Negative" = "#d95f02", "Neutral" = "#757575")) +
  labs(
    #title = "Percentage of Effect Sizes by Response Variable",
    x = "Response Variable",
    y = "Distribution of Effect Size [%]",
    fill = "Effect Size Category"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(linewidth = 0.5),
    legend.position = "top"
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, by = 20)
  )

# Step 4: Display the fixed plot
grouped_barplot_fixed_percent_effect_size
```


















# STEP 4 FORREST PLOT - OVERALL EFFECT SIZE DISTRIBUTION

```{r}
# Cleaning and preparing data
prepare_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- prepare_forest_data(
  data = imp_dataset,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)
```

```{r}
# Aggregating data
aggregate_forest_data <- function(data, yi_col, ci_cols, group_col) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      overall_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(ci_cols[1]), na.rm = TRUE),
      upper_ci = mean(!!sym(ci_cols[2]), na.rm = TRUE),
      num_observations = n(),
      num_studies = n_distinct(id_article),
      size_category = case_when(
        num_studies <= 2 ~ "1-2",
        num_studies <= 4 ~ "3-4",
        num_studies > 4 ~ "5+"
      ),
      .groups = "drop"
    ) %>%
    mutate(
      size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
      response_label = paste0(!!sym(group_col), " (", num_studies, " studies)"),
      mean_ci_label = paste0(
        sprintf("%.2f", overall_effect), " [",
        sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
      ),
      response_rank = rank(-overall_effect)  # Rank for sorting
    )
}

aggregated_data <- aggregate_forest_data(
  data = forest_data_clean,
  yi_col = "yi",
  ci_cols = c("lower_ci", "upper_ci"),
  group_col = "response_variable"
)

# Computing densities
compute_densities <- function(data, yi_col, group_col, aggregated_data) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      aggregated_data %>% select(response_variable, response_label, response_rank),
      by = "response_variable"
    )
}

density_data <- compute_densities(
  data = forest_data_clean,
  yi_col = "yi",
  group_col = "response_variable",
  aggregated_data = aggregated_data
)

# Adding "Overall" effect size
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = sum(num_observations, na.rm = TRUE),
    num_studies = sum(num_studies, na.rm = TRUE),
    size_category = NA,
    mean_ci_label = paste0(
      sprintf("%.2f", mean(overall_effect, na.rm = TRUE)), " [",
      sprintf("%.2f", mean(lower_ci, na.rm = TRUE)), ", ",
      sprintf("%.2f", mean(upper_ci, na.rm = TRUE)), "]"
    )
  ) %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)",
    response_rank = Inf  # Place "Overall" at the bottom
  )

# Combine aggregated data and overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  arrange(response_rank) %>%
  mutate(
    response_label = factor(response_label, levels = unique(response_label)),
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables"),
    facet_label = factor(facet_label, levels = c("Response Variables", "Overall Effect Size"))
  )
```

```{r}
# Define a custom color palette for response_variable
custom_colors <- c(
  "Biodiversity"             = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)
```


```{r}
# Forest plot with facets for "Overall" and "Response Variables"
# Arrange the `plot_data` by the number of studies
plot_data <- plot_data %>%
  arrange(desc(-num_studies)) %>%
  mutate(
    response_label = factor(
      response_label,
      levels = unique(response_label)  # Ensure levels follow the descending order of num_studies
    )
  )

# Update `density_data` to match the new order of response_label
density_data <- density_data %>%
  mutate(
    response_label = factor(
      response_label,
      levels = levels(plot_data$response_label)  # Use the same factor levels as in plot_data
    )
  )

####################################################################################################################

# Adjust text placement for CI values dynamically

# Updated Flexible Plotting Function
create_forest_plot <- function(
  density_data, 
  plot_data, 
  x_limits = NULL,    # Specify limits for the x-axis (e.g., c(-0.25, 0.5))
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Default breaks for the x-axis
  add_clipped_error_bars = FALSE,        # Include clipped error bars
  custom_colors = NULL                   # Custom fill colors
) {
  # Create a facet column to separate panels
  plot_data <- plot_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      )
    ) %>%
    arrange(desc(!num_studies)) %>%  # Sort by number of studies for the top panel
    mutate(
      response_label = factor(
        response_label,
        levels = unique(response_label)  # Ensure the order is preserved
      )
    )
  
  # Align density_data with plot_data
  density_data <- density_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      ),
      response_label = factor(
        response_label,
        levels = levels(plot_data$response_label)  # Match the order in plot_data
      )
    )
  
  # Explicitly order the facet levels
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )

  density_data <- density_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )
  
  # Adjust text placement for CI values
  plot_data <- plot_data %>%
    mutate(
      ci_text_position = ifelse(
        is.null(x_limits),
        max(x_breaks) + 0.1,  # Place text slightly beyond x-axis
        x_limits[2] - 0.2     # Keep text within x-axis limits
      )
    )
  
  # Build the base ggplot object
  forest_plot <- ggplot() +
    # Density ridges for individual response variables
    geom_ridgeline(
      data = density_data %>% filter(response_variable != "Overall"),
      aes(
        x = density_x,
        y = response_label,
        height = density_y,
        fill = response_variable
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Points for individual response variables
    geom_point(
      data = plot_data %>% filter(response_variable != "Overall"),
      aes(
        x = overall_effect,
        y = response_label,
        size = size_category
      ),
      color = "black"
    ) +
    # Diamond for overall effect size
    geom_point(
      data = plot_data %>% filter(response_variable == "Overall"),
      aes(
        x = overall_effect,
        y = response_label
      ),
      shape = 18,
      size = 5,
      color = "black"
    ) +
    # Error bars for all response variables
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = response_label
      ),
      height = 0.2,
      color = "darkgray"
    )
  
  # Optionally add clipped error bars
  if (add_clipped_error_bars && !is.null(x_limits)) {
    forest_plot <- forest_plot +
      geom_errorbarh(
        data = plot_data,
        aes(
          xmin = pmax(lower_ci, x_limits[1]),  # Clip lower bounds to x-axis limits
          xmax = pmin(upper_ci, x_limits[2]),  # Clip upper bounds to x-axis limits
          y = response_label
        ),
        height = 0.1,
        color = "darkgray",
        size = 0.8
      )
  }
  
  # Add text annotations for confidence intervals
  forest_plot <- forest_plot +
    geom_text(
      data = plot_data,
      aes(
        x = ci_text_position,  # Use dynamically calculated position
        y = response_label,
        label = mean_ci_label
      ),
      size = 3.5,
      hjust = 0,
      color = "black"
    ) +
    # Add vertical red dotted line for effect size = 0
    geom_vline(
      xintercept = 0,
      color = "red",
      linetype = "dotted",
      size = 0.7
    ) +
    # Adjust x-axis scale dynamically
    scale_x_continuous(
      limits = x_limits,     # Use the specified limits if provided
      breaks = x_breaks      # Use the specified breaks
    ) +
    # Customize fill colors and size scale
    scale_fill_manual(values = custom_colors, guide = "none") +
    scale_size_manual(
      values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
      name = "Number of Studies"
    ) +
    # Add labels and theme adjustments
    labs(
      title = "Forest Plot with Adjusted Density and Study Details",
      x = "Effect Size",
      y = NULL
    ) +
    # Separate panels for response variables and overall effect size
    facet_grid(
      facet_label ~ .,       # Facet by `facet_label`
      scales = "free_y",
      space = "free",
      switch = "y"           # Labels on the left
    ) +
    theme_minimal() +
    theme(
      strip.text.y = element_blank(),  # Remove facet labels
      strip.background = element_blank(),
      panel.spacing = unit(0.5, "lines"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 12),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      legend.position = "bottom",
      legend.title = element_text(size = 12),
      legend.text = element_text(size = 10)
    )
  
  return(forest_plot)
}
```

```{r}
forest_plot_density_narrow <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-0.25, 0.5),  # Narrow x-axis
  x_breaks = seq(-0.25, 0.5, by = 0.25),
  add_clipped_error_bars = TRUE,
  custom_colors = custom_colors
)
forest_plot_density_narrow
```

```{r}
forest_plot_density_wide <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-3, 3),  # Wide x-axis
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Adjust breaks to fit the wider range
  add_clipped_error_bars = FALSE,  # Optional: Set to FALSE for full error bars
  custom_colors = custom_colors
)
forest_plot_density_wide
```



```{r}
# Create the boxplot for the effect size (yi)
# Order response variables by descending median effect size
imp_data_rom_reorder <- imp_data_rom %>%
  mutate(response_variable = fct_reorder(response_variable, yi, .fun = median, .desc = FALSE))

# Create the boxplot with ordered response variables
boxplot_raw_effect_size <- imp_data_rom_reorder |> 
  ggplot(aes(y = response_variable, x = yi, fill = response_variable)) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", size = 0.8) + # Add red dotted line at x = 0
  geom_boxplot(alpha = 0.7, outlier.size = 1) +
  labs(
    title = "Raw Effect Sizes (yi) Across Response Variables",
    x = "Raw Effect Size (yi)",
    y = "Response Variable"
  ) +
  scale_x_continuous(
    trans = scales::pseudo_log_trans(sigma = 0.1), # Apply pseudo-log transformation
    breaks = c(0, 0.1, 1, 10, 100),       # Custom breaks
    labels = c("0", "0.1", "1", "10", "100") # Relatable labels
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 12), # Adjust y-axis text size
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels
    legend.position = "none"
  )

boxplot_raw_effect_size
```













FORREST PLOT - FOR INDIVIDUAL RESPONSE VARIABLES AND MODERATORS EFFECT SIZE DISTRIBUTION


```{r}
# Cleaning Data
clean_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- clean_forest_data(
  data = imp_dataset,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)
```

```{r}
forest_data_clean 

forest_moderator <- forest_data_clean |> 
  filter(response_variable %in% c("Biodiversity", "Crop yield", "Soil quality"))

forest_moderator |> str()
```

```{r}
create_forest_plot_moderator <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  x_limits = NULL,                     # Specify x-axis limits
  x_breaks = seq(-0.5, 1.0, by = 0.25),# Specify x-axis breaks
  custom_colors = NULL,                # Custom color palette
  add_clipped_error_bars = FALSE       # Add clipped error bars
) {
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = if (add_clipped_error_bars & !is.null(x_limits)) pmax(lower_ci, x_limits[1]) else lower_ci,
        xmax = if (add_clipped_error_bars & !is.null(x_limits)) pmin(upper_ci, x_limits[2]) else upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = if (is.null(x_limits)) max(x_breaks) + 0.1 else x_limits[2] - 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable
    facet_wrap(~ facet_label, ncol = 1, scales = "free_y") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # X-axis adjustments
    scale_x_continuous(limits = x_limits, breaks = x_breaks) +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Create a forest plot with tree_type as moderator
forest_plot_tree_type <- create_forest_plot_moderator(
  data = forest_moderator,
  response_var = "response_variable",
  moderator_var = "tree_type",
  x_limits = c(-1.0, 2.0),  # Wide x-axis
  x_breaks = seq(-1.0, 2.0, by = 0.5),
  custom_colors = custom_colors,
  add_clipped_error_bars = TRUE
)

# Display the plot
forest_plot_tree_type
```


Free x-axis for response variables (ecosystem services)
```{r}
create_forest_plot_moderator_freex <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  custom_colors = NULL                 # Custom color palette
) {

  
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = max(lower_ci, na.rm = TRUE) + 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable with free x-axis
    facet_wrap(~ facet_label, ncol = 1, scales = "free_x") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Generate the plot
forest_plot_free_x <- create_forest_plot_moderator_freex(data = forest_moderator, custom_colors = custom_colors)

forest_plot_free_x
```



















# STEP 5 FOREST PLOT (VIOLIN PLOT) EFFECT SIZES FOR ALL RESPONSE VARIABLES (THIS IS THE FINNAL FOREST PLOT)


```{r}
# Read the pre-saved combined evaluation results object
evaluation_results <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "MULTI_MODEL_EVALUATION_RESULTS", "evaluation_results_combined.rds")) 

evaluation_results
```

```{r}
evaluation_results$`Crop yield`$base_model
evaluation_results$`Biodiversity`$base_model
evaluation_results$`Soil quality`$base_model
```

```{r}
##########################################################################################################################################
# Function to Back-Transform log-ROM to ROM in Percentage for All Response Variables
##########################################################################################################################################

# Define the function
back_transform_logROM <- function(evaluation_results) {
  # Initialize an empty list to store back-transformed results
  back_transformed_results <- list()

  # Iterate through each response variable in evaluation_results
  for (response in names(evaluation_results)) {
    cat("Processing response variable:", response, "\n")

    # Extract models for the response variable
    models <- evaluation_results[[response]]

    # Initialize a list to store back-transformed values for this response variable
    response_results <- list()

    # Define a helper function to back-transform log-ROM values
    back_transform <- function(estimate, ci.lb, ci.ub) {
      list(
        ROM_percent = (exp(estimate) - 1) * 100,  # Convert to percentage
        ROM_lower_percent = (exp(ci.lb) - 1) * 100,
        ROM_upper_percent = (exp(ci.ub) - 1) * 100
      )
    }

    # Iterate through all models (null, minimal, moderators, full, interaction)
    for (model_name in names(models)) {
      model <- models[[model_name]]

      # Skip if the model is NULL
      if (is.null(model)) {
        response_results[[model_name]] <- NULL
        next
      }

      # Extract estimates and confidence intervals
      if (!is.null(model$b)) {
        response_results[[model_name]] <- back_transform(
          estimate = model$b[1],
          ci.lb = model$ci.lb[1],
          ci.ub = model$ci.ub[1]
        )
      } else {
        response_results[[model_name]] <- NULL
      }
    }

    # Store the back-transformed results for this response variable
    back_transformed_results[[response]] <- response_results
  }

  return(back_transformed_results)
}

##########################################################################################################################################
# Example: Apply the function to evaluation_results
##########################################################################################################################################

# Assuming evaluation_results is already loaded
back_transformed_results <- back_transform_logROM(evaluation_results)

# Inspect the back-transformed results for a specific response variable
# Choose a model from structured_results (e.g., "interaction_model", or "minimal_random_effects")

back_transformed_results$`Water quality`$base_model # <-------------- ! Chosen Model !
back_transformed_results$Biodiversity$base_model
```




```{r}
##########################################################################################################################################
# Combine Raw Effect Sizes and Back-Transformed Values into a Structured Data Frame
##########################################################################################################################################

# Function to create a structured data frame
combine_effect_sizes <- function(evaluation_results, back_transformed_results) {
  combined_results <- data.frame(
    ResponseVariable = character(),
    Model = character(),
    Estimate = numeric(),
    CI_Lower = numeric(),
    CI_Upper = numeric(),
    P_Value = character(),
    Significance = character(),
    ROM_Percent = numeric(),
    ROM_Lower_Percent = numeric(),
    ROM_Upper_Percent = numeric(),
    stringsAsFactors = FALSE
  )

  for (response in names(evaluation_results)) {
    models <- evaluation_results[[response]]
    back_transformed <- back_transformed_results[[response]]

    for (model_name in names(models)) {
      model <- models[[model_name]]
      if (!is.null(model) && !is.null(model$b) && !is.null(model$ci.lb) && !is.null(model$pval)) {
        p_value <- ifelse(model$pval[1] < 0.001, "<0.001", formatC(model$pval[1], format = "f", digits = 3))
        significance <- if (model$pval[1] < 0.001) {
          "***"
        } else if (model$pval[1] < 0.01) {
          "**"
        } else if (model$pval[1] < 0.05) {
          "*"
        } else if (model$pval[1] < 0.1) {
          "."
        } else {
          " "
        }

        combined_results <- rbind(combined_results, data.frame(
          ResponseVariable = response,
          Model = model_name,
          Estimate = model$b[1],
          CI_Lower = model$ci.lb[1],
          CI_Upper = model$ci.ub[1],
          P_Value = p_value,
          Significance = significance,
          ROM_Percent = back_transformed[[model_name]]$ROM_percent,
          ROM_Lower_Percent = back_transformed[[model_name]]$ROM_lower_percent,
          ROM_Upper_Percent = back_transformed[[model_name]]$ROM_upper_percent
        ))
      }
    }
  }

  return(combined_results)
}


# Apply the function
structured_results <- combine_effect_sizes(evaluation_results, back_transformed_results)


output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# Save structured_results in a combined file
saveRDS(structured_results, file = file.path(output_dir, "structured_results_all_effect_sizes.rds"))



# Inspect the structured data frame
structured_results |> glimpse()
```


```{r}
custom_colors <- c(
  "Biodiversity"             = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)
```

```{r}
##########################################################################################################################################

# Step 1: Choose a model from structured_results (e.g., "interaction_model", or "minimal_random_effects", "base_model")
chosen_model <- "base_model" # <-------------- ! Chosen Model !

# Step 2: Compute median effect sizes for ordering
median_effects <- imp_data_rom %>%
  group_by(response_variable) %>%
  summarise(median_effect = median(yi, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(median_effect))  # Order by descending median effect size

# Step 3: Calculate the number of studies (`n_studies`) and number of observations (`obs_count`) per response variable
study_counts <- imp_data_rom %>%
  group_by(response_variable) %>%
  summarise(
    n_studies = n_distinct(id_article),  # Count unique studies (articles)
    obs_count = n(),  # Count total observations
    .groups = "drop"
  )

# Step 4: Extract structured summary for the chosen model
structured_summary <- structured_results %>%
  filter(Model == chosen_model) %>%  # Ensure the selected model is used
  select(ResponseVariable, CI_Lower, CI_Upper, ROM_Lower_Percent, ROM_Upper_Percent, Significance)

# Step 5: Merge `study_counts` into `structured_summary`
structured_summary <- structured_summary %>%
  left_join(study_counts, by = c("ResponseVariable" = "response_variable")) %>%  # Ensure `n_studies` & `obs_count` are included
  mutate(ResponseVariable = factor(ResponseVariable, levels = median_effects$response_variable))  # Ensure ordering

# Step 6: Merge `study_counts` and `structured_summary` into `imp_data_rom_violin`
imp_data_rom_violin <- imp_data_rom %>%
  left_join(study_counts, by = "response_variable") %>%  
  left_join(structured_summary, by = c("response_variable" = "ResponseVariable"))  

# Step 7: Apply the ordered factor to `imp_data_rom_violin`
imp_data_rom_violin <- imp_data_rom_violin %>%
  mutate(response_variable = factor(response_variable, levels = median_effects$response_variable))

##########################################################################################################################################
# Step 6: Define fixed x-axis limits
x_min <- -0.30
x_max <- 1.00

# Step 7: Create the Forest Violin Plot
forest_violin_plot <- ggplot(imp_data_rom_violin, aes(x = yi, 
                                                      y = response_variable, 
                                                      fill = response_variable)) +
  geom_violin(trim = FALSE, alpha = 0.5, color = "black") +
  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.8, color = "black") +
  stat_summary(fun = mean, geom = "point", shape = 21, size = 3, fill = "white", stroke = 1.5) +
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2, color = "black", linewidth = 1) +
  scale_fill_manual(values = custom_colors) +
  labs(
    title = "", # "Overall Effects of SAF on Agroecosystem Services Relative to Monocrop"
    x = "Effect Size (logRR) Relative to Monocrop",
    y = "",
    fill = "Response Variable"
  ) +
  scale_x_continuous(limits = c(x_min, x_max), breaks = seq(x_min, x_max, by = 0.05)) +
  scale_x_break(c(0.35, 0.6), scales = "free") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(size = 14),
    axis.title = element_text(size = 20),
    axis.text.x.top = element_blank(),
    axis.title.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    axis.text.y = element_text(size = 20),
    legend.position = "none",
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),
    panel.grid.minor = element_blank(),
    axis.line = element_line(linewidth = 0.5)
  ) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", linewidth = 1.5)

# Step 8: Create the CI & Study Count Text Plot (Now Includes `obs_count`)
text_plot <- ggplot(structured_summary, aes(
    y = factor(ResponseVariable, levels = median_effects$response_variable),
    x = -0.5
  )) +
  geom_text(
    aes(
      label = paste0(
        "CI: [", round(CI_Lower, 3), ", ", round(CI_Upper, 3), "]\n",
        "Back-transformed: [", round(ROM_Lower_Percent, 3), "%, ", 
        round(ROM_Upper_Percent, 3), "%]\n",
        "n studies = ", n_studies, " | n obs = ", obs_count
      )
    ),
    hjust = 1,
    size = 4
  ) +
  expand_limits(x = -5) +
  labs(title = "") + # "Effect Size, Study & Observation Counts"
  theme_void() +
  theme(
    plot.title = element_text(size = 14, hjust = 1, face = "bold"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

# Step 9: Arrange both plots side by side
forest_violin_plot_final <- forest_violin_plot + text_plot + plot_layout(widths = c(6, 1))

# Display the plot
forest_violin_plot_final
```

```{r}
structured_summary
```


## Overall forest plot

```{r}
# Load and assess the model results from the saved file
# Load the combined model results
model_result <- readRDS(file = here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "fitted_models_all_new.rds"))
```

```{r}
# Extract back-transformed results from base_model for each response
forest_data <- lapply(names(model_results), function(response) {
  model <- model_results[[response]]$base_model
  if (is.null(model)) return(NULL)
  
  est <- model$b[1]
  ci.lb <- model$ci.lb[1]
  ci.ub <- model$ci.ub[1]
  pval <- model$pval[1]
  
  # Back-transform from logRR to ROM (%)
  rom_percent <- (exp(est) - 1) * 100
  rom_lower   <- (exp(ci.lb) - 1) * 100
  rom_upper   <- (exp(ci.ub) - 1) * 100
  
  tibble(
    response_variable = response,
    Estimate_logRR = est,
    CI_Lower_logRR = ci.lb,
    CI_Upper_logRR = ci.ub,
    ROM_percent = rom_percent,
    ROM_lower = rom_lower,
    ROM_upper = rom_upper,
    p_value = pval
  )
}) |> bind_rows()

# Order response variables by effect size or custom color order
forest_data <- forest_data %>%
  mutate(response_variable = factor(response_variable, levels = names(custom_colors))) %>%
  arrange(desc(ROM_percent))

# Forest plot (Back-transformed % Change from Control)
forest_plot_backtransformed <- forest_data |> 
  ggplot(aes(y = response_variable, x = ROM_percent, color = response_variable)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = ROM_lower, xmax = ROM_upper), height = 0.3, linewidth = 1) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = custom_colors) +
  scale_y_discrete(limits = rev(levels(forest_data$response_variable))) +
  labs(
    # title = "Summary Effects by Response Variable (Back-Transformed % Change)",
    x = "Effect size (lnRR)",
    y = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 12, face = "bold"),
    axis.title.x = element_text(size = 12),
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "none"
  )



forest_plot_backtransformed
```


## Final overall forest plot

```{r}
forest_data

imp_data_rom |> glimpse()
```

```{r}
# --- Step 1: Define response variables to include (omit Water quality)
custom_order <- c(
  "Crop yield", "Biodiversity", "Soil quality", 
  "Carbon sequestration", "Product quality", "Pest and disease control"
)

# --- Step 2: Compute study & obs count per response
summary_counts <- imp_data_rom %>%
  filter(response_variable %in% custom_order) %>%
  group_by(response_variable) %>%
  summarise(
    n_study = n_distinct(id_article),
    k = n_distinct(id_obs),
    .groups = "drop"
  )

# --- Step 3: Back-transform forest estimates
forest_data <- lapply(names(model_results), function(response) {
  model <- model_results[[response]]$base_model
  if (is.null(model)) return(NULL)

  est <- model$b[1]
  ci.lb <- model$ci.lb[1]
  ci.ub <- model$ci.ub[1]
  pval <- model$pval[1]

  tibble(
    response_variable = response,
    Estimate_logRR = est,
    CI_Lower_logRR = ci.lb,
    CI_Upper_logRR = ci.ub,
    ROM_percent = (exp(est) - 1) * 100,
    ROM_lower = (exp(ci.lb) - 1) * 100,
    ROM_upper = (exp(ci.ub) - 1) * 100,
    p_value = pval
  )
}) |> bind_rows() |> 
  mutate(
    significant = case_when(
      p_value < 0.001 ~ "***",
      p_value < 0.01  ~ "**",
      p_value < 0.05  ~ "*",
      p_value < 0.1   ~ ".",
      TRUE            ~ " "
    )
  )


# --- Step 4: Join counts and filter
forest_data_clean <- forest_data %>%
  filter(response_variable %in% custom_order) %>%
  left_join(summary_counts, by = "response_variable") %>%
  mutate(annotation_text = paste0(
    "(", n_study, " studies, ", k, " obs)\n",
    "Mean ES: ", round(ROM_percent, 1), "% ", significant)
  ) %>%
  mutate(response_variable = factor(response_variable, levels = rev(custom_order)))


# --- Step 5: Plot with annotations
forest_plot_annotated <- ggplot(forest_data_clean, aes(
  y = response_variable,
  x = ROM_percent,
  color = response_variable
)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = ROM_lower, xmax = ROM_upper), height = 0.3, linewidth = 1) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  geom_text(
    aes(label = annotation_text),
    hjust = 0,
    size = 4.2,
    color = "black",
    x = 52  # <- move labels to a fixed X on the far right
  ) +
  scale_color_manual(values = custom_colors) +
  scale_x_continuous(
    limits = c(-50, 60),  # add more space on the right
    breaks = seq(-50, 50, by = 10)
  ) +
  labs(
    x = "Effect size (lnRR)",
    y = NULL
  ) +
  theme_bw(base_size = 15) +
  theme(
    axis.text.y = element_text(size = 13, face = "bold"),
    axis.title.x = element_text(size = 13),
    plot.title = element_text(size = 15, face = "bold"),
    legend.position = "none"
  )

# --- Step 6: Display the plot
forest_plot_annotated
```


## Tree type and Crop type - Detailed Forest Plot - Moderator-specific 

```{r}
# Crop yield
model_result$`Crop yield`$tree_type
model_result$`Crop yield`$crop_type
# Biodiversity
model_result$`Biodiversity`$tree_type
model_result$`Biodiversity`$crop_type
# Product quality
model_result$`Product quality`$tree_type
model_result$`Product quality`$crop_type
# Soil quality
model_result$`Soil quality`$tree_type
model_result$`Soil quality`$crop_type
```

```{r}
# --- Step 1: Define inputs ---
responses_fig7 <- c("Crop yield", "Biodiversity", "Product quality", "Soil quality")
moderators_fig7 <- c("tree_type", "crop_type")

# Cap exponentiated effect size to avoid overflow
safe_exp <- function(x, cap = 80) {
  x[x > cap] <- cap
  x[x < -cap] <- -cap
  exp(x)
}


# --- Step 2: Helper function to extract & transform moderator effects ---
extract_moderator_from_model <- function(model, response, moderator) {
  if (is.null(model)) return(NULL)
  
  # Extract core stats
  estimates <- as.numeric(model$b[, 1])
  ci.lb <- as.numeric(model$ci.lb)
  ci.ub <- as.numeric(model$ci.ub)
  pvals <- as.numeric(model$pval)
  levels <- rownames(model$b)

  # Back-transform logRR → % change
  rom_percent <- (safe_exp(estimates) - 1) * 100
  rom_lower <- (safe_exp(ci.lb) - 1) * 100
  rom_upper <- (safe_exp(ci.ub) - 1) * 100

    # Significance codes
  signif_code <- dplyr::case_when(
    pvals < 0.001 ~ "***",
    pvals < 0.01  ~ "**",
    pvals < 0.05  ~ "*",
    pvals < 0.1   ~ ".",
    TRUE ~ ""
  )

  # Return as tibble
  tibble::tibble(
    response_variable = response,
    moderator_variable = moderator,
    moderator_level = levels,
    Estimate_logRR = estimates,
    CI_Lower_logRR = ci.lb,
    CI_Upper_logRR = ci.ub,
    ROM_percent = rom_percent,
    ROM_lower = rom_lower,
    ROM_upper = rom_upper,
    p_value = pvals,
    significant = signif_code
  )
}

# --- Step 3: Loop through responses and moderators ---
mod_forest_data_fig7 <- purrr::map_dfr(responses_fig7, function(resp) {
  purrr::map_dfr(moderators_fig7, function(mod) {
    model_obj <- evaluation_results[[resp]]$moderators[[mod]]
    extract_moderator_from_model(model_obj, response = resp, moderator = mod)
  })
})

# --- Step 4: Optional preview ---
mod_forest_data_fig7 |> glimpse()
```

```{r}
# --- Step 1: Subset data ---
mod_forest_tree <- mod_forest_data_fig7 %>%
  filter(moderator_variable == "tree_type")

mod_forest_crop <- mod_forest_data_fig7 %>%
  filter(moderator_variable == "crop_type")

# Optional: clean labels
mod_forest_tree <- mod_forest_data_fig7 %>%
  filter(moderator_variable == "tree_type", response_variable != "Soil quality") %>%
  mutate(
    # Clean up moderator_level
    moderator_level = gsub("tree_type", "", moderator_level),
    # Rename levels
    moderator_level = case_when(
      moderator_level == "Biomass" ~ "Biomass",
      moderator_level == "Fruit,nut & other" ~ "Fruit, nut and other",
      moderator_level == "Timber" ~ "Timber",
      TRUE ~ moderator_level
    ),
    # Set response order for plotting
    response_variable = factor(response_variable, levels = c(
      "Crop yield", "Biodiversity", "Product quality"
    ))
  )

mod_forest_crop <- mod_forest_crop %>%
  mutate(
    moderator_level = gsub("crop_type", "", moderator_level),
    moderator_level = case_when(
      moderator_level == "Cereal" ~ "Cereals", 
      moderator_level == "Legume" ~ "Legumes",
      moderator_level == "Tuber,root and other" ~ "Tubers, roots and other",
      TRUE ~ moderator_level
    ),
    response_variable = factor(response_variable, levels = c(
      "Crop yield", "Biodiversity", "Product quality", "Soil quality"
    ))
  )


# --- Step 2: Plot function ---
plot_moderator_forest <- function(data, moderator_label) {
  ggplot(data, aes(
    x = ROM_percent,
    y = moderator_level,
    color = response_variable
  )) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    geom_point(size = 3) +
    geom_errorbarh(
      aes(xmin = ROM_lower, xmax = ROM_upper),
      height = 0.25,
      linewidth = 0.8
    ) +
    geom_text(
      aes(label = significant),
      hjust = -0.5,
      size = 5,
      color = "black"
    ) +
    scale_color_manual(values = custom_colors) +
    facet_wrap(~response_variable, scales = "free_y", ncol = 1) +
    # Change x-axis limits according to the data (for tree type its -50 to 80, for crop type its -40 to 110)
    scale_x_continuous(
      name = "Effect size (lnRR)",
      limits = c(-40, 110),
      breaks = seq(-40, 110, by = 10)
    ) +
    labs(
      y = moderator_label,
      color = "Response Variable"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      axis.text.y = element_text(size = 12),
      axis.text.x = element_text(size = 12),
      axis.title = element_text(size = 13),
      strip.text = element_text(size = 13, face = "bold"),
      panel.spacing = unit(1, "lines"),
      legend.position = "none"
    )
}

# --- Step 3: Generate plots ---
fig7a_tree_type <- plot_moderator_forest(mod_forest_tree, moderator_label = "Tree Type")
fig7b_crop_type <- plot_moderator_forest(mod_forest_crop, moderator_label = "Crop Type")

# --- Step 4: Show plots ---
fig7a_tree_type
fig7b_crop_type
```

## Soil texture and Crop alley width - Detailed Forest Plot - Moderator-specific 

```{r}
# Crop yield
model_result$`Crop yield`$soil_texture
model_result$`Crop yield`$alley_width
# Biodiversity
model_result$`Biodiversity`$soil_texture
model_result$`Biodiversity`$alley_width
# Product quality
model_result$`Product quality`$soil_texture
model_result$`Product quality`$alley_width
# Soil quality
model_result$`Soil quality`$soil_texture
model_result$`Soil quality`$alley_width
```

```{r}
# Step 1: Define the new extraction function (for this structure)

extract_mod_effects_v2 <- function(model_list, response, moderator) {
  if (!is.list(model_list) || is.null(model_list$b)) return(NULL)

  levels <- rownames(model_list$b)
  estimates <- as.numeric(model_list$b[, 1])
  ci.lb <- as.numeric(model_list$ci.lb)
  ci.ub <- as.numeric(model_list$ci.ub)
  pval <- as.numeric(model_list$pval)

  tibble(
    response_variable = response,
    moderator_variable = moderator,
    moderator_level = levels,
    Estimate_logRR = estimates,
    CI_Lower_logRR = ci.lb,
    CI_Upper_logRR = ci.ub,
    ROM_percent = (exp(estimates) - 1) * 100,
    ROM_lower = ifelse(abs(ci.lb) > 50, NA, (exp(ci.lb) - 1) * 100),
    ROM_upper = ifelse(abs(ci.ub) > 50, NA, (exp(ci.ub) - 1) * 100),
    p_value = pval,
    significant = case_when(
      pval < 0.001 ~ "***",
      pval < 0.01  ~ "**",
      pval < 0.05  ~ "*",
      pval < 0.1   ~ ".",
      TRUE         ~ " "
    )
  )
}

# Step 2: Define responses and moderators for Figure 8

responses_fig8 <- c("Crop yield", "Biodiversity", "Product quality", "Soil quality")
moderators_fig8 <- c("soil_texture", "alley_width")

# Step 3: Loop through and extract

mod_forest_data_fig8 <- purrr::map_dfr(responses_fig8, function(resp) {
  purrr::map_dfr(moderators_fig8, function(mod) {
    model_obj <- model_result[[resp]][[mod]]
    extract_mod_effects_v2(model_obj, response = resp, moderator = mod)
  })
})


mod_forest_data_fig8 |> glimpse()
```
```{r}
# Split by moderator variable

mod_forest_soil <- mod_forest_data_fig8 %>%
  filter(moderator_variable == "soil_texture") %>%
  mutate(
    moderator_level = gsub("soil_texture", "", moderator_level),
    response_variable = factor(response_variable, levels = c(
      "Crop yield", "Biodiversity", "Product quality", "Soil quality"
    )),
    moderator_level = case_when(
      moderator_level == "Clay" ~ "Clay soils",
      moderator_level == "Sand" ~ "Sandy soils",
      moderator_level == "Silt" ~ "Silty soils",
      TRUE ~ moderator_level
    )
  )

mod_forest_alley <- mod_forest_data_fig8 %>%
  filter(moderator_variable == "alley_width") %>%
  mutate(
    moderator_level = gsub("alley_width", "", moderator_level),
    response_variable = factor(response_variable, levels = c(
      "Crop yield", "Biodiversity", "Product quality", "Soil quality"
    )),
    moderator_level = case_when(
      moderator_level == "Narrow" ~ "Narrow alleys",
      moderator_level == "Wide" ~ "Wide alleys",
      TRUE ~ moderator_level
    )
  )

# Step 2: Plot function for each moderator (facet by response_variable)

plot_forest_mod <- function(df, title_text) {
  ggplot(df, aes(y = moderator_level, x = ROM_percent)) +
    geom_point(aes(color = response_variable), size = 3) +
    geom_errorbarh(
      aes(xmin = ROM_lower, xmax = ROM_upper, color = response_variable),
      height = 0.3, linewidth = 1
    ) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    geom_text(
      aes(label = significant),
      hjust = -0.8, vjust = 0.5, size = 5, fontface = "bold"
    ) +
    facet_wrap(~response_variable, scales = "free_x") +
    scale_color_manual(values = custom_colors) +
    labs(
      title = title_text,
      x = "Effect Size (ROM % Change from Control)",
      y = NULL
    ) +
    theme_minimal(base_size = 15) +
    theme(
      axis.text.y = element_text(size = 13, face = "bold"),
      axis.text.x = element_text(size = 12),
      strip.text = element_text(size = 14, face = "bold"),
      legend.position = "none",
      plot.title = element_text(size = 15, face = "bold", hjust = 0)
    )
}

# Step 3: Generate plots for soil texture and crop alley width

forest_plot_soil <- plot_forest_mod(mod_forest_soil, "Soil Texture")
forest_plot_alley <- plot_forest_mod(mod_forest_alley, "Crop Alley Width")

forest_plot_soil
forest_plot_alley

```

## Cropping season and Age system - Detailed Forest Plot - Moderator-specific

```{r}
# Crop yield
model_result$`Crop yield`$season
model_result$`Crop yield`$age_system
# Biodiversity
model_result$`Biodiversity`$season
model_result$`Biodiversity`$age_system
# Product quality
model_result$`Product quality`$season
model_result$`Product quality`$age_system
# Soil quality
model_result$`Soil quality`$season
model_result$`Soil quality`$age_system
```
```{r}
# Step 1: Define the new extraction function (for this structure)


```










# STEP 6 EVIDENCE GAP MAP - HEAT MAP STYLE

```{r}
# imp_data_rom |> glimpse()
```

```{r}
custom_colors <- c(
  "Biodiversity"             = "#FF9999",
  "Carbon sequestration"     = "#66C266",
  "Product quality"          = "#FFC000",
  "Crop yield"               = "#FF9933",
  "Pest and disease control" = "#33CCCC",
  "Soil quality"             = "#9966CC",
  "Water quality"            = "#9999FF"
)
```

## Evidence and Gap Map (EGM): Response Variables x Crop Types

```{r}
# Evidence and Gap Map (EGM): Response Variables x Crop Types

# Create a count table of response_variable by crop_type (crop_genus)
# Step 1: Prepare data
egm_matrix <- imp_data_rom %>%
  drop_na(response_variable, crop_genus) %>%
  distinct(id_obs, response_variable, crop_genus) %>%
  count(response_variable, crop_genus) %>%
  mutate(
    crop_genus = as.factor(crop_genus),
    response_variable = as.factor(response_variable)
  )

# Step 2: Order crop and response levels
crop_levels <- egm_matrix %>%
  count(crop_genus, wt = n) %>%
  arrange(desc(n)) %>%
  pull(crop_genus)

response_levels <- egm_matrix %>%
  count(response_variable, wt = n) %>%
  arrange(desc(n)) %>%
  pull(response_variable)

# Step 3: Plot EGM
egm_plot_crop_vs_agroecosystem_service <- ggplot(
  egm_matrix,
  aes(
    x = factor(crop_genus, levels = crop_levels),
    y = factor(response_variable, levels = response_levels)
  )
) +
  geom_point(
    aes(size = n, fill = response_variable),
    shape = 21, color = "black", alpha = 0.9
  ) +
  scale_fill_manual(values = custom_colors, name = "Response Variable", guide = "none") +
  scale_size_continuous(
    range = c(2, 12),
    breaks = seq(25, 200, by = 25),
    limits = c(0, 200),
    name = "No. of Observations",
    guide = guide_legend(
      title.position = "top",
      title.hjust = 0.5,
      direction = "horizontal",
      nrow = 1,
      override.aes = list(shape = 21, fill = "white", color = "black")
    )
  ) +
  theme_bw(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 12),
    legend.position = "bottom",
    legend.box = "vertical",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    panel.grid = element_blank(),
    plot.margin = margin(10, 20, 10, 20)
  ) +
  labs(
    x = "Crop Genus",
    y = "Agroecosystem Service",
    size = "No. of Observations"
  )

# Show plot
egm_plot_crop_vs_agroecosystem_service
```

## Evidence and Gap Map (EGM): Response Variables x Tree Types

```{r}
# Evidence and Gap Map (EGM): Response Variables x Tree Types

# Step 1: Prepare data
egm_matrix_tree <- imp_data_rom %>%
  drop_na(response_variable, tree_genus) %>%
  distinct(id_obs, response_variable, tree_genus) %>%
  count(response_variable, tree_genus) %>%
  mutate(across(where(is.character), as.factor))

# Step 2: Order tree types and response variables by frequency
tree_levels <- egm_matrix_tree %>%
  count(tree_genus, wt = n) %>%
  arrange(desc(n)) %>%
  pull(tree_genus)

response_levels <- egm_matrix_tree %>%
  count(response_variable, wt = n) %>%
  arrange(desc(n)) %>%
  pull(response_variable)

# Step 3: Plot the EGM
egm_plot_tree_vs_agroecosystem_service <- ggplot(egm_matrix_tree, aes(x = factor(tree_genus, levels = tree_levels),
                                                                      y = factor(response_variable, levels = response_levels))) +
  geom_point(aes(size = n, fill = response_variable), shape = 21, color = "black", alpha = 0.9) +
  scale_fill_manual(values = custom_colors, name = "Response Variable", guide = "none") +
  # scale_size_continuous(range = c(2, 12)) +
  scale_size_continuous(
  range = c(2, 12),
  breaks = seq(25, 200, by = 25),
  limits = c(0, 200),
  name = "No. of Observations",
  guide = guide_legend(
    title.position = "top",
    title.hjust = 0.5,
    direction = "horizontal",     # Forces horizontal layout
    nrow = 1,                     # Single horizontal row
    override.aes = list(shape = 21, fill = "white", color = "black")
  )
) +
  theme_bw(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(size = 12),
    legend.position = "bottom",
    panel.grid = element_blank(),
    legend.box = "vertical",
  legend.title = element_text(size = 12),
  legend.text = element_text(size = 10)
  ) +
  labs(
    #title = "Evidence and Gap Map: Ecosystem Services vs Tree Types",
    x = "Tree Genus",
    y = "Agroecosystem Service",
    size = "No. of Observations"
  )

# Step 4: Display the plot
egm_plot_tree_vs_agroecosystem_service
```

## Evidence and Gap Map (EGM): Response Variables x Tree Types - Facetted for Crop Types

```{r}
# EGM: Response Variables x Tree Types, Facetted by Crop Type

# Step 1: Prepare data
egm_matrix_tree_crop <- imp_data_rom %>%
  drop_na(response_variable, tree_genus, crop_type) %>%
  distinct(id_obs, response_variable, tree_genus, crop_type) %>%
  count(response_variable, tree_genus, crop_type) %>%
  mutate(across(where(is.character), as.factor))

# Step 2: Order factors by total frequency
tree_levels <- egm_matrix_tree_crop %>%
  count(tree_genus, wt = n) %>%
  arrange(desc(n)) %>%
  pull(tree_genus)

response_levels <- egm_matrix_tree_crop %>%
  count(response_variable, wt = n) %>%
  arrange(desc(n)) %>%
  pull(response_variable)

crop_levels <- egm_matrix_tree_crop %>%
  count(crop_type, wt = n) %>%
  arrange(desc(n)) %>%
  pull(crop_type)


# Step 3: Plot facetted EGM (Tree Genus × Agroecosystem Services, Facetted by Crop Type)
egm_plot_tree_vs_agroecosystem_service_faceted_by_crop <- ggplot(
  egm_matrix_tree_crop %>%
    mutate(crop_type = as.factor(crop_type)),
  aes(
    x = factor(tree_genus, levels = tree_levels),
    y = factor(response_variable, levels = response_levels)
  )
) +
  geom_point(
    aes(size = n, fill = response_variable),
    shape = 21, color = "black", alpha = 0.9
  ) +
  scale_fill_manual(values = custom_colors, guide = "none") +
  scale_size_continuous(
    range = c(2, 12),
    breaks = seq(25, 200, by = 25),
    limits = c(0, 200),
    name = "No. of Observations",
    guide = guide_legend(
      title.position = "top",
      title.hjust = 0.5,
      direction = "horizontal",
      nrow = 1,
      override.aes = list(shape = 21, fill = "white", color = "black")
    )
  ) +
  ggforce::facet_row(
    facets = vars(crop_type),
    space = "free",
    scales = "free_x"
  ) +
  theme_bw(base_size = 14) +
  theme(
    strip.text = element_text(size = 13, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 12),
    legend.position = "bottom",
    legend.box = "vertical",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    panel.grid = element_blank(),
    panel.spacing = unit(1.5, "lines"),
    plot.margin = margin(10, 20, 10, 20)
  ) +
  labs(
    x = "Tree Genus",
    y = "Agroecosystem Service",
    size = "No. of Observations"
  )

# Step 4: Show Plot
egm_plot_tree_vs_agroecosystem_service_faceted_by_crop
```

