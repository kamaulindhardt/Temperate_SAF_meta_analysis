---
title: "5_FINAL_VISUALIZATIONS"
author: "M.K.K. Lindhardt"
date: "2024-11-17"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between


Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

# STEP 0 PREPARING SCRIPT AND READ IN THE DATA

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    patchwork,        # ggplot2 API for sequentially building up a plot
    purrr, 
    ggridges,
    ggbreak,
    tidyr,
    forcats,
    ###################################################################################################################
    # Spatial Data
    tidygeocoder,     # Unified interface for performing both forward and reverse geocoding queries
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    rnaturalearth,    # For world map data
    rnaturalearthdata, 
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})


###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("extract", "raster")
conflict_prefer("intersect", "base")
```


Loading the datasets

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())


# Define your working directory using 'here'
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load datasets
non_imp_dataset <- readRDS(file.path(output_dir, "non_imp_data_rom.rds"))
imp_dataset <- readRDS(file.path(output_dir, "imp_data_rom.rds"))
```

```{r}
# Custom colors for each response variable
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)
```


```{r}
imp_dataset |> glimpse()
```



# STEP 1 MAP OF STUDIES' GEOGRAPHIC DISTRIBUTION - SPATIAL DISTRIBUTION 


Publication-ready map that visualizes the ecosystem services (response variables) reported in each study (id_article),
```{r}
# Step 1: Simplify the dataset for visualization
geo_data <- imp_dataset %>%
  group_by(lat = final_lat, lon = final_lon, response_variable) %>%
  summarize(
    n_studies = n_distinct(id_article),
    .groups = "drop"
  ) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

geo_data
```


```{r}
# Step 2: Base world map
world_map <- map_data("world")


# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)


# Step 3: Create the enhanced map
geo_distribution_of_studies_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add jittered points for studies
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 2, height = 1.5)
  ) +
  # Apply custom colors
  scale_color_manual(values = custom_colors, 
                     name = "Ecosystem Service",
                     # Enlarges color legend symbols
                     guide = guide_legend(override.aes = list(size = 5))
                     ) +
  scale_size_continuous(
    name = "Number of Studies",
    range = c(4, 8),  # Adjust size range for better visibility
    breaks = c(1, 2, 5),  # Customize breaks based on study count
    labels = c("1", "2", "5+")
  ) +
  # Add labels and enhance the theme
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry Studies",
    subtitle = "Larger Map for Clearer Visualization",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_quickmap() +  # Quick world map projection
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
geo_distribution_of_studies_map
```

```{r}
# Define European bounding box (Longitude and Latitude limits)
europe_bbox <- c(xmin = -15, xmax = 25, ymin = 34, ymax = 58)

# Create the zoomed-in map for Europe
geo_distribution_europe_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add jittered points for studies (Europe only)
  geom_point(
    data = geo_data %>% filter(lon >= europe_bbox["xmin"], lon <= europe_bbox["xmax"],
                               lat >= europe_bbox["ymin"], lat <= europe_bbox["ymax"]),
    aes(x = lon, y = lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 1, height = 0.8)
  ) +
  # Apply custom colors and increase legend dot size
  scale_color_manual(
    values = custom_colors,
    name = "Ecosystem Service",
    guide = guide_legend(override.aes = list(size = 5))  # Enlarges legend symbols
  ) +
  # Keep study size legend unchanged
  scale_size_continuous(
    name = "Number of Studies",
    range = c(4, 8),  
    breaks = c(1, 2, 5),  
    labels = c("1", "2", "5+")
  ) +
  # Add labels and enhance the theme
  labs(
    title = "",
    subtitle = "",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "none",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the zoomed-in Europe map
geo_distribution_europe_map
```

```{r}
# Combining the maps in a nicely designed multiplot


# Global map
geo_distribution_of_studies_map

# Zoomed-in Europe map
geo_distribution_europe_map
```


```{r}
imp_dataset |> glimpse()
```

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(sf)
library(ggnewscale)  # Allows multiple fill scales in ggplot2
library(rnaturalearth)
library(rnaturalearthdata)
library(ggforce)  # For drawing pie charts
```

```{r}
# Step 2: Aggregate Data at Country Level
country_study_counts <- imp_dataset %>%
  group_by(location) %>%
  summarize(n_studies = n_distinct(id_article), .groups = "drop") 

country_study_counts |> glimpse()

# Step 3: Calculate Response Variable Proportions Per Country
response_proportions <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop") %>%
  group_by(location) %>%
  mutate(proportion = n_articles / sum(n_articles)) 

response_proportions |> glimpse()
```

```{r}
# Load world map
world_map <- map_data("world")

world <- ne_countries(scale = "medium", returnclass = "sf")
colnames(world)  # Check available columns

# Step 2: Aggregate number of studies per country
country_study_counts <- imp_dataset %>%
  group_by(location) %>%
  summarize(n_studies = n_distinct(id_article), .groups = "drop")

# Ensure correct numeric format
country_study_counts$n_studies <- as.numeric(country_study_counts$n_studies)

# Step 3: Calculate response variable proportions per country
response_proportions <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop") %>%
  group_by(location) %>%
  mutate(proportion = n_articles / sum(n_articles)) 

# Step 4: Merge study counts with world map
world_data <- world %>%
  left_join(country_study_counts, by = c("name" = "location"))

# Step 5: Compute centroids for pie chart placement
# Convert to MULTIPOINT for countries with multiple polygons
centroids <- world_data %>%
  filter(!is.na(n_studies)) %>%
  st_as_sf() %>%
  st_centroid(of_largest_polygon = TRUE)  # Ensures correct placement

# Standardize country names before merging
country_study_counts <- country_study_counts %>%
  mutate(location = recode(location,
                           "UK" = "United Kingdom",
                           "USA" = "United States of America",
                           "Canada" = "Canada",
                           "Hungary" = "Hungary"))

response_proportions <- response_proportions %>%
  mutate(location = recode(location,
                           "UK" = "United Kingdom",
                           "USA" = "United States of America",
                           "Canada" = "Canada",
                           "Hungary" = "Hungary"))

# Merge study counts with world map
world_data <- world %>%
  left_join(country_study_counts, by = c("name" = "location"))

unique(world_data$name)


# Compute centroids again
centroids <- world_data %>%
  filter(!is.na(n_studies)) %>%
  st_as_sf() %>%
  st_centroid(of_largest_polygon = TRUE)  # Ensures correct placement

# Merge response proportions
pie_chart_data <- centroids %>%
  left_join(response_proportions, by = c("name" = "location")) %>%
  filter(!is.na(proportion))  # Remove missing proportions

# pie_chart_data$sovereignt
# 
# world_data$sovereignt
# 
# centroids$sovereignt
```


```{r}
# Define custom colors for response variables
response_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Step 1: Load world map
world <- ne_countries(scale = "medium", returnclass = "sf")


# Step 6: Generate the final map
final_global_map <- ggplot() +
  # Base map with study counts as color
  geom_sf(data = world_data, aes(fill = n_studies), color = "black", size = 0.3) +
  #scale_fill_viridis_c(name = "Number of Studies", na.value = "gray90", option = "magma") + 
  # Use "Blues" scale from RColorBrewer (monochrome blue gradient)
  scale_fill_distiller(
    name = "Number of Studies",
    palette = "Blues",  # Set to Brewer's "Blues" scale
    direction = 1,  # Light to dark
    na.value = "gray90",
    limits = c(0, max(world_data$n_studies, na.rm = TRUE))
  ) + 

  # Add new fill scale for pie charts (prevents conflicts)
  new_scale_fill() +

  # Add pie charts at country centroids
  geom_arc_bar(
    data = pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],  # X-coord (longitude)
      y0 = st_coordinates(geometry)[,2],  # Y-coord (latitude)
      r0 = 0, r = 3,  # Adjust radius size as needed
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Response Variable", values = response_colors) +  # Use categorical scale

  # Labels & theme adjustments
  labs(
    title = "Geographical Distribution of Ecosystem Services in Agroforestry Studies",
    subtitle = "Pie chart shows proportion of each response variable; color indicates study count",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50")
  )

# Display the final global map
print(final_global_map)
```



```{r}
# Step 1: Define European Bounding Box
europe_bbox <- c(xmin = -10, xmax = 30, ymin = 35, ymax = 65)

# Step 1: Extract all European countries (even those without studies)
europe_all_countries <- world %>%
  filter(continent == "Europe") %>%
  select(name, geometry)  # Keep only necessary columns

# Step 2: Merge `europe_all_countries` with study count data
europe_all_countries <- europe_all_countries %>%
  left_join(country_study_counts, by = c("name" = "location")) %>%
  mutate(n_studies = ifelse(is.na(n_studies), 0, n_studies))  # Fill missing values with 0

# Step 3: Filter Pie Chart Data for Europe Only (Only for Countries with Data)
europe_pie_chart_data <- pie_chart_data %>%
  filter(st_coordinates(geometry)[,1] >= europe_bbox["xmin"],
         st_coordinates(geometry)[,1] <= europe_bbox["xmax"],
         st_coordinates(geometry)[,2] >= europe_bbox["ymin"],
         st_coordinates(geometry)[,2] <= europe_bbox["ymax"])

# Step 4: Filter Pie Chart Data for Europe Only (Only for Countries with Data)
# Step 4: Generate the Zoomed-in Europe Map
geo_distribution_europe_map <- ggplot() +
  # Base map: Show all countries in Europe (with or without data)
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  
  # Ensure `scale_fill_viridis_c()` starts at 1, and gray out countries without data
  # scale_fill_viridis_c(
  #   name = "Number of Studies",
  #   option = "viridis",
  #   na.value = "gray90",
  #   limits = c(1, max(europe_all_countries$n_studies, na.rm = TRUE))
  # ) + 
  # Use "Blues" scale from RColorBrewer (monochrome blue gradient)
  scale_fill_distiller(
    name = "Number of Studies",
    palette = "Blues",  # Set to Brewer's "Blues" scale
    direction = 1,  # Light to dark
    na.value = "gray90",
    limits = c(0, max(europe_all_countries$n_studies, na.rm = TRUE))
  ) + 

  # Fix: Add `new_scale_fill()` before pie charts to separate categorical colors
  new_scale_fill() +

  # Add pie charts at country centroids for response proportions (Only where data exists)
  geom_arc_bar(
    data = europe_pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],  # Longitude
      y0 = st_coordinates(geometry)[,2],  # Latitude
      r0 = 0, r = 2,  # Adjust radius size
      fill = response_variable,  # Ensure this is NOT mapped to `n_studies`
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Response Variable", values = response_colors) +  # Use categorical scale

  # Labels and theme adjustments
  labs(
    title = "Geographical Distribution of Ecosystem Services in Agroforestry Studies (Europe)",
    subtitle = "Pie chart shows proportion of each response variable; color indicates study count",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50")
  )

# Step 5: Display the Zoomed-in Europe Map
print(geo_distribution_europe_map)
```

```{r}
# Generating combined map

# Step 1: Define Bounding Box for Europe
europe_bbox <- c(xmin = -10, xmax = 30, ymin = 35, ymax = 65)

# Step 2: Remove Antarctica and Generate the Global Map
global_map <- ggplot() +
  geom_sf(data = world_data %>% filter(continent != "Antarctica"), aes(fill = n_studies), color = "black", size = 0.3) +
  scale_fill_distiller(
    name = "No. of Studies",  # Renamed legend
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(0, max(world_data$n_studies, na.rm = TRUE)),
    breaks = c(1,3,5,7,9)
    ) + 
  new_scale_fill() +
  geom_arc_bar(
    data = pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 3,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(name = "Ecosystem Service", values = response_colors) +  # Updated legend title) +  
  theme_void() +
  theme(legend.position = "top") +
  guides(fill = guide_legend(nrow = 1, ncol = 8,
                             byrow = TRUE)) 

global_map

# Step 3: Generate the Zoomed-in Europe Map using BBOX
europe_map <- ggplot() +
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  scale_fill_distiller(
    palette = "Greens",
    direction = 1,
    na.value = "gray95",
    limits = c(1, 8)
  ) + 
  new_scale_fill() +
  geom_arc_bar(
    data = europe_pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 2,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(values = response_colors) +  
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  theme_void() +
  theme(
    legend.position = "none",
    panel.background = element_rect(fill = "white"),
    panel.border = element_rect(color = "black", size = 3,
                                fill = NA)
  )

europe_map

# Step 4: Combine the Global and Europe Maps using Patchwork
final_combined_map_of_study_geodistribution <- global_map + 
  inset_element(europe_map, 
                left = 0.10, 
                bottom = 0.18, 
                right = 0.95, 
                top = 0.63, 
                align_to = "panel")

# Step 6: Display the Final Merged Map
final_combined_map_of_study_geodistribution
```


















































# STEP 2 TEMPORAL DISTRIBUTION

```{r}
imp_dataset |> glimpse()
```


Step 2: Distribution of Studies Over Time

```{r}
# Define a function for generating the chart
create_stacked_bar_chart <- function(data, id_column, custom_colors, title = "Distribution Over Time", y_label = "Count") {
  
  # Prepare the data: Count unique entries by year and response variable
  grouped_data <- data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |>
    distinct(!!sym(id_column), Year, response_variable) |>  # Dynamically use id_column
    count(Year, response_variable)
  
  # Create the stacked bar chart
  ggplot(grouped_data, aes(x = Year, y = n, fill = response_variable)) +
    geom_bar(stat = "identity", position = "stack", color = "black") +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(grouped_data$Year, na.rm = TRUE), max(grouped_data$Year, na.rm = TRUE), by = 5),  # Adjust as needed
      labels = scales::number_format(scale = 1, accuracy = 1)
    ) +
    labs(
      title = title,
      subtitle = "Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette

# Generate chart for id_article (studies)
chart_studies <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_article",  # Switch to "id_obs" for observations
  custom_colors = custom_colors,
  title = "Distribution of Studies Over Time",
  y_label = "Number of Studies"
)

# Generate chart for id_obs (observations)
chart_observations <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_obs",
  custom_colors = custom_colors,
  title = "Distribution of Observations Over Time",
  y_label = "Number of Observations"
)

# Print the chart for studies
print(chart_studies)

# Print the chart for observations
print(chart_observations)
```

```{r}
# Function to create cumulative data in long format with proper carry-forward logic
prepare_cumulative_data <- function(data, id_column) {
  data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |> # Extract Year
    distinct(!!sym(id_column), Year, response_variable) |>               # Ensure unique id_column
    count(Year, response_variable) |>                                    # Count unique ids per year
    complete(Year = seq(min(Year, na.rm = TRUE), max(Year, na.rm = TRUE), by = 1),  # Add missing years
             response_variable,                                          # Ensure all response_variables exist for all years
             fill = list(n = 0)) |>                                      # Fill missing counts with 0
    group_by(response_variable) |>                                       # Group by response_variable
    arrange(Year) |>                                                     # Ensure correct year order within each group
    mutate(cumulative_n = cumsum(n)) |>                                  # Calculate cumulative sum
    ungroup()                                                            # Remove grouping
}

# Function to plot cumulative stacked area chart
create_cumulative_chart <- function(data, custom_colors, title = "Cumulative Distribution Over Time", y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", size = 0.3) +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5)
    ) +
    labs(
      title = title,
      subtitle = "Cumulative Count Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette


# Prepare cumulative data for articles
cumulative_data_articles <- prepare_cumulative_data(
  data = imp_dataset,
  id_column = "id_article"
)

# Generate the cumulative chart for articles
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# Print the chart
print(cumulative_articles_chart)
```

```{r}
n_distinct(imp_dataset$id_article)
```

```{r}

# Function to prepare cumulative data for each response_variable
prepare_cumulative_data <- function(data) {
  data |>
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |> # Extract Year
    distinct(id_article, Year, response_variable) |>                     # Ensure unique id_article per Year & response_variable
    group_by(Year, response_variable) |>                                 # Group by Year & response_variable
    summarise(unique_articles = n_distinct(id_article), .groups = "drop") |> # Count unique id_articles per group
    complete(Year = seq(min(Year, na.rm = TRUE), max(Year, na.rm = TRUE), by = 1),  # Add missing years
             response_variable,                                          # Ensure all response_variables exist for all years
             fill = list(unique_articles = 0)) |>                        # Fill missing counts with 0
    group_by(response_variable) |>                                      # Group by response_variable
    arrange(Year) |>                                                    # Ensure correct year order within each group
    mutate(cumulative_n = cumsum(unique_articles)) |>                   # Calculate cumulative sum
    ungroup()                                                           # Remove grouping
}

# Function to prepare cumulative data for the total unique articles
prepare_unique_articles <- function(data) {
  data %>%
    mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) %>%  # Extract year from date column
    group_by(id_article) %>%  # Group by id_article
    summarise(first_year = min(Year, na.rm = TRUE), .groups = "drop") %>%  # Get the first year each article appears
    count(first_year, name = "unique_articles") %>%  # Count unique articles by their first year
    arrange(first_year) %>%  # Ensure data is sorted by year
    mutate(cumulative_unique = cumsum(unique_articles)) %>%  # Calculate cumulative sum
    rename(Year = first_year)  # Rename column for consistency
}

# Function to create the cumulative chart
create_cumulative_chart <- function(data, unique_articles_data, custom_colors, title = "Cumulative Distribution Over Time", y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", size = 0.3) +  # Stacked area chart
    geom_line(data = unique_articles_data, aes(x = Year, y = cumulative_unique), 
              inherit.aes = FALSE, color = "black", size = 1.2) +  # Black line for cumulative unique articles
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5)
    ) +
    labs(
      title = title,
      subtitle = "Cumulative Count Stacked by Response Variable with Total Unique Articles",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette


# Prepare the data
cumulative_data_articles <- prepare_cumulative_data(imp_dataset)  # Data for the stacked area chart
cumulative_unique_articles <- prepare_unique_articles(imp_dataset)  # Data for the black line

# Generate the cumulative chart
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  unique_articles_data = cumulative_unique_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# Print the chart
print(cumulative_articles_chart)
```
```{r}
n_distinct(imp_dataset$id_article)  # Should match the final value in cumulative_unique
print(cumulative_unique_articles) |> glimpse() # Ensure no year surpasses the total unique articles
```


```{r}
# Prepare the data: Count unique studies by year and response variable
stacked_data <- imp_dataset |>
  mutate(Year = as.integer(format(as.Date(experiment_year), "%Y"))) |>
  distinct(id_article, Year, response_variable) |>  # Ensure unique studies
  count(Year, response_variable)

# Define custom color palette


# Create stacked bar chart
studies_over_time <- stacked_data |> 
  ggplot(aes(x = Year, y = n, fill = response_variable)) +
  geom_bar(stat = "identity", position = "stack", color = "black") +
  scale_fill_manual(values = custom_colors) +
  scale_x_continuous(
    breaks = seq(min(stacked_data$Year), max(stacked_data$Year), by = 5),  # Adjust as needed
    labels = scales::number_format(scale = 1, accuracy = 1)
  ) +
  labs(
    title = "Distribution of Studies Over Time",
    subtitle = "Stacked by Response Variable",
    x = "Year",
    y = "Number of Studies",
    fill = "Response Variable"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    legend.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

studies_over_time
```


```{r}
# Histogram for the distribution of unique studies over time
imp_dataset |> 
  distinct(id_article, experiment_year) |> 
ggplot(aes(x = as.Date(experiment_year))) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Unique Studies Over Time", x = "Year", y = "Number of Studies") +
  theme_minimal()
```

Step 3: Distribution of Studies Across Moderators

```{r}
# Bar plot for crop_type
data_distribution_crop_type <- 
  imp_dataset |> 
  ggplot(aes(x = crop_type)) +
  geom_bar(fill = "cornflowerblue") +
  labs(title = "Distribution of Entries Across Tree-Crop Combinations", x = "Tree-Crop Combination", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_crop_type
```
```{r}
# Bar plot for season
data_distribution_season <- 
  imp_dataset |>
  ggplot(aes(x = season)) +
  geom_bar(fill = "darkorange") +
  labs(title = "Distribution of Entries Across Seasons", x = "Season", y = "Number of Entries") +
  theme_minimal()

data_distribution_season
```

```{r}
# Bar plot for alley width category
```

Step 4: Effect Size Distribution



Step 5: Explore Moderators and Levels

```{r}
# Faceted bar plots for different moderators
data_distribution_moderator_levels_seasen <- 
  imp_dataset |> 
  ggplot(aes(x = response_variable)) +
  geom_bar(aes(fill = season)) +
  facet_wrap(~ crop_type) +
  labs(title = "Number of Entries by Response Variable and Season", x = "Response Variable", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_moderator_levels_seasen
```

Step 6: Distribution in Space (Using generated Categorized Location Column)

```{r}
# Bar plot for the number of studies per sub_region
data_distribution_studies_agroclimregion <- 
  imp_dataset |> 
  ggplot(aes(x = bioclim_sub_regions)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Number of Studies Per Continent", x = "Continent", y = "Number of Studies") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1))

data_distribution_studies_agroclimregion
```






# STEP 3 PROPORTION OF EFFECT SIZE DIRECTION AS NUMBER OF POSITIVE, NEUTRAL, AND NEGATIVE


Based on the minimal_random_effects model

```{r}
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))
```

```{r}
# Step 1: Calculate Z-score, p-value, and categorize effect sizes
imp_data_rom_calc <- imp_data_rom %>%
  mutate(
    Z = yi / sqrt(vi),  # Calculate Z-score
    Pval = 2 * (1 - pnorm(abs(Z))),  # Two-tailed p-value
    category = case_when(
      Pval < 0.05 & yi > 0 ~ "Positive",  # Significant positive effect
      Pval < 0.05 & yi < 0 ~ "Negative",  # Significant negative effect
      TRUE ~ "Neutral"  # Non-significant or neutral effect
    )
  )

# Step 2: Count effect sizes per category for each response variable
summary_data_for_eff_calc <- imp_data_rom_calc %>%
  count(response_variable, category, name = "count") %>%  # More efficient than group_by() + summarise(n())
  group_by(response_variable) %>%
  mutate(
    total_count = sum(count),  # Total effect sizes per response variable
    percentage = (count / total_count) * 100  # Compute proportion
  ) %>%
  ungroup()  # Remove grouping after calculation

# Step 3: Ensure response variables are ordered by total effect sizes
summary_data_for_eff_calc <- summary_data_for_eff_calc %>%
  mutate(response_variable = fct_reorder(response_variable, total_count, .fun = sum))

# Step 4: Create stacked bar plot for effect size proportions
stacked_barplot <- ggplot(summary_data_for_eff_calc, aes(x = response_variable, y = percentage, fill = category)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("Positive" = "#1b9e77", "Negative" = "#d95f02", "Neutral" = "#757575")) +
  labs(
    title = "Percentage of Effect Sizes by Response Variable",
    x = "Response Variable",
    y = "Percentage of Effect Sizes",
    fill = "Effect Size Category"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),  # Keep only y-axis grid lines
    panel.grid.major.x = element_blank(),  # Remove x-axis grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    axis.line = element_line(linewidth = 0.5)  # Keep axis lines visible
  ) +
  scale_y_continuous(
    limits = c(0, 100),  # Y-axis fixed between 0 and 100%
    breaks = seq(0, 100, by = 20)  # Set breaks at 20% intervals
  )

# Step 5: Display the plot
print(stacked_barplot)
```



















# STEP 4 FORREST PLOT - OVERALL EFFECT SIZE DISTRIBUTION


```{r}
# Cleaning and preparing data
prepare_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- prepare_forest_data(
  data = imp_dataset,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)
```

```{r}
# Aggregating data
aggregate_forest_data <- function(data, yi_col, ci_cols, group_col) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      overall_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(ci_cols[1]), na.rm = TRUE),
      upper_ci = mean(!!sym(ci_cols[2]), na.rm = TRUE),
      num_observations = n(),
      num_studies = n_distinct(id_article),
      size_category = case_when(
        num_studies <= 2 ~ "1-2",
        num_studies <= 4 ~ "3-4",
        num_studies > 4 ~ "5+"
      ),
      .groups = "drop"
    ) %>%
    mutate(
      size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
      response_label = paste0(!!sym(group_col), " (", num_studies, " studies)"),
      mean_ci_label = paste0(
        sprintf("%.2f", overall_effect), " [",
        sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
      ),
      response_rank = rank(-overall_effect)  # Rank for sorting
    )
}

aggregated_data <- aggregate_forest_data(
  data = forest_data_clean,
  yi_col = "yi",
  ci_cols = c("lower_ci", "upper_ci"),
  group_col = "response_variable"
)

# Computing densities
compute_densities <- function(data, yi_col, group_col, aggregated_data) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      aggregated_data %>% select(response_variable, response_label, response_rank),
      by = "response_variable"
    )
}

density_data <- compute_densities(
  data = forest_data_clean,
  yi_col = "yi",
  group_col = "response_variable",
  aggregated_data = aggregated_data
)

# Adding "Overall" effect size
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = sum(num_observations, na.rm = TRUE),
    num_studies = sum(num_studies, na.rm = TRUE),
    size_category = NA,
    mean_ci_label = paste0(
      sprintf("%.2f", mean(overall_effect, na.rm = TRUE)), " [",
      sprintf("%.2f", mean(lower_ci, na.rm = TRUE)), ", ",
      sprintf("%.2f", mean(upper_ci, na.rm = TRUE)), "]"
    )
  ) %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)",
    response_rank = Inf  # Place "Overall" at the bottom
  )

# Combine aggregated data and overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  arrange(response_rank) %>%
  mutate(
    response_label = factor(response_label, levels = unique(response_label)),
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables"),
    facet_label = factor(facet_label, levels = c("Response Variables", "Overall Effect Size"))
  )
```

```{r}
# Define a custom color palette for response_variable
custom_colors <- c(
  "Biodiversity" = "#FF9999",             # Light red
  "Greenhouse gas emission" = "#66C266",  # Green
  "Product quality" = "#FFC000",          # Yellow
  "Crop yield" = "#FF9933",               # Orange
  "Pest and Disease" = "#33CCCC",         # Teal
  "Soil quality" = "#9966CC",             # Purple
  "Water quality" = "#9999FF"             # Light blue
)
```


```{r}
# Forest plot with facets for "Overall" and "Response Variables"
# Arrange the `plot_data` by the number of studies
plot_data <- plot_data %>%
  arrange(desc(-num_studies)) %>%
  mutate(
    response_label = factor(
      response_label,
      levels = unique(response_label)  # Ensure levels follow the descending order of num_studies
    )
  )

# Update `density_data` to match the new order of response_label
density_data <- density_data %>%
  mutate(
    response_label = factor(
      response_label,
      levels = levels(plot_data$response_label)  # Use the same factor levels as in plot_data
    )
  )

####################################################################################################################

# Adjust text placement for CI values dynamically

# Updated Flexible Plotting Function
create_forest_plot <- function(
  density_data, 
  plot_data, 
  x_limits = NULL,    # Specify limits for the x-axis (e.g., c(-0.25, 0.5))
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Default breaks for the x-axis
  add_clipped_error_bars = FALSE,        # Include clipped error bars
  custom_colors = NULL                   # Custom fill colors
) {
  # Create a facet column to separate panels
  plot_data <- plot_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      )
    ) %>%
    arrange(desc(!num_studies)) %>%  # Sort by number of studies for the top panel
    mutate(
      response_label = factor(
        response_label,
        levels = unique(response_label)  # Ensure the order is preserved
      )
    )
  
  # Align density_data with plot_data
  density_data <- density_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      ),
      response_label = factor(
        response_label,
        levels = levels(plot_data$response_label)  # Match the order in plot_data
      )
    )
  
  # Explicitly order the facet levels
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )

  density_data <- density_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )
  
  # Adjust text placement for CI values
  plot_data <- plot_data %>%
    mutate(
      ci_text_position = ifelse(
        is.null(x_limits),
        max(x_breaks) + 0.1,  # Place text slightly beyond x-axis
        x_limits[2] - 0.2     # Keep text within x-axis limits
      )
    )
  
  # Build the base ggplot object
  forest_plot <- ggplot() +
    # Density ridges for individual response variables
    geom_ridgeline(
      data = density_data %>% filter(response_variable != "Overall"),
      aes(
        x = density_x,
        y = response_label,
        height = density_y,
        fill = response_variable
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Points for individual response variables
    geom_point(
      data = plot_data %>% filter(response_variable != "Overall"),
      aes(
        x = overall_effect,
        y = response_label,
        size = size_category
      ),
      color = "black"
    ) +
    # Diamond for overall effect size
    geom_point(
      data = plot_data %>% filter(response_variable == "Overall"),
      aes(
        x = overall_effect,
        y = response_label
      ),
      shape = 18,
      size = 5,
      color = "black"
    ) +
    # Error bars for all response variables
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = response_label
      ),
      height = 0.2,
      color = "darkgray"
    )
  
  # Optionally add clipped error bars
  if (add_clipped_error_bars && !is.null(x_limits)) {
    forest_plot <- forest_plot +
      geom_errorbarh(
        data = plot_data,
        aes(
          xmin = pmax(lower_ci, x_limits[1]),  # Clip lower bounds to x-axis limits
          xmax = pmin(upper_ci, x_limits[2]),  # Clip upper bounds to x-axis limits
          y = response_label
        ),
        height = 0.1,
        color = "darkgray",
        size = 0.8
      )
  }
  
  # Add text annotations for confidence intervals
  forest_plot <- forest_plot +
    geom_text(
      data = plot_data,
      aes(
        x = ci_text_position,  # Use dynamically calculated position
        y = response_label,
        label = mean_ci_label
      ),
      size = 3.5,
      hjust = 0,
      color = "black"
    ) +
    # Add vertical red dotted line for effect size = 0
    geom_vline(
      xintercept = 0,
      color = "red",
      linetype = "dotted",
      size = 0.7
    ) +
    # Adjust x-axis scale dynamically
    scale_x_continuous(
      limits = x_limits,     # Use the specified limits if provided
      breaks = x_breaks      # Use the specified breaks
    ) +
    # Customize fill colors and size scale
    scale_fill_manual(values = custom_colors, guide = "none") +
    scale_size_manual(
      values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
      name = "Number of Studies"
    ) +
    # Add labels and theme adjustments
    labs(
      title = "Forest Plot with Adjusted Density and Study Details",
      x = "Effect Size",
      y = NULL
    ) +
    # Separate panels for response variables and overall effect size
    facet_grid(
      facet_label ~ .,       # Facet by `facet_label`
      scales = "free_y",
      space = "free",
      switch = "y"           # Labels on the left
    ) +
    theme_minimal() +
    theme(
      strip.text.y = element_blank(),  # Remove facet labels
      strip.background = element_blank(),
      panel.spacing = unit(0.5, "lines"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 12),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      legend.position = "bottom",
      legend.title = element_text(size = 12),
      legend.text = element_text(size = 10)
    )
  
  return(forest_plot)
}
```

```{r}
forest_plot_density_narrow <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-0.25, 0.5),  # Narrow x-axis
  x_breaks = seq(-0.25, 0.5, by = 0.25),
  add_clipped_error_bars = TRUE,
  custom_colors = custom_colors
)
forest_plot_density_narrow
```

```{r}
forest_plot_density_wide <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-3, 3),  # Wide x-axis
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Adjust breaks to fit the wider range
  add_clipped_error_bars = FALSE,  # Optional: Set to FALSE for full error bars
  custom_colors = custom_colors
)
forest_plot_density_wide
```



```{r}
# Create the boxplot for the effect size (yi)
# Order response variables by descending median effect size
imp_data_rom_reorder <- imp_data_rom %>%
  mutate(response_variable = fct_reorder(response_variable, yi, .fun = median, .desc = FALSE))

# Create the boxplot with ordered response variables
boxplot_raw_effect_size <- imp_data_rom_reorder |> 
  ggplot(aes(y = response_variable, x = yi, fill = response_variable)) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", size = 0.8) + # Add red dotted line at x = 0
  geom_boxplot(alpha = 0.7, outlier.size = 1) +
  labs(
    title = "Raw Effect Sizes (yi) Across Response Variables",
    x = "Raw Effect Size (yi)",
    y = "Response Variable"
  ) +
  scale_x_continuous(
    trans = pseudo_log_trans(sigma = 0.1), # Apply pseudo-log transformation
    breaks = c(0, 0.1, 1, 10, 100),       # Custom breaks
    labels = c("0", "0.1", "1", "10", "100") # Relatable labels
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 12), # Adjust y-axis text size
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels
    legend.position = "none"
  )

boxplot_raw_effect_size
```













FORREST PLOT - FOR INDIVIDUAL RESPONSE VARIABLES AND MODERATORS EFFECT SIZE DISTRIBUTION


```{r}
# Cleaning Data
clean_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- clean_forest_data(
  data = imp_dataset,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)
```

```{r}
forest_data_clean 

forest_moderator <- forest_data_clean |> 
  filter(response_variable %in% c("Biodiversity", "Crop yield", "Soil quality"))

forest_moderator |> str()
```

```{r}
create_forest_plot_moderator <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  x_limits = NULL,                     # Specify x-axis limits
  x_breaks = seq(-0.5, 1.0, by = 0.25),# Specify x-axis breaks
  custom_colors = NULL,                # Custom color palette
  add_clipped_error_bars = FALSE       # Add clipped error bars
) {
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = if (add_clipped_error_bars & !is.null(x_limits)) pmax(lower_ci, x_limits[1]) else lower_ci,
        xmax = if (add_clipped_error_bars & !is.null(x_limits)) pmin(upper_ci, x_limits[2]) else upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = if (is.null(x_limits)) max(x_breaks) + 0.1 else x_limits[2] - 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable
    facet_wrap(~ facet_label, ncol = 1, scales = "free_y") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # X-axis adjustments
    scale_x_continuous(limits = x_limits, breaks = x_breaks) +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Create a forest plot with tree_type as moderator
forest_plot_tree_type <- create_forest_plot_moderator(
  data = forest_moderator,
  response_var = "response_variable",
  moderator_var = "tree_type",
  x_limits = c(-1.0, 2.0),  # Wide x-axis
  x_breaks = seq(-1.0, 2.0, by = 0.5),
  custom_colors = custom_colors,
  add_clipped_error_bars = TRUE
)

# Display the plot
forest_plot_tree_type
```


Free x-axis for response variables (ecosystem services)
```{r}
create_forest_plot_moderator_freex <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  custom_colors = NULL                 # Custom color palette
) {

  
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = max(lower_ci, na.rm = TRUE) + 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable with free x-axis
    facet_wrap(~ facet_label, ncol = 1, scales = "free_x") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Generate the plot
forest_plot_free_x <- create_forest_plot_moderator_freex(data = forest_moderator, custom_colors = custom_colors)

forest_plot_free_x
```



















# STEP 5 FOREST PLOT (VIOLIN PLOT) EFFECT SIZES FOR ALL RESPONSE VARIABLES (THIS IS THE FINNAL FOREST PLOT)


```{r}
# Read the pre-saved combined evaluation results object
evaluation_results <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "MULTI_MODEL_EVALUATION_RESULTS", "evaluation_results_combined.rds")) 

evaluation_results
```

```{r}
evaluation_results$`Crop yield`$minimal_random_effects
evaluation_results$`Biodiversity`$minimal_random_effects
evaluation_results$`Soil quality`$minimal_random_effects
```

```{r}
##########################################################################################################################################
# Function to Back-Transform log-ROM to ROM in Percentage for All Response Variables
##########################################################################################################################################

# Define the function
back_transform_logROM <- function(evaluation_results) {
  # Initialize an empty list to store back-transformed results
  back_transformed_results <- list()

  # Iterate through each response variable in evaluation_results
  for (response in names(evaluation_results)) {
    cat("Processing response variable:", response, "\n")

    # Extract models for the response variable
    models <- evaluation_results[[response]]

    # Initialize a list to store back-transformed values for this response variable
    response_results <- list()

    # Define a helper function to back-transform log-ROM values
    back_transform <- function(estimate, ci.lb, ci.ub) {
      list(
        ROM_percent = (exp(estimate) - 1) * 100,  # Convert to percentage
        ROM_lower_percent = (exp(ci.lb) - 1) * 100,
        ROM_upper_percent = (exp(ci.ub) - 1) * 100
      )
    }

    # Iterate through all models (null, minimal, moderators, full, interaction)
    for (model_name in names(models)) {
      model <- models[[model_name]]

      # Skip if the model is NULL
      if (is.null(model)) {
        response_results[[model_name]] <- NULL
        next
      }

      # Extract estimates and confidence intervals
      if (!is.null(model$b)) {
        response_results[[model_name]] <- back_transform(
          estimate = model$b[1],
          ci.lb = model$ci.lb[1],
          ci.ub = model$ci.ub[1]
        )
      } else {
        response_results[[model_name]] <- NULL
      }
    }

    # Store the back-transformed results for this response variable
    back_transformed_results[[response]] <- response_results
  }

  return(back_transformed_results)
}

##########################################################################################################################################
# Example: Apply the function to evaluation_results
##########################################################################################################################################

# Assuming evaluation_results is already loaded
back_transformed_results <- back_transform_logROM(evaluation_results)

# Inspect the back-transformed results for a specific response variable
# Choose a model from structured_results (e.g., "interaction_model", or "minimal_random_effects")

back_transformed_results$`Water quality`$minimal_random_effects # <-------------- ! Chosen Model !
back_transformed_results$Biodiversity$minimal_random_effects
```




```{r}
##########################################################################################################################################
# Combine Raw Effect Sizes and Back-Transformed Values into a Structured Data Frame
##########################################################################################################################################

# Function to create a structured data frame
combine_effect_sizes <- function(evaluation_results, back_transformed_results) {
  combined_results <- data.frame(
    ResponseVariable = character(),
    Model = character(),
    Estimate = numeric(),
    CI_Lower = numeric(),
    CI_Upper = numeric(),
    P_Value = character(),
    Significance = character(),
    ROM_Percent = numeric(),
    ROM_Lower_Percent = numeric(),
    ROM_Upper_Percent = numeric(),
    stringsAsFactors = FALSE
  )

  for (response in names(evaluation_results)) {
    models <- evaluation_results[[response]]
    back_transformed <- back_transformed_results[[response]]

    for (model_name in names(models)) {
      model <- models[[model_name]]
      if (!is.null(model) && !is.null(model$b) && !is.null(model$ci.lb) && !is.null(model$pval)) {
        p_value <- ifelse(model$pval[1] < 0.001, "<0.001", formatC(model$pval[1], format = "f", digits = 3))
        significance <- if (model$pval[1] < 0.001) {
          "***"
        } else if (model$pval[1] < 0.01) {
          "**"
        } else if (model$pval[1] < 0.05) {
          "*"
        } else if (model$pval[1] < 0.1) {
          "."
        } else {
          " "
        }

        combined_results <- rbind(combined_results, data.frame(
          ResponseVariable = response,
          Model = model_name,
          Estimate = model$b[1],
          CI_Lower = model$ci.lb[1],
          CI_Upper = model$ci.ub[1],
          P_Value = p_value,
          Significance = significance,
          ROM_Percent = back_transformed[[model_name]]$ROM_percent,
          ROM_Lower_Percent = back_transformed[[model_name]]$ROM_lower_percent,
          ROM_Upper_Percent = back_transformed[[model_name]]$ROM_upper_percent
        ))
      }
    }
  }

  return(combined_results)
}


# Apply the function
structured_results <- combine_effect_sizes(evaluation_results, back_transformed_results)


output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# Save structured_results in a combined file
saveRDS(structured_results, file = file.path(output_dir, "structured_results_all_effect_sizes.rds"))



# Inspect the structured data frame
structured_results |> glimpse()
```


```{r}
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Carbon sequestration" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)
```

```{r}
##########################################################################################################################################

# Step 1: Choose a model from structured_results (e.g., "interaction_model", or "minimal_random_effects")
chosen_model <- "minimal_random_effects" # <-------------- ! Chosen Model !

# Step 2: Compute median effect sizes for ordering
median_effects <- imp_data_rom %>%
  group_by(response_variable) %>%
  summarise(median_effect = median(yi, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(median_effect))  # Order by descending median effect size

# Step 3: Calculate the number of studies (`n_studies`) and number of observations (`obs_count`) per response variable
study_counts <- imp_data_rom %>%
  group_by(response_variable) %>%
  summarise(
    n_studies = n_distinct(id_article),  # Count unique studies (articles)
    obs_count = n(),  # Count total observations
    .groups = "drop"
  )

# Step 4: Extract structured summary for the chosen model
structured_summary <- structured_results %>%
  filter(Model == chosen_model) %>%  # Ensure the selected model is used
  select(ResponseVariable, CI_Lower, CI_Upper, ROM_Lower_Percent, ROM_Upper_Percent, Significance)

# Step 5: Merge `study_counts` into `structured_summary`
structured_summary <- structured_summary %>%
  left_join(study_counts, by = c("ResponseVariable" = "response_variable")) %>%  # Ensure `n_studies` & `obs_count` are included
  mutate(ResponseVariable = factor(ResponseVariable, levels = median_effects$response_variable))  # Ensure ordering

# Step 6: Merge `study_counts` and `structured_summary` into `imp_data_rom_violin`
imp_data_rom_violin <- imp_data_rom %>%
  left_join(study_counts, by = "response_variable") %>%  
  left_join(structured_summary, by = c("response_variable" = "ResponseVariable"))  

# Step 7: Apply the ordered factor to `imp_data_rom_violin`
imp_data_rom_violin <- imp_data_rom_violin %>%
  mutate(response_variable = factor(response_variable, levels = median_effects$response_variable))

##########################################################################################################################################
# Step 6: Define fixed x-axis limits
x_min <- -0.30
x_max <- 1.00

# Step 7: Create the Forest Violin Plot
forest_violin_plot <- ggplot(imp_data_rom_violin, aes(x = yi, 
                                                      y = response_variable, 
                                                      fill = response_variable)) +
  geom_violin(trim = FALSE, alpha = 0.5, color = "black") +
  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.8, color = "black") +
  stat_summary(fun = mean, geom = "point", shape = 21, size = 3, fill = "white", stroke = 1.5) +
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2, color = "black", linewidth = 1) +
  scale_fill_manual(values = custom_colors) +
  labs(
    title = "", # "Overall Effects of SAF on Agroecosystem Services Relative to Monocrop"
    x = "Effect Size (logRR) Relative to Monocrop",
    y = "",
    fill = "Response Variable"
  ) +
  scale_x_continuous(limits = c(x_min, x_max), breaks = seq(x_min, x_max, by = 0.05)) +
  scale_x_break(c(0.35, 0.6), scales = "free") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(size = 14),
    axis.title = element_text(size = 20),
    axis.text.x.top = element_blank(),
    axis.title.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    axis.text.y = element_text(size = 20),
    legend.position = "none",
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),
    panel.grid.minor = element_blank(),
    axis.line = element_line(linewidth = 0.5)
  ) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", linewidth = 1.5)

# Step 8: Create the CI & Study Count Text Plot (Now Includes `obs_count`)
text_plot <- ggplot(structured_summary, aes(
    y = factor(ResponseVariable, levels = median_effects$response_variable),
    x = -0.5
  )) +
  geom_text(
    aes(
      label = paste0(
        "CI: [", round(CI_Lower, 3), ", ", round(CI_Upper, 3), "]\n",
        "Back-transformed: [", round(ROM_Lower_Percent, 3), "%, ", 
        round(ROM_Upper_Percent, 3), "%]\n",
        "n studies = ", n_studies, " | n obs = ", obs_count
      )
    ),
    hjust = 1,
    size = 4
  ) +
  expand_limits(x = -5) +
  labs(title = "") + # "Effect Size, Study & Observation Counts"
  theme_void() +
  theme(
    plot.title = element_text(size = 14, hjust = 1, face = "bold"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

# Step 9: Arrange both plots side by side
forest_violin_plot_final <- forest_violin_plot + text_plot + plot_layout(widths = c(6, 1))

# Display the plot
forest_violin_plot_final
```

```{r}
structured_summary
```








# STEP 6 PROPORTION OF EFFECT SIZE DIRECTION AS NUMBER OF POSITIVE, NEUTRAL, AND NEGATIVE


Based on the minimal_random_effects model

Change everything!

```{r}
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))
```

```{r}
# Step 1: Calculate Z-score, p-value, and categorize effect sizes
imp_data_rom_calc <- imp_data_rom %>%
  mutate(
    Z = yi / sqrt(vi),  # Calculate Z-score
    Pval = 2 * (1 - pnorm(abs(Z))),  # Two-tailed p-value
    category = case_when(
      Pval < 0.05 & yi > 0 ~ "Positive",  # Significant positive effect
      Pval < 0.05 & yi < 0 ~ "Negative",  # Significant negative effect
      TRUE ~ "Neutral"  # Non-significant or neutral effect
    )
  )

# Step 2: Count effect sizes per category for each response variable
summary_data_for_eff_calc <- imp_data_rom_calc %>%
  count(response_variable, category, name = "count") %>%  # More efficient than group_by() + summarise(n())
  group_by(response_variable) %>%
  mutate(
    total_count = sum(count),  # Total effect sizes per response variable
    percentage = (count / total_count) * 100  # Compute proportion
  ) %>%
  ungroup()  # Remove grouping after calculation

# Step 3: Ensure response variables are ordered by total effect sizes
summary_data_for_eff_calc <- summary_data_for_eff_calc %>%
  mutate(response_variable = fct_reorder(response_variable, total_count, .fun = sum))

# Step 4: Create stacked bar plot for effect size proportions
stacked_barplot <- ggplot(summary_data_for_eff_calc, aes(x = response_variable, y = percentage, fill = category)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("Positive" = "#1b9e77", "Negative" = "#d95f02", "Neutral" = "#757575")) +
  labs(
    title = "Percentage of Effect Sizes by Response Variable",
    x = "Response Variable",
    y = "Percentage of Effect Sizes", # %
    fill = "Effect Size Category"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),  # Keep only y-axis grid lines
    panel.grid.major.x = element_blank(),  # Remove x-axis grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    axis.line = element_line(linewidth = 0.5)  # Keep axis lines visible
  ) +
  scale_y_continuous(
    limits = c(0, 100),  # Y-axis fixed between 0 and 100%
    breaks = seq(0, 100, by = 20)  # Set breaks at 20% intervals
  )

# Step 5: Display the plot
print(stacked_barplot)
```
 
 
 
 







# STEP 7 EVIDENCE GAP MAP - HEAT MAP STYLE

```{r}
imp_data_rom |> glimpse()
```
```{r}
# Evidence and Gap Map (EGM): Response Variables x Crop Types

# Create a count table of response_variable by crop_type (crop_genus)
egm_matrix <- imp_data_rom %>%
  drop_na(response_variable, crop_genus) %>%
  distinct(id_obs, response_variable, crop_genus) %>%
  count(response_variable, crop_genus) %>%
  mutate(across(where(is.character), as.factor))

# Optional: order rows and columns
crop_levels <- egm_matrix %>% 
  count(crop_genus, wt = n) %>% 
  arrange(desc(n)) %>% 
  pull(crop_genus)

response_levels <- egm_matrix %>% 
  count(response_variable, wt = n) %>% 
  arrange(desc(n)) %>% 
  pull(response_variable)


custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Carbon sequestration" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)


# Plot the EGM with reordered axes based on counts
egm_plot <- ggplot(egm_matrix, aes(x = factor(crop_genus, levels = crop_levels),
                                   y = factor(response_variable, levels = response_levels))) +
  geom_point(aes(size = n, fill = response_variable), shape = 21, color = "black", alpha = 0.9) +
  scale_fill_manual(values = custom_colors, name = "Response Variable", guide = "none") +
  scale_size_continuous(range = c(2, 12)) +
  theme_bw(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    panel.grid = element_blank()
  ) +
  labs(
    title = "Evidence and Gap Map: Ecosystem Services vs Crop Types",
    x = "Crop Type",
    y = "Ecosystem Service (Response Variable)",
    size = "No. of Observations"
  )

print(egm_plot)

```






