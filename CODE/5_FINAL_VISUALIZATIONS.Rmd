---
title: "5_FINAL_VISUALIZATIONS"
author: "M.K.K. Lindhardt"
date: "2024-11-17"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    patchwork,        # ggplot2 API for sequentially building up a plot
    purrr, 
    ggridges,
    ###################################################################################################################
    # Spatial Data
    tidygeocoder,     # Unified interface for performing both forward and reverse geocoding queries
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    rnaturalearth,    # For world map data
    rnaturalearthdata, 
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("extract", "raster")
conflict_prefer("intersect", "base")
```


Loading the datasets

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())


# Define your working directory using 'here'
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load datasets
non_imp_dataset <- readRDS(file.path(output_dir, "non_imp_dataset.rds"))
imp_dataset <- readRDS(file.path(output_dir, "imp_dataset.rds"))
```

```{r}
# Custom colors for each response variable
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)
```


```{r}
imp_dataset |> glimpse()
```



#############
# STEP 1
##########################################################################################################################################
MAP OF STUDIES' GEOGRAPHIC DISTRIBUTION - SPATIAL DISTRIBUTION 
##########################################################################################################################################


Publication-ready map that visualizes the ecosystem services (response variables) reported in each study (id_article),
```{r}
# Step 1: Simplify the dataset for visualization
geo_data <- imp_dataset %>%
  group_by(lat = final_lat, lon = final_lon, response_variable) %>%
  summarize(
    n_studies = n_distinct(id_article),
    .groups = "drop"
  ) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

# Step 2: Base world map
world_map <- map_data("world")


# Step 4: Create the map
geo_distribution_of_studies_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
   # Add jittered points for studies
  geom_jitter(
    data = geo_data,
    aes(x = lon, y = lat, color = response_variable, size = as.factor(n_studies)),
    alpha = 0.8,
    width = 0.8,  # Adjust jitter width (longitude)
    height = 0.75  # Adjust jitter height (latitude)
  ) +
  # Apply custom colors
  scale_color_manual(values = custom_colors, name = "Ecosystem Service") +
  scale_size_manual(
    name = "n studies for the given locataion",
    values = c(2, 3, 4, 5, 6, 7), # Assign size values for bins 1-6
    breaks = as.character(1:6),  # Ensure breaks correspond to whole numbers
    labels = as.character(1:6)   # Labels for the legend
  ) +
  # Add labels and theme adjustments
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry Studies (Northern Hemisphere)",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(ylim = c(20, 90)) +  # Restrict to the northern hemisphere
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50")
  )

geo_distribution_of_studies_map
```







#############
# STEP 2
##########################################################################################################################################
TEMPORAL DISTRIBUTION
##########################################################################################################################################


Step 2: Distribution of Studies Over Time

```{r}
# Define a function for generating the chart
create_stacked_bar_chart <- function(data, id_column, custom_colors, title = "Distribution Over Time", y_label = "Count") {
  
  # Prepare the data: Count unique entries by year and response variable
  grouped_data <- data |>
    mutate(Year = as.integer(format(as.Date(study_year_start), "%Y"))) |>
    distinct(!!sym(id_column), Year, response_variable) |>  # Dynamically use id_column
    count(Year, response_variable)
  
  # Create the stacked bar chart
  ggplot(grouped_data, aes(x = Year, y = n, fill = response_variable)) +
    geom_bar(stat = "identity", position = "stack", color = "black") +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(grouped_data$Year, na.rm = TRUE), max(grouped_data$Year, na.rm = TRUE), by = 5),  # Adjust as needed
      labels = scales::number_format(scale = 1, accuracy = 1)
    ) +
    labs(
      title = title,
      subtitle = "Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette

# Generate chart for id_article (studies)
chart_studies <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_article",  # Switch to "id_obs" for observations
  custom_colors = custom_colors,
  title = "Distribution of Studies Over Time",
  y_label = "Number of Studies"
)

# Generate chart for id_obs (observations)
chart_observations <- create_stacked_bar_chart(
  data = imp_dataset,
  id_column = "id_obs",
  custom_colors = custom_colors,
  title = "Distribution of Observations Over Time",
  y_label = "Number of Observations"
)

# Print the chart for studies
print(chart_studies)

# Print the chart for observations
print(chart_observations)
```

```{r}
# Function to create cumulative data in long format with proper carry-forward logic
prepare_cumulative_data <- function(data, id_column) {
  data |>
    mutate(Year = as.integer(format(as.Date(study_year_start), "%Y"))) |> # Extract Year
    distinct(!!sym(id_column), Year, response_variable) |>               # Ensure unique id_column
    count(Year, response_variable) |>                                   # Count unique ids per year
    complete(Year = seq(min(Year, na.rm = TRUE), max(Year, na.rm = TRUE), by = 1),  # Add missing years
             response_variable,                                          # Ensure all response_variables exist for all years
             fill = list(n = 0)) |>                                      # Fill missing counts with 0
    group_by(response_variable) |>                                      # Group by response_variable
    arrange(Year) |>                                                    # Ensure correct year order within each group
    mutate(cumulative_n = cumsum(n)) |>                                 # Calculate cumulative sum
    ungroup()                                                           # Remove grouping
}

# Function to plot cumulative stacked area chart
create_cumulative_chart <- function(data, custom_colors, title = "Cumulative Distribution Over Time", y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", size = 0.3) +
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5)
    ) +
    labs(
      title = title,
      subtitle = "Cumulative Count Stacked by Response Variable",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette


# Prepare cumulative data for articles
cumulative_data_articles <- prepare_cumulative_data(
  data = imp_dataset,
  id_column = "id_article"
)

# Generate the cumulative chart for articles
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# Print the chart
print(cumulative_articles_chart)
```

```{r}
n_distinct(imp_dataset$id_article)
```

```{r}

# Function to prepare cumulative data for each response_variable
prepare_cumulative_data <- function(data) {
  data |>
    mutate(Year = as.integer(format(as.Date(study_year_start), "%Y"))) |> # Extract Year
    distinct(id_article, Year, response_variable) |>                     # Ensure unique id_article per Year & response_variable
    group_by(Year, response_variable) |>                                 # Group by Year & response_variable
    summarise(unique_articles = n_distinct(id_article), .groups = "drop") |> # Count unique id_articles per group
    complete(Year = seq(min(Year, na.rm = TRUE), max(Year, na.rm = TRUE), by = 1),  # Add missing years
             response_variable,                                          # Ensure all response_variables exist for all years
             fill = list(unique_articles = 0)) |>                        # Fill missing counts with 0
    group_by(response_variable) |>                                      # Group by response_variable
    arrange(Year) |>                                                    # Ensure correct year order within each group
    mutate(cumulative_n = cumsum(unique_articles)) |>                   # Calculate cumulative sum
    ungroup()                                                           # Remove grouping
}

# Function to prepare cumulative data for the total unique articles
prepare_unique_articles <- function(data) {
  data %>%
    mutate(Year = as.integer(format(as.Date(study_year_start), "%Y"))) %>%  # Extract year from date column
    group_by(id_article) %>%  # Group by id_article
    summarise(first_year = min(Year, na.rm = TRUE), .groups = "drop") %>%  # Get the first year each article appears
    count(first_year, name = "unique_articles") %>%  # Count unique articles by their first year
    arrange(first_year) %>%  # Ensure data is sorted by year
    mutate(cumulative_unique = cumsum(unique_articles)) %>%  # Calculate cumulative sum
    rename(Year = first_year)  # Rename column for consistency
}

# Function to create the cumulative chart
create_cumulative_chart <- function(data, unique_articles_data, custom_colors, title = "Cumulative Distribution Over Time", y_label = "Cumulative Count") {
  ggplot(data, aes(x = Year, y = cumulative_n, fill = response_variable)) +
    geom_area(alpha = 0.8, color = "black", size = 0.3) +  # Stacked area chart
    geom_line(data = unique_articles_data, aes(x = Year, y = cumulative_unique), 
              inherit.aes = FALSE, color = "black", size = 1.2) +  # Black line for cumulative unique articles
    scale_fill_manual(values = custom_colors) +
    scale_x_continuous(
      breaks = seq(min(data$Year, na.rm = TRUE), max(data$Year, na.rm = TRUE), by = 5)
    ) +
    labs(
      title = title,
      subtitle = "Cumulative Count Stacked by Response Variable with Total Unique Articles",
      x = "Year",
      y = y_label,
      fill = "Response Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      axis.title = element_text(size = 12, face = "bold"),
      legend.title = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Define custom color palette


# Prepare the data
cumulative_data_articles <- prepare_cumulative_data(imp_dataset)  # Data for the stacked area chart
cumulative_unique_articles <- prepare_unique_articles(imp_dataset)  # Data for the black line

# Generate the cumulative chart
cumulative_articles_chart <- create_cumulative_chart(
  data = cumulative_data_articles,
  unique_articles_data = cumulative_unique_articles,
  custom_colors = custom_colors,
  title = "Cumulative Number of Articles Over Time",
  y_label = "Cumulative Articles"
)

# Print the chart
print(cumulative_articles_chart)
```
```{r}
n_distinct(imp_dataset$id_article)  # Should match the final value in cumulative_unique
print(cumulative_unique_articles) |> glimpse() # Ensure no year surpasses the total unique articles
```


```{r}
# Prepare the data: Count unique studies by year and response variable
stacked_data <- imp_dataset |>
  mutate(Year = as.integer(format(as.Date(study_year_start), "%Y"))) |>
  distinct(id_article, Year, response_variable) |>  # Ensure unique studies
  count(Year, response_variable)

# Define custom color palette


# Create stacked bar chart
studies_over_time <- stacked_data |> 
  ggplot(aes(x = Year, y = n, fill = response_variable)) +
  geom_bar(stat = "identity", position = "stack", color = "black") +
  scale_fill_manual(values = custom_colors) +
  scale_x_continuous(
    breaks = seq(min(stacked_data$Year), max(stacked_data$Year), by = 5),  # Adjust as needed
    labels = scales::number_format(scale = 1, accuracy = 1)
  ) +
  labs(
    title = "Distribution of Studies Over Time",
    subtitle = "Stacked by Response Variable",
    x = "Year",
    y = "Number of Studies",
    fill = "Response Variable"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    legend.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

studies_over_time
```


```{r}
# Histogram for the distribution of unique studies over time
imp_dataset |> 
  distinct(id_article, study_year_start) |> 
ggplot(aes(x = as.Date(study_year_start))) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Unique Studies Over Time", x = "Year", y = "Number of Studies") +
  theme_minimal()
```

Step 3: Distribution of Studies Across Moderators

```{r}
# Bar plot for crop_type
data_distribution_crop_type <- 
  imp_dataset |> 
  ggplot(aes(x = crop_type)) +
  geom_bar(fill = "cornflowerblue") +
  labs(title = "Distribution of Entries Across Tree-Crop Combinations", x = "Tree-Crop Combination", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_crop_type
```
```{r}
# Bar plot for season
data_distribution_season <- 
  imp_dataset |>
  ggplot(aes(x = season)) +
  geom_bar(fill = "darkorange") +
  labs(title = "Distribution of Entries Across Seasons", x = "Season", y = "Number of Entries") +
  theme_minimal()

data_distribution_season
```

```{r}
# Bar plot for alley width category
```

Step 4: Effect Size Distribution



Step 5: Explore Moderators and Levels

```{r}
# Faceted bar plots for different moderators
data_distribution_moderator_levels_seasen <- 
  imp_dataset |> 
  ggplot(aes(x = response_variable)) +
  geom_bar(aes(fill = season)) +
  facet_wrap(~ crop_type) +
  labs(title = "Number of Entries by Response Variable and Season", x = "Response Variable", y = "Number of Entries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

data_distribution_moderator_levels_seasen
```

Step 6: Distribution in Space (Using generated Categorized Location Column)

```{r}
# Bar plot for the number of studies per sub_region
data_distribution_studies_agroclimregion <- 
  imp_dataset |> 
  ggplot(aes(x = sub_region)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Number of Studies Per Continent", x = "Continent", y = "Number of Studies") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1))

data_distribution_studies_agroclimregion
```






















































#############
# STEP 2
##########################################################################################################################################
FORREST PLOT - OVERALL EFFECT SIZE DISTRIBUTION
##########################################################################################################################################

```{r}
# Cleaning and preparing data
prepare_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- prepare_forest_data(
  data = imp_dataset_imputed,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)
```

```{r}
# Aggregating data
aggregate_forest_data <- function(data, yi_col, ci_cols, group_col) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      overall_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(ci_cols[1]), na.rm = TRUE),
      upper_ci = mean(!!sym(ci_cols[2]), na.rm = TRUE),
      num_observations = n(),
      num_studies = n_distinct(id_article),
      size_category = case_when(
        num_studies <= 2 ~ "1-2",
        num_studies <= 4 ~ "3-4",
        num_studies > 4 ~ "5+"
      ),
      .groups = "drop"
    ) %>%
    mutate(
      size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
      response_label = paste0(!!sym(group_col), " (", num_studies, " studies)"),
      mean_ci_label = paste0(
        sprintf("%.2f", overall_effect), " [",
        sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
      ),
      response_rank = rank(-overall_effect)  # Rank for sorting
    )
}

aggregated_data <- aggregate_forest_data(
  data = forest_data_clean,
  yi_col = "yi",
  ci_cols = c("lower_ci", "upper_ci"),
  group_col = "response_variable"
)

# Computing densities
compute_densities <- function(data, yi_col, group_col, aggregated_data) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      aggregated_data %>% select(response_variable, response_label, response_rank),
      by = "response_variable"
    )
}

density_data <- compute_densities(
  data = forest_data_clean,
  yi_col = "yi",
  group_col = "response_variable",
  aggregated_data = aggregated_data
)

# Adding "Overall" effect size
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = sum(num_observations, na.rm = TRUE),
    num_studies = sum(num_studies, na.rm = TRUE),
    size_category = NA,
    mean_ci_label = paste0(
      sprintf("%.2f", mean(overall_effect, na.rm = TRUE)), " [",
      sprintf("%.2f", mean(lower_ci, na.rm = TRUE)), ", ",
      sprintf("%.2f", mean(upper_ci, na.rm = TRUE)), "]"
    )
  ) %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)",
    response_rank = Inf  # Place "Overall" at the bottom
  )

# Combine aggregated data and overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  arrange(response_rank) %>%
  mutate(
    response_label = factor(response_label, levels = unique(response_label)),
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables"),
    facet_label = factor(facet_label, levels = c("Response Variables", "Overall Effect Size"))
  )
```

```{r}
# Define a custom color palette for response_variable
custom_colors <- c(
  "Biodiversity" = "#FF9999",             # Light red
  "Greenhouse gas emission" = "#66C266",  # Green
  "Product quality" = "#FFC000",          # Yellow
  "Crop yield" = "#FF9933",               # Orange
  "Pest and Disease" = "#33CCCC",         # Teal
  "Soil quality" = "#9966CC",             # Purple
  "Water quality" = "#9999FF"             # Light blue
)
```


```{r}
# Forest plot with facets for "Overall" and "Response Variables"
# Arrange the `plot_data` by the number of studies
plot_data <- plot_data %>%
  arrange(desc(-num_studies)) %>%
  mutate(
    response_label = factor(
      response_label,
      levels = unique(response_label)  # Ensure levels follow the descending order of num_studies
    )
  )

# Update `density_data` to match the new order of response_label
density_data <- density_data %>%
  mutate(
    response_label = factor(
      response_label,
      levels = levels(plot_data$response_label)  # Use the same factor levels as in plot_data
    )
  )

####################################################################################################################

# Adjust text placement for CI values dynamically

# Updated Flexible Plotting Function
create_forest_plot <- function(
  density_data, 
  plot_data, 
  x_limits = NULL,    # Specify limits for the x-axis (e.g., c(-0.25, 0.5))
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Default breaks for the x-axis
  add_clipped_error_bars = FALSE,        # Include clipped error bars
  custom_colors = NULL                   # Custom fill colors
) {
  # Create a facet column to separate panels
  plot_data <- plot_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      )
    ) %>%
    arrange(desc(!num_studies)) %>%  # Sort by number of studies for the top panel
    mutate(
      response_label = factor(
        response_label,
        levels = unique(response_label)  # Ensure the order is preserved
      )
    )
  
  # Align density_data with plot_data
  density_data <- density_data %>%
    mutate(
      facet_label = ifelse(
        response_variable == "Overall",
        "Overall Effect Size",
        "Response Variables"
      ),
      response_label = factor(
        response_label,
        levels = levels(plot_data$response_label)  # Match the order in plot_data
      )
    )
  
  # Explicitly order the facet levels
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )

  density_data <- density_data %>%
    mutate(
      facet_label = factor(
        facet_label,
        levels = c("Response Variables", "Overall Effect Size")  # Ensure overall is last
      )
    )
  
  # Adjust text placement for CI values
  plot_data <- plot_data %>%
    mutate(
      ci_text_position = ifelse(
        is.null(x_limits),
        max(x_breaks) + 0.1,  # Place text slightly beyond x-axis
        x_limits[2] - 0.2     # Keep text within x-axis limits
      )
    )
  
  # Build the base ggplot object
  forest_plot <- ggplot() +
    # Density ridges for individual response variables
    geom_ridgeline(
      data = density_data %>% filter(response_variable != "Overall"),
      aes(
        x = density_x,
        y = response_label,
        height = density_y,
        fill = response_variable
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Points for individual response variables
    geom_point(
      data = plot_data %>% filter(response_variable != "Overall"),
      aes(
        x = overall_effect,
        y = response_label,
        size = size_category
      ),
      color = "black"
    ) +
    # Diamond for overall effect size
    geom_point(
      data = plot_data %>% filter(response_variable == "Overall"),
      aes(
        x = overall_effect,
        y = response_label
      ),
      shape = 18,
      size = 5,
      color = "black"
    ) +
    # Error bars for all response variables
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = response_label
      ),
      height = 0.2,
      color = "darkgray"
    )
  
  # Optionally add clipped error bars
  if (add_clipped_error_bars && !is.null(x_limits)) {
    forest_plot <- forest_plot +
      geom_errorbarh(
        data = plot_data,
        aes(
          xmin = pmax(lower_ci, x_limits[1]),  # Clip lower bounds to x-axis limits
          xmax = pmin(upper_ci, x_limits[2]),  # Clip upper bounds to x-axis limits
          y = response_label
        ),
        height = 0.1,
        color = "darkgray",
        size = 0.8
      )
  }
  
  # Add text annotations for confidence intervals
  forest_plot <- forest_plot +
    geom_text(
      data = plot_data,
      aes(
        x = ci_text_position,  # Use dynamically calculated position
        y = response_label,
        label = mean_ci_label
      ),
      size = 3.5,
      hjust = 0,
      color = "black"
    ) +
    # Add vertical red dotted line for effect size = 0
    geom_vline(
      xintercept = 0,
      color = "red",
      linetype = "dotted",
      size = 0.7
    ) +
    # Adjust x-axis scale dynamically
    scale_x_continuous(
      limits = x_limits,     # Use the specified limits if provided
      breaks = x_breaks      # Use the specified breaks
    ) +
    # Customize fill colors and size scale
    scale_fill_manual(values = custom_colors, guide = "none") +
    scale_size_manual(
      values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
      name = "Number of Studies"
    ) +
    # Add labels and theme adjustments
    labs(
      title = "Forest Plot with Adjusted Density and Study Details",
      x = "Effect Size",
      y = NULL
    ) +
    # Separate panels for response variables and overall effect size
    facet_grid(
      facet_label ~ .,       # Facet by `facet_label`
      scales = "free_y",
      space = "free",
      switch = "y"           # Labels on the left
    ) +
    theme_minimal() +
    theme(
      strip.text.y = element_blank(),  # Remove facet labels
      strip.background = element_blank(),
      panel.spacing = unit(0.5, "lines"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 12),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      legend.position = "bottom",
      legend.title = element_text(size = 12),
      legend.text = element_text(size = 10)
    )
  
  return(forest_plot)
}
```

```{r}
forest_plot_density_narrow <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-0.25, 0.5),  # Narrow x-axis
  x_breaks = seq(-0.25, 0.5, by = 0.25),
  add_clipped_error_bars = TRUE,
  custom_colors = custom_colors
)
forest_plot_density_narrow
```

```{r}
forest_plot_density_wide <- create_forest_plot(
  density_data = density_data,
  plot_data = plot_data,
  x_limits = c(-3, 3),  # Wide x-axis
  x_breaks = seq(-0.5, 1.0, by = 0.25),  # Adjust breaks to fit the wider range
  add_clipped_error_bars = FALSE,  # Optional: Set to FALSE for full error bars
  custom_colors = custom_colors
)
forest_plot_density_wide
```



```{r}
# Define the output folder path
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Define the file paths for saving
output_path_png <- file.path(output_dir, "forest_plot_density.png")
output_path_pdf <- file.path(output_dir, "forest_plot_density.pdf")

# Save the plot as PNG
ggsave(
  filename = output_path_png,
  plot = forest_plot_density,
  width = 16, height = 8, dpi = 600
)

# Save the plot as PDF
ggsave(
  filename = output_path_pdf,
  plot = forest_plot_density,
  width = 16, height = 8
)
```












##########################################################################################################################################
FORREST PLOT - FOR INDIVIDUAL RESPONSE VARIABLES AND MODERATORS EFFECT SIZE DISTRIBUTION
##########################################################################################################################################


```{r}
forest_data_clean 

forest_moderator <- forest_data_clean |> 
  filter(response_variable %in% c("Biodiversity", "Crop yield", "Soil quality"))

forest_moderator |> str()
```

```{r}
create_forest_plot_moderator <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  x_limits = NULL,                     # Specify x-axis limits
  x_breaks = seq(-0.5, 1.0, by = 0.25),# Specify x-axis breaks
  custom_colors = NULL,                # Custom color palette
  add_clipped_error_bars = FALSE       # Add clipped error bars
) {
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = if (add_clipped_error_bars & !is.null(x_limits)) pmax(lower_ci, x_limits[1]) else lower_ci,
        xmax = if (add_clipped_error_bars & !is.null(x_limits)) pmin(upper_ci, x_limits[2]) else upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = if (is.null(x_limits)) max(x_breaks) + 0.1 else x_limits[2] - 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable
    facet_wrap(~ facet_label, ncol = 1, scales = "free_y") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # X-axis adjustments
    scale_x_continuous(limits = x_limits, breaks = x_breaks) +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Create a forest plot with tree_type as moderator
forest_plot_tree_type <- create_forest_plot_moderator(
  data = forest_moderator,
  response_var = "response_variable",
  moderator_var = "tree_type",
  x_limits = c(-1.0, 2.0),  # Wide x-axis
  x_breaks = seq(-1.0, 2.0, by = 0.5),
  custom_colors = custom_colors,
  add_clipped_error_bars = TRUE
)

# Display the plot
forest_plot_tree_type
```


Free x-axis for response variables (ecosystem services)
```{r}
create_forest_plot_moderator_freex <- function(
  data, 
  response_var = "response_variable",  # Column for response variables
  moderator_var = "tree_type",         # Column for moderators
  yi_col = "yi",                       # Column for effect sizes
  lower_ci_col = "lower_ci",           # Column for lower CI
  upper_ci_col = "upper_ci",           # Column for upper CI
  custom_colors = NULL                 # Custom color palette
) {

  
  # Step 1: Preprocess Data for Plot
  plot_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      mean_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(lower_ci_col), na.rm = TRUE),
      upper_ci = mean(!!sym(upper_ci_col), na.rm = TRUE),
      num_studies = n_distinct(id_article),  # Count unique studies
      ci_label = sprintf(
        "%.2f [%.2f, %.2f]", 
        mean(!!sym(yi_col), na.rm = TRUE),
        mean(!!sym(lower_ci_col), na.rm = TRUE),
        mean(!!sym(upper_ci_col), na.rm = TRUE)
      ),
      .groups = "drop"
    ) %>%
    mutate(
      moderator_label = paste0(!!sym(moderator_var), " (n=", num_studies, ")"),
      facet_label = !!sym(response_var)  # Create a facet column
    )

  # Step 2: Prepare Density Data
  density_data <- data %>%
    group_by(!!sym(response_var), !!sym(moderator_var)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      plot_data %>% select(!!sym(response_var), !!sym(moderator_var), moderator_label, facet_label),
      by = c(response_var, moderator_var)
    )

  # Step 3: Ensure Proper Facet and Moderator Order
  plot_data <- plot_data %>%
    mutate(
      facet_label = factor(facet_label, levels = unique(plot_data[[response_var]])),
      moderator_label = factor(moderator_label, levels = unique(plot_data$moderator_label))
    )
  
  density_data <- density_data %>%
    mutate(
      facet_label = factor(facet_label, levels = levels(plot_data$facet_label)),
      moderator_label = factor(moderator_label, levels = levels(plot_data$moderator_label))
    )

  # Step 4: Build the Plot
  forest_plot <- ggplot() +
    # Density ridges for the distribution of effect sizes
    geom_ridgeline(
      data = density_data,
      aes(
        x = density_x,
        y = moderator_label,
        height = density_y,
        fill = !!sym(response_var)
      ),
      alpha = 0.3,
      scale = 0.05,
      color = NA
    ) +
    # Mean effect size (black dots)
    geom_point(
      data = plot_data,
      aes(
        x = mean_effect,
        y = moderator_label
      ),
      size = 3,
      color = "black"
    ) +
    # Error bars for confidence intervals
    geom_errorbarh(
      data = plot_data,
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = moderator_label
      ),
      height = 0.2,
      size = 1,
      color = "darkgray"
    ) +
    # Confidence interval text
    geom_text(
      data = plot_data,
      aes(
        x = max(lower_ci, na.rm = TRUE) + 0.1,
        y = moderator_label,
        label = ci_label
      ),
      size = 3,
      hjust = 0
    ) +
    # Faceting by response variable with free x-axis
    facet_wrap(~ facet_label, ncol = 1, scales = "free_x") +
    # Vertical reference line at zero
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # Colors
    scale_fill_manual(values = custom_colors) +
    # Labels and theme adjustments
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = paste(moderator_var, "(with Number of Studies)")
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      legend.position = "none",
      panel.spacing = unit(1, "lines")
    )
  
  return(forest_plot)
}
```

```{r}
# Generate the plot
forest_plot_free_x <- create_forest_plot_moderator_freex(data = forest_moderator, custom_colors = custom_colors)

forest_plot_free_x
```










































