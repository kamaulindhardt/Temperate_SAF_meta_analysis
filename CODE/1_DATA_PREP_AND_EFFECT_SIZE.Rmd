---
title: "1_DATA_PREP"
author: "M.K.K. Lindhardt"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    ###################################################################################################################
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science (data wrangling, visualization, etc.)
    readr,            # For fast reading and writing of CSV files
    dlookr,           # Diagnose, explore, and transform data efficiently
    skimr,            # Summary statistics for data frames, tibbles, and vectors
    janitor,          # Cleaning and renaming data columns for tidy data
    readxl,           # Reading Excel files
    vroom,            # High-performance reading of large datasets
    missForest,       # Random forest imputation for missing data
    mice,             # Multiple imputation for multivariate missing data
    missRanger,       # Chained random forest imputation for large datasets
    conflicted,       # Resolves conflicts in function names across packages
    future,           # Enables parallel processing for faster computation
    future.apply,     # Apply functions in parallel over lists or arrays
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization (part of tidyverse)
    patchwork,        # Combine and arrange ggplots in complex layouts
    RColorBrewer,     # Color palettes for visualizations
    gt,               # Generate stylish publication-ready tables
    corrplot,         # Correlation matrix visualization
    scales,           # Generate pseudo-log scale plots and other scaling options
    forcats,          # For working with and reordering factors
    ###################################################################################################################
    # Spatial Data Analysis
    tidygeocoder,     # Unified geocoding interface for forward and reverse geocoding
    raster,           # Handle raster data and analyze spatial layers
    sp,               # Provides spatial data classes and methods
    sf,               # Simple feature handling for vector data
    rnaturalearth,    # Access world map data for visualization
    rnaturalearthdata, # Supporting data for rnaturalearth
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # Conduct meta-analyses and calculate effect sizes
    clubSandwich,     # Cluster-robust variance estimators for linear regression
    philentropy,      # Compute Jensen-Shannon Divergence and other divergence measures
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # Streamline exploratory data analysis
    SmartEDA,         # Automated exploratory data analysis
    inspectdf,        # Inspect data frames for structure and quality
    naniar,           # Explore and visualize missing data patterns
    VIM,              # Visualize and impute missing values
    corrplot,         # Visualize correlations between variables
    ###################################################################################################################
    # Machine Learning and Modeling (Tidymodels Framework)
    tidymodels,       # Unified framework for machine learning and modeling
    vip,              # Visualize variable importance from models
    caret,            # Train machine learning models with cross-validation
    randomForest,     # Train random forest models and evaluate variable importance
    recipes,          # Preprocessing data for machine learning
    ranger,           # High-performance random forests
    yardstick,        # Metrics for model performance evaluation
    tune,             # Hyperparameter tuning for machine learning models
    rsample,          # Generate resamples and cross-validation folds
    workflows,        # Combine recipes and models into workflows
    parsnip,          # Define machine learning models
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy and robust file referencing
    styler            # Automatically format and style R code
)
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("extract", "raster")
conflict_prefer("col_factor", "scales")
conflict_prefer("geocode", "tidygeocoder")
conflict_prefer("chisq.test", "stats")
conflict_prefer("step", "stats")
conflict_prefer("margin", "ggplot2")
```
```{r}
# Set a global theme and color scale
# Define the global color palette
global_palette <- c(
  "#ffd700", 
  "#ffb14e", 
  "#fa8775", 
  "#ea5f94", 
  "#cd34b5", 
  "#9d02d7", 
  "#0000ff"
)

# Define global ggplot2 scales for color and fill
scale_fill_global <- scale_fill_viridis_d(option = "D")  # Discrete
scale_color_global <- scale_color_viridis_d(option = "D")  # Discrete

scale_fill_global <- scale_fill_viridis_c(option = "D")  # Continuous
scale_color_global <- scale_color_viridis_c(option = "D")  # Continuous
```


Loading the dataset (main metadata database)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({
  # Final database as 'meta-data'
  # Manually modifying the SD columns to "control_sd" and "silvo_sd" in Excel!
  database <- readxl::read_excel(
    here("DATA", "Meta_data_v4.xlsx"), 
    sheet = "Quantatitive data"
  )
  
  # Dummy data where silvo_mean has been multiplied with 1.2 to check for larger effect size estimates
  # database_dummy <- readxl::read_excel(
  #   here("DATA", "Meta_data_dummy_test_high_silvo_mean_se.xlsx"), 
  #   sheet = "Quantatitive data"
  # )
    
})


```

**Glimpse (taking a look at the data)**
```{r Glimpse the dataset, eval=FALSE}
database %>% dplyr::glimpse() 

# Rows: 1,075
# Columns: 35
# Rows: 1,126
# Columns: 35
```

```{r}
database %>% summary() 
```

```{r}
database |> skim()
```


#############
# STEP 1
##########################################################################################################################################
DATA PREPROCESSING
##########################################################################################################################################

####################################
GENERIC PREPROCESSING
####################################

Manually modifying the SD columns to "control_sd" and "silvo_sd" in Excel!

And in step 4, generate unique study identifier ('exp_id')

```{r}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Data Preprocessing
database_clean <- database |>
  # Step 1: Clean column names
  janitor::clean_names() |>
  
  # Step 2: Convert id_article and id_obs to integer
  mutate(
    id_article = as.integer(id_article),
    id_obs = as.integer(id_obs)
  ) |>
  
  # Step 3: Convert standard errors and other numeric columns
  mutate(
    silvo_mean = safe_as_numeric(silvo_mean),
    silvo_se = safe_as_numeric(silvo_se),
    silvo_sd = safe_as_numeric(silvo_sd),
    silvo_n = safe_as_numeric(silvo_n),
    control_mean = safe_as_numeric(control_mean),
    control_se = safe_as_numeric(control_se),
    control_sd = safe_as_numeric(control_sd),
    control_n = safe_as_numeric(control_n),
    tree_age = safe_as_numeric(tree_age),
    no_tree_per_m = as.character(no_tree_per_m)) |> 
  
  # Step 4: Create Identifiers (Experiment, Treatment, Common Control)
  # Group data by relevant columns for Treatment ID
  group_by(id_article, tree_type, crop_type, location, experiment_year) |>
  mutate(treat_id = cur_group_id()) |>
  ungroup() |>
  
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
  # the experiment_year (the year the experiment was conducted)
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  # Step 5: Ensure no infinite or NaN values are present in any columns
  mutate(across(everything(), ~ifelse(is.infinite(.) | is.nan(.), NA, .))
         ) |> 
  
  # Step 6: Convert "NA" strings to real NA values, excluding 'id_article' and 'id_obs'
  mutate(
    across(
      .cols = where(is.character) & !c("id_article", "id_obs"),
      .fns = ~ na_if(., "NA")
    )
  ) |>
  
  # Step 7: Convert year columns to date format
 # Convert to proper Date format using "YYYY-01-01"
 mutate(
    experiment_year = as.Date(paste0(experiment_year, "-01-01")),
    year_est_exp = as.Date(paste0(year_est_exp, "-01-01")),
    #study_year_start = as.Date(paste0(study_year_start, "-01-01")),
    #study_year_end = as.Date(paste0(study_year_end, "-01-01"))
  ) |> 
  # Step 8: Rename Latitude and Longitude to lat and lon
  rename(
    lat = latitude,
    lon = longitude
  ) |>
  
  # Step 9: Convert lat and lon to numeric coordinates
  mutate(
    lat = str_replace_all(lat, "[°NS]", "") |> safe_as_numeric(),
    lon = str_replace_all(lon, "[°EW]", "") |> safe_as_numeric(),
    lat = if_else(str_detect(lat, "S$"), -lat, lat),
    lon = if_else(str_detect(lon, "W$"), -lon, lon)
  ) |>
  
  # Step 10: Create a Coherent 'site_x' Column
  mutate(
    # If `lat` and `lon` are present, use them; otherwise, use the `location` name
    site_x = case_when(
      !is.na(lat) & !is.na(lon) ~ paste(lat, lon, sep = ", "),
      !is.na(location) ~ location,
      TRUE ~ NA_character_
    )
  ) 
```

```{r, eval=FALSE}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Data Preprocessing
database_clean_dummy <- database_dummy |>
  # Step 1: Clean column names
  janitor::clean_names() |>
  
  # Step 2: Convert id_article and id_obs to integer
  mutate(
    id_article = as.integer(id_article),
    id_obs = as.integer(id_obs)
  ) |>
  
  # Step 3: Convert standard errors and other numeric columns
  mutate(
    silvo_mean = safe_as_numeric(silvo_mean),
    silvo_se = safe_as_numeric(silvo_se),
    silvo_sd = safe_as_numeric(silvo_sd),
    silvo_n = safe_as_numeric(silvo_n),
    control_mean = safe_as_numeric(control_mean),
    control_se = safe_as_numeric(control_se),
    control_sd = safe_as_numeric(control_sd),
    control_n = safe_as_numeric(control_n),
    tree_age = safe_as_numeric(tree_age),
    no_tree_per_m = as.character(no_tree_per_m)) |> 
  
  # Step 4: Create Identifiers (Experiment, Treatment, Common Control)
  # Group data by relevant columns for Treatment ID
  group_by(id_article, tree_type, crop_type, location, experiment_year) |>
  mutate(treat_id = cur_group_id()) |>
  ungroup() |>
  
  # Group data by relevant columns for Experiment ID
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  # Step 5: Ensure no infinite or NaN values are present in any columns
  mutate(across(everything(), ~ifelse(is.infinite(.) | is.nan(.), NA, .))
         ) |> 
  
  # Step 6: Convert "NA" strings to real NA values, excluding 'id_article' and 'id_obs'
  mutate(
    across(
      .cols = where(is.character) & !c("id_article", "id_obs"),
      .fns = ~ na_if(., "NA")
    )
  ) |>
  
 # Step 7: Convert year columns to date format
 # Convert to proper Date format using "YYYY-01-01"
 mutate(
    experiment_year = as.Date(paste0(experiment_year, "-01-01")),
    year_est_exp = as.Date(paste0(year_est_exp, "-01-01")),
    study_year_start = as.Date(paste0(study_year_start, "-01-01")),
    study_year_end = as.Date(paste0(study_year_end, "-01-01"))
  ) |> 
  # Step 8: Rename Latitude and Longitude to lat and lon
  rename(
    lat = latitude,
    lon = longitude
  ) |>
  
  # Step 9: Convert lat and lon to numeric coordinates
  mutate(
    lat = str_replace_all(lat, "[°NS]", "") |> safe_as_numeric(),
    lon = str_replace_all(lon, "[°EW]", "") |> safe_as_numeric(),
    lat = if_else(str_detect(lat, "S$"), -lat, lat),
    lon = if_else(str_detect(lon, "W$"), -lon, lon)
  ) |>
  
  # Step 10: Create a Coherent 'site_x' Column
  mutate(
    # If `lat` and `lon` are present, use them; otherwise, use the `location` name
    site_x = case_when(
      !is.na(lat) & !is.na(lon) ~ paste(lat, lon, sep = ", "),
      !is.na(location) ~ location,
      TRUE ~ NA_character_
    )
  ) 
```

```{r}
database_clean
```

####################################
GEOSPATIAL PREPROCESSING
####################################

Manually changing 'France (south west)' to 'France' and 'South East England(Cambridgeshire)' to 'Cambridgeshire, England' and 
'Bramham in northern England' to 'Bramham, England'
```{r}
# Step 1: Extract Coordinates from `site_x` if available
database_clean <- database_clean |>
  mutate(
    # Extract latitude and longitude from `site_x` if it contains coordinates
    extracted_lat = str_extract(site_x, "[-]?\\d+\\.\\d+(?=, )") |> as.numeric(),
    extracted_lon = str_extract(site_x, "(?<=, )[-]?\\d+\\.\\d+") |> as.numeric()
  )

# Step 2: Identify rows that need geocoding (i.e., where coordinates are missing)
locations_to_geocode <- database_clean |>
  filter(is.na(extracted_lat) | is.na(extracted_lon)) |>
  distinct(location) |>
  filter(!is.na(location))

# Step 3: Geocode Location Names
geocoded_locations <- locations_to_geocode |>
  geocode(address = location, method = "osm", lat = "geo_lat", long = "geo_lon")

# Step 4: Merge Geocoded Coordinates Back to the Dataset
database_clean <- database_clean |>
  left_join(geocoded_locations, by = "location") |>
  mutate(
    # Use extracted coordinates if available, otherwise use geocoded coordinates
    final_lat = coalesce(extracted_lat, geo_lat),
    final_lon = coalesce(extracted_lon, geo_lon),
    # Create the `exp_site_loc` column with final coordinates
    exp_site_loc = if_else(!is.na(final_lat) & !is.na(final_lon),
                           paste(final_lat, final_lon, sep = ", "),
                           NA_character_)
  ) |>
  select(-extracted_lat, -extracted_lon, -geo_lat, -geo_lon)|> 
  
  
  
  # Step 5: Relocate columns to the desired order
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Passing 23 addresses to the Nominatim single address geocoder
# Query completed in: 23.4 seconds

# Last run (03/12-2024)
# Passing 4 addresses to the Nominatim single address geocoder
# Query completed in: 4.3 seconds

# Last run (01/01-2025)
# Passing 3 addresses to the Nominatim single address geocoder
# Query completed in: 3 seconds
```

Checking if any missing values in coordinates

```{r}
# Filter rows where either final_lat or final_lon is missing
missing_coordinates <- database_clean |>
  filter(is.na(final_lat) | is.na(final_lon))

# View the rows with missing coordinates
missing_coordinates

# No missing coordinates
# 0 rows | 1-10 of 39 columns
```


Add geographical sub-regions to the dataset

```{r}
# Load Köppen-Geiger Climate Data (as a raster file)
kg_climate <- raster(here("DATA", "koppen_geiger_tif", "1991_2020", "koppen_geiger_0p1.tif"))


# Preserve final_lat and final_lon before conversion
database_clean <- database_clean |> 
  mutate(
    preserved_lat = final_lat,
    preserved_lon = final_lon
  )

# Convert your dataset to an sf object using preserved lat/lon columns
database_clean_sf <- database_clean |>
  drop_na(preserved_lat, preserved_lon) |>
  st_as_sf(coords = c("preserved_lon", "preserved_lat"), crs = 4326)

# Extract climate zone for each observation using spatial overlay
# Beck, H.E., T.R. McVicar, N. Vergopolan, A. Berg, N.J. Lutsko, A. Dufour, Z. Zeng, X. Jiang, A.I.J.M. van Dijk, D.G. MirallesHigh-resolution (1 km) Köppen-Geiger maps for 1901–2099 based on constrained CMIP6 projectionsScientific Data 10, 724, doi:10.1038/s41597-023–02549‑6 (2023)
# The variable 'climate_zone' represents the climate classification code assigned to each data point based on its geographical coordinates (latitude and longitude) from the Köppen-Geiger map. The climate_zone information was extracted using a spatial overlay.
database_clean_sf <- database_clean_sf |>
  mutate(
    climate_zone = extract(kg_climate, st_coordinates(database_clean_sf))
  )

# Classify sub-regions based on the climate zone
# Refine classifications for temperate climates and assign broader regions for others
database_clean_sf <- database_clean_sf %>%
  mutate(
    # Assign specific Köppen-Geiger classifications to climate zones
    climate_zone = case_when(
      climate_zone == 1 ~ "Tropical, rainforest",
      climate_zone == 2 ~ "Tropical, monsoon",
      climate_zone == 3 ~ "Tropical, savannah",
      climate_zone == 4 ~ "Arid, desert, hot",
      climate_zone == 5 ~ "Arid, desert, cold",
      climate_zone == 6 ~ "Arid, steppe, hot",
      climate_zone == 7 ~ "Arid, steppe, cold",
      climate_zone == 8 ~ "Temperate, dry summer, hot summer",
      climate_zone == 9 ~ "Temperate, dry summer, warm summer",
      climate_zone == 10 ~ "Temperate, dry summer, cold summer",
      climate_zone == 11 ~ "Temperate, dry winter, hot summer",
      climate_zone == 12 ~ "Temperate, dry winter, warm summer",
      climate_zone == 13 ~ "Temperate, dry winter, cold summer",
      climate_zone == 14 ~ "Temperate, no dry season, hot summer",
      climate_zone == 15 ~ "Temperate, no dry season, warm summer",
      climate_zone == 16 ~ "Temperate, no dry season, cold summer",
      climate_zone == 17 ~ "Cold, dry summer, hot summer",
      climate_zone == 18 ~ "Cold, dry summer, warm summer",
      climate_zone == 19 ~ "Cold dry summer, cold summer",
      climate_zone == 20 ~ "Cold dry summer, very cold winter",
      climate_zone == 21 ~ "Cold, dry winter, hot summer",
      climate_zone == 22 ~ "Cold, dry winter, warm summer",
      climate_zone == 23 ~ "Cold, dry winter, cold summer",
      climate_zone == 24 ~ "Cold, dry winter, very cold winter",
      climate_zone == 25 ~ "Cold, no dry season, hot summer",
      climate_zone == 26 ~ "Cold, no dry season, warm summer",
      climate_zone == 27 ~ "Cold, no dry season, cold summer",
      climate_zone == 28 ~ "Cold, no dry season, very cold winter",
      climate_zone == 29 ~ "Polar, tundra",
      climate_zone == 30 ~ "Polar, frost",
      TRUE ~ "Unclassified"
    ),
    
    # Assign refined geographical regions
    bioclim_sub_regions = case_when(
      # Tropical climates
      climate_zone %in% c(
        "Tropical, rainforest", "Tropical, monsoon", "Tropical, savannah"
      ) ~ "Tropical Climates",
      
      # Arid climates
      climate_zone %in% c(
        "Arid, desert, hot", "Arid, desert, cold", "Arid, steppe, hot", "Arid, steppe, cold"
      ) ~ "Arid Climates",
      
      # Refined temperate climates
      climate_zone %in% c(
        "Temperate, dry summer, hot summer", "Temperate, dry summer, warm summer", 
        "Temperate, dry winter, hot summer", "Temperate, dry winter, warm summer"
      ) ~ "Dry and Warm Temperate",
      
      climate_zone %in% c(
        "Temperate, no dry season, hot summer", "Temperate, no dry season, warm summer"
      ) ~ "Wet and Warm Temperate",
      
      climate_zone %in% c(
        "Temperate, dry summer, cold summer", "Temperate, dry winter, cold summer"
      ) ~ "Dry and Cold Temperate",
      
      climate_zone %in% c(
        "Temperate, no dry season, cold summer"
      ) ~ "Wet and Cold Temperate",
      
      # Cold climates
      climate_zone %in% c(
        "Cold, dry summer, hot summer", "Cold, dry summer, warm summer", 
        "Cold dry summer, cold summer", "Cold dry summer, very cold winter", 
        "Cold, dry winter, hot summer", "Cold, dry winter, warm summer", 
        "Cold, dry winter, cold summer", "Cold, dry winter, very cold winter",
        "Cold, no dry season, hot summer", "Cold, no dry season, warm summer", 
        "Cold, no dry season, cold summer", "Cold, no dry season, very cold winter"
      ) ~ "Cold Climates",
      
      # Polar climates
      climate_zone %in% c(
        "Polar, tundra", "Polar, frost"
      ) ~ "Polar Climates",
      
      TRUE ~ "Unclassified"
    )
  )
```
Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type
10	366	Leeds	England	53.883333° N	1.5491° W	NA	1992	7	Monoculture	Biomass	Cereal

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
29	873	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	874	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	875	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	876	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	877	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	878	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	879	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	880	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	881	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	882	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	883	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	884	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	885	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	886	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	887	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	888	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	889	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	890	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	891	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	892	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	893	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	894	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	895	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	896	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	897	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	898	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	899	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	900	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	901	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	902	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	903	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	904	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	905	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	906	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	907	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	908	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
36	1074	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011
36	1075	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011



Checking for unclassified in climate_zone and bioclim_sub_regions

```{r}
# Check classification coverage
database_clean_sf %>%
  filter(climate_zone == "Unclassified" | bioclim_sub_regions == "Unclassified") %>%
  head()
```




```{r}
# Rename back to 'database_clean'

# Relocate columns to the desired order
database_clean <- database_clean_sf |>
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, climate_zone, bioclim_sub_regions, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Preview the resulting data
database_clean |> 
  glimpse()

# Rows: 1,126
# Columns: 44
```


```{r}
# Create the bar chart
database_clean |> 
  ggplot(aes(x = bioclim_sub_regions)) +
  geom_bar(fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(
    title = "Distribution of Observations by BioClim Subregions",
    x = "BioClim Subregions",
    y = "Count of Observations"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
```


################################################################################
ASSESSING THE GENERATED RANDOM-FACTOR VARIABLES exp_id and treat_id 
################################################################################

Missingness Assessment for exp_id Visualization

```{r}
database_clean |> as.data.frame() |>
  str()
```


```{r}
# Visualize exp_id distribution across components
database_clean %>%
  as.data.frame() %>%
  select(-geometry) %>%
  mutate(
    # Convert all columns to character for consistency
    id_article = as.character(id_article),
    experiment_year = as.character(experiment_year)
  ) %>%
  group_by(id_article, location, experiment_year) %>%
  summarise(exp_id_count = n_distinct(exp_id), .groups = "drop") %>%
  pivot_longer(
    cols = c(id_article, location, experiment_year),
    names_to = "component",
    values_to = "value"
  ) %>%
  ggplot(aes(x = component, y = exp_id_count, fill = component)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Distribution of exp_id Across Components",
    x = "Component",
    y = "Count of exp_id"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
```{r}
# Heatmap for year-location relationship
# Function to plot heatmap for year and a chosen variable
plot_heatmap <- function(data, y_var) {
  data %>%
    as.data.frame() %>%
    select(-geometry) %>%
    group_by(experiment_year, !!sym(y_var)) %>%
    summarise(n_exp_id = n_distinct(exp_id), .groups = "drop") %>%
    ggplot(aes(x = experiment_year, y = !!sym(y_var), fill = n_exp_id)) +
    geom_tile() +
    labs(
      title = paste("Heatmap of exp_id by Experiment Year and", y_var),
      x = "Experiment Year",
      y = y_var,
      fill = "Count of exp_id"
    ) +
    scale_fill_viridis_c() +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Example usage: Plot for location
plot_heatmap(database_clean, "location")

# Example usage: Plot for site
plot_heatmap(database_clean, "site")

# Example usage: Plot for bioclim_sub_region
plot_heatmap(database_clean, "bioclim_sub_regions")
```

```{r}
# Function to plot heatmap for year and chosen variable (moderators or response variables)
plot_heatmap_moderator_response <- function(data, var_type) {
  data %>%
    as.data.frame() %>%
    select(-geometry) %>%
    group_by(experiment_year, !!sym(var_type)) %>%
    summarise(n_exp_id = n_distinct(exp_id), .groups = "drop") %>%
    ggplot(aes(x = experiment_year, y = !!sym(var_type), fill = n_exp_id)) +
    geom_tile() +
    labs(
      title = paste("Heatmap of exp_id by Experiment Year and", var_type),
      x = "Experiment Year",
      y = var_type,
      fill = "Count of exp_id"
    ) +
    scale_fill_viridis_c() +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Example usage: Plot for moderators
plot_heatmap_moderator_response(database_clean, "tree_type")

# Example usage: Plot for response variables
plot_heatmap_moderator_response(database_clean, "response_variable")

```


```{r}
# Step 2: Explore Aggregation Level
# Count unique `exp_id` values at each location level
aggregation_summary <- database_clean %>%
  as.data.frame() |> 
  select(-geometry) |> 
  group_by(location) %>%
  summarise(
    n_exp_id = n_distinct(exp_id),
    n_obs = n(),
    .groups = "drop"
  )

# View summary of aggregation
aggregation_summary

# Step 3: Visualize Aggregation Level
aggregation_summary |> 
  ggplot(aes(x = reorder(location, n_exp_id), y = n_exp_id)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Distribution of exp_id Across Locations",
    x = "Location",
    y = "Number of exp_id"
  ) +
  theme_minimal()
```
```{r}
# Step 4: Map Visualization (Optional)
# Simplify the dataset
geo_data <- database_clean %>%
  select(lat, lon, exp_id) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

# Base map
world_map <- map_data("world")

# Plot the map with points
ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.3
  ) +
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = as.factor(exp_id)),
    size = 3, alpha = 0.7
  ) +
  scale_color_viridis_d() +
  labs(
    title = "Geographical Distribution of exp_id",
    x = "Longitude",
    y = "Latitude",
    color = "Experiment ID"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "right"
  )

```

```{r}
# Summarize missingness across moderators and response variables
missingness_summary <- database_clean %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(exp_id) %>%
  summarise(
    # Count missing values for moderators
    missing_moderators = sum(
      is.na(tree_type) | is.na(crop_type) | is.na(age_system) | 
      is.na(tree_age) | is.na(season) | is.na(soil_texture) | 
      is.na(no_tree_per_m) | is.na(tree_height) | is.na(alley_width)
    ),
    # Count missing values for response variables
    missing_response = sum(
      is.na(silvo_mean) | is.na(control_mean)
    ),
    total_missing = missing_moderators + missing_response,
    n_obs = n(),
    .groups = "drop"
  )
```

Visualize Missingness (Facet Plot for Moderators and Responses)
```{r}
# Reshape the missingness summary for visualization
missingness_long <- missingness_summary %>%
  pivot_longer(
    cols = c(missing_moderators, missing_response),
    names_to = "missingness_type",
    values_to = "missing_count"
  )

# Summarize missingness data for better grouping and understanding
missingness_long_summary <- missingness_long %>%
  group_by(missingness_type, n_obs_group = cut(n_obs, breaks = seq(0, max(n_obs, na.rm = TRUE), by = 10))) %>%
  summarise(mean_missing = mean(missing_count, na.rm = TRUE), .groups = "drop")

# Separate data for moderators and response variables
moderators_data <- missingness_summary %>%
  select(exp_id, n_obs, missing_moderators) %>%
  mutate(missingness_type = "Moderators")

response_variables_data <- missingness_summary %>%
  select(exp_id, n_obs, missing_response) %>%
  mutate(missingness_type = "Response Variables")

# Plot Moderators Missingness
moderators_data %>%
  ggplot(aes(x = as.factor(n_obs), y = missing_moderators, fill = missingness_type)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Missingness Overview for Moderators",
    x = "Number of Observations per exp_id (Grouped)",
    y = "Average Missing Values Count"
  ) +
  scale_fill_manual(values = c("blue")) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```


Bar Chart: Missingness by Moderators (Exp ID Count)
```{r}
# Count missing values for each moderator
# Ensure all columns have the same data type before pivoting
missingness_moderators <- database_clean %>%
  as.data.frame() %>%
  select(
    exp_id, tree_type, crop_type, age_system, tree_age, season, 
    soil_texture, no_tree_per_m, tree_height, alley_width
  ) %>%
  mutate(across(-exp_id, as.character)) %>%  # Convert all non-exp_id columns to character
  pivot_longer(
    cols = -exp_id,
    names_to = "moderator",
    values_to = "value"
  ) %>%
  group_by(moderator) %>%
  summarise(
    exp_id_with_missing = sum(is.na(value)),  # Count the number of exp_id with missing values
    .groups = "drop"
  )

# Bar chart of missingness across moderators
missingness_moderators |> 
  ggplot(aes(x = moderator, y = exp_id_with_missing, fill = moderator)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Missingness in Moderators",
    subtitle = "Number of exp_id with Missing Values for Each Moderator",
    x = "Moderator",
    y = "Count of exp_id with Missing Values"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```












#############
# STEP 2
##########################################################################################################################################
CALCULATING META-ANALYSIS QUANTITATIVE DATA (Std Dev., Std. Err. etc.)
##########################################################################################################################################

Code for Calculating Meta-Analysis Quantitative Data

```{r}
# Dunctions for converting different measures of variability to SD - (however, in our data we only have SE)

# SE to SD
# SEtoSD <- function(SE, n) {
#   SE * sqrt(n)
# }

# # LSD to SD
# LSDtoSD <- function(LSD, n) {
#   LSD / (qt(0.975, n - 1)) * (sqrt(n) / sqrt(2))
# }
# 
# # CV to SD
# CVtoSD <- function(CV, mean) {
#   (CV / 100) * mean
# }
# 
# # MSE to SD
# MSEtoSD <- function(MSE) {
#   sqrt(MSE)
# }
```

```{r}
# Calculate standard deviations from standard errors and sample sizes
database_clean_sd <- database_clean |>
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  )
```


Checking for missing data in control_sd and silvo_sd

```{r}
# Calculate percentage of missing SD values for control and silvo groups
missing_control_sd <- sum(is.na(database_clean_sd$control_sd)) / nrow(database_clean_sd) * 100
missing_silvo_sd <- sum(is.na(database_clean_sd$silvo_sd)) / nrow(database_clean_sd) * 100

message("Percentage of missing SD values for control group: ", round(missing_control_sd, 2), "%")
message("Percentage of missing SD values for silvo group: ", round(missing_silvo_sd, 2), "%")

# Percentage of missing SD values for control group: 17.95%
# Percentage of missing SD values for silvo group: 17.95%

missing_sd <- database_clean_sd |>
  filter(is.na(control_sd) | is.na(silvo_sd)) |> 
  relocate(id_article, id_obs, response_variable, location, 
           # Quantitative mata-analysis effect size info
           silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n)

missing_sd
```

Dataset that is used as non-imputed

```{r}
# This is the dataset that is used for analysis before imputation
database_clean_sd |> glimpse()
```



#############
# STEP 3
##########################################################################################################################################
ASSESS MISSINGNESS PATTERNS OF DATA BEFORE IMPUTATION
##########################################################################################################################################

```{r}
# Check missingness summary for `control_sd` and `silvo_sd`
missingness_summary <- database_clean_sd %>%
  as.data.frame() %>%
  summarise(
    total_rows = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    percent_missing_control_sd = mean(is.na(control_sd)) * 100,
    percent_missing_silvo_sd = mean(is.na(silvo_sd)) * 100
  )



# Add missingness indicators to the dataset
database_clean_sd_missingness <- database_clean_sd %>%
  as.data.frame() %>%
  mutate(
    missing_control_sd = ifelse(is.na(control_sd), 1, 0),
    missing_silvo_sd = ifelse(is.na(silvo_sd), 1, 0)
  )

missingness_summary
```
```{r}
# Visualize missingness across response variables and moderators
missingness_plot <- database_clean_sd_missingness %>%
  select(response_variable, missing_control_sd, missing_silvo_sd) %>%
  group_by(response_variable) %>%
  summarise(
    missing_control_sd = mean(missing_control_sd) * 100,
    missing_silvo_sd = mean(missing_silvo_sd) * 100
  ) %>%
  pivot_longer(cols = c(missing_control_sd, missing_silvo_sd),
               names_to = "Variable",
               values_to = "Percent_Missing") %>%
  ggplot(aes(x = response_variable, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Percentage of Standard Deviation Across Response Variables",
    x = "Response Variable",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

missingness_plot
```
```{r}
# Filter the dataset for rows with missing sd (either control_sd or silvo_sd)
missing_sd_data <- database_clean_sd_missingness %>%
  filter(is.na(control_sd) | is.na(silvo_sd))

# Summarize the counts of response variables for the filtered data
missing_sd_summary <- missing_sd_data %>%
  group_by(response_variable) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Plot the distribution of response variables for missing sd data
ggplot(missing_sd_summary, aes(x = reorder(response_variable, -count), y = count, fill = response_variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Distribution of Response Variables with Missing Standard Deviation",
    x = "Response Variable",
    y = "Count of Missing Entries"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# Filter the dataset for rows with missing sd (either control_sd or silvo_sd)
missing_sd_data_rel <- database_clean_sd_missingness %>%
  mutate(missing_sd = ifelse(is.na(control_sd) | is.na(silvo_sd), 1, 0))

# database_clean_sd_missingness |> filter(response_variable == "Soil quality") |> nrow()

# Calculate the percentage of missing sd values relative to total observations for each response variable
missing_sd_summary_rel <- missing_sd_data_rel %>%
  group_by(response_variable) %>%
  summarise(
    total_observations = n(),
    missing_sd_count = sum(missing_sd),
    percent_missing = (missing_sd_count / total_observations) * 100
  ) %>%
  arrange(desc(percent_missing))

missing_sd_summary_rel
```
```{r}
# Plot the relative missingness
missing_sd_summary_rel |> 
  ggplot(aes(x = reorder(response_variable, -percent_missing), y = percent_missing, fill = response_variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Relative Missingness of Standard Deviation by Response Variable",
    x = "Response Variable",
    y = "Percent Missing",
    fill = "Response Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Assess missingness patterns across moderators
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

missingness_by_moderators <- database_clean_sd_missingness %>%
  select(all_of(moderators), missing_control_sd, missing_silvo_sd) %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100))

missingness_by_moderators

# Visualize missingness with a heatmap
heatmap_missingness <- database_clean_sd_missingness %>%
  select(control_sd, silvo_sd, response_variable, all_of(moderators)) %>%
  naniar::gg_miss_upset()

heatmap_missingness
```

##########################################################################################################################################
Assess missingness patterns for _sd variables by location and study ID (id_article)
##########################################################################################################################################

```{r}
# Calculate missingness by location
missing_by_location <- database_clean_sd_missingness %>%
  group_by(location) %>%
  summarise(
    total = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    perc_missing_control_sd = 100 * mean(is.na(control_sd)),
    perc_missing_silvo_sd = 100 * mean(is.na(silvo_sd))
  ) %>%
  arrange(desc(perc_missing_control_sd), desc(perc_missing_silvo_sd))

# Print missingness summary by location
cat("\nMissingness by Location:\n")
print(missing_by_location)
```

```{r}
# Missingness by location
ggplot(missing_by_location, aes(x = reorder(location, -perc_missing_control_sd), y = perc_missing_control_sd)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  geom_bar(aes(y = perc_missing_silvo_sd), stat = "identity", fill = "red", alpha = 0.7) +
  labs(
    title = "Missingness Percentage by Location",
    x = "Location",
    y = "Percentage Missing",
    fill = "Missingness Type"
  ) +
  theme_minimal() +
  coord_flip()
```

```{r}
# Calculate missingness by study ID (id_article)
missing_by_study <- database_clean_sd_missingness %>%
  group_by(id_article) %>%
  summarise(
    total = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    perc_missing_control_sd = 100 * mean(is.na(control_sd)),
    perc_missing_silvo_sd = 100 * mean(is.na(silvo_sd))
  ) %>%
  arrange(desc(perc_missing_control_sd), desc(perc_missing_silvo_sd))

# Print missingness summary by study ID
cat("\nMissingness by Study ID:\n")
print(missing_by_study)
```

```{r}
# Missingness by study ID
ggplot(missing_by_study, aes(x = reorder(as.factor(id_article), -perc_missing_control_sd), y = perc_missing_control_sd)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  geom_bar(aes(y = perc_missing_silvo_sd), stat = "identity", fill = "red", alpha = 0.7) +
  labs(
    title = "Missingness Percentage by Study ID",
    x = "Study ID",
    y = "Percentage Missing",
    fill = "Missingness Type"
  ) +
  theme_minimal() +
  coord_flip()
```


##########################################################################################################################################
Little's MCAR test for missingness
##########################################################################################################################################

Implications of missing.patterns:
A high number of missing patterns indicates complex missingness in your dataset, which might suggest that the data is not Missing Completely at Random (MCAR). Instead, it might be Missing at Random (MAR) or Not Missing at Random (NMAR).

```{r}
####################################################################################################################
# Prepare the data for missingness assessment
database_clean_sd_df <- database_clean_sd |> as.data.frame() |> select(-geometry) 
####################################################################################################################



# Select the variables for the test
test_data <- database_clean_sd_df %>%
  select(control_sd, silvo_sd, everything())  # Include control_sd, silvo_sd, and all variables

# Convert non-numeric columns to numeric using one-hot encoding or factor levels
test_data_numeric <- test_data %>%
  mutate(across(where(is.character), ~ as.numeric(as.factor(.)))) %>%  # Convert characters to numeric
  mutate(across(where(is.factor), as.numeric)) %>%                     # Convert factors to numeric
  select(where(~ sum(!is.na(.)) > 0))                                  # Remove columns with all missing values

# Check for problematic values
problematic_values <- test_data_numeric %>%
  summarise(across(everything(), ~ sum(is.infinite(.)) + sum(is.nan(.)) + sum(is.na(.))))

problematic_values |> glimpse()

# Exclude columns with more than 50% missing values
test_data_cleaned <- test_data_numeric %>%
  # Keep control_sd and silvo_sd
  select(control_sd, silvo_sd, response_variable, experiment_year, year_est_exp, exp_id,
         # Moderators info
         tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width) %>%  
  # Remove columns with >50% missing
  select(where(~ sum(is.na(.)) < nrow(test_data_numeric) * 0.5)) |> 
  # Convert date-time columns to numeric (e.g., extract the year)
  mutate(
    experiment_year = as.numeric(format(experiment_year, "%Y")),
    year_est_exp = as.numeric(format(year_est_exp, "%Y"))
  )

# Confirm all columns are numeric
all(sapply(test_data_cleaned, is.numeric)) 

# Check which columns are not numeric
non_numeric_cols <- sapply(test_data_cleaned, function(col) !is.numeric(col))
names(non_numeric_cols[non_numeric_cols])

# Inspect missing values after cleaning
colSums(is.na(test_data_cleaned))

# Perform Little's MCAR test on cleaned data
mcar_test <- naniar::mcar_test(test_data_cleaned)
mcar_test
```

```{r}
# Visualize missingness pattern
md.pattern(database_clean_sd_df)

# Heatmap of missing data

aggr_plot <- VIM::aggr(database_clean_sd_df, col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE, 
                  labels = names(database_clean_sd_df), cex.axis = .7, gap = 3, ylab = c("Missingness", "Pattern"))
aggr_plot
```

```{r}
# Add missing indicators for control_sd and silvo_sd
test_data_cleaned_sd <- test_data_cleaned %>%
  mutate(
    missing_control_sd = is.na(control_sd),
    missing_silvo_sd = is.na(silvo_sd)
  )

# Check relationship between missingness and other variables
ggplot(test_data_cleaned_sd, aes(x = tree_age, fill = missing_control_sd)) +
  geom_histogram(position = "dodge") +
  labs(title = "Relationship Between Tree Age and Missingness in control_sd")
```

Fit logistic regression models to see if missingness depends on observed variables.

```{r}
# Test if missingness depends on observed variables using logistic regression

# Add missingness indicators for control_sd and silvo_sd
test_data_cleaned_sd_test <- test_data_cleaned %>%
  mutate(
    missing_control_sd = as.numeric(is.na(control_sd)),
    missing_silvo_sd = as.numeric(is.na(silvo_sd))
  )
# Check levels of categorical variables
levels(test_data_cleaned_sd_test$tree_type)
levels(test_data_cleaned_sd_test$crop_type)
levels(test_data_cleaned_sd_test$age_system)
levels(test_data_cleaned_sd_test$season)
levels(test_data_cleaned_sd_test$soil_texture)
levels(test_data_cleaned_sd_test$no_tree_per_m)
levels(test_data_cleaned_sd_test$alley_width)


# Test 1: Logistic regression using specified moderators
missing_control_model_1 <- glm(
  missing_control_sd ~ tree_type + crop_type + age_system + tree_age + season + soil_texture + no_tree_per_m + tree_height + alley_width,
  data = test_data_cleaned_sd_test, family = binomial
)

missing_silvo_model_1 <- glm(
  missing_silvo_sd ~ tree_type + crop_type + age_system + tree_age + season + soil_texture + no_tree_per_m + tree_height + alley_width,
  data = test_data_cleaned_sd_test, family = binomial
)

# Summarize results for Test 1
cat("\nTest 1: Logistic regression results using specified moderators\n")
cat("\nControl SD Missingness:\n")
summary(missing_control_model_1)

cat("\nSilvo SD Missingness:\n")
summary(missing_silvo_model_1)

# Test 2: Logistic regression using response variables
missing_control_model_2 <- glm(
  missing_control_sd ~ response_variable,
  data = test_data_cleaned_sd_test, family = binomial
)

missing_silvo_model_2 <- glm(
  missing_silvo_sd ~ response_variable,
  data = test_data_cleaned_sd_test, family = binomial
)

# Summarize results for Test 2
cat("\nTest 2: Logistic regression results using response variables\n")
cat("\nControl SD Missingness:\n")
summary(missing_control_model_2)

cat("\nSilvo SD Missingness:\n")
summary(missing_silvo_model_2)

```



The updated analysis confirms that missingness in both `control_sd` and `silvo_sd` is systematically linked to specific observed variables, reinforcing that the missing data are not random. Tree type, crop type, season, soil texture, tree age, and tree height significantly influence missingness patterns. For example, "timber" tree types exhibit notably lower missingness rates, while "fruit/nut & other" tree types are associated with higher levels of missing data, likely due to variations in experimental settings or measurement challenges. Similarly, cereal crops consistently display lower missingness, likely reflecting standardized data collection protocols, whereas tuber/root crops show disproportionately higher missingness, potentially due to logistical or methodological complexities in these systems.

Environmental factors such as season and soil texture further contribute to missingness. Difficult soil textures like clay or extreme seasonal conditions may hinder measurement and reporting. Additionally, tree age inversely correlates with missingness, as older systems often reflect greater experimental stability and completeness. Conversely, taller trees are associated with increased missingness, likely stemming from challenges in measuring or monitoring these systems. Complex response variables, such as biodiversity or greenhouse gas emissions, are also more prone to missing data due to their indirect and multifaceted nature.

These findings underscore that missingness follows a "Missing at Random" (MAR) pattern, where it is conditional on observed variables. This justifies the use of multiple imputation techniques that incorporate these predictors—tree type, crop type, environmental conditions, and response variables—into the imputation process. By accounting for these systematic relationships, imputation models can reduce bias while maintaining the data's structural integrity. The results highlight the critical role of understanding missingness mechanisms to ensure robust imputation and support reliable downstream analyses, particularly in diverse experimental contexts where missingness is inherently variable.



```{r}
# Step 1: Summarize Missingness by Tree Type and Crop Type
missing_summary_tree_crop <- test_data_cleaned_sd_test %>%
  group_by(tree_type, crop_type) %>%
  summarise(
    missing_control_sd = mean(missing_control_sd, na.rm = TRUE) * 100,
    missing_silvo_sd = mean(missing_silvo_sd, na.rm = TRUE) * 100,
    .groups = "drop"
  )

# Print Summary Table
missing_summary_tree_crop
```
```{r}
# Step 2: Visualize Missingness by Tree Type and Crop Type

# Plot for Control SD Missingness
plot_control_missingness <- ggplot(missing_summary_tree_crop, aes(x = tree_type, y = missing_control_sd, fill = crop_type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Control SD Missingness by Tree Type and Crop Type",
    x = "Tree Type",
    y = "% Missing Control SD",
    fill = "Crop Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot for Silvo SD Missingness
plot_silvo_missingness <- ggplot(missing_summary_tree_crop, aes(x = tree_type, y = missing_silvo_sd, fill = crop_type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Silvo SD Missingness by Tree Type and Crop Type",
    x = "Tree Type",
    y = "% Missing Silvo SD",
    fill = "Crop Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



plot_control_missingness
plot_silvo_missingness
```


```{r}
# Step 3: Statistical Tests to Confirm Associations
# Chi-Square Test for Independence between Tree Type and Control SD Missingness
chi_sq_tree_control <- chisq.test(table(test_data_cleaned_sd_test$tree_type, test_data_cleaned_sd_test$missing_control_sd))
chi_sq_tree_control

# Chi-Square Test for Independence between Crop Type and Control SD Missingness
chi_sq_crop_control <- chisq.test(table(test_data_cleaned_sd_test$crop_type, test_data_cleaned_sd_test$missing_control_sd))
chi_sq_crop_control

# Chi-Square Test for Independence between Tree Type and Silvo SD Missingness
chi_sq_tree_silvo <- chisq.test(table(test_data_cleaned_sd_test$tree_type, test_data_cleaned_sd_test$missing_silvo_sd))
chi_sq_tree_silvo

# Chi-Square Test for Independence between Crop Type and Silvo SD Missingness
chi_sq_crop_silvo <- chisq.test(table(test_data_cleaned_sd_test$crop_type, test_data_cleaned_sd_test$missing_silvo_sd))
chi_sq_crop_silvo

# Step 4: Logistic Regression Models for Tree Type and Crop Type
# Logistic Regression for Control SD
logit_control <- glm(missing_control_sd ~ tree_type + crop_type, data = test_data_cleaned_sd_test, family = binomial)
logit_control

# Logistic Regression for Silvo SD
logit_silvo <- glm(missing_silvo_sd ~ tree_type + crop_type, data = test_data_cleaned_sd_test, family = binomial)
logit_silvo
```

The statistical tests confirm strong associations between missingness in both `control_sd` and `silvo_sd` and the observed variables `tree_type` and `crop_type`. The chi-square tests reveal highly significant relationships for both tree type and crop type, with p-values well below 0.05. Specifically, the chi-square statistic for tree type is extremely high (X² = 76.96, p < 2.2e-16), indicating a strong dependency between tree type and missingness in both `control_sd` and `silvo_sd`. Similarly, crop type shows a significant association, though weaker (X² = 9.10, p = 0.01058).

The logistic regression models further substantiate these findings. Both tree type and crop type have positive coefficients in predicting missingness for `control_sd` and `silvo_sd`, indicating that certain tree and crop types increase the likelihood of missing data. The intercept reflects a baseline low probability of missingness, but the inclusion of tree type and crop type increases this probability, as suggested by the positive coefficients (tree type: 0.083, crop type: 0.389). The models also demonstrate good fit, with reduced residual deviance compared to null deviance, and an AIC value of 999.6 for both models.

These results underscore the systematic nature of missingness, linked to specific experimental conditions represented by tree and crop types. This confirms that missingness is not random but likely "Missing at Random" (MAR), as it depends on observable factors. The findings justify the use of these predictors in imputation models to accurately address and correct for the missing data while preserving the underlying patterns.

The results of the missingness analysis reveal systematic dependencies between missing data in `control_sd` and `silvo_sd` and observed variables such as `tree_type` and `crop_type`. This suggests that missingness is "Missing at Random" (MAR), meaning it is influenced by known factors within the dataset. These findings have significant implications for the setup of the final meta-regression analysis. First, variables like `tree_type` and `crop_type` must be included as moderators in the model to account for variability linked to experimental conditions. Their inclusion ensures that the analysis properly reflects the relationships in the data and reduces bias in effect size estimates.

Addressing missing data is essential, and imputation methods such as the upper quartile approach or predictive mean matching are suitable for handling MAR scenarios. These methods preserve the dataset’s structure by leveraging observed predictors to estimate plausible values. Given the systematic associations identified, it is also critical to test for heterogeneity across studies by incorporating random effects or interaction terms into the model, ensuring that variability is accurately captured.

Additionally, sensitivity analyses are necessary to evaluate the impact of imputation methods on the results. Comparing models based on imputed and non-imputed datasets will help validate the robustness of findings. The inclusion of tree and crop type as moderators may influence relationships with other variables, warranting the exploration of interactions and effect modifications. Finally, diagnostic tests for publication bias and model performance must be incorporated to ensure the validity and reliability of the results. In summary, the systematic nature of missingness underscores the need for careful model specification, robust handling of missing data, and comprehensive diagnostic evaluations to produce sound and interpretable conclusions in the meta-regression analysis.


```{r}
# Correctly aggregate missingness percentages by tree_type and crop_type
missingness_distribution_corrected <- database_clean_sd_df %>%
  group_by(tree_type, crop_type) %>%
  summarise(
    missing_control_sd = sum(missing_control_sd, na.rm = TRUE) / n() * 100,
    missing_silvo_sd = sum(missing_silvo_sd, na.rm = TRUE) / n() * 100,
    .groups = "drop"
  ) %>%
  pivot_longer(
    cols = c(missing_control_sd, missing_silvo_sd),
    names_to = "Variable",
    values_to = "Percent_Missing"
  )

# Re-generate plots with corrected aggregation
missingness_tree_plot_corrected <- ggplot(missingness_distribution_corrected, aes(x = tree_type, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Distribution by Tree Type",
    x = "Tree Type",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

missingness_crop_plot_corrected <- ggplot(missingness_distribution_corrected, aes(x = crop_type, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Distribution by Crop Type",
    x = "Crop Type",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the corrected plots
print(missingness_tree_plot_corrected)
print(missingness_crop_plot_corrected)
```



The updated visualizations highlight the percentage of missing values in `control_sd` and `silvo_sd` across tree type and crop type categories. For tree type, missingness patterns reveal that "fruit/nut & other" exhibits the highest proportion of missing values, exceeding 60% for both variables. In contrast, "biomass" shows moderate missingness, with rates around 40%, while "timber" has the lowest missingness, approximately 20%. These findings indicate that missingness is strongly dependent on tree type, with "fruit/nut & other" particularly affected, possibly due to measurement challenges or variability in experimental setups.

Similarly, crop type exhibits a pronounced influence on missingness rates. Cereal crops demonstrate minimal missingness, with percentages below 20%, suggesting standardized or easier measurement practices for this crop type. In contrast, legumes exhibit moderate missingness, averaging 30-40%, while tuber/root crops face the highest rates, exceeding 60% for both variables. This substantial variability underscores that specific crop types, like tuber/root crops, may present greater challenges in data collection or experimental consistency.

These patterns reinforce the importance of tailoring imputation strategies to account for category-specific influences. For example, tree type and crop type should be included as key predictors in imputation models to address the observed disparities. Additionally, the significantly higher missingness in "fruit/nut & other" tree types and tuber/root crops indicates that these categories warrant particular attention to mitigate biases introduced by missing data. Further validation through statistical testing can refine these insights and guide the application of targeted imputation techniques to preserve data integrity across diverse categories.

















#############
# STEP 4
##########################################################################################################################################
HANDELING OF MISSING VALUES IN THE DATASET
##########################################################################################################################################

Perform imputation on  silvo_se, control_se using 
"mice" (Multivariate Imputation by Chained Equations), 
Upper Quartile, 
Mean Imputation,
Bayesian
Linear regression imputation (norm.predict)


```{r}
database_clean_sd |> glimpse()
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
#######################################################################################
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
#######################################################################################
impute_data <- function(data, method_name) {
  if (method_name == "mean_imputation") {
    #######################################################################################
    # Mean Imputation (mean)
    #######################################################################################
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)

  } else if (method_name == "upper_quartile") {
    #######################################################################################
    # Upper Quartile Imputation (uq)
    #######################################################################################
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")

    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)

  } else if (method_name == "linear_imputation") {
    #######################################################################################
    # Linear Regression Imputation (lr)
    #######################################################################################
    data <- data %>%
      mutate(
        crop_type = as.numeric(as.factor(crop_type)),
        tree_type = as.numeric(as.factor(tree_type)),
        bioclim_sub_regions = as.numeric(as.factor(bioclim_sub_regions)),
        alley_width = as.numeric(as.factor(alley_width))
      )

    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 

    method <- c(
      "silvo_se" = "norm.predict",   # Imputed using linear regression
      "control_se" = "norm.predict",   # Imputed using linear regression
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else if (method_name == "bayesian") {
    #######################################################################################
    # Bayesian Imputation (by)
    #######################################################################################
    data <- data %>%
      mutate(
        silvo_se = ifelse(silvo_se < 0, 0, silvo_se),
        control_se = ifelse(control_se < 0, 0, control_se)
      )

    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 

    method <- c(
      "silvo_se" = "norm.nob",   # Imputed using Bayesian regression
      "control_se" = "norm.nob",   # Imputed using Bayesian regression
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else if (method_name == "pmm") {
    #######################################################################################
    # Predictive Mean Matching (pmm)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 

    method <- c(
      "silvo_se" = "pmm",   # Imputed using predictive mean matching
      "control_se" = "pmm", # Imputed using predictive mean matching
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else if (method_name == "rf") {
    #######################################################################################
    # Random Forest Imputation (rf)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 

    method <- c(
      "silvo_se" = "rf",   # Imputed using random forest
      "control_se" = "rf",   # Imputed using random forest
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
#######################################################################################
imputation_methods <- c("mean_imputation", "upper_quartile", "linear_imputation", "bayesian", "pmm", "rf")
imputed_datasets <- list()

# Separate storage for raw mids objects
imputed_mids_pmm <- NULL
imputed_mids_rf <- NULL
imputed_mids_bayesian <- NULL
imputed_mids_linear <- NULL

# Iterate through imputation methods
for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  
  tryCatch({
    if (method_name %in% c("pmm", "rf", "bayesian", "linear_imputation")) {
      imputed_mids <- impute_data(col_for_impute, method_name)
      
      # Save mids objects for diagnostics
      if (method_name == "pmm") imputed_mids_pmm <- imputed_mids
      if (method_name == "rf") imputed_mids_rf <- imputed_mids
      if (method_name == "bayesian") imputed_mids_bayesian <- imputed_mids
      if (method_name == "linear_imputation") imputed_mids_linear <- imputed_mids
      
      # Store completed dataset
      imputed_datasets[[method_name]] <- mice::complete(imputed_mids)
    } else {
      # Direct dataset modification for other methods
      imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
    }
  }, error = function(e) {
    cat("Error applying", method_name, "imputation:", e$message, "\n")
  })
}

# Summary of results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  if (!is.null(imputed_datasets[[method_name]])) {
    print(summary(imputed_datasets[[method_name]]))
  } else {
    cat("No data available for", method_name, "\n")
  }
}

#######################################################################################
# Step 4: Compare Results
#######################################################################################
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed and capped datasets


##########################################################################
# Last run (04/01-25)
# Total time taken: 18.98046 secs

# Last run (05/01-25)
# Total time taken: 2.224936 mins 

# Last run (11/01-25)
# Total time taken: 3.70053 mins
```

```{r}
#######################################################################################
# Step 5: Extract Fitted Data and Prepare for Visualization
#######################################################################################

# Ensure consistent data types across datasets
consistent_col_types <- function(data) {
  data %>%
    mutate(
      crop_type = as.factor(crop_type),
      tree_type = as.factor(tree_type),
      bioclim_sub_regions = as.factor(bioclim_sub_regions),
      alley_width = as.factor(alley_width)
    )
}

# Apply the function to ensure consistency
col_for_impute <- consistent_col_types(col_for_impute)

for (method_name in names(imputed_datasets)) {
  imputed_datasets[[method_name]] <- consistent_col_types(imputed_datasets[[method_name]])
}

# Combine original and imputed datasets into one
visualization_data <- col_for_impute %>%
  mutate(source = "Original") %>%
  bind_rows(
    imputed_datasets[["linear_imputation"]] %>% mutate(source = "Linear Imputation"),
    imputed_datasets[["pmm"]] %>% mutate(source = "PMM"),
    imputed_datasets[["rf"]] %>% mutate(source = "Random Forest"),
    imputed_datasets[["bayesian"]] %>% mutate(source = "Bayesian"),
    imputed_datasets[["upper_quartile"]] %>% mutate(source = "Upper Quartile"),
    imputed_datasets[["mean_imputation"]] %>% mutate(source = "Mean Imputation")
  )

# Visualize imputed values against original
library(ggplot2)

# Scatterplot for silvo_se
ggplot(visualization_data, aes(x = source, y = silvo_se, color = source)) +
  geom_jitter(width = 0.2, alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Comparison of Imputed Values for silvo_se",
    x = "Data Source",
    y = "silvo_se"
  )

# Scatterplot for control_se
ggplot(visualization_data, aes(x = source, y = control_se, color = source)) +
  geom_jitter(width = 0.2, alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Comparison of Imputed Values for control_se",
    x = "Data Source",
    y = "control_se"
  )

```


```{r}
# Check the summary of imputed dataset list
imputed_datasets |> str()
```



```{r}
# Evaluate the imputed datasets
# Check convergence diagnostics
plot(imputed_mids_pmm)
```



```{r}
# Use stripplot to compare observed and imputed values
stripplot(imputed_mids_pmm, silvo_se + control_se ~ .imp,
  cex = c(1, 2), pch = c(20, 20), jitter = TRUE, alpha = 0.4, scales = "free")
```


```{r}
# Step 1: Summarize each imputed dataset
# Quantitative Assessment:
# Calculate summary statistics for each imputed dataset, focusing on proximity to medians (mean proximity for silvo_se and control_se).
# Incorporate additional metrics like variance, range, and RMSE for better decision-making.

imputed_summaries <- list()

for (i in 1:20) {
  data <- mice::complete(imputed_mids_pmm, i) # Extract the i-th imputed dataset

  # Calculate summary statistics for each imputation
  summary <- data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_silvo_se = max(silvo_se, na.rm = TRUE) - min(silvo_se, na.rm = TRUE),
      range_control_se = max(control_se, na.rm = TRUE) - min(control_se, na.rm = TRUE)
    )

  imputed_summaries[[i]] <- summary
}

# Combine all summaries into a single data frame
imputed_summaries_df <- bind_rows(imputed_summaries, .id = "imputation")

# Calculate medians for silvo_se and control_se
median_silvo_se <- median(imputed_summaries_df$mean_silvo_se)
median_control_se <- median(imputed_summaries_df$mean_control_se)

# Add a column calculating Euclidean distance to medians
imputed_summaries_df <- imputed_summaries_df %>%
  mutate(
    distance_from_median = sqrt(
      (mean_silvo_se - median_silvo_se)^2 + (mean_control_se - median_control_se)^2
    )
  )
```

```{r}
# Step 2: Advanced Quantitative Metrics
# Root Mean Squared Error (RMSE):
# Add RMSE comparison between observed and imputed values for silvo_se and control_se.

# RMSE calculation function
calculate_rmse <- function(observed, imputed) {
  sqrt(mean((observed - imputed)^2, na.rm = TRUE))
}

# Add RMSE to imputed summaries
imputed_summaries_df <- imputed_summaries_df %>%
  rowwise() %>%
  mutate(
    rmse_silvo_se = calculate_rmse(
      col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$silvo_se[is.na(col_for_impute$silvo_se)]
    ),
    rmse_control_se = calculate_rmse(
      col_for_impute$control_se[!is.na(col_for_impute$control_se)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$control_se[is.na(col_for_impute$control_se)]
    )
  )

imputed_summaries_df
```
```{r}
# Step 3: Choose the Best Imputation
# Select the imputation based on a weighted score that combines distance to medians, RMSE, and possibly variance or range.

# Add weighted score for selection and select the top-ranked imputation
chosen_imputation <- imputed_summaries_df %>% 
  as.data.frame() |> 
  # Calculate the total_score (modify weights if necessary)
  mutate(total_score = distance_from_median + rmse_silvo_se + rmse_control_se) %>%
  # Arrange by total_score (ascending order)
  arrange(total_score) %>%
  # Select the top row with the lowest total_score
  slice(1)

# Extract the corresponding dataset
chosen_imputation_number <- as.integer(chosen_imputation$imputation)
imputed_col_data <- mice::complete(imputed_mids_pmm, chosen_imputation_number)
imputed_datasets$pmm_best <- imputed_col_data

imputed_datasets$pmm_best 
```
```{r}
# imputation_plot_data_all |> str()
# # Check if "linear" is included as a method
# unique(imputation_plot_data_all$method)
# 
# imputation_plot_data_all %>%
#     filter(method == "linear_imputation") %>%
#     filter(type == "Imputed")
# count()
```

##########################################################################################################################################
Visually examine the imputed values
##########################################################################################################################################


```{r}
# Step 4: Visual Assessment
# Density Plots for Each Method and Variable:
# Compare observed and imputed distributions for silvo_se and control_se across all imputation methods.

# Prepare data for visualization
observed_values <- list(
  silvo_se = col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)],
  control_se = col_for_impute$control_se[!is.na(col_for_impute$control_se)]
)

# Combine observed and imputed values for plotting
combined_plot_data <- list()

for (variable in c("silvo_se", "control_se")) {
  for (method_name in names(imputed_datasets)) {
    imputed_values <- imputed_datasets[[method_name]][[variable]][is.na(col_for_impute[[variable]])]

    combined_plot_data[[paste(variable, method_name, sep = "_")]] <- data.frame(
      value = c(observed_values[[variable]], imputed_values),
      type = c(rep("Original", length(observed_values[[variable]])),
               rep("Imputed", length(imputed_values))),
      method = method_name,
      variable = variable
    )
  }
}

###############################################################################
# Combine all data into one frame
imputation_plot_data_all <- bind_rows(combined_plot_data)

###############################################################################
# Remove linear imputation for better visualization
imputation_plot_data_no_linear <- 
  imputation_plot_data_all |> 
  filter(method != "linear_imputation")
```



```{r}
# Generic function to generate density plots
generate_density_plot <- function(data, title_suffix, scale_type = "linear") {
  plot <- ggplot(data, aes(x = value, fill = type)) +
    geom_density(alpha = 0.5) +
    facet_grid(variable ~ method) +
    labs(
      title = paste("Density Comparison:", title_suffix),
      x = ifelse(scale_type == "linear", "Value", "Value (Pseudo-Log Scale)"),
      y = "Density"
    ) +
    theme_minimal() +
    scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
    theme(strip.text = element_text(size = 10, face = "bold"))
  
  if (scale_type == "pseudo_log") {
    plot <- plot + scale_x_continuous(trans = pseudo_log_trans(sigma = 0.1)) + scale_y_continuous(trans = pseudo_log_trans(sigma = 0.1))
  }
  
  return(plot)
}

# Generate plots

plot_imputation_data_all_linear <- generate_density_plot(
  data = imputation_plot_data_all, 
  title_suffix = "Original vs. Imputed Values (Linear Scale)",
  scale_type = "linear"
)

plot_imputation_data_all_pseudo <- generate_density_plot(
  data = imputation_plot_data_all, 
  title_suffix = "Original vs. Imputed Values (Pseudo-Log Scale)",
  scale_type = "pseudo_log"
)

plot_imputation_data_no_linear_linear <- generate_density_plot(
  data = imputation_plot_data_no_linear, 
  title_suffix = "Excluding Linear Imputation (Linear Scale)",
  scale_type = "linear"
)

plot_imputation_data_no_linear_pseudo <- generate_density_plot(
  data = imputation_plot_data_no_linear, 
  title_suffix = "Excluding Linear Imputation (Pseudo-Log Scale)",
  scale_type = "pseudo_log"
)

# Print
plot_imputation_data_all_pseudo 
plot_imputation_data_all_linear 
plot_imputation_data_no_linear_linear 
plot_imputation_data_no_linear_pseudo 
```

Save the plots
```{r}
# Define the output folder path
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 80),        # Increase title size
    axis.text = element_text(size = 30),        # Increase axis text size
    axis.title = element_text(size = 50),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 60),      # Increase legend text size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.x = element_text(angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
plot_imputation_data_all_pseudo <- plot_imputation_data_all_pseudo + theme_custom
plot_imputation_data_all_linear <- plot_imputation_data_all_linear + theme_custom
plot_imputation_data_no_linear_linear <- plot_imputation_data_no_linear_linear + theme_custom
plot_imputation_data_no_linear_pseudo <- plot_imputation_data_no_linear_pseudo + theme_custom


# Save the plots
ggsave(
  filename = file.path(output_dir, "plot_imputation_data_all_linear.png"),
  plot = plot_imputation_data_all_linear,
  width = 14, height = 6, dpi = 600,
  bg = "white"
)

ggsave(
  filename = file.path(output_dir, "plot_imputation_data_all_pseudo.png"),
  plot = plot_imputation_data_all_pseudo,
  width = 14, height = 6, dpi = 600,
  bg = "white"
)

ggsave(
  filename = file.path(output_dir, "plot_imputation_data_no_linear_linear.png"),
  plot = plot_imputation_data_no_linear_linear,
  width = 14, height = 6, dpi = 600,
  bg = "white"
)

ggsave(
  filename = file.path(output_dir, "plot_imputation_data_no_linear_pseudo.png"),
  plot = plot_imputation_data_no_linear_pseudo,
  width = 14, height = 6, dpi = 600,
  bg = "white"
)
```






##########################################################################################################################################
More quantitative statistical assessment
##########################################################################################################################################

```{r}
# imputed_datasets |> str()
```

```{r}
# Variable importance from a random forest model

# Remove rows with missing values
rf_data <- na.omit(imputed_datasets$rf)  

# Exclude specific variables as predictors
rf_model <- randomForest(silvo_se ~ . - control_se - control_n - silvo_n - id_obs - treat_id - exp_id,
                         data = rf_data)

# View variable importance
importance(rf_model)

# Plot variable importance
varImpPlot(rf_model)
```


```{r}
# a. Descriptive Statistics
# Calculate and compare mean, standard deviation, and range for key variables (e.g., silvo_se, control_se).

# a. Descriptive Statistics
# Calculate descriptive statistics for each imputation method
compare_stats <- lapply(imputed_datasets, function(data) {
  # Check if data is of class 'mids', and extract the completed data
  if (inherits(data, "mids")) {
    data <- mice::complete(data)
  }
  
  data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      range_silvo_se = diff(range(silvo_se, na.rm = TRUE)),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_control_se = diff(range(control_se, na.rm = TRUE))
    )
}) %>%
  bind_rows(.id = "method")

# View results
print(compare_stats)



# b. Variance Explained
# Calculate variance for silvo_se and control_se
compare_variance <- lapply(imputed_datasets, function(data) {
  # Check if data is of class 'mids', and extract the completed data
  if (inherits(data, "mids")) {
    data <- mice::complete(data)
  }
  
  data %>%
    summarise(
      var_silvo_se = var(silvo_se, na.rm = TRUE),
      var_control_se = var(control_se, na.rm = TRUE)
    )
}) %>%
  bind_rows(.id = "method")

# View results
print(compare_variance)




# Calculate Jensen-Shannon divergence for each method - The Jensen-Shannon Divergence (JSD) is a statistical measure of similarity (or dissimilarity) between two 
# probability distributions. It quantifies how different one probability distribution is from another and is widely used in information theory, machine learning, 
# and statistics.
library(philentropy)

# Function to calculate JSD safely
calculate_jsd <- function(observed, imputed) {
  if (length(imputed) > 1 && length(observed) > 1) {
    observed_density <- density(observed)$y
    imputed_density <- density(imputed)$y
    
    # Normalize to probabilities
    observed_density <- observed_density / sum(observed_density)
    imputed_density <- imputed_density / sum(imputed_density)
    
    # Calculate JSD
    return(JSD(rbind(observed_density, imputed_density)))
  } else {
    return(NA) # Return NA if densities cannot be calculated
  }
}

# Calculate JSD for each method
compare_jsd <- lapply(imputed_datasets, function(data) {
  if (inherits(data, "mids")) {
    data <- mice::complete(data) # Handle mids objects
  }
  
  # Filter numeric columns for density calculation
  observed_silvo <- col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)]
  imputed_silvo <- data$silvo_se[is.na(col_for_impute$silvo_se)]
  
  observed_control <- col_for_impute$control_se[!is.na(col_for_impute$control_se)]
  imputed_control <- data$control_se[is.na(col_for_impute$control_se)]
  
  list(
    jsd_silvo = calculate_jsd(observed_silvo, imputed_silvo),
    jsd_control = calculate_jsd(observed_control, imputed_control)
  )
}) %>%
  bind_rows(.id = "method")

# View results
print(compare_jsd)


# Add an identifier column to each data frame and convert them to long format
compare_stats_long <- compare_stats %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Descriptive Stats")

compare_variance_long <- compare_variance %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Variance Comparison")

compare_jsd_long <- compare_jsd %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "JSD Comparison")

# Combine all data frames into one
combined_metrics <- bind_rows(compare_stats_long, compare_variance_long, compare_jsd_long)

# View the combined data frame
combined_metrics
```

```{r}
# Ensure `database_clean_sd` is converted to a plain data frame and the `geometry` column is removed
database_clean_sd_df <- database_clean_sd %>%
  as.data.frame() |> 
  select(-geometry)

# Recalculate the metrics for the original dataset
original_metrics <- list(
  # Descriptive statistics
  descriptive_stats = database_clean_sd_df %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      range_silvo_se = diff(range(silvo_se, na.rm = TRUE)),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_control_se = diff(range(control_se, na.rm = TRUE))
    ) %>%
    mutate(method = "original"),
  
  # Variance comparison
  variance = database_clean_sd_df %>%
    summarise(
      var_silvo_se = var(silvo_se, na.rm = TRUE),
      var_control_se = var(control_se, na.rm = TRUE)
    ) %>%
    mutate(method = "original"),
  
  # Jensen-Shannon divergence (JSD)
  jsd = list(
    jsd_silvo = 0, # No divergence for original dataset
    jsd_control = 0
  ) %>%
    bind_rows() %>%
    mutate(method = "original")
)

# Convert metrics to long format for visualization
original_descriptive_long <- original_metrics$descriptive_stats %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Descriptive Stats")

original_variance_long <- original_metrics$variance %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Variance Comparison")

original_jsd_long <- original_metrics$jsd %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "JSD Comparison")

# Combine original metrics with imputed dataset metrics
original_combined <- bind_rows(original_descriptive_long, original_variance_long, original_jsd_long)
combined_metrics_with_original <- bind_rows(combined_metrics, original_combined)
```

```{r}
prepared_data_gt <- combined_metrics_with_original %>%
  # Pivot wider to make methods the columns
  pivot_wider(names_from = method, values_from = value) %>%
  # Add absolute relative difference columns
  mutate(
    across(
      -c(metric, original, category), # Exclude non-numeric columns
      ~ ifelse(
        grepl("^jsd_", metric) & . == 0, NA,  # Set `NA` where `jsd_` metrics are 0.00
        ifelse(is.na(original) | original == 0, NA, abs((. - original) / original)) # Otherwise compute relative difference
      ),
      .names = "{.col}_relative"
    )
  ) %>%
  # Add a new column to extract the prefix (e.g., mean_, sd_, etc.)
  mutate(metric_group = sub("_.*", "", metric)) %>%
  # Sort rows first by the group (e.g., mean, sd) and then by the original metric order
  arrange(metric_group, metric)

# Display structure for verification
prepared_data_gt |> glimpse()
```


```{r}
# Create the updated plot
combined_metrics_comparison_with_original <- combined_metrics_with_original |> 
  ggplot(aes(x = method, y = value, fill = method)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = 0.8) +
  facet_wrap(~ category + metric, scales = "free_y", ncol = 3) +
  labs(
    title = "Comparison of Imputation Methods Across Metrics (Including Original Dataset)",
    x = "Method",
    y = "Value",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "top"
  ) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8)  # Discrete color scale



combined_metrics_comparison_with_original
```

Save the comparison diagnostics bar chart
```{r}
# Specify the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists (optional step)
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save as PDF

combined_metrics_comparison_with_original <- combined_metrics_comparison_with_original + theme_custom

ggsave(
  filename = file.path(output_dir, "combined_metrics_comparison_with_original.png"),
  plot = combined_metrics_comparison_with_original,
  width = 14, height = 18, dpi = 400,
  bg = "white"
)
```

```{r}
# Apply color shading to all `_relative` columns
comparison_gt <- prepared_data_gt %>%
   # Reorder the columns in the data frame before passing to gt
  select(
    metric, category, original, 
    linear_imputation,
    linear_imputation_relative,
    mean_imputation, 
    mean_imputation_relative,
    upper_quartile, 
    upper_quartile_relative, 
    bayesian,
    bayesian_relative,
    pmm, 
    pmm_relative,  
    pmm_best,  
    pmm_best_relative,
    rf,
    rf_relative
    ) |> 
  # Set `metric` as row names
  gt() %>%
  data_color(
    columns = ends_with("_relative"), # Target all _relative columns
    method = "numeric", # Use numeric method for gradient mapping
    palette = c(
    "#00FF00", # Bright Green
    "#80FF00", # Light Green
    "#A8FF00", # Lime Green
    "#FFFF00", # Yellow
    "#FFA500", # Orange
    "#FF4500", # Orange-Red
    "#FF0000", # Bright Red
    "#8B0000"  # Dark Red
),
    domain = c(0, 0.25), # Domain for relative differences
    na_color = "#f0f0f0" # Light gray for missing/NA values
  ) %>%
  tab_header(
    title = "Comparison of Imputation Methods Across Metrics",
    subtitle = "Including Original Data and Relative Differences"
  ) %>%
  cols_label(
    original = "Original",
    linear_imputation = "Linear Imputation",
    linear_imputation_relative = "Linear Imputation Relative",
    mean_imputation = "Mean Imputation",
    mean_imputation_relative = "Mean Imputation Relative",
    upper_quartile = "Upper Quartile",
    upper_quartile_relative = "Upper Quartile Relative",
    bayesian = "Bayesian",
    bayesian_relative = "Bayesian Relative",
    pmm = "PMM",
    pmm_relative = "PMM Relative",
    pmm_best = "PMM Best",
    pmm_best_relative = "PMM Best Relative",
    rf = "Random Forest",
    rf_relative = "Random Forest Relative"
    ) %>%
  fmt_number(
    columns = c(original, 
                linear_imputation,
                mean_imputation,
                upper_quartile,
                pmm, 
                pmm_best),
    decimals = 2
  ) %>%
  fmt_number(
    columns = ends_with("_relative"),
    decimals = 3
  ) %>%
  sub_missing(
    columns = ends_with("_relative"),
    missing_text = "NA"
  ) %>%
  tab_options(
    table.font.size = "small",
    column_labels.font.size = "medium"
  )

# Display the table
comparison_gt
```



The visualization compares various imputation methods across key metrics, including mean, range, standard deviation, variance, and Jensen-Shannon divergence (JSD), applied to silvo and control standard errors (SEs). The original dataset serves as a baseline for comparison.

The **upper quartile**, **rf**, and **pmm_best** methods emerge as strong contenders, consistently preserving the statistical properties of the original data. The **upper quartile** method achieves particularly favorable results, demonstrating low mean SE values for both silvo and control, coupled with balanced handling of data variability. Unlike Bayesian and mean imputation, which introduce noticeable upward bias in the mean SE values, the upper quartile method avoids distortions, ensuring the imputed data aligns closely with the original distribution. The **rf** and **pmm_best** methods also perform well in terms of distributional preservation, with **rf** achieving low relative differences but slightly higher variance and JSD values compared to upper quartile. 

The range of SEs is stable across all imputation methods, indicating that extreme values in the dataset are well-preserved. The standard deviation and variance metrics show minimal variation across methods, reflecting consistent handling of the dataset’s internal variability. Notably, the **upper quartile** method retains these properties without the complexity of predictive modeling, offering a robust and straightforward approach.

The JSD values reinforce these findings. While **pmm_best** and **rf** achieve the lowest JSD scores (indicating high similarity to the original data distribution), the **upper quartile** method maintains competitive JSD values (0.90 for both silvo and control SEs), highlighting its ability to balance simplicity with effective data preservation. Bayesian and mean imputation, by contrast, exhibit the highest JSD values, diverging considerably from the original data.

Overall, while **rf** and **pmm_best** excel in certain metrics, the **upper quartile** method delivers the best overall performance, combining low relative differences, robust statistical preservation, and distributional alignment, making it the most suitable choice for imputation in this analysis.


Save the comparison gt table
```{r}
# Specify the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists (optional step)
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save as HTML
gtsave(data = comparison_gt, filename = file.path(output_dir, "comparison_gt_table.html"))

# Save as PDF
# gtsave(data = comparison_gt, filename = file.path(output_dir, "comparison_gt_table.pdf"))

```


#############
# STEP 5
##########################################################################################################################################
MERGING THE IMPUTED DATASET BACK TO THE ORIGINAL DATA AND VISUALISE
##########################################################################################################################################

```{r}
##############
# MERGING IMPUTED DATA BACK TO THE ORIGINAL DATASET AND VISUALIZING
############################################################################
# Objective:
# Combine the original dataset (`database_clean_sd`) with the imputed dataset 
# (using the Upper Quartile imputation method, selected as the most robust approach).
# This allows for a comparison of original vs. imputed values and ensures clarity by keeping columns distinct.

# Merging process
merged_data <- database_clean_sd_df %>%
  full_join(
    # Selecting the Upper Quartile imputed dataset:
    imputed_datasets$upper_quartile %>%
      # Keep only relevant imputed columns
      select(id_article, id_obs, silvo_se, control_se), 
    # Merge based on unique identifiers
    by = c("id_article", "id_obs"), 
    # Suffix to distinguish original vs. imputed columns
    suffix = c("_original", "_imputed") 
  )

# Preview the structure of the merged dataset to ensure the merge was successful
glimpse(merged_data)

# Dataset Summary:
# - Rows: 1,126
# - Columns: Updated based on merged dataset

# Additional Context:
# - The Upper Quartile method was selected as the most robust imputation method based on:
#   - The lowest total relative differences (14.92%) across silvo and control SEs.
#   - Competitive Jensen-Shannon Divergence (JSD) values (0.90), indicating strong alignment with the original distribution.
#   - Consistent performance across key metrics like variance, standard deviation, and range.
#   - Its simplicity and effectiveness in minimizing distortions compared to more complex methods.

# Note:
# The evaluations/assessments of the imputation methods, including relative differences, variance comparisons, 
# and Jensen-Shannon Divergence (JSD), are performed in the subsequent sections below. These assessments 
# substantiate the selection of the Upper Quartile method as the best approach for this dataset.


# The merged dataset is now ready for further analysis and visualization, allowing for transparent comparisons between the original and imputed values.

```
```{r}
# Replace missing values in `silvo_se` and `control_se` with imputed values if originals are NA

merged_data <- merged_data %>%
  mutate(
    # For `silvo_se`, check if the original value is missing (NA)
    # If missing, use the imputed value (`silvo_se_imputed`)
    # Otherwise, retain the original value (`silvo_se_original`)
    silvo_se = ifelse(is.na(silvo_se_original), silvo_se_imputed, silvo_se_original),
    
    # For `control_se`, check if the original value is missing (NA)
    # If missing, use the imputed value (`control_se_imputed`)
    # Otherwise, retain the original value (`control_se_original`)
    control_se = ifelse(is.na(control_se_original), control_se_imputed, control_se_original)
  )

# merged_data <- merged_data %>%
#   select(-silvo_se_original, -silvo_se_imputed, -control_se_original, -control_se_imputed)
```

```{r}
# Identify rows where imputation occurred
imputation_evaluation <- merged_data %>%
  filter(
    (is.na(silvo_se_original) & !is.na(silvo_se_imputed)) |
    (is.na(control_se_original) & !is.na(control_se_imputed))
  ) %>%
  select(id_article, id_obs, exp_id) %>%
  distinct()

# Count the number of unique articles where imputation occurred
n_imputed_studies <- imputation_evaluation %>%
  distinct(id_article) %>%
  nrow()

cat("Number of unique articles with imputed values:", n_imputed_studies, "\n")
# Number of unique articles with imputed values: 11 
```


```{r}
# Calculate missing counts and proportions
missing_summary <- merged_data %>%
  summarise(
    total_obs = n(),

    # Missing counts for original dataset
    silvo_se_original_missing = sum(is.na(silvo_se_original)),
    control_se_original_missing = sum(is.na(control_se_original)),

    # Missing counts for imputed dataset
    silvo_se_imputed_missing = sum(is.na(silvo_se_imputed)),
    control_se_imputed_missing = sum(is.na(control_se_imputed))
  ) %>%
  mutate(
    # Proportions for original dataset
    silvo_se_original_proportion = silvo_se_original_missing / total_obs,
    control_se_original_proportion = control_se_original_missing / total_obs,

    # Proportions for imputed dataset
    silvo_se_imputed_proportion = silvo_se_imputed_missing / total_obs,
    control_se_imputed_proportion = control_se_imputed_missing / total_obs
  )

missing_summary
```

```{r}
original_data <- database_clean_sd_df %>%
  select(id_article, id_obs, silvo_se, control_se) %>%
  mutate(data_source = "Original")

imputed_data <- merged_data %>%
  select(id_article, id_obs, silvo_se, control_se) %>%
  mutate(data_source = "Imputed")

# Combine original and imputed data
combined_data <- bind_rows(original_data, imputed_data)
```

```{r}
# Check for duplicates
duplicates <- merged_data %>%
  group_by(id_article, id_obs) %>%
  filter(n() > 1)

# Check for rows with imputed values
imputation_check <- merged_data %>%
  filter(is.na(silvo_se_original) & !is.na(silvo_se_imputed))

# View summaries
print(duplicates)
print(imputation_check)
```


##########################################################################################################################################
VISUAL DIAGNOSTIGS OF MERGED DATA WITH SELECTED IMPUTATION METHOD
##########################################################################################################################################

```{r}
# imputed_datasets |> str()
```


```{r}
# imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation", "rf", "bayesian")

# Combine all imputed datasets with the original data

# Convert 'mids' objects to complete data frames
valid_imputation_methods <- imputed_datasets %>%
  keep(~ !is.null(.)) %>%
  lapply(function(dataset) {
    if (inherits(dataset, "mids")) {
      # Convert mids object to complete data frame
      complete(dataset)
    } else {
      dataset
    }
  })

# Convert the original data to a standard data frame
original_data <- database_clean_sd %>% as.data.frame()

# Combine all valid imputed datasets with the original data
se_comparison_data <- valid_imputation_methods %>%
  lapply(function(imp_data) {
    # Convert imputed data to data.frame if necessary
    imp_data <- imp_data %>%
      as.data.frame() 
    
    # Summarize the original and imputed data by response_variable or other grouping variables
    summarized_data <- original_data %>%
      full_join(
        imp_data %>% select(id_article, id_obs, silvo_se, control_se),
        by = c("id_article", "id_obs"),
        suffix = c("_original", "_imputed")
      ) %>%
      group_by(response_variable) %>%
      summarise(
        silvo_se_original_mean = mean(silvo_se_original, na.rm = TRUE),
        silvo_se_imputed_mean = mean(silvo_se_imputed, na.rm = TRUE),
        control_se_original_mean = mean(control_se_original, na.rm = TRUE),
        control_se_imputed_mean = mean(control_se_imputed, na.rm = TRUE)
      ) %>%
      mutate(
        silvo_diff = abs(silvo_se_original_mean - silvo_se_imputed_mean),
        silvo_rel_diff = if_else(silvo_se_original_mean > 0, 
                                 (silvo_diff / silvo_se_original_mean) * 100, NA_real_),
        control_diff = abs(control_se_original_mean - control_se_imputed_mean),
        control_rel_diff = if_else(control_se_original_mean > 0, 
                                   (control_diff / control_se_original_mean) * 100, NA_real_)
      )
    
    # Return the summarized data
    summarized_data
  })


# Check the structure of one of the resulting datasets
str(se_comparison_data[[1]])
se_comparison_data |> str()
```

```{r}
# Combine all the comparison data into a single dataframe for plotting
se_comparison_combined <- bind_rows(
  lapply(names(se_comparison_data), function(method) {
    se_comparison_data[[method]] %>%
      mutate(imputation_method = method)
  }),
  .id = "method"
)

# Reshape data for plotting relative differences
se_comparison_long <- se_comparison_combined %>%
  select(response_variable, imputation_method, silvo_rel_diff, control_rel_diff) %>%
  pivot_longer(cols = c(silvo_rel_diff, control_rel_diff),
               names_to = "type",
               values_to = "relative_difference")

# Plot relative differences
# Bar chart
se_comparison_plot_mods <- se_comparison_long |> 
  ggplot(aes(x = imputation_method, y = relative_difference, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ response_variable, scales = "free_y") +
  labs(
    title = "Relative Differences Between Original and Imputed SE",
    x = "Imputation Method",
    y = "Relative Difference (%)",
    fill = "SE Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot relative differences with response variables on the x-axis
se_comparison_plot_resp <- se_comparison_long |> 
  ggplot(aes(x = response_variable, y = relative_difference, fill = imputation_method)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ type, scales = "free_y") +
  labs(
    title = "Relative Differences Across Response Variables and Imputation Methods",
    x = "Response Variable",
    y = "Relative Difference (%)",
    fill = "Imputation Method"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plots
se_comparison_plot_mods
se_comparison_plot_resp
```

Saving the plots of relative differences
```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 80),        # Increase title size
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 50),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 60),      # Increase legend text size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 80),
    axis.text.x = element_text(size = 80,
                               angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
se_comparison_plot_mods <- se_comparison_plot_mods + theme_custom
se_comparison_plot_resp <- se_comparison_plot_resp + theme_custom


# Save the plots
ggsave(
  filename = file.path(output_dir, "se_comparison_plot_mods.png"),
  plot = se_comparison_plot_mods,
  width = 10, height = 8, dpi = 600,
  bg = "white"
)

ggsave(
  filename = file.path(output_dir, "se_comparison_plot_resp.png"),
  plot = se_comparison_plot_resp,
  width = 10, height = 8, dpi = 600,
  bg = "white"
)
```



```{r}
# Identify the imputation method with the least relative difference
# se_comparison_combined |> str()

# Calculate the mean relative differences for each imputation method
imp_method_summary <- se_comparison_combined %>%
  group_by(imputation_method) %>%
  summarize(
    mean_silvo_rel_diff = mean(silvo_rel_diff, na.rm = TRUE),
    mean_control_rel_diff = mean(control_rel_diff, na.rm = TRUE),
    total_rel_diff = mean_silvo_rel_diff + mean_control_rel_diff
  ) |> 
  arrange(total_rel_diff)

imp_method_summary

# A tibble:7 × 4
# imputation_method
# <chr>
# mean_silvo_rel_diff
# <dbl>
# mean_control_rel_diff
# <dbl>
# total_rel_diff
# <dbl>
# bayesian	23.516743	42.415572	65.93232	
# linear_imputation	16.330035	21.200899	37.53093	
# mean_imputation	15.642138	21.686978	37.32912	
# pmm	10.123358	5.253195	15.37655	
# pmm_best	7.750850	13.484360	21.23521	
# rf	8.349813	8.251046	16.60086	
# upper_quartile	7.409419	7.513123	14.92254	

# Identify the imputation method with the least relative difference
best_imp_method_based_on_se_rel_diff <- imp_method_summary %>%
  arrange(!total_rel_diff) %>%
  slice(1)

# Display the best method
best_imp_method_based_on_se_rel_diff
```

Generate this table as a gt table
```{r}
# imp_method_summary |> str()

# Create a publication-ready gt table
imp_method_real_diff_table <- imp_method_summary %>%
  gt() %>%
  tab_header(
    title = "Comparison of Imputation Methods",
    subtitle = "Relative Differences Across Metrics"
  ) %>%
  fmt_number(
    columns = c(mean_silvo_rel_diff, mean_control_rel_diff, total_rel_diff),
    decimals = 2
  ) %>%
  cols_label(
    imputation_method = "Imputation Method",
    mean_silvo_rel_diff = "Mean Silvo Rel. Diff (%)",
    mean_control_rel_diff = "Mean Control Rel. Diff (%)",
    total_rel_diff = "Total Rel. Diff (%)"
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "lightblue"),
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = px(14),  # Adjust font size for readability
    table.width = pct(100)    # Adjust table width
  )

# View the table
print(imp_method_real_diff_table)
```


Taking into account the additional summary metrics above, which quantify the average relative differences (silvo, control, and total) across all imputation methods, we can further refine the evaluation of the imputation methods. The relative differences provide insight into how much the imputed values deviate from the original data on average.

The imputation methods upper_quartile and pmm demonstrate the lowest total relative differences (14.92% and 15.38%, respectively). This indicates that these methods introduce the least overall distortion into the imputed data compared to the original dataset. Notably, upper_quartile has a slightly better balance between silvo and control SEs, with almost equal mean relative differences (7.41% and 7.51%).

The rf method also performs well with a total relative difference of 16.60%, but its slightly higher relative differences for both silvo and control SEs indicate that it introduces a bit more variability than upper_quartile and pmm.

On the other hand, methods like mean_imputation, linear_imputation, and bayesian exhibit much higher total relative differences (37.33%, 37.53%, and 65.93%, respectively). These methods significantly deviate from the original data, with bayesian showing the highest total relative difference. Such large deviations suggest that these methods are less effective at preserving the original data's characteristics, especially for analyses where minimizing distortion is critical.

Considering both the visualization and these summary statistics, upper_quartile emerges as the most robust imputation method. It not only minimizes total relative differences but also maintains consistency across silvo and control SEs. The pmm method is a close second, providing similarly low total relative differences but with a slightly less balanced performance between silvo and control SEs. Methods like rf may be suitable in cases where slight deviations are acceptable, but mean_imputation, linear_imputation, and bayesian should generally be avoided unless specific methodological justifications exist.


###########################################################


Based on the analysis of the visualizations, summary metrics, and Jensen-Shannon Divergence (JSD), the **upper_quartile** imputation method stands out as the best choice. This method demonstrates strong performance across all metrics, preserving the statistical properties of the original dataset while introducing minimal distortion.

The **upper_quartile** method achieves the lowest total relative difference, with an overall value of **14.92%**, and balances the relative differences for silvo and control SEs, with mean values of **7.41%** and **7.51%**, respectively. Additionally, its JSD values (0.90 for both silvo and control SEs) indicate strong preservation of the data distribution, outperforming more complex methods like Bayesian (0.98 for control and 0.92 for silvo). While **pmm** achieves slightly lower JSD values (0.25 for control and 0.07 for silvo), the **upper_quartile** method remains competitive in distributional similarity while excelling in minimizing relative differences.

Other methods, such as Bayesian, mean_imputation, and linear_imputation, exhibit significantly higher relative differences, with Bayesian reaching a total of **65.93%**. These methods tend to introduce notable distortions into the data, making them less suitable for analyses where preserving the integrity of the original dataset is critical.

From a practical standpoint, the simplicity and robustness of **upper_quartile** make it a dependable choice. It avoids overfitting or imposing relationships on the data, which can introduce biases, as seen in methods like Bayesian or random forests. While **pmm** (15.38% total relative difference) and **rf** (16.60%) also perform well, **upper_quartile** provides a superior balance between simplicity, distributional preservation, and low distortion.

In conclusion, the **upper_quartile** method delivers the best combination of minimal relative differences, robust performance across metrics, and strong preservation of the data distribution, making it the most suitable choice for imputation in this analysis.


###########################################################

The upper quartile imputation method imputes missing values using the 75th percentile of the observed data distribution. This simple approach ensures imputed values remain within the dataset's plausible range while slightly favoring higher values. Unlike complex predictive models, it does not rely on assumptions about relationships between variables, making it robust against outliers and overfitting.

This method outperforms others because it preserves the distribution of the original data. By relying on a robust summary statistic, it avoids introducing bias or unrealistic imputations often seen in mean or model-based imputation methods. Its focus on maintaining variability and range ensures that the statistical properties of the dataset remain intact. Furthermore, the method is consistent across different variables, minimizing distortion in both silvo and control SEs.

Analyses show that upper quartile imputation has the lowest total relative differences across all response variables, outperforming more complex methods like Bayesian or random forests. It balances simplicity and accuracy, achieving reliable imputation without introducing unnecessary complexity or deviations from the original data.

The success of upper quartile imputation lies in its simplicity and reliance on data-driven statistics rather than predictive modeling. This makes it especially effective in cases with skewed distributions or when preserving data integrity is a priority. While advanced methods may offer predictive power, they risk adding noise or bias. Upper quartile imputation provides a straightforward and reliable alternative, making it an ideal choice when the goal is to maintain the dataset's original characteristics with minimal distortion.










Quality check of imputation using QQ-plot 
```{r}
# Pivot data to long format for easier plotting
qq_imp_data <- combined_data %>%
  pivot_longer(
    cols = c(silvo_se, control_se),
    names_to = "variable",
    values_to = "value"
  ) |> 
  mutate(value = ifelse(is.na(value), 0, value)) # Replace NA with 0 (or another strategy if preferred)



# Compute theoretical quantiles and prepare the data for jitter
qq_data_jitter <- qq_imp_data %>%
  group_by(variable, data_source) %>%
  mutate(
    theoretical = qnorm((rank(value, na.last = "keep") - 0.5) / sum(!is.na(value))) # Compute theoretical quantiles
  ) %>%
  ungroup() 


# Enhanced Q-Q plot for publication (with jitter)
# Enhanced Q-Q plot with different point types
qqplot_imp_combined <- qq_data_jitter %>%
  ggplot(aes(x = theoretical, y = value, color = data_source, shape = data_source)) + # Map shape to data_source
  geom_point(
    position = position_jitter(width = 0.1, height = 0.1),
    alpha = 0.9,
    size = 1
  ) + # Adjust jitter, transparency, and size
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black", size = 1) + # Reference line
  facet_wrap(~variable, scales = "free", ncol = 2) + # Adjust layout
  labs(
    title = "Q-Q Plots for Original vs. Imputed Data",
    subtitle = "Assessing Distributional Similarity with Jitter",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
    color = "Data Source",
    shape = "Data Source" # Add legend for shape
  ) +
  scale_color_viridis_d(
    option = "D",
    begin = 0.3,
    end = 0.9,
    name = "Data Source"
  ) +
  scale_shape_manual(
    values = c(16, 17) # Specify shapes: 16 (solid circle), 17 (triangle)
  ) +
  guides(
    color = guide_legend(override.aes = list(size = 3)), # Increase color legend point size
    shape = guide_legend(override.aes = list(size = 3))  # Increase shape legend point size
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5), # Centered title
    plot.subtitle = element_text(size = 14, face = "italic", hjust = 0.5), # Italicized subtitle
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    strip.text = element_text(size = 14, face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 12)
  )


qqplot_imp_combined
```


Saving QQ-plot

```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 80),        # Increase title size
    plot.subtitle = element_text(size = 70),
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 50),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 80),      # Increase legend text size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 100),
    axis.text.x = element_text(size = 100,
                               angle = 0, hjust = 0) # Rotate x-axis text
  )

# Apply theme modifications to each plot
qqplot_imp_combined <- qqplot_imp_combined + theme_custom


# Save the enhanced plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
ggsave(
  filename = file.path(output_dir, "qqplot_imp_combined_enhanced.png"),
  plot = qqplot_imp_combined,
  width = 8, height = 6, dpi = 600,
  bg = "white"
)
```



Again, checking the imputed control_se and silvo_se density distribution (imputation = upper quartile)
```{r}
# Combine the data for silvo_se and control_se into long format
density_data <- combined_data %>%
  pivot_longer(
    cols = c(silvo_se, control_se),
    names_to = "variable",
    values_to = "value"
  )

# Filter out non-positive values for log transformation
density_data_clean <- density_data %>%
  filter(value > 0) # Keep only positive values

# Improved density plot with custom x-axis labels
density_plot_clean <- density_data_clean %>%
  ggplot(aes(x = value, color = data_source, fill = data_source)) +
  geom_density(alpha = 0.4, na.rm = TRUE) + # Add density plot with transparency
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x), # Define breaks at log10 intervals
    labels = scales::trans_format("log10", scales::math_format(10^.x)) # Format labels as 10^x
  ) +
  labs(
    title = "Density Distribution of silvo_se and control_se (Log-Transformed)",
    x = "Value (Log Scale)",
    y = "Density",
    color = "Data Source",
    fill = "Data Source"
  ) +
  facet_wrap(~variable, scales = "free_x", ncol = 2) + # Separate plots for silvo_se and control_se
  scale_color_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom"
  )

# Display the density plot
density_plot_clean
```

Save density distribution plot of imputed control_se and silvo_se

```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 80),        # Increase title size
    plot.subtitle = element_text(size = 70),
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 50),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 80),      # Increase legend text size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 100),
    axis.text.x = element_text(size = 100,
                               angle = 0, hjust = 0) # Rotate x-axis text
  )

# Apply theme modifications to each plot
density_plot_clean <- density_plot_clean + theme_custom


# Save the enhanced plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
ggsave(
  filename = file.path(output_dir, "density_plot_clean.png"),
  plot = density_plot_clean,
  width = 8, height = 6, dpi = 600,
  bg = "white"
)
```




















#############
# STEP 6
##########################################################################################################################################
SAVING TWO VERSIONS OF PREPROCESSED DATA FOR FURTHER ANALYSIS AND VISUALIZATION
##########################################################################################################################################


```{r}
# Imputed dataset
imp_pmm_best <- merged_data |> 
  # Modify the imp_pmm_best dataset before saving
  # Remove existing columns
  select(-c(control_se, silvo_se, control_se_original, silvo_se_original)) |>  
  rename(
    # Rename control_se_imputed to control_se
    control_se = control_se_imputed, 
    # Rename silvo_se_imputed to silvo_se
    silvo_se = silvo_se_imputed      
  ) |> 
  as.data.frame()|> 
  # RECALCALCULATE STANDARD DEVIATION FOR IMPUTED DATASET
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


# Non-imputed dataset (remove geometry if necessary)
non_imp_dataset <- database_clean_sd |> 
  as.data.frame() |> 
  # RECALCALCULATE STANDARD DEVIATION 
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
```



```{r}
# Calculate standard deviations from standard errors and sample sizes
imp_pmm_best 

```












```{r}
# Visualize differences in categorical distributions
inspect_cat(imp_pmm_best, non_imp_dataset) %>% show_plot()

# Visualize differences in numerical distributions
# Select numeric columns
imp_pmm_best_numeric <- imp_pmm_best %>% select(where(is.numeric))
non_imp_dataset_numeric <- non_imp_dataset %>% select(where(is.numeric))

# Check column counts
if (ncol(imp_pmm_best_numeric) == 0 || ncol(non_imp_dataset_numeric) == 0) {
  stop("One or both datasets have no numeric columns to compare.")
}

# Custom function to filter columns with valid variance
filter_valid_columns <- function(df) {
  df %>% select(where(~ !all(is.na(.)) && var(., na.rm = TRUE) > 0))
}

# Apply the filter to remove problematic columns
imp_pmm_best_numeric <- filter_valid_columns(imp_pmm_best_numeric)
non_imp_dataset_numeric <- filter_valid_columns(non_imp_dataset_numeric)

# Combine datasets for comparison
imp_pmm_best_numeric$dataset <- "imputed"
non_imp_dataset_numeric$dataset <- "non-imputed"
combined_data <- bind_rows(imp_pmm_best_numeric, non_imp_dataset_numeric)

# Melt into long format for ggplot
combined_long <- combined_data %>%
  pivot_longer(cols = -dataset, names_to = "variable", values_to = "value")

# Plot distributions for all variables
ggplot(combined_long, aes(x = value, fill = dataset)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Density Plots for Numeric Variables", x = "Value", y = "Density") +
  theme_minimal()

# Check for column-wise similarity
inspect_cor(imp_pmm_best, non_imp_dataset) %>% show_plot()

# Missingness comparison
inspect_na(imp_pmm_best, non_imp_dataset) %>% show_plot()
```


```{r}
# SAVING TWO VERSIONS OF PREPROCESSED DATA AS RDS

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# Ensure the output directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save the non-imputed dataset as RDS
saveRDS(non_imp_dataset,
        file = file.path(output_dir, "non_imp_dataset.rds"))


# Save the best-imputed dataset as RDS
saveRDS(imp_pmm_best,
        file = file.path(output_dir, "imp_pmm_best_dataset.rds"))

# Print confirmation messages
cat("Datasets saved successfully:\n")
cat("- Non-imputed dataset: non_imp_dataset.rds\n")
cat("- Best-imputed dataset: imp_pmm_best_dataset.rds\n")
```





#############
# STEP 7
##########################################################################################################################################
CALCULATING EFFECT SIZES FOR IMPUTED AND NON-IMPUTED DATASETS
##########################################################################################################################################

Loading datasets
```{r}
# Load the non-imputed and imputed datasets
non_imp_dataset <- readRDS(file.path(output_dir, "non_imp_dataset.rds"))

# Load the imputed dataset
imp_pmm_best <- readRDS(file.path(output_dir, "imp_pmm_best_dataset.rds"))
```



```{r}
non_imp_dataset |> glimpse()
imp_pmm_best |> glimpse()
```




##########################################################################################################################################
EFFECT SIZE CALCULATION
##########################################################################################################################################

```{r}
# Function for data preparation and effect size calculation

# This function takes a dataset as input and applies several transformations
# to clean, filter, and prepare it for effect size calculation.
prep_dataset_for_rom <- function(data) {
  data %>%
    # Step 1: Filter out rows where the standard errors are zero or negative
    # - Standard errors (SE) must be positive for valid statistical calculations.
    # - Removing these rows prevents mathematical errors in subsequent calculations (e.g., division by zero).
    filter(silvo_se > 0, control_se > 0) %>%
    
    # Step 2: Adjust the sign of mean values for specific response variables
    # - For variables like "Greenhouse gas emissions," "Pests and Diseases," and "Water quality,"
    #   lower values are considered better (e.g., lower emissions or fewer pests).
    # - We negate the mean values to reflect this interpretation correctly.
    mutate(
      silvo_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"),
                          -silvo_mean, silvo_mean),
      control_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"),
                            -control_mean, control_mean)
    ) %>%
    
    # Step 3: Exclude specific response variables from analysis
    # - "Soil water content" is excluded due to inconsistent data or limited measurements.
    filter(response_variable != "Soil water content") %>%
    
    # Step 4: Remove rows with missing values in key columns
    # - Ensures that the data is complete for effect size calculation.
    # - Excludes rows where any of the following are missing: mean, sample size, or standard error for both groups.
    filter(!is.na(silvo_mean) & !is.na(control_mean) &
           !is.na(silvo_n) & !is.na(control_n) &
           !is.na(silvo_se) & !is.na(control_se)) %>%
    
    # Step 5: Calculate standard deviations from standard errors and sample sizes
    # - Standard deviation (SD) is calculated using the formula: SD = SE * sqrt(n)
    # - This step is necessary for effect size calculations, which often require SDs rather than SEs.
    mutate(
      silvo_sd = silvo_se * sqrt(silvo_n),
      control_sd = control_se * sqrt(control_n),
      
      # Step 6: Shift mean values to be positive
      # - We calculate a shift value (min_value_shift) based on the absolute minimum value
      #   of the means, ensuring all values are positive.
      # - This shift is necessary for certain transformations (e.g., log transformations),
      #   which require positive inputs.
      min_value_shift = abs(min(c(silvo_mean, control_mean), na.rm = TRUE)) + 1,
      
      # Apply the shift to the mean values for both groups
      silvo_mean = silvo_mean + min_value_shift,
      control_mean = control_mean + min_value_shift
    ) %>%
    
    # Step 7: Reorder columns for better readability and organization
    # - Places important columns (e.g., identifiers, means, SEs, SDs, sample sizes) at the front of the data frame.
    relocate(id_article, response_variable, measured_metrics, measured_unit,
             silvo_mean, silvo_se, silvo_sd, silvo_n,
             control_mean, control_se, control_sd, control_n) %>%
    
    # Step 8: Sort the data by article ID and response variable for consistency
    # - Sorting helps ensure that the data is organized and facilitates easier inspection and analysis.
    arrange(id_article, response_variable)
}
```

```{r}
# Apply the data preparation function to both non-imputed and imputed datasets
# - This creates cleaned and prepared datasets for effect size calculation.
non_imp_data_prep <- prep_dataset_for_rom(non_imp_dataset)
imp_data_prep <- prep_dataset_for_rom(imp_pmm_best)
```

```{r}
# Verify consistency between imputed _se and recalculated _sd
verify_sd_calculation <- function(data) {
  data <- data %>%
    mutate(
      recalculated_silvo_sd = silvo_se * sqrt(silvo_n),
      recalculated_control_sd = control_se * sqrt(control_n),
      silvo_sd_diff = abs(silvo_sd - recalculated_silvo_sd),
      control_sd_diff = abs(control_sd - recalculated_control_sd)
    ) %>%
    filter(silvo_sd_diff > 1e-5 | control_sd_diff > 1e-5) # Filter rows with differences

  return(data)
}

# Apply to both non-imputed and imputed datasets
non_imp_inconsistencies <- verify_sd_calculation(non_imp_data_prep)
imp_inconsistencies <- verify_sd_calculation(imp_data_prep)

# Visualize inconsistencies (if any)
list(
  non_imputed = non_imp_inconsistencies,
  imputed = imp_inconsistencies
)

```


```{r}
# Generic function for effect size calculation using `escalc()`

# This function can be applied to both imputed and non-imputed datasets.

calculate_effect_sizes <- function(data, measure = "ROM") {
  # Check if the required columns are present in the dataset
  required_columns <- c("silvo_mean", "silvo_sd", "silvo_n",
                        "control_mean", "control_sd", "control_n",
                        "id_article", "experiment_year")
  
  if (!all(required_columns %in% names(data))) {
    stop("The dataset is missing one or more required columns.")
  }
  
  # Calculate effect sizes using `escalc()`
  result <- escalc(
    measure = measure,           # Specify the effect size measure (default is "ROM").
    
    # Experimental group (silvo_) parameters:
    m1i = silvo_mean,            # Mean of the experimental (silvo) group.
    sd1i = silvo_sd,             # Standard deviation of the experimental (silvo) group.
    n1i = silvo_n,               # Sample size of the experimental (silvo) group.
    
    # Control group (control_) parameters:
    m2i = control_mean,          # Mean of the control group.
    sd2i = control_sd,           # Standard deviation of the control group.
    n2i = control_n,             # Sample size of the control group.
    
    # Study labels for identification:
    slab = paste(id_article, ", ", experiment_year, sep = ""),
    
    # The input dataset:
    data = data
  ) %>%
    as.data.frame()              # Convert the result to a data frame for easier handling.
  
  # Return the resulting data frame with calculated effect sizes
  return(result)
}
```

```{r}
# Inspect the input data to `escalc()`
imp_data_prep %>%
  select(id_article, response_variable, silvo_mean, silvo_sd, silvo_n, control_mean, control_sd, control_n) %>%
  filter(is.na(silvo_sd) | is.na(control_sd) | silvo_sd <= 0 | control_sd <= 0)
```

```{r}
# Apply the function to both non-imputed and imputed datasets

non_imp_data_rom <- calculate_effect_sizes(non_imp_data_prep,
                                           # SMD (Standardized Mean Difference)
                                           # ROM (Log-Transformed Ratio)
                                           measure = "ROM") 

imp_data_rom <- calculate_effect_sizes(imp_data_prep,
                                       # SMD (Standardized Mean Difference)
                                       # ROM (Log-Transformed Ratio)
                                       measure = "ROM")
```

```{r}
imp_data_rom |> glimpse()
```

```{r}
# Create the boxplot for the effect size (yi)
# Order response variables by descending median effect size
imp_data_rom_reorder <- imp_data_rom %>%
  mutate(response_variable = fct_reorder(response_variable, yi, .fun = median, .desc = FALSE))

# Create the boxplot with ordered response variables
boxplot_raw_effect_size <- imp_data_rom_reorder |> 
  ggplot(aes(y = response_variable, x = yi, fill = response_variable)) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", size = 0.8) + # Add red dotted line at x = 0
  geom_boxplot(alpha = 0.7, outlier.size = 1) +
  labs(
    title = "Raw Effect Sizes (yi) Across Response Variables",
    x = "Raw Effect Size (yi)",
    y = "Response Variable"
  ) +
  scale_x_continuous(
    trans = pseudo_log_trans(sigma = 0.1), # Apply pseudo-log transformation
    breaks = c(0, 0.1, 1, 10, 100),       # Custom breaks
    labels = c("0", "0.1", "1", "10", "100") # Relatable labels
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 12), # Adjust y-axis text size
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels
    legend.position = "none"
  )

boxplot_raw_effect_size

```


Save raw effect size plot of each response variable

```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 120),        # Increase title size
    plot.subtitle = element_text(size = 70),
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 100),       # Increase axis title size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 100),
    legend.position = "none",
    axis.text.x = element_text(size = 100,
                               angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
boxplot_raw_effect_size <- boxplot_raw_effect_size + theme_custom


# Save the enhanced plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
ggsave(
  filename = file.path(output_dir, "boxplot_raw_effect_size.png"),
  plot = boxplot_raw_effect_size,
  width = 16, height = 8, dpi = 600,
  bg = "white"
)
```














```{r}
# Prepare the data for plotting
data_long <- imp_data_rom %>%
  select(response_variable, control_mean, silvo_mean) %>%
  pivot_longer(
    cols = c(control_mean, silvo_mean),
    names_to = "Group",
    values_to = "Mean"
  )

# Create the boxplot with a log-transformed y-axis
ggplot(data_long, aes(x = Group, y = Mean, fill = Group)) +
  geom_boxplot(alpha = 0.7, outlier.size = 1) +
  facet_wrap(~ response_variable, scales = "free") +
  labs(
    title = "Comparison of Mean Values for Control and Silvo Groups by Response Variable (Log Scale)",
    x = "Group",
    y = "Mean Value (Log Scale)"
  ) +
  scale_y_log10() +  # Logarithmic transformation for the y-axis
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )
```

```{r}
# Boxplot comparison for control and silvo groups
imp_data_rom_long <- imp_data_rom %>%
  select(response_variable, control_mean, silvo_mean) %>%
  pivot_longer(cols = c(control_mean, silvo_mean), names_to = "Group", values_to = "Mean")

ggplot(imp_data_rom_long, aes(x = Group, y = Mean, fill = Group)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~ response_variable, scales = "free") +  # Facet by response variable
  scale_y_log10() +  # Logarithmic transformation for the y-axis
  labs(
    title = "Distribution of Means for Control and Silvo Groups",
    x = "Group",
    y = "Mean Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```



Identify Extreme Variances
For response variables like "Crop yield" with extreme variances, identify rows with unusually high or low values in the variance column (vi).

```{r}
# Summary statistics for variance
summary(imp_data_rom$vi)

# Identify rows with extreme variances
extreme_variance_rows <- imp_data_rom %>%
  filter(vi > quantile(vi, 0.95) #| vi < quantile(vi, 0.05)
         ) %>%
  arrange(desc(vi)) |> 
  relocate(yi, vi, id_article, response_variable,
           silvo_mean, control_mean, 
           silvo_se, control_se,
           silvo_sd, control_sd, 
           silvo_n, control_n)

extreme_variance_rows |> str()

# Set high and low variance thresholds (e.g., 95th and 5th percentiles)
high_variance_threshold <- quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Filter rows with extreme variances
extreme_variance_articles <- imp_data_rom %>%
  filter(vi > high_variance_threshold #| vi < low_variance_threshold
         ) %>%
  summarise(
    num_obs = n(),
    avg_variance = mean(vi, na.rm = TRUE),
    max_variance = max(vi, na.rm = TRUE),
    min_variance = min(vi, na.rm = TRUE),
    response_variables = paste(unique(response_variable), collapse = ", ")
  ) %>%
  arrange(desc(avg_variance))

# Display the summary of articles with high variance
extreme_variance_articles |> str()

extreme_variance_rows
```
```{r}
# Identify high-variance observations
high_variance_rows <- imp_data_rom %>%
  filter(vi > quantile(vi, 0.95, na.rm = TRUE)) %>%
  select(yi, vi,
         id_obs, id_article, response_variable, 
         silvo_mean, control_mean, 
         silvo_se, control_se,
         silvo_sd, control_sd, 
         silvo_n, control_n)

high_variance_row_isolated <- high_variance_rows |> 
  relocate(yi, vi, id_article, id_obs, response_variable,
           silvo_mean, control_mean, 
           silvo_se, control_se,
           silvo_sd, control_sd, 
           silvo_n, control_n)

high_variance_row_isolated |> glimpse()
```

```{r}
# Define thresholds for high and non-high variance
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Create separate datasets
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold | vi < low_variance_threshold)

non_high_variance_data <- imp_data_rom %>%
  filter(!(vi > high_variance_threshold | vi < low_variance_threshold))
```

```{r}
# Plot for high variance
high_variance_plot <- ggplot(high_variance_data, aes(x = id_article, y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "red", alpha = 0.7) +
  labs(title = "High Variance Observations", x = "Article ID", y = "Variance (vi)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  # Remove legend for better visualization
  theme(legend.position = "none")

# Plot for non-high variance
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = id_article, y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(title = "Non-High Variance Observations", x = "Article ID", y = "Variance (vi)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine plots side by side
combined_plot <- high_variance_plot + non_high_variance_plot + 
  plot_layout(ncol = 2)

# Display the combined plot
print(combined_plot)
```

```{r}
# High variance plot for individual observations with increased jitter
high_variance_plot <- ggplot(high_variance_data, aes(x = as.factor(id_article), y = vi, color = response_variable)) +
  geom_point(position = position_jitter(width = 0.3, height = 0), size = 3, alpha = 0.7) +
  geom_text_repel(
    aes(label = id_obs),
    position = position_jitter(width = 0.3, height = 0),
    size = 3,
    max.overlaps = Inf
  ) +
  labs(
    title = "High Variance Observations (Individual, Jittered)",
    x = "Article ID",
    y = "Variance (vi) [pseudo log transformed]",
    color = "Response Variable"
  ) +
  scale_color_manual(values = global_palette) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.1, 1, 10, 30),
    labels = c("0", "0.1", "1", "10", "30")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Non-high variance plot with aggregated boxplots
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = as.factor(id_article), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(
    title = "Non-High Variance Observations (Boxplots)",
    x = "Article ID",
    y = "Variance (vi) [pseudo log transformed]",
    fill = "Response Variable"
  ) +
  scale_fill_manual(values = global_palette) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.01, 0.1, 1),
    labels = c("0", "0.01", "0.1", "1")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Combine the two plots
combined_plot_high_var_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2, widths = c(1, 1)) &
  theme(
    plot.margin = margin(10, 10, 10, 10)
  )

# Display the combined plot
print(combined_plot_high_var_plot)
```

Save the high-variance plot
```{r}
# Saving manually from the 'Plots -> Export -> Save as image -> dimension (2000::800)'
```

```{r}
# Calculate variance contributions from SE columns
extreme_variance_analysis <- extreme_variance_rows %>%
  mutate(
    silvo_variance = silvo_se^2 / silvo_n,        # Variance from silvo SE
    control_variance = control_se^2 / control_n,  # Variance from control SE
    total_pre_escalc_variance = silvo_variance + control_variance  # Combined variance
  ) %>%
  arrange(desc(vi)) %>%  # Sort by high vi
  select(
    yi, vi, id_article, response_variable,
    silvo_mean, control_mean, 
    silvo_se, control_se, silvo_sd, control_sd, 
    silvo_n, control_n, total_pre_escalc_variance, silvo_variance, control_variance
  )

extreme_variance_analysis |> glimpse()
```

```{r}
discrepancies <- extreme_variance_analysis %>%
  mutate(discrepancy = vi - total_pre_escalc_variance) %>%
  arrange(desc(discrepancy))

print(discrepancies)
```
```{r}
extreme_se <- extreme_variance_analysis %>%
  filter(silvo_se > quantile(silvo_se, 0.75) | control_se > quantile(control_se, 0.75))

print(extreme_se)
```
```{r}
# Recompute vi for rows with high variance
recomputed_vi <- escalc(
  measure = "ROM", 
  m1i = silvo_mean, sd1i = silvo_sd, n1i = silvo_n,
  m2i = control_mean, sd2i = control_sd, n2i = control_n,
  data = extreme_variance_analysis
)

# Compare recomputed vi with original vi
comparison <- extreme_variance_analysis %>%
  mutate(recomputed_vi = recomputed_vi$vi) %>%
  select(yi, vi, recomputed_vi, total_pre_escalc_variance)

print(comparison)
```









####################################################################################################################################################

Heterogeneity Analysis
Compare the heterogeneity statistics between the meta-analyses conducted on the imputed and non-imputed datasets. This will help you understand if the imputation has influenced the heterogeneity of the studies.
####################################################################################################################################################

```{r}
# Prepare the original data
original_data_x <- non_imp_data_rom %>%
  select(id_article, id_obs, exp_id, id_obs, response_variable, 
         silvo_se, control_se, yi, vi) |> 
  mutate(data_source = "Original") |> 
  as.data.frame() 

imputed_data_y <- imp_data_rom |> 
  select(id_article, id_obs, exp_id, id_obs, response_variable, 
         silvo_se, control_se, yi, vi) |> 
  mutate(data_source = "Imputed") 

# Combine the original and imputed data
combined_data <- bind_rows(original_data_x, imputed_data_y)

# Join the original and imputed data to directly compare
comparison_data <- original_data_x %>%
  full_join(imputed_data_y, by = c("id_article", "exp_id", "id_obs", "response_variable"), 
            suffix = c("_original", "_imputed")) %>%
  distinct()

# Advarsel: Detected an unexpected many-to-many relationship between `x` and `y`.

comparison_data |> glimpse()
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Fit random-effects models on both datasets
rma_non_imp <- rma(yi = yi_original, vi = vi_original, data = comparison_data)
rma_imp <- rma(yi = yi_imputed, vi = vi_imputed, data = comparison_data)

# Print heterogeneity statistics
cat("Non-Imputed Data - Heterogeneity (I^2):", rma_non_imp$I2, "\n")
cat("Non-Imputed Data - Between-study variance (tau^2):", rma_non_imp$tau2, "\n")

cat("Imputed Data - Heterogeneity (I^2):", rma_imp$I2, "\n")
cat("Imputed Data - Between-study variance (tau^2):", rma_imp$tau2, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 4.055149 secs
# Advarsel: 186 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 10.13463 
# Non-Imputed Data - Between-study variance (tau^2): 1.468685e-06 
# Imputed Data - Heterogeneity (I^2): 99.93572 
# Imputed Data - Between-study variance (tau^2): 0.009922055 
# Time difference of 4.055149 secs

# Last go (01/01-2025)
# Time difference of 5.424392 secs
# Advarsel: 176 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93018 
# Imputed Data - Between-study variance (tau^2): 0.01053153 
# Time difference of 5.424392 secs

# Last go (02/01-2025)
# Time difference of 6.113353 secs
# Advarsel: 177 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93201 
# Imputed Data - Between-study variance (tau^2): 0.01083873 
# Time difference of 6.113353 secs

# Last go (04/01-2025)
# Advarsel: 212 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.89571 
# Imputed Data - Between-study variance (tau^2): 0.0174592 
# Time difference of 10.74234 secs

# Last go (04/01-2025)
# Advarsel: 184 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93201 
# Imputed Data - Between-study variance (tau^2): 0.01006262 
# Time difference of 5.507228 secs


# Last go (12/01-2025)
# Advarsel: 184 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93201 
# Imputed Data - Between-study variance (tau^2): 0.01006262 
# Time difference of 3.647738 secs
```
 
 
 
Kolmogorov-Smirnov Test
The Kolmogorov-Smirnov (KS) test can be used to compare the distributions of effect sizes.

```{r}
# Kolmogorov-Smirnov test
ks_test_result <- ks.test(non_imp_data_rom$yi, imp_data_rom$yi)
print(ks_test_result)

# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.040734, p-value = 0.4179
# alternative hypothesis: two-sided

# Last go (01/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.030426, p-value = 0.7468
# alternative hypothesis: two-sided

# Last go (02/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.031966, p-value = 0.6897
# alternative hypothesis: two-sided

# Last go (04/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.025547, p-value = 0.897
# alternative hypothesis: two-sided

# Last go (05/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.032653, p-value = 0.6621
# alternative hypothesis: two-sided

# Last go (12/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.032653, p-value = 0.6621
# alternative hypothesis: two-sided
```

The Kolmogorov-Smirnov (KS) test was used to compare the distributions of effect sizes (ROM) between the non-imputed and imputed datasets. The test yielded a statistic \(D = 0.0327\) with a p-value of 0.6621, indicating no significant difference between the two distributions. This suggests that the imputed and non-imputed datasets share a similar underlying distribution, and the null hypothesis cannot be rejected. Although a warning was raised about approximate p-values due to ties in the data, the large sample size minimizes the impact of this issue on the overall interpretation.

The random-effects models revealed notable differences between the two datasets. In the non-imputed dataset, heterogeneity was low (I² = 9.35%) with a minimal between-study variance (\(\tau^2 = 1.44 \times 10^{-6}\)), indicating that the variability among studies was small. In contrast, the imputed dataset showed extremely high heterogeneity (I² = 99.93%) and a substantially increased between-study variance (\(\tau^2 = 0.0101\)). These results suggest that the imputation process introduced additional variability not present in the original dataset. Both models also produced warnings about large ratios of the smallest to largest sampling variances, highlighting the potential instability of the results, particularly in the imputed dataset.

The KS test results indicate that the imputation process effectively preserved the distribution of effect sizes. This supports the reliability of the imputation approach and ensures that comparisons made within the meta-analysis remain meaningful. However, the increased heterogeneity in the imputed dataset signals the introduction of additional variability, which should be carefully considered when interpreting the results.

Overall, these findings suggest that while the imputation process successfully maintained the distributional properties of the data, it also added variability that influenced the heterogeneity and between-study variance. This has implications for the conclusions of the meta-analysis, as the imputed dataset may present a broader range of uncertainty compared to the original data. It is essential to transparently report these differences, perform sensitivity analyses, and interpret the results cautiously to account for the potential impact of imputation on the overall findings.
 
```{r}
# Descriptive statistics for non-imputed data
summary_non_imp <- non_imp_data_rom %>%
  summarise(
    mean_yi = mean(yi, na.rm = TRUE),
    median_yi = median(yi, na.rm = TRUE),
    sd_yi = sd(yi, na.rm = TRUE)
  )

# Descriptive statistics for imputed data
summary_imp <- imp_data_rom %>%
  summarise(
    mean_yi = mean(yi, na.rm = TRUE),
    median_yi = median(yi, na.rm = TRUE),
    sd_yi = sd(yi, na.rm = TRUE)
  )

# Print summaries
summary_non_imp
summary_imp

# Last go (01/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02660392	0.002633719	0.2446765	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02660392	0.002633719	0.2446765

# Last go (02/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02767394	0.003205885	0.2501374	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02369684	0.001916168	0.2386678	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02767394	0.003205885	0.2501374	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.03006166	0.003048475	0.2482813

# Last go (05/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02767394	0.003205885	0.2501374
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02423623	0.00187394	0.2387344	

# Last go (12/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02767394	0.003205885	0.2501374		
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02423623	0.00187394	0.2387344		
```
 
```{r}
# Density plot for non-imputed data
density_plot_non_imp <- ggplot(non_imp_data_rom, aes(x = yi)) +
  geom_density(fill = "#0072B2", alpha = 0.5) +
  labs(title = "Density Plot: Non-Imputed Data",
       x = "Effect Size (yi)", y = "Density") +
  theme_minimal() +
  # Use pseudo-log transformation for x-axis
  scale_x_continuous(trans = pseudo_log_trans(sigma = 0.1))

# Density plot for imputed data
density_plot_imp <- ggplot(imp_data_rom, aes(x = yi)) +
  geom_density(fill = "#E69F00", alpha = 0.5) +
  labs(title = "Density Plot: Imputed Data",
       x = "Effect Size (yi)", y = "Density") +
  theme_minimal() +
  # Use pseudo-log transformation for x-axis
  scale_x_continuous(trans = pseudo_log_trans(sigma = 0.1))

# Boxplot for non-imputed data
boxplot_non_imp <- ggplot(non_imp_data_rom, aes(y = yi)) +
  geom_boxplot(fill = "#0072B2", alpha = 0.5) +
  labs(title = "Boxplot: Non-Imputed Data",
       y = "Effect Size (yi)") +
  theme_minimal() +
  # Use pseudo-log transformation for y-axis
  scale_y_continuous(trans = pseudo_log_trans(sigma = 0.1))

# Boxplot for imputed data
boxplot_imp <- ggplot(imp_data_rom, aes(y = yi)) +
  geom_boxplot(fill = "#E69F00", alpha = 0.5) +
  labs(title = "Boxplot: Imputed Data",
       y = "Effect Size (yi)") +
  theme_minimal() +
  # Use pseudo-log transformation for y-axis
  scale_y_continuous(trans = pseudo_log_trans(sigma = 0.1))

# Arrange the plots in a 2x2 layout
(density_plot_non_imp | boxplot_non_imp) /
(density_plot_imp | boxplot_imp)

```


##########################################################################################################################################
ASSESSING CORRELATION BETWEEN RESPONSE VARIABLES
##########################################################################################################################################

```{r}
non_imp_data_rom |> glimpse()
imp_data_rom |> glimpse()
```

```{r}
##########################################################################
# Visual Diagnostics for Predictor Redundancy and Model Fit
##########################################################################

# Set up data
# Testing on the imputed dataset
moderators_data <- imp_data_rom %>%
  select(tree_type, crop_type, age_system, season, soil_texture) %>%
  mutate(across(everything(), as.factor)) %>%
  as.data.frame()

##########################################################################
# Fit a linear model with moderators
##########################################################################

lm_model <- lm(yi ~ tree_type + crop_type + age_system + season + soil_texture, 
               data = imp_data_rom)
```

```{r}
##########################################################################
# 1. Correlation Analysis
##########################################################################
# Convert categorical variables to numeric for correlation matrix
moderators_numeric <- model.matrix(~ . + 0, data = moderators_data) %>% as.data.frame()

# Calculate correlation matrix
cor_matrix <- cor(moderators_numeric, use = "pairwise.complete.obs")

# Visualize correlation matrix
corrplot::corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, title = "Correlation Matrix of Moderators")
```

```{r}
##########################################################################
# 2. Variance Inflation Factor (VIF)
##########################################################################

# Calculate VIF for predictors
vif_values <- car::vif(lm_model)

# Print VIF values
print(vif_values)

# Plot VIF values
barplot(vif_values, main = "Variance Inflation Factors (VIF)",
        col = "blue", horiz = TRUE, las = 1, xlab = "VIF Value")
```

The Variance Inflation Factor (VIF) helps identify if predictors in a regression model are too similar or correlated with each other. When predictors are highly related, it can make it difficult to determine the individual impact of each predictor, as their effects overlap. VIF measures how much the variance of a predictor's coefficient is inflated due to this overlap.

For categorical variables with multiple levels, the Generalized VIF (GVIF) is used. GVIF adjusts for the number of levels in a categorical variable, providing a way to compare them with other predictors. A scaled version of GVIF helps interpret these adjusted values more easily.

In this analysis, the predictors all have low VIF values. The largest adjusted GVIF is 1.17 for soil texture, which is far below the commonly used thresholds of 5 or 10 for concern. This indicates there is no significant multicollinearity among the predictors, meaning they are not overly correlated with one another.

Since none of the predictors exhibit high VIF values, no corrective actions are needed, such as removing variables or combining them into fewer components. The predictors are providing independent and valuable information to the model, and the results can be interpreted without concerns about redundancy or instability.

In summary, the VIF analysis confirms that the model's predictors are sufficiently independent and do not cause issues with multicollinearity. This ensures the model's results are clear, reliable, and interpretable.






```{r}
# Convert categorical variables to factors (if not already factors)
predictors <- imp_data_rom %>%
  select(tree_age, crop_type, tree_type, soil_texture, experiment_year, alley_width) %>%
  mutate(across(where(is.character), as.factor))

# Create dummy variables for categorical predictors
predictors_numeric <- model.matrix(~ . - 1, data = predictors) %>%  # -1 to exclude the intercept
  as.data.frame()

# Calculate pairwise correlations
correlation_matrix <- cor(predictors_numeric, use = "pairwise.complete.obs")

# Visualize the correlation matrix
corrplot::corrplot(correlation_matrix, method = "circle")

# Visualize correlations with a heatmap
heatmap(correlation_matrix, symm = TRUE)

# PCA
pca_results <- prcomp(predictors_numeric, scale. = TRUE)
summary(pca_results)
biplot(pca_results)
```

```{r}
# Convert categorical variables to factors and create dummy variables
predictors <- imp_data_rom %>%
  select(tree_age, crop_type, tree_type, soil_texture, experiment_year, alley_width) %>%
  mutate(across(where(is.character), as.factor))

# Create numeric predictors with dummy variables
predictors_numeric <- model.matrix(~ . - 1, data = predictors) %>%
  as.data.frame()

# Ensure the response is aligned with predictors
# Keep only rows where both predictors and response are not missing
aligned_data <- imp_data_rom %>%
  select(tree_age, crop_type, tree_type, soil_texture, experiment_year, alley_width, yi) %>%
  drop_na()

# Split aligned data into predictors and response
predictors_numeric <- model.matrix(~ . - 1, data = aligned_data %>% 
                                     select(-yi)) %>% 
  as.data.frame()


response_filled <- aligned_data$yi

# Fit the Random Forest model
library(randomForest)
rf_model <- randomForest(x = predictors_numeric, y = response_filled, importance = TRUE)

# Display feature importance
varImpPlot(rf_model)
```

The results from the random forest analysis and the correlation matrix provide insight into how the predictors (tree type, crop type, age system, season, and soil texture) influence the variability in your dataset and how they relate to each other.

The random forest model was employed to evaluate the relative importance of variables considered as moderators in the meta-regression model (`rma.mv`). Random forest is particularly suitable for this purpose due to its ability to handle complex interactions and non-linear relationships between predictors, making it an effective tool for identifying which moderators most strongly influence the response variable. The model assessed the importance of these moderators using two key metrics: the percentage increase in mean squared error (%IncMSE) and increase in node purity (IncNodePurity). These metrics reflect how much each moderator improves the model's predictive power and reduces uncertainty.

###############################################

The results highlight tree age and experiment year as the most influential moderators. These variables significantly impact both %IncMSE and IncNodePurity, indicating their critical role in explaining variations in the response variable within the meta-analytic framework. Their importance suggests that temporal factors, such as the age of the tree systems and the year of the experiments, are vital in shaping the observed outcomes across studies.

Moderators related to soil texture, such as sand and silt, were also identified as moderately important, reflecting the role of soil properties in influencing the outcomes under study. Tree type and crop type, including specific categories like "Fruit, nut & other" and "Legume," were somewhat less influential but still contributed meaningfully to explaining variability in the response. Their inclusion underscores the importance of agroforestry and crop-specific characteristics as moderators in the meta-regression.

Variables like alley width and certain crop or tree types, such as tuber/root crops and timber trees, showed relatively lower importance. This indicates that their moderating effects are less pronounced in this context, suggesting they may not need to be prioritized in the meta-regression model.

The random forest results emphasize the hierarchy of moderator importance, with tree age and experiment year standing out as dominant predictors. This refined understanding can improve the meta-regression model by focusing on the most impactful moderators, simplifying the model structure while maintaining its explanatory and predictive power.

###########################################

The correlation matrix reveals the relationships between predictors. Strong correlations (shown as larger, darker circles) suggest that some predictors may overlap in the information they provide. For example, if "tree type" and "crop type" have a high positive or negative correlation, this could mean they are not entirely independent, potentially leading to redundancy in the model. This aligns with earlier findings of multicollinearity, where predictors interfere with each other.

From the PCA results, the first three principal components (PC1, PC2, and PC3) explain approximately 79% of the variance in the data. This means most of the variation in your dataset can be summarized using fewer dimensions, reducing complexity while retaining the critical patterns. The decreasing proportion of variance explained by PC4 and PC5 suggests these components add less unique information.

Overall, while "tree type" and "crop type" seem to be the most influential predictors, their high correlation with other variables might complicate the interpretation. You should consider techniques like feature selection, dimensionality reduction, or removing redundant variables to improve model clarity and performance.


###########################################


The results of the random forest analysis reveal important limitations and implications for reporting and communicating overall meta-analysis findings. By identifying tree age and experiment year as the dominant moderators, the analysis highlights a key limitation of heterogeneity within the dataset. This underscores the challenge of attributing outcomes to specific factors when a few moderators exert disproportionate influence. Such dominance may mask the contributions of less impactful moderators and complicate the interpretation of their individual roles in the meta-analysis.

One limitation lies in the reliance on moderators that capture temporal or environmental variability, such as experiment year or soil texture. These factors, while essential, may vary considerably across studies, making it difficult to disentangle their effects from study-specific conditions. Additionally, the relatively lower importance of certain moderators, such as alley width or specific tree and crop types, raises questions about whether these variables are consistently measured or reported across studies. Inconsistent data collection or reporting practices may contribute to their diminished influence, emphasizing the importance of standardized reporting in future studies.

Communicating the meta-analysis results requires careful consideration of these limitations. Overemphasis on the dominant moderators, such as tree age, may oversimplify the nuanced interactions present in agroforestry systems. Researchers and stakeholders must be cautious not to generalize findings without acknowledging the variability that less prominent moderators can introduce. This is particularly relevant when translating findings into practical recommendations, as the applicability of results to diverse settings may be constrained.

The findings also highlight the need for transparency when reporting results. Explicitly addressing the hierarchical importance of moderators, as identified by the random forest analysis, can help contextualize the results and guide readers in understanding the limitations of the analysis. Clear communication about the variability and potential biases introduced by dominant moderators is essential for ensuring that the conclusions of the meta-analysis are interpreted appropriately and applied responsibly in decision-making contexts.

###########################################


```{r}
# Prepare the data
# Convert categorical variables to factors
imp_data_rom <- imp_data_rom %>%
  mutate(across(where(is.character), as.factor))

# Define the predictors and response
predictors <- imp_data_rom %>%
  select(tree_age, crop_type, tree_type, soil_texture, experiment_year, alley_width)


response <- imp_data_rom$yi

# Combine predictors and response into a single dataset
complete_data <- cbind(predictors, response = response) %>%
  na.omit() # Remove rows with missing values to ensure compatibility


# complete_data |> str()
# 'data.frame':	808 obs. of  7 variables:
#  $ tree_age       : num  2 2 2 9 9 9 9 8 8 8 ...
#  $ crop_type      : Factor w/ 3 levels "Cereal","Legume",..: 1 1 1 1 1 1 1 1 1 1 ...
#  $ tree_type      : Factor w/ 3 levels "Biomass","Fruit,nut & other",..: 2 2 2 2 2 2 2 1 1 1 ...
#  $ soil_texture   : Factor w/ 3 levels "Clay","Sand",..: 1 1 1 1 1 1 1 3 3 3 ...
#  $ experiment_year: Date, format: "2011-01-01" "2011-01-01" "2011-01-01" "2011-01-01" ...
#  $ alley_width    : Factor w/ 2 levels "Narrow","Wide": 2 2 2 2 2 2 2 2 2 2 ...
#  $ response       : num  0.633 0.199 0.771 0.137 0.182 ...
#  - attr(*, "na.action")= 'omit' Named int [1:290] 329 330 331 332 333 334 335 336 337 338 ...
#   ..- attr(*, "names")= chr [1:290] "329" "330" "331" "332" ...

# Tidymodels Random Forest Workflow

# Step 1: Create a recipe for preprocessing
rf_recipe <- recipe(response ~ ., data = complete_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%  # Convert categorical predictors to dummies
  step_normalize(all_numeric_predictors())       # Normalize numeric predictors

# Step 2: Specify the Random Forest model
rf_spec <- rand_forest(
  mode = "regression",
  mtry = tune(),         # Number of predictors sampled at each split
  trees = 500,           # Number of trees
  min_n = tune()         # Minimum samples per node
) %>%
  set_engine("ranger", importance = "permutation")  # Use permutation importance

# Step 3: Define a workflow
rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_spec)

# Step 4: Create cross-validation folds
set.seed(123)

cv_splits <- vfold_cv(complete_data, v = 5)

# Step 5: Tune the model
rf_res <- tune_grid(
  rf_workflow,
  resamples = cv_splits,
  grid = 10,  # Number of parameter combinations to try
  metrics = metric_set(rmse, rsq)  # Evaluation metrics
)

# Step 6: Finalize the model with the best parameters
# Finalize the model with the best parameters
best_rf <- finalize_workflow(
  rf_workflow,
  select_best(rf_res, metric = "rmse") # Explicitly name the argument 'metric'
)

# Step 7: Fit the finalized model
final_rf_fit <- fit(best_rf, data = complete_data)

# Step 8: Compute and visualize variable importance
# Extract the fitted model
rf_final_model <- final_rf_fit %>% extract_fit_parsnip()

# Visualize Variable Importance
vip(rf_final_model) +
  ggtitle("Variable Importance from Random Forest Model") +
  theme_minimal()

```

##########################################################################
# 3. Stepwise Regression
##########################################################################

Stepwise regression iteratively adds or removes predictors from a model to optimize the Akaike Information Criterion (AIC), balancing goodness-of-fit with model complexity. Starting with an initial model, predictors are evaluated through forward selection (adding predictors) and backward elimination (removing predictors). This process identifies the most relevant variables while discarding redundant or insignificant ones, improving model interpretability and predictive performance. It mitigates multicollinearity and avoids overfitting by ensuring parsimony. However, results should be interpreted cautiously, as stepwise methods may exclude theoretically important predictors.

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Set up data (imputed dataset)
moderators_data <- imp_data_rom %>%
  select(tree_type, crop_type, age_system, season, soil_texture) %>%
  mutate(across(everything(), as.factor)) %>%
  as.data.frame()

##########################################################################
# Fit a linear model with moderators
lm_model <- lm(yi ~ tree_type + crop_type + age_system + season + soil_texture, 
               data = imp_data_rom)

##########################################################################
# Stepwise regression using AIC
stepwise_model <- step(lm_model, direction = "both", trace = TRUE)

# Summary of the stepwise regression model
summary(stepwise_model)



##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (12/01-2025)
# Time difference of  mins
```
The updated stepwise regression analysis assessed the relationship between moderators (tree type, crop type, age system, season, and soil texture) and the response variable (yi). The final model retained all predictors, as removing any would increase the AIC from its optimal value (-3147.24). This suggests that each predictor contributes, albeit to varying degrees, to the overall model.

Tree type displayed mixed effects. Timber trees significantly reduced the response variable compared to the reference group, indicating their distinct influence. Crop type emerged as a strong predictor, with legumes showing a positive and significant association with the response variable, whereas tuber, root, and other crops showed no significant effect. System age had a notable impact, with younger systems significantly increasing the response, suggesting they may offer greater benefits during earlier stages of development. Seasonal effects were also evident, with winter seasons linked to significantly lower response values, highlighting seasonal variations in observed effects. Soil texture showed weaker influence, with sand and silt not exhibiting significant effects, though silt approached significance, suggesting context-dependent influences.

The model explained 6.6% of the variance in the response variable, as indicated by the R-squared value. While this shows that the moderators contribute meaningfully, a large proportion of variability remains unexplained, signaling the potential importance of additional moderators or interactions not included in this analysis. Despite the low explanatory power, the model as a whole is statistically significant (F-statistic p-value = 2.46e-12), confirming that the predictors collectively have an impact on the response variable.

In summary, stepwise regression highlights key moderators such as crop type, tree type, age system, and season as important influencers of the response variable. However, the relatively low R-squared suggests the need for further refinement, either by including additional moderators, exploring interactions between variables, or considering non-linear relationships to better capture the complexity inherent in the dataset.


The findings from the stepwise regression analysis have several implications for reporting and communicating the results of the overall meta-analysis. These implications can shape how the relationships between moderators and the response variable are presented and how uncertainties or gaps in the analysis are conveyed.
**Highlighting Key Moderators**: The stepwise regression identified significant moderators, such as crop type, tree type, age system, and season. These findings suggest that these variables are pivotal in shaping the response variable and should be emphasized in the meta-analysis results. For instance, the positive association between legumes and the response variable or the negative effect of timber trees provides actionable insights that can be linked to specific ecological or management contexts.
**Acknowledging Context-Dependent Effects**: The weaker or non-significant effects of variables like soil texture and certain crop and tree types imply that their influence may vary across studies or contexts. This highlights the importance of discussing heterogeneity in the meta-analysis, emphasizing that not all moderators have uniform impacts. Including such nuance ensures the results are interpreted with an understanding of the variability in underlying datasets.
**Explaining Limited Variance Explained**: With the R-squared value at 6.6%, the stepwise regression indicates that the included moderators only explain a small proportion of the variance in the response variable. This limitation should be transparently communicated to avoid overstating the explanatory power of the meta-analysis. It suggests that additional factors—such as interactions between moderators, unmeasured variables, or study-level characteristics—may also influence the response.
**Interpreting Seasonal and Temporal Trends**: The significant seasonal and age system effects underscore the dynamic nature of the response variable over time and seasons. Communicating these findings can guide recommendations for management practices or experimental designs that consider seasonal or developmental variability.
**Need for Caution in Overgeneralization**: Given the low explanatory power, it is crucial to avoid overgeneralizing the findings across all agroforestry systems. Instead, the results should be framed as indicative of broader trends, with the acknowledgment that the model does not fully capture the complexity of the data.
**Incorporating Insights into Meta-Analysis Models**: The findings from the stepwise regression can inform the structure of the meta-analysis models, particularly in prioritizing the inclusion of significant moderators like crop type, tree type, and season. Interactions between these variables should also be explored to better understand their combined effects.
**Communicating Uncertainty**: The presence of unexplained variance should be communicated as a limitation in the meta-analysis. This uncertainty could be due to missing data, unmeasured moderators, or study-specific idiosyncrasies. Reporting this openly ensures the results are interpreted with appropriate caution.

In summary, the stepwise regression findings provide valuable insights into the roles of specific moderators, but they also highlight the complexity and limitations of the relationships in the dataset. This dual narrative—of meaningful trends and unexplained variability—should guide the reporting and communication of the meta-analysis results to ensure they are both impactful and scientifically rigorous.


##########################################################################
# 4. Principal Component Analysis (PCA)
##########################################################################

```{r}
# Perform PCA on the numeric moderator data
pca_results <- prcomp(moderators_numeric, scale. = TRUE)

pca_results |> str()

screeplot(pca_results, type = "lines", main = "Scree Plot for PCA")

# Calculate proportion of variance explained
explained_variance <- (pca_results$sdev^2) / sum(pca_results$sdev^2)

# Create a scree plot
plot(explained_variance, type = "b", pch = 19, col = "blue",
     xlab = "Principal Components", ylab = "Proportion of Variance Explained",
     main = "Scree Plot for PCA")

barplot(explained_variance, names.arg = paste0("PC", 1:length(explained_variance)),
        col = "skyblue", xlab = "Principal Components", ylab = "Proportion of Variance",
        main = "Variance Explained by Principal Components")

# Biplot of the first two components
biplot(pca_results, scale = 0, main = "PCA Biplot")
```

##########################################################################
# 5. Cross-Validation
##########################################################################

```{r}
# Define predictors and response
predictors <- imp_data_rom %>% 
  select(tree_type, crop_type, age_system, season, soil_texture) %>%
  mutate(across(everything(), as.factor)) %>% # Convert to factors first
  mutate(across(everything(), as.numeric))   # Convert to numeric
response <- imp_data_rom$yi

# Check the structure of predictors
str(predictors)

# Remove rows with missing values in predictors or response
complete_cases <- complete.cases(predictors, response)
predictors <- predictors[complete_cases, ]
response <- response[complete_cases]

# Create a train/test split
set.seed(123)
train_index <- caret::createDataPartition(response, p = 0.8, list = FALSE)
train_data <- predictors[train_index, ]
train_response <- response[train_index]
test_data <- predictors[-train_index, ]
test_response <- response[-train_index]

# Ensure training data has no missing values
if (anyNA(train_data) || anyNA(train_response)) {
  stop("Training data contains missing values. Please clean your data.")
}

# Train a linear model with cross-validation
cv_model <- caret::train(
  x = train_data, 
  y = train_response, 
  method = "lm", 
  trControl = caret::trainControl(method = "cv", number = 10)
)

# Evaluate on test set
predictions <- predict(cv_model, newdata = test_data)
results <- caret::postResample(predictions, test_response)

# Print cross-validation results
print(cv_model)
print("Performance on test data:")
print(results)
```

The cross-validation test evaluated how well the selected meta-analysis moderators—tree type, crop type, age system, season, and soil texture—explain the variability in the response variable, **yi**. The dataset, containing 1,098 observations, was cleaned to remove missing values, and numeric transformations were applied to categorical variables to enable linear regression analysis. The dataset was split into training (80%) and testing (20%) subsets, and 10-fold cross-validation was used to assess the model’s predictive performance.

The linear regression model produced an RMSE (Root Mean Square Error) of **0.2332** during cross-validation, indicating the average deviation of predictions from observed values. The MAE (Mean Absolute Error), which reflects the average size of prediction errors without considering direction, was **0.1223**. The R-squared value, at **0.0445**, revealed that only 4.5% of the variation in **yi** was explained by the moderators in the model, suggesting that their overall contribution to explaining the response variable was minimal.

When the model was applied to the independent test set, the results were consistent. The test set evaluation yielded an RMSE of **0.1805**, an MAE of **0.1122**, and an R-squared value of **0.0355**. These findings reaffirm that the model’s predictions are generally close to the observed values but that the moderators collectively explain only a small fraction of the variability in the response variable.

The low R-squared values in both the cross-validation and test phases indicate that while the moderators may have some role, they are not the primary determinants of the observed variability. This suggests that other unmeasured factors or interactions might have a more substantial influence on **yi**. 

The results imply that further refinement of the meta-analysis is needed, either by identifying additional moderators, exploring potential interactions among existing moderators, or employing more sophisticated models that can capture complex relationships. These findings should be clearly communicated to contextualize the limited explanatory power of the current moderators and emphasize the need for caution when interpreting their effects on the response variable.



#############
# STEP 8
##########################################################################################################################################
SAVING FINAL PREPROCESSED META-DATASETS (IMPUTED AND NON-IMPUTED) - THESE ARE USED FOR RMA.MV() MODELLING
##########################################################################################################################################


Saving the datasets that is used for the rma.mv() modelling

```{r}
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Ensure the output directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save as RDS files

## NON-IMPUTED
saveRDS(non_imp_data_rom,
        file = here::here(output_dir, "non_imp_data_rom.rds"))

## IMPUTED
saveRDS(imp_data_rom,
        file = here::here(output_dir, "imp_data_rom.rds"))



# Confirmation message
cat("Files saved successfully to:", output_dir, "\n")
```

git fetch origin
git pull origin main
git push origin HEAD:refs/heads/main --force

