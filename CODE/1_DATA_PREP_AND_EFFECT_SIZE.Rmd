---
title: "1_DATA_PREP"
author: "M.K.K. Lindhardt"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    patchwork,        # ggplot2 API for sequentially building up a plot
    RColorBrewer,
    gt,               # gt Tables
    ###################################################################################################################
    # Spatial Data
    tidygeocoder,     # Unified interface for performing both forward and reverse geocoding queries
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    rnaturalearth,    # For world map data
    rnaturalearthdata, 
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    philentropy,      # Computing divergence matrix or divergence value based on the Jensen-Shannon Divergence
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    inspectdf,
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("extract", "raster")
conflict_prefer("col_factor", "scales")
conflict_prefer("geocode", "tidygeocoder")
conflict_prefer("chisq.test", "stats")
```
```{r}
# Set a global theme and color scale
# Define the global color palette
global_palette <- c(
  "#ffd700", 
  "#ffb14e", 
  "#fa8775", 
  "#ea5f94", 
  "#cd34b5", 
  "#9d02d7", 
  "#0000ff"
)

# Define global ggplot2 scales for color and fill
scale_fill_global <- scale_fill_viridis_d(option = "D")  # Discrete
scale_color_global <- scale_color_viridis_d(option = "D")  # Discrete

scale_fill_global <- scale_fill_viridis_c(option = "D")  # Continuous
scale_color_global <- scale_color_viridis_c(option = "D")  # Continuous
```


Loading the dataset (main metadata database)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({
  # Final database as 'meta-data'
  # Manually modifying the SD columns to "control_sd" and "silvo_sd" in Excel!
  database <- readxl::read_excel(
    here("DATA", "Meta_data_v4.xlsx"), 
    sheet = "Quantatitive data"
  )
  
  # Dummy data where silvo_mean has been multiplied with 1.2 to check for larger effect size estimates
  # database_dummy <- readxl::read_excel(
  #   here("DATA", "Meta_data_dummy_test_high_silvo_mean_se.xlsx"), 
  #   sheet = "Quantatitive data"
  # )
    
})


```

**Glimpse (taking a look at the data)**
```{r Glimpse the dataset, eval=FALSE}
database %>% dplyr::glimpse() 

# Rows: 1,075
# Columns: 35
# Rows: 1,126
# Columns: 35
```

```{r}
database %>% summary() 
```

```{r}
database |> skim()
```


#############
# STEP 1
##########################################################################################################################################
DATA PREPROCESSING
##########################################################################################################################################

####################################
GENERIC PREPROCESSING
####################################

Manually modifying the SD columns to "control_sd" and "silvo_sd" in Excel!

And in step 4, generate unique study identifier ('exp_id')

```{r}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Data Preprocessing
database_clean <- database |>
  # Step 1: Clean column names
  janitor::clean_names() |>
  
  # Step 2: Convert id_article and id_obs to integer
  mutate(
    id_article = as.integer(id_article),
    id_obs = as.integer(id_obs)
  ) |>
  
  # Step 3: Convert standard errors and other numeric columns
  mutate(
    silvo_mean = safe_as_numeric(silvo_mean),
    silvo_se = safe_as_numeric(silvo_se),
    silvo_sd = safe_as_numeric(silvo_sd),
    silvo_n = safe_as_numeric(silvo_n),
    control_mean = safe_as_numeric(control_mean),
    control_se = safe_as_numeric(control_se),
    control_sd = safe_as_numeric(control_sd),
    control_n = safe_as_numeric(control_n),
    tree_age = safe_as_numeric(tree_age),
    no_tree_per_m = as.character(no_tree_per_m)) |> 
  
  # Step 4: Create Identifiers (Experiment, Treatment, Common Control)
  # Group data by relevant columns for Treatment ID
  group_by(id_article, tree_type, crop_type, location, experiment_year) |>
  mutate(treat_id = cur_group_id()) |>
  ungroup() |>
  
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
  # the experiment_year (the year the experiment was conducted)
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  # Step 5: Ensure no infinite or NaN values are present in any columns
  mutate(across(everything(), ~ifelse(is.infinite(.) | is.nan(.), NA, .))
         ) |> 
  
  # Step 6: Convert "NA" strings to real NA values, excluding 'id_article' and 'id_obs'
  mutate(
    across(
      .cols = where(is.character) & !c("id_article", "id_obs"),
      .fns = ~ na_if(., "NA")
    )
  ) |>
  
  # Step 7: Convert year columns to date format
 # Convert to proper Date format using "YYYY-01-01"
 mutate(
    experiment_year = as.Date(paste0(experiment_year, "-01-01")),
    year_est_exp = as.Date(paste0(year_est_exp, "-01-01")),
    #study_year_start = as.Date(paste0(study_year_start, "-01-01")),
    #study_year_end = as.Date(paste0(study_year_end, "-01-01"))
  ) |> 
  # Step 8: Rename Latitude and Longitude to lat and lon
  rename(
    lat = latitude,
    lon = longitude
  ) |>
  
  # Step 9: Convert lat and lon to numeric coordinates
  mutate(
    lat = str_replace_all(lat, "[°NS]", "") |> safe_as_numeric(),
    lon = str_replace_all(lon, "[°EW]", "") |> safe_as_numeric(),
    lat = if_else(str_detect(lat, "S$"), -lat, lat),
    lon = if_else(str_detect(lon, "W$"), -lon, lon)
  ) |>
  
  # Step 10: Create a Coherent 'site_x' Column
  mutate(
    # If `lat` and `lon` are present, use them; otherwise, use the `location` name
    site_x = case_when(
      !is.na(lat) & !is.na(lon) ~ paste(lat, lon, sep = ", "),
      !is.na(location) ~ location,
      TRUE ~ NA_character_
    )
  ) 
```

```{r, eval=FALSE}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Data Preprocessing
database_clean_dummy <- database_dummy |>
  # Step 1: Clean column names
  janitor::clean_names() |>
  
  # Step 2: Convert id_article and id_obs to integer
  mutate(
    id_article = as.integer(id_article),
    id_obs = as.integer(id_obs)
  ) |>
  
  # Step 3: Convert standard errors and other numeric columns
  mutate(
    silvo_mean = safe_as_numeric(silvo_mean),
    silvo_se = safe_as_numeric(silvo_se),
    silvo_sd = safe_as_numeric(silvo_sd),
    silvo_n = safe_as_numeric(silvo_n),
    control_mean = safe_as_numeric(control_mean),
    control_se = safe_as_numeric(control_se),
    control_sd = safe_as_numeric(control_sd),
    control_n = safe_as_numeric(control_n),
    tree_age = safe_as_numeric(tree_age),
    no_tree_per_m = as.character(no_tree_per_m)) |> 
  
  # Step 4: Create Identifiers (Experiment, Treatment, Common Control)
  # Group data by relevant columns for Treatment ID
  group_by(id_article, tree_type, crop_type, location, experiment_year) |>
  mutate(treat_id = cur_group_id()) |>
  ungroup() |>
  
  # Group data by relevant columns for Experiment ID
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  # Step 5: Ensure no infinite or NaN values are present in any columns
  mutate(across(everything(), ~ifelse(is.infinite(.) | is.nan(.), NA, .))
         ) |> 
  
  # Step 6: Convert "NA" strings to real NA values, excluding 'id_article' and 'id_obs'
  mutate(
    across(
      .cols = where(is.character) & !c("id_article", "id_obs"),
      .fns = ~ na_if(., "NA")
    )
  ) |>
  
 # Step 7: Convert year columns to date format
 # Convert to proper Date format using "YYYY-01-01"
 mutate(
    experiment_year = as.Date(paste0(experiment_year, "-01-01")),
    year_est_exp = as.Date(paste0(year_est_exp, "-01-01")),
    study_year_start = as.Date(paste0(study_year_start, "-01-01")),
    study_year_end = as.Date(paste0(study_year_end, "-01-01"))
  ) |> 
  # Step 8: Rename Latitude and Longitude to lat and lon
  rename(
    lat = latitude,
    lon = longitude
  ) |>
  
  # Step 9: Convert lat and lon to numeric coordinates
  mutate(
    lat = str_replace_all(lat, "[°NS]", "") |> safe_as_numeric(),
    lon = str_replace_all(lon, "[°EW]", "") |> safe_as_numeric(),
    lat = if_else(str_detect(lat, "S$"), -lat, lat),
    lon = if_else(str_detect(lon, "W$"), -lon, lon)
  ) |>
  
  # Step 10: Create a Coherent 'site_x' Column
  mutate(
    # If `lat` and `lon` are present, use them; otherwise, use the `location` name
    site_x = case_when(
      !is.na(lat) & !is.na(lon) ~ paste(lat, lon, sep = ", "),
      !is.na(location) ~ location,
      TRUE ~ NA_character_
    )
  ) 
```


####################################
GEOSPATIAL PREPROCESSING
####################################

Manually changing 'France (south west)' to 'France' and 'South East England(Cambridgeshire)' to 'Cambridgeshire, England' and 
'Bramham in northern England' to 'Bramham, England'
```{r}
# Step 1: Extract Coordinates from `site_x` if available
database_clean <- database_clean |>
  mutate(
    # Extract latitude and longitude from `site_x` if it contains coordinates
    extracted_lat = str_extract(site_x, "[-]?\\d+\\.\\d+(?=, )") |> as.numeric(),
    extracted_lon = str_extract(site_x, "(?<=, )[-]?\\d+\\.\\d+") |> as.numeric()
  )

# Step 2: Identify rows that need geocoding (i.e., where coordinates are missing)
locations_to_geocode <- database_clean |>
  filter(is.na(extracted_lat) | is.na(extracted_lon)) |>
  distinct(location) |>
  filter(!is.na(location))

# Step 3: Geocode Location Names
geocoded_locations <- locations_to_geocode |>
  geocode(address = location, method = "osm", lat = "geo_lat", long = "geo_lon")

# Step 4: Merge Geocoded Coordinates Back to the Dataset
database_clean <- database_clean |>
  left_join(geocoded_locations, by = "location") |>
  mutate(
    # Use extracted coordinates if available, otherwise use geocoded coordinates
    final_lat = coalesce(extracted_lat, geo_lat),
    final_lon = coalesce(extracted_lon, geo_lon),
    # Create the `exp_site_loc` column with final coordinates
    exp_site_loc = if_else(!is.na(final_lat) & !is.na(final_lon),
                           paste(final_lat, final_lon, sep = ", "),
                           NA_character_)
  ) |>
  select(-extracted_lat, -extracted_lon, -geo_lat, -geo_lon)|> 
  
  
  
  # Step 5: Relocate columns to the desired order
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Passing 23 addresses to the Nominatim single address geocoder
# Query completed in: 23.4 seconds

# Last run (03/12-2024)
# Passing 4 addresses to the Nominatim single address geocoder
# Query completed in: 4.3 seconds

# Last run (01/01-2025)
# Passing 3 addresses to the Nominatim single address geocoder
# Query completed in: 3 seconds
```

Checking if any missing values in coordinates

```{r}
# Filter rows where either final_lat or final_lon is missing
missing_coordinates <- database_clean |>
  filter(is.na(final_lat) | is.na(final_lon))

# View the rows with missing coordinates
missing_coordinates

# No missing coordinates
# 0 rows | 1-10 of 39 columns
```


Add geographical sub-regions to the dataset

```{r}
# Load Köppen-Geiger Climate Data (as a raster file)
kg_climate <- raster(here("DATA", "koppen_geiger_tif", "1991_2020", "koppen_geiger_0p1.tif"))


# Preserve final_lat and final_lon before conversion
database_clean <- database_clean |> 
  mutate(
    preserved_lat = final_lat,
    preserved_lon = final_lon
  )

# Convert your dataset to an sf object using preserved lat/lon columns
database_clean_sf <- database_clean |>
  drop_na(preserved_lat, preserved_lon) |>
  st_as_sf(coords = c("preserved_lon", "preserved_lat"), crs = 4326)

# Extract climate zone for each observation using spatial overlay
# Beck, H.E., T.R. McVicar, N. Vergopolan, A. Berg, N.J. Lutsko, A. Dufour, Z. Zeng, X. Jiang, A.I.J.M. van Dijk, D.G. MirallesHigh-resolution (1 km) Köppen-Geiger maps for 1901–2099 based on constrained CMIP6 projectionsScientific Data 10, 724, doi:10.1038/s41597-023–02549‑6 (2023)
# The variable 'climate_zone' represents the climate classification code assigned to each data point based on its geographical coordinates (latitude and longitude) from the Köppen-Geiger map. The climate_zone information was extracted using a spatial overlay.
database_clean_sf <- database_clean_sf |>
  mutate(
    climate_zone = extract(kg_climate, st_coordinates(database_clean_sf))
  )

# Classify sub-regions based on the climate zone
# Refine classifications for temperate climates and assign broader regions for others
database_clean_sf <- database_clean_sf %>%
  mutate(
    # Assign specific Köppen-Geiger classifications to climate zones
    climate_zone = case_when(
      climate_zone == 1 ~ "Tropical, rainforest",
      climate_zone == 2 ~ "Tropical, monsoon",
      climate_zone == 3 ~ "Tropical, savannah",
      climate_zone == 4 ~ "Arid, desert, hot",
      climate_zone == 5 ~ "Arid, desert, cold",
      climate_zone == 6 ~ "Arid, steppe, hot",
      climate_zone == 7 ~ "Arid, steppe, cold",
      climate_zone == 8 ~ "Temperate, dry summer, hot summer",
      climate_zone == 9 ~ "Temperate, dry summer, warm summer",
      climate_zone == 10 ~ "Temperate, dry summer, cold summer",
      climate_zone == 11 ~ "Temperate, dry winter, hot summer",
      climate_zone == 12 ~ "Temperate, dry winter, warm summer",
      climate_zone == 13 ~ "Temperate, dry winter, cold summer",
      climate_zone == 14 ~ "Temperate, no dry season, hot summer",
      climate_zone == 15 ~ "Temperate, no dry season, warm summer",
      climate_zone == 16 ~ "Temperate, no dry season, cold summer",
      climate_zone == 17 ~ "Cold, dry summer, hot summer",
      climate_zone == 18 ~ "Cold, dry summer, warm summer",
      climate_zone == 19 ~ "Cold dry summer, cold summer",
      climate_zone == 20 ~ "Cold dry summer, very cold winter",
      climate_zone == 21 ~ "Cold, dry winter, hot summer",
      climate_zone == 22 ~ "Cold, dry winter, warm summer",
      climate_zone == 23 ~ "Cold, dry winter, cold summer",
      climate_zone == 24 ~ "Cold, dry winter, very cold winter",
      climate_zone == 25 ~ "Cold, no dry season, hot summer",
      climate_zone == 26 ~ "Cold, no dry season, warm summer",
      climate_zone == 27 ~ "Cold, no dry season, cold summer",
      climate_zone == 28 ~ "Cold, no dry season, very cold winter",
      climate_zone == 29 ~ "Polar, tundra",
      climate_zone == 30 ~ "Polar, frost",
      TRUE ~ "Unclassified"
    ),
    
    # Assign refined geographical regions
    bioclim_sub_regions = case_when(
      # Tropical climates
      climate_zone %in% c(
        "Tropical, rainforest", "Tropical, monsoon", "Tropical, savannah"
      ) ~ "Tropical Climates",
      
      # Arid climates
      climate_zone %in% c(
        "Arid, desert, hot", "Arid, desert, cold", "Arid, steppe, hot", "Arid, steppe, cold"
      ) ~ "Arid Climates",
      
      # Refined temperate climates
      climate_zone %in% c(
        "Temperate, dry summer, hot summer", "Temperate, dry summer, warm summer", 
        "Temperate, dry winter, hot summer", "Temperate, dry winter, warm summer"
      ) ~ "Dry and Warm Temperate",
      
      climate_zone %in% c(
        "Temperate, no dry season, hot summer", "Temperate, no dry season, warm summer"
      ) ~ "Wet and Warm Temperate",
      
      climate_zone %in% c(
        "Temperate, dry summer, cold summer", "Temperate, dry winter, cold summer"
      ) ~ "Dry and Cold Temperate",
      
      climate_zone %in% c(
        "Temperate, no dry season, cold summer"
      ) ~ "Wet and Cold Temperate",
      
      # Cold climates
      climate_zone %in% c(
        "Cold, dry summer, hot summer", "Cold, dry summer, warm summer", 
        "Cold dry summer, cold summer", "Cold dry summer, very cold winter", 
        "Cold, dry winter, hot summer", "Cold, dry winter, warm summer", 
        "Cold, dry winter, cold summer", "Cold, dry winter, very cold winter",
        "Cold, no dry season, hot summer", "Cold, no dry season, warm summer", 
        "Cold, no dry season, cold summer", "Cold, no dry season, very cold winter"
      ) ~ "Cold Climates",
      
      # Polar climates
      climate_zone %in% c(
        "Polar, tundra", "Polar, frost"
      ) ~ "Polar Climates",
      
      TRUE ~ "Unclassified"
    )
  )
```
Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type
10	366	Leeds	England	53.883333° N	1.5491° W	NA	1992	7	Monoculture	Biomass	Cereal

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
29	873	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	874	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	875	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	876	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	877	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	878	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	879	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	880	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	881	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	882	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	883	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	884	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	885	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	886	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	887	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	888	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	889	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	890	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	891	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	892	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	893	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	894	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	895	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	896	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	897	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	898	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	899	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	900	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	901	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	902	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	903	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	904	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	905	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	906	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	907	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	908	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
36	1074	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011
36	1075	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011



Checking for unclassified in climate_zone and bioclim_sub_regions

```{r}
# Check classification coverage
database_clean_sf %>%
  filter(climate_zone == "Unclassified" | bioclim_sub_regions == "Unclassified") %>%
  head()
```




```{r}
# Rename back to 'database_clean'

# Relocate columns to the desired order
database_clean <- database_clean_sf |>
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, climate_zone, bioclim_sub_regions, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Preview the resulting data
database_clean |> 
  glimpse()

# Rows: 1,126
# Columns: 44
```


```{r}
# Create the bar chart
database_clean |> 
  ggplot(aes(x = bioclim_sub_regions)) +
  geom_bar(fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(
    title = "Distribution of Observations by BioClim Subregions",
    x = "BioClim Subregions",
    y = "Count of Observations"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
```


################################################################################
ASSESSING THE GENERATED RANDOM-FACTOR VARIABLES exp_id and treat_id 
################################################################################

Missingness Assessment for exp_id Visualization

```{r}
database_clean |> as.data.frame() |>
  str()
```


```{r}
# Visualize exp_id distribution across components
database_clean %>%
  as.data.frame() %>%
  select(-geometry) %>%
  mutate(
    # Convert all columns to character for consistency
    id_article = as.character(id_article),
    experiment_year = as.character(experiment_year)
  ) %>%
  group_by(id_article, location, experiment_year) %>%
  summarise(exp_id_count = n_distinct(exp_id), .groups = "drop") %>%
  pivot_longer(
    cols = c(id_article, location, experiment_year),
    names_to = "component",
    values_to = "value"
  ) %>%
  ggplot(aes(x = component, y = exp_id_count, fill = component)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Distribution of exp_id Across Components",
    x = "Component",
    y = "Count of exp_id"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
```{r}
# Heatmap for year-location relationship
# Function to plot heatmap for year and a chosen variable
plot_heatmap <- function(data, y_var) {
  data %>%
    as.data.frame() %>%
    select(-geometry) %>%
    group_by(experiment_year, !!sym(y_var)) %>%
    summarise(n_exp_id = n_distinct(exp_id), .groups = "drop") %>%
    ggplot(aes(x = experiment_year, y = !!sym(y_var), fill = n_exp_id)) +
    geom_tile() +
    labs(
      title = paste("Heatmap of exp_id by Experiment Year and", y_var),
      x = "Experiment Year",
      y = y_var,
      fill = "Count of exp_id"
    ) +
    scale_fill_viridis_c() +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Example usage: Plot for location
plot_heatmap(database_clean, "location")

# Example usage: Plot for site
plot_heatmap(database_clean, "site")

# Example usage: Plot for bioclim_sub_region
plot_heatmap(database_clean, "bioclim_sub_regions")
```

```{r}
# Function to plot heatmap for year and chosen variable (moderators or response variables)
plot_heatmap_moderator_response <- function(data, var_type) {
  data %>%
    as.data.frame() %>%
    select(-geometry) %>%
    group_by(experiment_year, !!sym(var_type)) %>%
    summarise(n_exp_id = n_distinct(exp_id), .groups = "drop") %>%
    ggplot(aes(x = experiment_year, y = !!sym(var_type), fill = n_exp_id)) +
    geom_tile() +
    labs(
      title = paste("Heatmap of exp_id by Experiment Year and", var_type),
      x = "Experiment Year",
      y = var_type,
      fill = "Count of exp_id"
    ) +
    scale_fill_viridis_c() +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Example usage: Plot for moderators
plot_heatmap_moderator_response(database_clean, "tree_type")

# Example usage: Plot for response variables
plot_heatmap_moderator_response(database_clean, "response_variable")

```


```{r}
# Step 2: Explore Aggregation Level
# Count unique `exp_id` values at each location level
aggregation_summary <- database_clean %>%
  as.data.frame() |> 
  select(-geometry) |> 
  group_by(location) %>%
  summarise(
    n_exp_id = n_distinct(exp_id),
    n_obs = n(),
    .groups = "drop"
  )

# View summary of aggregation
aggregation_summary

# Step 3: Visualize Aggregation Level
aggregation_summary |> 
  ggplot(aes(x = reorder(location, n_exp_id), y = n_exp_id)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Distribution of exp_id Across Locations",
    x = "Location",
    y = "Number of exp_id"
  ) +
  theme_minimal()
```
```{r}
# Step 4: Map Visualization (Optional)
# Simplify the dataset
geo_data <- database_clean %>%
  select(lat, lon, exp_id) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

# Base map
world_map <- map_data("world")

# Plot the map with points
ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.3
  ) +
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = as.factor(exp_id)),
    size = 3, alpha = 0.7
  ) +
  scale_color_viridis_d() +
  labs(
    title = "Geographical Distribution of exp_id",
    x = "Longitude",
    y = "Latitude",
    color = "Experiment ID"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "right"
  )

```

```{r}
# Summarize missingness across moderators and response variables
missingness_summary <- database_clean %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(exp_id) %>%
  summarise(
    # Count missing values for moderators
    missing_moderators = sum(
      is.na(tree_type) | is.na(crop_type) | is.na(age_system) | 
      is.na(tree_age) | is.na(season) | is.na(soil_texture) | 
      is.na(no_tree_per_m) | is.na(tree_height) | is.na(alley_width)
    ),
    # Count missing values for response variables
    missing_response = sum(
      is.na(silvo_mean) | is.na(control_mean)
    ),
    total_missing = missing_moderators + missing_response,
    n_obs = n(),
    .groups = "drop"
  )
```

Visualize Missingness (Facet Plot for Moderators and Responses)
```{r}
# Reshape the missingness summary for visualization
missingness_long <- missingness_summary %>%
  pivot_longer(
    cols = c(missing_moderators, missing_response),
    names_to = "missingness_type",
    values_to = "missing_count"
  )

# Summarize missingness data for better grouping and understanding
missingness_long_summary <- missingness_long %>%
  group_by(missingness_type, n_obs_group = cut(n_obs, breaks = seq(0, max(n_obs, na.rm = TRUE), by = 10))) %>%
  summarise(mean_missing = mean(missing_count, na.rm = TRUE), .groups = "drop")

# Separate data for moderators and response variables
moderators_data <- missingness_summary %>%
  select(exp_id, n_obs, missing_moderators) %>%
  mutate(missingness_type = "Moderators")

response_variables_data <- missingness_summary %>%
  select(exp_id, n_obs, missing_response) %>%
  mutate(missingness_type = "Response Variables")

# Plot Moderators Missingness
moderators_data %>%
  ggplot(aes(x = as.factor(n_obs), y = missing_moderators, fill = missingness_type)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Missingness Overview for Moderators",
    x = "Number of Observations per exp_id (Grouped)",
    y = "Average Missing Values Count"
  ) +
  scale_fill_manual(values = c("blue")) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```


Bar Chart: Missingness by Moderators (Exp ID Count)
```{r}
# Count missing values for each moderator
# Ensure all columns have the same data type before pivoting
missingness_moderators <- database_clean %>%
  as.data.frame() %>%
  select(
    exp_id, tree_type, crop_type, age_system, tree_age, season, 
    soil_texture, no_tree_per_m, tree_height, alley_width
  ) %>%
  mutate(across(-exp_id, as.character)) %>%  # Convert all non-exp_id columns to character
  pivot_longer(
    cols = -exp_id,
    names_to = "moderator",
    values_to = "value"
  ) %>%
  group_by(moderator) %>%
  summarise(
    exp_id_with_missing = sum(is.na(value)),  # Count the number of exp_id with missing values
    .groups = "drop"
  )

# Bar chart of missingness across moderators
missingness_moderators |> 
  ggplot(aes(x = moderator, y = exp_id_with_missing, fill = moderator)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Missingness in Moderators",
    subtitle = "Number of exp_id with Missing Values for Each Moderator",
    x = "Moderator",
    y = "Count of exp_id with Missing Values"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```












#############
# STEP 2
##########################################################################################################################################
CALCULATING META-ANALYSIS QUANTITATIVE DATA (Std Dev., Std. Err. etc.)
##########################################################################################################################################

Code for Calculating Meta-Analysis Quantitative Data

```{r}
# Dunctions for converting different measures of variability to SD - (however, in our data we only have SE)

# SE to SD
# SEtoSD <- function(SE, n) {
#   SE * sqrt(n)
# }

# # LSD to SD
# LSDtoSD <- function(LSD, n) {
#   LSD / (qt(0.975, n - 1)) * (sqrt(n) / sqrt(2))
# }
# 
# # CV to SD
# CVtoSD <- function(CV, mean) {
#   (CV / 100) * mean
# }
# 
# # MSE to SD
# MSEtoSD <- function(MSE) {
#   sqrt(MSE)
# }
```

```{r}
# Calculate standard deviations from standard errors and sample sizes
database_clean_sd <- database_clean |>
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  )
```

```{r, eval=FALSE}
# Calculate standard deviations from standard errors and sample sizes
database_clean_sd_dummy <- database_clean_dummy |>
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  )
```

Checking for missing data in control_sd and silvo_sd

```{r}
# Calculate percentage of missing SD values for control and silvo groups
missing_control_sd <- sum(is.na(database_clean_sd$control_sd)) / nrow(database_clean_sd) * 100
missing_silvo_sd <- sum(is.na(database_clean_sd$silvo_sd)) / nrow(database_clean_sd) * 100

message("Percentage of missing SD values for control group: ", round(missing_control_sd, 2), "%")
message("Percentage of missing SD values for silvo group: ", round(missing_silvo_sd, 2), "%")

# Percentage of missing SD values for control group: 17.95%
# Percentage of missing SD values for silvo group: 17.95%

missing_sd <- database_clean_sd |>
  filter(is.na(control_sd) | is.na(silvo_sd)) |> 
  relocate(id_article, id_obs, response_variable, location, 
           # Quantitative mata-analysis effect size info
           silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n)

missing_sd
```

Dataset that is used as non-imputed

```{r}
# This is the dataset that is used for analysis before imputation
database_clean_sd |> glimpse()
```



#############
# STEP 3
##########################################################################################################################################
ASSESS MISSINGNESS PATTERNS OF DATA BEFORE IMPUTATION
##########################################################################################################################################

```{r}
# Check missingness summary for `control_sd` and `silvo_sd`
missingness_summary <- database_clean_sd %>%
  as.data.frame() %>%
  summarise(
    total_rows = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    percent_missing_control_sd = mean(is.na(control_sd)) * 100,
    percent_missing_silvo_sd = mean(is.na(silvo_sd)) * 100
  )



# Add missingness indicators to the dataset
database_clean_sd_missingness <- database_clean_sd %>%
  as.data.frame() %>%
  mutate(
    missing_control_sd = ifelse(is.na(control_sd), 1, 0),
    missing_silvo_sd = ifelse(is.na(silvo_sd), 1, 0)
  )

missingness_summary
```
```{r}
# Visualize missingness across response variables and moderators
missingness_plot <- database_clean_sd_missingness %>%
  select(response_variable, missing_control_sd, missing_silvo_sd) %>%
  group_by(response_variable) %>%
  summarise(
    missing_control_sd = mean(missing_control_sd) * 100,
    missing_silvo_sd = mean(missing_silvo_sd) * 100
  ) %>%
  pivot_longer(cols = c(missing_control_sd, missing_silvo_sd),
               names_to = "Variable",
               values_to = "Percent_Missing") %>%
  ggplot(aes(x = response_variable, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Percentage of Standard Deviation Across Response Variables",
    x = "Response Variable",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

missingness_plot
```
```{r}
# Filter the dataset for rows with missing sd (either control_sd or silvo_sd)
missing_sd_data <- database_clean_sd_missingness %>%
  filter(is.na(control_sd) | is.na(silvo_sd))

# Summarize the counts of response variables for the filtered data
missing_sd_summary <- missing_sd_data %>%
  group_by(response_variable) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Plot the distribution of response variables for missing sd data
ggplot(missing_sd_summary, aes(x = reorder(response_variable, -count), y = count, fill = response_variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Distribution of Response Variables with Missing Standard Deviation",
    x = "Response Variable",
    y = "Count of Missing Entries"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# Filter the dataset for rows with missing sd (either control_sd or silvo_sd)
missing_sd_data_rel <- database_clean_sd_missingness %>%
  mutate(missing_sd = ifelse(is.na(control_sd) | is.na(silvo_sd), 1, 0))

# database_clean_sd_missingness |> filter(response_variable == "Soil quality") |> nrow()

# Calculate the percentage of missing sd values relative to total observations for each response variable
missing_sd_summary_rel <- missing_sd_data_rel %>%
  group_by(response_variable) %>%
  summarise(
    total_observations = n(),
    missing_sd_count = sum(missing_sd),
    percent_missing = (missing_sd_count / total_observations) * 100
  ) %>%
  arrange(desc(percent_missing))

missing_sd_summary_rel
```
```{r}
# Plot the relative missingness
missing_sd_summary_rel |> 
  ggplot(aes(x = reorder(response_variable, -percent_missing), y = percent_missing, fill = response_variable)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Relative Missingness of Standard Deviation by Response Variable",
    x = "Response Variable",
    y = "Percent Missing",
    fill = "Response Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Assess missingness patterns across moderators
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

missingness_by_moderators <- database_clean_sd_missingness %>%
  select(all_of(moderators), missing_control_sd, missing_silvo_sd) %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100))

missingness_by_moderators

# Visualize missingness with a heatmap
heatmap_missingness <- database_clean_sd_missingness %>%
  select(control_sd, silvo_sd, response_variable, all_of(moderators)) %>%
  gg_miss_upset()

heatmap_missingness
```

##########################################################################################################################################
Assess missingness patterns for _sd variables by location and study ID (id_article)
##########################################################################################################################################

```{r}
# Calculate missingness by location
missing_by_location <- database_clean_sd_missingness %>%
  group_by(location) %>%
  summarise(
    total = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    perc_missing_control_sd = 100 * mean(is.na(control_sd)),
    perc_missing_silvo_sd = 100 * mean(is.na(silvo_sd))
  ) %>%
  arrange(desc(perc_missing_control_sd), desc(perc_missing_silvo_sd))

# Print missingness summary by location
cat("\nMissingness by Location:\n")
print(missing_by_location)
```

```{r}
# Missingness by location
ggplot(missing_by_location, aes(x = reorder(location, -perc_missing_control_sd), y = perc_missing_control_sd)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  geom_bar(aes(y = perc_missing_silvo_sd), stat = "identity", fill = "red", alpha = 0.7) +
  labs(
    title = "Missingness Percentage by Location",
    x = "Location",
    y = "Percentage Missing",
    fill = "Missingness Type"
  ) +
  theme_minimal() +
  coord_flip()
```

```{r}
# Calculate missingness by study ID (id_article)
missing_by_study <- database_clean_sd_missingness %>%
  group_by(id_article) %>%
  summarise(
    total = n(),
    missing_control_sd = sum(is.na(control_sd)),
    missing_silvo_sd = sum(is.na(silvo_sd)),
    perc_missing_control_sd = 100 * mean(is.na(control_sd)),
    perc_missing_silvo_sd = 100 * mean(is.na(silvo_sd))
  ) %>%
  arrange(desc(perc_missing_control_sd), desc(perc_missing_silvo_sd))

# Print missingness summary by study ID
cat("\nMissingness by Study ID:\n")
print(missing_by_study)
```

```{r}
# Missingness by study ID
ggplot(missing_by_study, aes(x = reorder(as.factor(id_article), -perc_missing_control_sd), y = perc_missing_control_sd)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  geom_bar(aes(y = perc_missing_silvo_sd), stat = "identity", fill = "red", alpha = 0.7) +
  labs(
    title = "Missingness Percentage by Study ID",
    x = "Study ID",
    y = "Percentage Missing",
    fill = "Missingness Type"
  ) +
  theme_minimal() +
  coord_flip()
```


##########################################################################################################################################
Little's MCAR test for missingness
##########################################################################################################################################

Implications of missing.patterns:
A high number of missing patterns indicates complex missingness in your dataset, which might suggest that the data is not Missing Completely at Random (MCAR). Instead, it might be Missing at Random (MAR) or Not Missing at Random (NMAR).

```{r}
####################################################################################################################
# Prepare the data for missingness assessment
database_clean_sd_df <- database_clean_sd |> as.data.frame() |> select(-geometry) 
####################################################################################################################



# Select the variables for the test
test_data <- database_clean_sd_df %>%
  select(control_sd, silvo_sd, everything())  # Include control_sd, silvo_sd, and all variables

# Convert non-numeric columns to numeric using one-hot encoding or factor levels
test_data_numeric <- test_data %>%
  mutate(across(where(is.character), ~ as.numeric(as.factor(.)))) %>%  # Convert characters to numeric
  mutate(across(where(is.factor), as.numeric)) %>%                     # Convert factors to numeric
  select(where(~ sum(!is.na(.)) > 0))                                  # Remove columns with all missing values

# Check for problematic values
problematic_values <- test_data_numeric %>%
  summarise(across(everything(), ~ sum(is.infinite(.)) + sum(is.nan(.)) + sum(is.na(.))))

problematic_values |> glimpse()

# Exclude columns with more than 50% missing values
test_data_cleaned <- test_data_numeric %>%
  # Keep control_sd and silvo_sd
  select(control_sd, silvo_sd, response_variable, experiment_year, year_est_exp, exp_id,
         # Moderators info
         tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width) %>%  
  # Remove columns with >50% missing
  select(where(~ sum(is.na(.)) < nrow(test_data_numeric) * 0.5)) |> 
  # Convert date-time columns to numeric (e.g., extract the year)
  mutate(
    experiment_year = as.numeric(format(experiment_year, "%Y")),
    year_est_exp = as.numeric(format(year_est_exp, "%Y"))
  )

# Confirm all columns are numeric
all(sapply(test_data_cleaned, is.numeric)) 

# Check which columns are not numeric
non_numeric_cols <- sapply(test_data_cleaned, function(col) !is.numeric(col))
names(non_numeric_cols[non_numeric_cols])

# Inspect missing values after cleaning
colSums(is.na(test_data_cleaned))

# Perform Little's MCAR test on cleaned data
mcar_test <- naniar::mcar_test(test_data_cleaned)
mcar_test
```

```{r}
# Visualize missingness pattern
md.pattern(database_clean_sd_df)

# Heatmap of missing data

aggr_plot <- aggr(database_clean_sd_df, col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE, 
                  labels = names(database_clean_sd_df), cex.axis = .7, gap = 3, ylab = c("Missingness", "Pattern"))
aggr_plot
```

```{r}
# Add missing indicators for control_sd and silvo_sd
test_data_cleaned_sd <- test_data_cleaned %>%
  mutate(
    missing_control_sd = is.na(control_sd),
    missing_silvo_sd = is.na(silvo_sd)
  )

# Check relationship between missingness and other variables
ggplot(test_data_cleaned_sd, aes(x = tree_age, fill = missing_control_sd)) +
  geom_histogram(position = "dodge") +
  labs(title = "Relationship Between Tree Age and Missingness in control_sd")
```

Fit logistic regression models to see if missingness depends on observed variables.

```{r}
# Test if missingness depends on observed variables using logistic regression

# Add missingness indicators for control_sd and silvo_sd
test_data_cleaned_sd_test <- test_data_cleaned %>%
  mutate(
    missing_control_sd = as.numeric(is.na(control_sd)),
    missing_silvo_sd = as.numeric(is.na(silvo_sd))
  )

# Test 1: Logistic regression using specified moderators
missing_control_model_1 <- glm(
  missing_control_sd ~ tree_type + crop_type + age_system + tree_age + season + soil_texture + no_tree_per_m + tree_height + alley_width,
  data = test_data_cleaned_sd_test, family = binomial
)

missing_silvo_model_1 <- glm(
  missing_silvo_sd ~ tree_type + crop_type + age_system + tree_age + season + soil_texture + no_tree_per_m + tree_height + alley_width,
  data = test_data_cleaned_sd_test, family = binomial
)

# Summarize results for Test 1
cat("\nTest 1: Logistic regression results using specified moderators\n")
cat("\nControl SD Missingness:\n")
summary(missing_control_model_1)

cat("\nSilvo SD Missingness:\n")
summary(missing_silvo_model_1)

# Test 2: Logistic regression using response variables
missing_control_model_2 <- glm(
  missing_control_sd ~ response_variable,
  data = test_data_cleaned_sd_test, family = binomial
)

missing_silvo_model_2 <- glm(
  missing_silvo_sd ~ response_variable,
  data = test_data_cleaned_sd_test, family = binomial
)

# Summarize results for Test 2
cat("\nTest 2: Logistic regression results using response variables\n")
cat("\nControl SD Missingness:\n")
summary(missing_control_model_2)

cat("\nSilvo SD Missingness:\n")
summary(missing_silvo_model_2)

```

### Missingness Assessment Interpretation:

For both `control_sd` and `silvo_sd`, the patterns of missing data appear to depend on several observed factors (`Moderators`) as well as the type of response variable (`response_variable`):

1. **Key Factors Associated with Missingness**:
   - **Tree Type and Crop Type**: These categories are significantly associated with whether data is missing. This means that certain types of trees or crops are more likely to have missing values for `control_sd` and `silvo_sd`.
   - **Season and Soil Texture**: Missing data is more common in certain seasons and specific soil textures, suggesting these conditions might make it harder to collect or report standard deviation data.
   - **Tree Age**: Older trees tend to have fewer missing values, perhaps because experiments on mature trees are more stable or complete.
   - **Tree Height**: Taller trees are linked to higher missingness, possibly because they are part of more complex or harder-to-measure systems.

2. **Response Variable Type**:
   - The type of response variable (e.g., biodiversity, crop yield) is also a strong predictor of missingness. This could mean that some variables are inherently harder to measure accurately, or researchers focus less on reporting standard deviations for specific types of responses.

---

### Why Do Categorical Variables Like `Tree Type` and `Crop Type` Have Significant Associations with Missingness?

Categorical variables can influence missingness if the categories represent different conditions or contexts where data collection might vary. For example:

- **Tree Type**: Different types of trees might be studied in varying experimental setups. For instance:
  - Fruit trees might be in highly controlled settings with fewer missing values.
  - Timber trees in diverse, large-scale environments might have more missing data due to logistical challenges.

- **Crop Type**: Similarly, certain crops might be associated with specific research focuses or reporting standards:
  - Cereals might be studied in detail with less missingness because of their importance.
  - Specialty crops like herbs might have more missing values due to less emphasis on certain measurements.

The "positive association" means that when a particular category (e.g., a specific tree or crop type) is present, the odds of missing data increase compared to a baseline category.

---

### Next Steps:

Since missingness is likely **Missing at Random (MAR)** (dependent on observed factors like tree type, crop type, etc.), we can proceed with **multiple imputation**:

1. Use the significant predictors (`tree_type`, `crop_type`, `season`, etc.) as covariates in an imputation model.
2. Test different imputation methods, like predictive mean matching or linear or Bayesian regression, to see which works best.
3. Validate the imputed data by checking its consistency with known trends or patterns in your data and assess on AIC and BIC.


Elucidate Associations of Tree Type and Crop Type with Missingness

```{r}
# Step 1: Summarize Missingness by Tree Type and Crop Type
missing_summary_tree_crop <- test_data_cleaned_sd_test %>%
  group_by(tree_type, crop_type) %>%
  summarise(
    missing_control_sd = mean(missing_control_sd, na.rm = TRUE) * 100,
    missing_silvo_sd = mean(missing_silvo_sd, na.rm = TRUE) * 100,
    .groups = "drop"
  )

# Print Summary Table
missing_summary_tree_crop
```
```{r}
# Step 2: Visualize Missingness by Tree Type and Crop Type

# Plot for Control SD Missingness
plot_control_missingness <- ggplot(missing_summary_tree_crop, aes(x = tree_type, y = missing_control_sd, fill = crop_type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Control SD Missingness by Tree Type and Crop Type",
    x = "Tree Type",
    y = "% Missing Control SD",
    fill = "Crop Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot for Silvo SD Missingness
plot_silvo_missingness <- ggplot(missing_summary_tree_crop, aes(x = tree_type, y = missing_silvo_sd, fill = crop_type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Silvo SD Missingness by Tree Type and Crop Type",
    x = "Tree Type",
    y = "% Missing Silvo SD",
    fill = "Crop Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



plot_control_missingness
plot_silvo_missingness
```


```{r}
# Step 3: Statistical Tests to Confirm Associations
# Chi-Square Test for Independence between Tree Type and Control SD Missingness
chi_sq_tree_control <- chisq.test(table(test_data_cleaned_sd_test$tree_type, test_data_cleaned_sd_test$missing_control_sd))
chi_sq_tree_control

# Chi-Square Test for Independence between Crop Type and Control SD Missingness
chi_sq_crop_control <- chisq.test(table(test_data_cleaned_sd_test$crop_type, test_data_cleaned_sd_test$missing_control_sd))
chi_sq_crop_control

# Chi-Square Test for Independence between Tree Type and Silvo SD Missingness
chi_sq_tree_silvo <- chisq.test(table(test_data_cleaned_sd_test$tree_type, test_data_cleaned_sd_test$missing_silvo_sd))
chi_sq_tree_silvo

# Chi-Square Test for Independence between Crop Type and Silvo SD Missingness
chi_sq_crop_silvo <- chisq.test(table(test_data_cleaned_sd_test$crop_type, test_data_cleaned_sd_test$missing_silvo_sd))
chi_sq_crop_silvo

# Step 4: Logistic Regression Models for Tree Type and Crop Type
# Logistic Regression for Control SD
logit_control <- glm(missing_control_sd ~ tree_type + crop_type, data = test_data_cleaned_sd_test, family = binomial)
logit_control

# Logistic Regression for Silvo SD
logit_silvo <- glm(missing_silvo_sd ~ tree_type + crop_type, data = test_data_cleaned_sd_test, family = binomial)
logit_silvo
```

```{r}
database_clean_sd_df |> glimpse()
```
```{r}
# Aggregate missingness percentages by tree_type and crop_type
missingness_distribution <- database_clean_sd_df %>%
  group_by(tree_type, crop_type) %>%
  summarise(
    missing_control_sd = mean(missing_control_sd, na.rm = TRUE) * 100,
    missing_silvo_sd = mean(missing_silvo_sd, na.rm = TRUE) * 100,
    .groups = "drop"
  ) %>%
  pivot_longer(
    cols = c(missing_control_sd, missing_silvo_sd),
    names_to = "Variable",
    values_to = "Percent_Missing"
  )

# Plot missingness distribution for tree_type
missingness_tree_plot <- ggplot(missingness_distribution, aes(x = tree_type, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Distribution by Tree Type",
    x = "Tree Type",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot missingness distribution for crop_type
missingness_crop_plot <- ggplot(missingness_distribution, aes(x = crop_type, y = Percent_Missing, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Missingness Distribution by Crop Type",
    x = "Crop Type",
    y = "Percent Missing",
    fill = "Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plots
print(missingness_tree_plot)
print(missingness_crop_plot)
```

The visualizations demonstrate the percentage of missing values in `control_sd` and `silvo_sd` across levels of tree type and crop type. For tree type, there is no missingness for the category "fruit/nut & other," while "biomass" and "timber" have consistent missingness rates of around 40% for both variables. This suggests that the missingness for tree type is category-specific and potentially influenced by the characteristics or measurement challenges associated with "biomass" and "timber." For crop type, cereals show a much lower percentage of missingness (approximately 20%) compared to legumes and tuber/root crops, where missingness rates exceed 40-50%. This indicates that crop type has a more pronounced impact on missingness, with cereals potentially benefiting from more standardized measurement protocols or fewer data collection challenges. The absence of missing data in "fruit/nut & other" tree types highlights the importance of considering specific categories when addressing missing data. These patterns suggest that imputation strategies should account for crop type as a key predictor, while also recognizing the absence of missingness in certain tree type categories, such as "fruit/nut & other," which do not require imputation. Further statistical analysis could validate these observations and refine imputation models.






#############
# STEP 4
##########################################################################################################################################
HANDELING OF MISSING VALUES IN THE DATASET
##########################################################################################################################################

Perform imputation on  silvo_se, control_se using 
"mice" (Multivariate Imputation by Chained Equations), 
Upper Quartile, 
Mean Imputation,
Linear regression imputation (norm.predict)



```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    # Predictive Mean Matching
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0

    ##########################################################################
    # Define imputation method for PMM
    method <- c(
      "silvo_se" = "pmm",
      "control_se" = "pmm",
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",         # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    # Perform imputation using mice
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "upper_quartile") {
    ##########################################################################
    # Upper Quartile Imputation for Variance
    upper_quartile_variance <- data %>%
      # The 75th percentile represents a value higher than the median, ensuring that the imputed variances 
      # are not unrealistically small while 
      # still grounded in observed data. This helps maintain a conservative weighting in the meta-analysis.
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")

    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)

  } else if (method_name == "linear_imputation") {
    ##########################################################################
    # Linear Regression Imputation (norm.predict)
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0

    method <- c(
      "silvo_se" = "norm.predict",
      "control_se" = "norm.predict",
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",         # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation")
imputed_datasets <- list()

# Separate storage for the raw mids objects
imputed_mids_pmm <- NULL
imputed_mids_linear <- NULL

for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  if (method_name == "pmm") {
    imputed_mids_pmm <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_pmm)
  } else if (method_name == "linear_imputation") {
    imputed_mids_linear <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_linear)
  } else {
    imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
  }
}

##########################################################################
# Step 4: Compare results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed datasets for all methods

##########################################################################
# Last run (02/01-25)
# Total time taken: 21.27449 
```

```{r}
# Check the summary of imputed dataset list
imputed_datasets |> str()
```
```{r}
# Evaluate the imputed datasets
# Check convergence diagnostics
plot(imputed_mids_pmm)
```


```{r}
# Use stripplot to compare observed and imputed values
stripplot(imputed_mids_pmm, silvo_se + control_se ~ .imp,
  cex = c(1, 2), pch = c(20, 20), jitter = TRUE, alpha = 0.4, scales = "free")
```


```{r}
# Step 1: Summarize each imputed dataset
# Quantitative Assessment:
# Calculate summary statistics for each imputed dataset, focusing on proximity to medians (mean proximity for silvo_se and control_se).
# Incorporate additional metrics like variance, range, and RMSE for better decision-making.

imputed_summaries <- list()

for (i in 1:20) {
  data <- mice::complete(imputed_mids_pmm, i) # Extract the i-th imputed dataset

  # Calculate summary statistics for each imputation
  summary <- data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_silvo_se = max(silvo_se, na.rm = TRUE) - min(silvo_se, na.rm = TRUE),
      range_control_se = max(control_se, na.rm = TRUE) - min(control_se, na.rm = TRUE)
    )

  imputed_summaries[[i]] <- summary
}

# Combine all summaries into a single data frame
imputed_summaries_df <- bind_rows(imputed_summaries, .id = "imputation")

# Calculate medians for silvo_se and control_se
median_silvo_se <- median(imputed_summaries_df$mean_silvo_se)
median_control_se <- median(imputed_summaries_df$mean_control_se)

# Add a column calculating Euclidean distance to medians
imputed_summaries_df <- imputed_summaries_df %>%
  mutate(
    distance_from_median = sqrt(
      (mean_silvo_se - median_silvo_se)^2 + (mean_control_se - median_control_se)^2
    )
  )
```

```{r}
# Step 2: Advanced Quantitative Metrics
# Root Mean Squared Error (RMSE):
# Add RMSE comparison between observed and imputed values for silvo_se and control_se.

# RMSE calculation function
calculate_rmse <- function(observed, imputed) {
  sqrt(mean((observed - imputed)^2, na.rm = TRUE))
}

# Add RMSE to imputed summaries
imputed_summaries_df <- imputed_summaries_df %>%
  rowwise() %>%
  mutate(
    rmse_silvo_se = calculate_rmse(
      col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$silvo_se[is.na(col_for_impute$silvo_se)]
    ),
    rmse_control_se = calculate_rmse(
      col_for_impute$control_se[!is.na(col_for_impute$control_se)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$control_se[is.na(col_for_impute$control_se)]
    )
  )
```
```{r}
# Step 3: Choose the Best Imputation
# Select the imputation based on a weighted score that combines distance to medians, RMSE, and possibly variance or range.

# Add weighted score for selection and select the top-ranked imputation
chosen_imputation <- imputed_summaries_df %>% 
  as.data.frame() |> 
  # Calculate the total_score (modify weights if necessary)
  mutate(total_score = distance_from_median + rmse_silvo_se + rmse_control_se) %>%
  # Arrange by total_score (ascending order)
  arrange(total_score) %>%
  # Select the top row with the lowest total_score
  slice(1)

# Extract the corresponding dataset
chosen_imputation_number <- as.integer(chosen_imputation$imputation)
imputed_col_data <- mice::complete(imputed_mids_pmm, chosen_imputation_number)
imputed_datasets$pmm_best <- imputed_col_data

imputed_datasets$pmm_best
```


##########################################################################################################################################
Visually examine the imputed values
##########################################################################################################################################


```{r}
# Step 4: Visual Assessment
# Density Plots for Each Method and Variable:
# Compare observed and imputed distributions for silvo_se and control_se across all imputation methods.

# Prepare data for visualization
observed_values <- list(
  silvo_se = col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)],
  control_se = col_for_impute$control_se[!is.na(col_for_impute$control_se)]
)

# Combine observed and imputed values for plotting
combined_plot_data <- list()

for (variable in c("silvo_se", "control_se")) {
  for (method_name in names(imputed_datasets)) {
    imputed_values <- imputed_datasets[[method_name]][[variable]][is.na(col_for_impute[[variable]])]

    combined_plot_data[[paste(variable, method_name, sep = "_")]] <- data.frame(
      value = c(observed_values[[variable]], imputed_values),
      type = c(rep("Original", length(observed_values[[variable]])),
               rep("Imputed", length(imputed_values))),
      method = method_name,
      variable = variable
    )
  }
}

# Combine all data into one frame
imputation_plot_data_all <- bind_rows(combined_plot_data)

imputation_plot_data_all

# Remove linear imputation for better visualization
imputation_plot_data_no_linear <- 
  imputation_plot_data_all |> 
  filter(method != "linear_imputation")
```

```{r}
# Plot densities for all methods and variables
plot_imputation_data_all <- 
  imputation_plot_data_all |> 
  ggplot(aes(x = value, fill = type)) +
  geom_density(alpha = 0.5) +
  facet_grid(variable ~ method) +
  labs(
    title = "Density Comparison: Original vs. Imputed Values",
    x = "Value",
    y = "Density"
  ) +
  theme_minimal() +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +  # Discrete color scale 
   scale_x_log10() + # Apply log10 scale to x-axis
  theme(strip.text = element_text(size = 10, face = "bold"))

plot_imputation_data_all


# Plot densities for all methods and variables without linear imputation
plot_imputation_data_no_linear <- 
  imputation_plot_data_no_linear |> 
  ggplot(aes(x = value, fill = type)) +
  geom_density(alpha = 0.5) +
  facet_grid(variable ~ method) +
  labs(
    title = "Density Comparison: Original vs. Imputed Values (Excluding Linear Imputation)",
    x = "Value",
    y = "Density"
  ) +
  theme_minimal() +
   scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +  # Discrete color scale 
   scale_x_log10() + # Apply log10 scale to x-axis
  theme(strip.text = element_text(size = 10, face = "bold"))


plot_imputation_data_all
plot_imputation_data_no_linear
```






##########################################################################################################################################
More quantitative statistical assessment
##########################################################################################################################################

```{r}
# a. Descriptive Statistics
# Calculate and compare mean, standard deviation, and range for key variables (e.g., silvo_se, control_se).

# Calculate descriptive statistics for each imputation method
compare_stats <- lapply(imputed_datasets, function(data) {
  data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      range_silvo_se = diff(range(silvo_se, na.rm = TRUE)),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_control_se = diff(range(control_se, na.rm = TRUE))
    )
}) %>%
  bind_rows(.id = "method")


# b. Variance Explained
# Assess variance consistency across imputations, as it may indicate reliability.

# Calculate variance for silvo_se and control_se
compare_variance <- lapply(imputed_datasets, function(data) {
  data %>%
    summarise(
      var_silvo_se = var(silvo_se, na.rm = TRUE),
      var_control_se = var(control_se, na.rm = TRUE)
    )
}) %>%
  bind_rows(.id = "method")




# Calculate Jensen-Shannon divergence for each method - The Jensen-Shannon Divergence (JSD) is a statistical measure of similarity (or dissimilarity) between two 
# probability distributions. It quantifies how different one probability distribution is from another and is widely used in information theory, machine learning, 
# and statistics.
# Adjust the code to calculate JSD using philentropy
compare_jsd <- lapply(imputed_datasets, function(data) {
  observed_silvo <- density(col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)])$y
  imputed_silvo <- density(data$silvo_se[is.na(col_for_impute$silvo_se)])$y
  
  observed_control <- density(col_for_impute$control_se[!is.na(col_for_impute$control_se)])$y
  imputed_control <- density(data$control_se[is.na(col_for_impute$control_se)])$y
  
  # Ensure probabilities sum to 1 before calculating divergence
  observed_silvo <- observed_silvo / sum(observed_silvo)
  imputed_silvo <- imputed_silvo / sum(imputed_silvo)
  
  observed_control <- observed_control / sum(observed_control)
  imputed_control <- imputed_control / sum(imputed_control)
  
  list(
    jsd_silvo = JSD(rbind(observed_silvo, imputed_silvo)),
    jsd_control = JSD(rbind(observed_control, imputed_control))
  )
}) %>%
  bind_rows(.id = "method")


# Add an identifier column to each data frame and convert them to long format
compare_stats_long <- compare_stats %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Descriptive Stats")

compare_variance_long <- compare_variance %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Variance Comparison")

compare_jsd_long <- compare_jsd %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "JSD Comparison")

# Combine all data frames into one
combined_metrics <- bind_rows(compare_stats_long, compare_variance_long, compare_jsd_long)

# View the combined data frame
combined_metrics
```

```{r}
# Ensure `database_clean_sd` is converted to a plain data frame and the `geometry` column is removed
database_clean_sd <- database_clean_sd %>%
  as.data.frame() %>%
  select(-geometry)

# Recalculate the metrics for the original dataset
original_metrics <- list(
  # Descriptive statistics
  descriptive_stats = database_clean_sd %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      range_silvo_se = diff(range(silvo_se, na.rm = TRUE)),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE),
      range_control_se = diff(range(control_se, na.rm = TRUE))
    ) %>%
    mutate(method = "original"),
  
  # Variance comparison
  variance = database_clean_sd %>%
    summarise(
      var_silvo_se = var(silvo_se, na.rm = TRUE),
      var_control_se = var(control_se, na.rm = TRUE)
    ) %>%
    mutate(method = "original"),
  
  # Jensen-Shannon divergence (JSD)
  jsd = list(
    jsd_silvo = 0, # No divergence for original dataset
    jsd_control = 0
  ) %>%
    bind_rows() %>%
    mutate(method = "original")
)

# Convert metrics to long format for visualization
original_descriptive_long <- original_metrics$descriptive_stats %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Descriptive Stats")

original_variance_long <- original_metrics$variance %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "Variance Comparison")

original_jsd_long <- original_metrics$jsd %>%
  pivot_longer(cols = -method, names_to = "metric", values_to = "value") %>%
  mutate(category = "JSD Comparison")

# Combine original metrics with imputed dataset metrics
original_combined <- bind_rows(original_descriptive_long, original_variance_long, original_jsd_long)
combined_metrics_with_original <- bind_rows(combined_metrics, original_combined)
```

```{r}
prepared_data_gt <- combined_metrics_with_original %>%
  # Pivot wider to make methods the columns
  pivot_wider(names_from = method, values_from = value) %>%
  # Add absolute relative difference columns
  mutate(
    across(
      -c(metric, original, category), # Exclude non-numeric columns
      ~ ifelse(
        grepl("^jsd_", metric) & . == 0, NA,  # Set `NA` where `jsd_` metrics are 0.00
        ifelse(is.na(original) | original == 0, NA, abs((. - original) / original)) # Otherwise compute relative difference
      ),
      .names = "{.col}_relative"
    )
  ) %>%
  # Add a new column to extract the prefix (e.g., mean_, sd_, etc.)
  mutate(metric_group = sub("_.*", "", metric)) %>%
  # Sort rows first by the group (e.g., mean, sd) and then by the original metric order
  arrange(metric_group, metric)

# Display structure for verification
prepared_data_gt |> glimpse()
```


```{r}
# Create the updated plot
ggplot(combined_metrics_with_original, aes(x = method, y = value, fill = method)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = 0.8) +
  facet_wrap(~ category + metric, scales = "free_y", ncol = 3) +
  labs(
    title = "Comparison of Imputation Methods Across Metrics (Including Original Dataset)",
    x = "Method",
    y = "Value",
    fill = "Method"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "top"
  ) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8)  # Discrete color scale
```

```{r}
# Create a formatted table using `gt`
comparison_gt <- prepared_data_gt %>%
  # Reorder the columns in the data frame before passing to gt
  select(
    metric, category, original, 
    linear_imputation,
    linear_imputation_relative,
    mean_imputation, 
    mean_imputation_relative,
    upper_quartile, 
    upper_quartile_relative, 
    pmm, 
    pmm_relative,  
    pmm_best,  
    pmm_best_relative, 
         ) |> 
  # Set `metric` as row names
  gt(rowname_col = "metric") %>%  
  tab_header(
    title = "Comparison of Imputation Methods Across Metrics",
    subtitle = "Including Original Data and Relative Differences"
  ) %>%
  # Rename columns for clarity
  cols_label(
    original = "Original",
    linear_imputation = "Linear Imputation",
    linear_imputation_relative = "Linear Imputation Relative",
    upper_quartile = "Upper Quartile",
    upper_quartile_relative = "Upper Quartile Relative",
    mean_imputation = "Mean Imputation",
    mean_imputation_relative = "Mean Imputation Relative",
    pmm = "PMM",
    pmm_relative = "PMM Relative",    
    pmm_best = "PMM Best",
    pmm_best_relative = "PMM Best Relative"
  ) %>%
  # Format numeric columns to two decimal places
  fmt_number(
    columns = c(pmm, upper_quartile, mean_imputation, pmm_best, original),
    decimals = 2
  ) %>%
  fmt_number(
    columns = ends_with("_relative"),  # Format relative difference columns
    decimals = 3
  ) %>%
  # Replace 0 values in the relative columns with "NA"
  fmt_missing(
    columns = ends_with("_relative"),
    missing_text = "NA"
  ) %>%
  # Add horizontal and vertical lines for clarity
  tab_style(
    style = list(
      cell_fill(color = "#f9f9f9"),
      cell_borders(sides = "all", color = "gray", weight = px(1))
    ),
    locations = cells_body()
  ) %>%
  # Adjust table options
  tab_options(
    table.font.size = "small",
    table.border.top.color = "gray",
    table.border.bottom.color = "gray",
    column_labels.font.size = "medium",
    row_group.font.size = "small"
  ) %>%
  # Add footnotes for explanation of relative differences
  tab_footnote(
    footnote = "Relative differences are calculated as |(imputed - original) / original|.",
    locations = cells_column_labels(ends_with("_relative"))
  )

# Optionally, export the table
output_folder <- here("DATA", "OUTPUT_FROM_R")
gtsave(comparison_gt, file.path(output_folder, "comparison_table.html"))
gtsave(comparison_gt, file.path(output_folder, "comparison_table.pdf"))

# Display the table
comparison_gt

```

```{r}
# Apply color shading to all `_relative` columns
comparison_gt <- prepared_data_gt %>%
   # Reorder the columns in the data frame before passing to gt
  select(
    metric, category, original, 
    linear_imputation,
    linear_imputation_relative,
    mean_imputation, 
    mean_imputation_relative,
    upper_quartile, 
    upper_quartile_relative, 
    pmm, 
    pmm_relative,  
    pmm_best,  
    pmm_best_relative
    ) |> 
  # Set `metric` as row names
  gt() %>%
  data_color(
    columns = ends_with("_relative"), # Target all _relative columns
    method = "numeric", # Use numeric method for gradient mapping
    palette = c(
    "#00FF00", # Bright Green
    "#80FF00", # Light Green
    "#A8FF00", # Lime Green
    "#FFFF00", # Yellow
    "#FFA500", # Orange
    "#FF4500", # Orange-Red
    "#FF0000", # Bright Red
    "#8B0000"  # Dark Red
),
    domain = c(0, 0.3), # Domain for relative differences
    na_color = "#f0f0f0" # Light gray for missing/NA values
  ) %>%
  tab_header(
    title = "Comparison of Imputation Methods Across Metrics",
    subtitle = "Including Original Data and Relative Differences"
  ) %>%
  cols_label(
    original = "Original",
    linear_imputation = "Linear Imputation",
    linear_imputation_relative = "Linear Imputation Relative",
    mean_imputation = "Mean Imputation",
    mean_imputation_relative = "Mean Imputation Relative",
    upper_quartile = "Upper Quartile",
    upper_quartile_relative = "Upper Quartile Relative",
    pmm = "PMM",
    pmm_relative = "PMM Relative",
    pmm_best = "PMM Best",
    pmm_best_relative = "PMM Best Relative"
    ) %>%
  fmt_number(
    columns = c(original, 
                linear_imputation,
                mean_imputation,
                upper_quartile,
                pmm, 
                pmm_best),
    decimals = 2
  ) %>%
  fmt_number(
    columns = ends_with("_relative"),
    decimals = 3
  ) %>%
  fmt_missing(
    columns = ends_with("_relative"),
    missing_text = "NA"
  ) %>%
  tab_options(
    table.font.size = "small",
    column_labels.font.size = "medium"
  )

# Display the table
comparison_gt
```








#############
# STEP 5
##########################################################################################################################################
MERGING THE IMPUTED DATASET BACK TO THE ORIGINAL DATA AND VISUALISE
##########################################################################################################################################

```{r}
merged_data <- database_clean_sd %>%
  full_join(
    # selecting pmm (out of all the imputation methods, pmm was selected as the best method out of all the imputation methods:
    # mice, upper quartile, mean, linear regression imputation (norm.predict)), based on JSD and variance comparison
    imputed_datasets$pmm %>% 
      select(id_article, id_obs, silvo_se, control_se), # Select relevant imputed columns
    by = c("id_article", "id_obs"),
    suffix = c("_original", "_imputed") # Keep original vs. imputed columns distinct
  )

# View the structure of the merged dataset
merged_data %>%
  glimpse()

# Rows: 1,126
# Columns: 47
```
```{r}
merged_data <- merged_data %>%
  mutate(
    silvo_se = ifelse(is.na(silvo_se_original), silvo_se_imputed, silvo_se_original),
    control_se = ifelse(is.na(control_se_original), control_se_imputed, control_se_original)
  )

# merged_data <- merged_data %>%
#   select(-silvo_se_original, -silvo_se_imputed, -control_se_original, -control_se_imputed)
```

```{r}
# Identify rows where imputation occurred
imputation_evaluation <- merged_data %>%
  filter(
    (is.na(silvo_se_original) & !is.na(silvo_se_imputed)) |
    (is.na(control_se_original) & !is.na(control_se_imputed))
  ) %>%
  select(id_article, id_obs, exp_id) %>%
  distinct()

# Count the number of unique articles where imputation occurred
n_imputed_studies <- imputation_evaluation %>%
  distinct(id_article) %>%
  nrow()

cat("Number of unique articles with imputed values:", n_imputed_studies, "\n")
# Number of unique articles with imputed values: 11 
```


```{r}
# Calculate missing counts and proportions
missing_summary <- merged_data %>%
  summarise(
    total_obs = n(),

    # Missing counts for original dataset
    silvo_se_original_missing = sum(is.na(silvo_se_original)),
    control_se_original_missing = sum(is.na(control_se_original)),

    # Missing counts for imputed dataset
    silvo_se_imputed_missing = sum(is.na(silvo_se_imputed)),
    control_se_imputed_missing = sum(is.na(control_se_imputed))
  ) %>%
  mutate(
    # Proportions for original dataset
    silvo_se_original_proportion = silvo_se_original_missing / total_obs,
    control_se_original_proportion = control_se_original_missing / total_obs,

    # Proportions for imputed dataset
    silvo_se_imputed_proportion = silvo_se_imputed_missing / total_obs,
    control_se_imputed_proportion = control_se_imputed_missing / total_obs
  )

missing_summary
```

```{r}
original_data <- database_clean_sd %>%
  select(id_article, id_obs, silvo_se, control_se) %>%
  mutate(data_source = "Original")

imputed_data <- merged_data %>%
  select(id_article, id_obs, silvo_se, control_se) %>%
  mutate(data_source = "Imputed")

# Combine original and imputed data
combined_data <- bind_rows(original_data, imputed_data)
```

```{r}
# Check for duplicates
duplicates <- merged_data %>%
  group_by(id_article, id_obs) %>%
  filter(n() > 1)

# Check for rows with imputed values
imputation_check <- merged_data %>%
  filter(is.na(silvo_se_original) & !is.na(silvo_se_imputed))

# View summaries
print(duplicates)
print(imputation_check)
```


##########################################################################################################################################
VISUAL DIAGNOSTIGS OF MERGED DATA
##########################################################################################################################################

```{r}
# Pivot data to long format for easier plotting
qq_data <- combined_data %>%
  pivot_longer(
    cols = c(silvo_se, control_se),
    names_to = "variable",
    values_to = "value"
  )



# Compute theoretical quantiles and prepare the data for jitter
qq_data_jitter <- qq_data %>%
  group_by(variable, data_source) %>%
  mutate(
    theoretical = qnorm((rank(value, na.last = "keep") - 0.5) / sum(!is.na(value))) # Compute theoretical quantiles
  ) %>%
  ungroup()

# Create the Q-Q plot with jitter
qqplot_combined <- qq_data_jitter %>%
  ggplot(aes(x = theoretical, y = value, color = data_source)) +
  geom_point(position = position_jitter(width = 0.1, height = 0.1), alpha = 0.5) + # Add jitter to points
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + # Add reference line
  facet_wrap(~variable, scales = "free") + # Separate facets for silvo_se and control_se
  labs(
    title = "Q-Q Plots for Original vs. Imputed Data (with Jitter)",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
    color = "Data Source"
  ) +
  scale_color_viridis_d(
    option = "D",
    begin = 0.2,
    end = 0.8,
    direction = 1
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom"
  )

# View the QQ plot
qqplot_combined
```

```{r}
# Combine the data for silvo_se and control_se into long format
density_data <- combined_data %>%
  pivot_longer(
    cols = c(silvo_se, control_se),
    names_to = "variable",
    values_to = "value"
  )

# Filter out non-positive values for log transformation
density_data_clean <- density_data %>%
  filter(value > 0) # Keep only positive values

# Create density plots
density_plot_clean <- density_data_clean %>%
  ggplot(aes(x = value, color = data_source, fill = data_source)) +
  geom_density(alpha = 0.4, na.rm = TRUE) + # Add density plot with transparency
  scale_x_log10() + # Log-transform the x-axis
  labs(
    title = "Density Distribution of silvo_se and control_se (Log-Transformed)",
    x = "Log-transformed Value",
    y = "Density",
    color = "Data Source",
    fill = "Data Source"
  ) +
  facet_wrap(~variable, scales = "free_x", ncol = 2) + # Separate plots for silvo_se and control_se
  scale_color_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom"
  )

# Display the density plot
density_plot_clean
```


#############
# STEP 6
##########################################################################################################################################
SAVING TWO VERSIONS OF PREPROCESSED DATA FOR FURTHER ANALYSIS AND VISUALIZATION
##########################################################################################################################################


```{r}
# Imputed dataset
imp_pmm_best <- merged_data |> 
  # Modify the imp_pmm_best dataset before saving
  # Remove existing columns
  select(-c(control_se, silvo_se, control_se_original, silvo_se_original)) |>  
  rename(
    # Rename control_se_imputed to control_se
    control_se = control_se_imputed, 
    # Rename silvo_se_imputed to silvo_se
    silvo_se = silvo_se_imputed      
  ) |> 
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


# Non-imputed dataset (remove geometry if necessary)
non_imp_dataset <- database_clean_sd |> 
  as.data.frame() |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
```

```{r}
# Visualize differences in categorical distributions
inspect_cat(imp_pmm_best, non_imp_dataset) %>% show_plot()

# Visualize differences in numerical distributions
# Select numeric columns
imp_pmm_best_numeric <- imp_pmm_best %>% select(where(is.numeric))
non_imp_dataset_numeric <- non_imp_dataset %>% select(where(is.numeric))

# Check column counts
if (ncol(imp_pmm_best_numeric) == 0 || ncol(non_imp_dataset_numeric) == 0) {
  stop("One or both datasets have no numeric columns to compare.")
}

# Custom function to filter columns with valid variance
filter_valid_columns <- function(df) {
  df %>% select(where(~ !all(is.na(.)) && var(., na.rm = TRUE) > 0))
}

# Apply the filter to remove problematic columns
imp_pmm_best_numeric <- filter_valid_columns(imp_pmm_best_numeric)
non_imp_dataset_numeric <- filter_valid_columns(non_imp_dataset_numeric)

# Combine datasets for comparison
imp_pmm_best_numeric$dataset <- "imputed"
non_imp_dataset_numeric$dataset <- "non-imputed"
combined_data <- bind_rows(imp_pmm_best_numeric, non_imp_dataset_numeric)

# Melt into long format for ggplot
combined_long <- combined_data %>%
  pivot_longer(cols = -dataset, names_to = "variable", values_to = "value")

# Plot distributions for all variables
ggplot(combined_long, aes(x = value, fill = dataset)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Density Plots for Numeric Variables", x = "Value", y = "Density") +
  theme_minimal()

# Check for column-wise similarity
inspect_cor(imp_pmm_best, non_imp_dataset) %>% show_plot()

# Missingness comparison
inspect_na(imp_pmm_best, non_imp_dataset) %>% show_plot()
```


```{r}
# SAVING TWO VERSIONS OF PREPROCESSED DATA AS RDS

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# Ensure the output directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save the non-imputed dataset as RDS
saveRDS(non_imp_dataset,
        file = file.path(output_dir, "non_imp_dataset.rds"))


# Save the best-imputed dataset as RDS
saveRDS(imp_pmm_best,
        file = file.path(output_dir, "imp_pmm_best_dataset.rds"))

# Print confirmation messages
cat("Datasets saved successfully:\n")
cat("- Non-imputed dataset: non_imp_dataset.rds\n")
cat("- Best-imputed dataset: imp_pmm_best_dataset.rds\n")
```





#############
# STEP 7
##########################################################################################################################################
CALCULATING EFFECT SIZES FOR IMPUTED AND NON-IMPUTED DATASETS
##########################################################################################################################################


```{r}
non_imp_dataset |> glimpse()
imp_pmm_best |> glimpse()
```




##########################################################################################################################################
EFFECT SIZE CALCULATION
##########################################################################################################################################

```{r}
# Function for data preparation and effect size calculation

# This function takes a dataset as input and applies several transformations
# to clean, filter, and prepare it for effect size calculation.
prep_dataset_for_rom <- function(data) {
  data %>%
    # Step 1: Filter out rows where the standard errors are zero or negative
    # - Standard errors (SE) must be positive for valid statistical calculations.
    # - Removing these rows prevents mathematical errors in subsequent calculations (e.g., division by zero).
    filter(silvo_se > 0, control_se > 0) %>%
    
    # Step 2: Adjust the sign of mean values for specific response variables
    # - For variables like "Greenhouse gas emissions," "Pests and Diseases," and "Water quality,"
    #   lower values are considered better (e.g., lower emissions or fewer pests).
    # - We negate the mean values to reflect this interpretation correctly.
    mutate(
      silvo_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"),
                          -silvo_mean, silvo_mean),
      control_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"),
                            -control_mean, control_mean)
    ) %>%
    
    # Step 3: Exclude specific response variables from analysis
    # - "Soil water content" is excluded due to inconsistent data or limited measurements.
    filter(response_variable != "Soil water content") %>%
    
    # Step 4: Remove rows with missing values in key columns
    # - Ensures that the data is complete for effect size calculation.
    # - Excludes rows where any of the following are missing: mean, sample size, or standard error for both groups.
    filter(!is.na(silvo_mean) & !is.na(control_mean) &
           !is.na(silvo_n) & !is.na(control_n) &
           !is.na(silvo_se) & !is.na(control_se)) %>%
    
    # Step 5: Calculate standard deviations from standard errors and sample sizes
    # - Standard deviation (SD) is calculated using the formula: SD = SE * sqrt(n)
    # - This step is necessary for effect size calculations, which often require SDs rather than SEs.
    mutate(
      silvo_sd = silvo_se * sqrt(silvo_n),
      control_sd = control_se * sqrt(control_n),
      
      # Step 6: Shift mean values to be positive
      # - We calculate a shift value (min_value_shift) based on the absolute minimum value
      #   of the means, ensuring all values are positive.
      # - This shift is necessary for certain transformations (e.g., log transformations),
      #   which require positive inputs.
      min_value_shift = abs(min(c(silvo_mean, control_mean), na.rm = TRUE)) + 1,
      
      # Apply the shift to the mean values for both groups
      silvo_mean = silvo_mean + min_value_shift,
      control_mean = control_mean + min_value_shift
    ) %>%
    
    # Step 7: Reorder columns for better readability and organization
    # - Places important columns (e.g., identifiers, means, SEs, SDs, sample sizes) at the front of the data frame.
    relocate(id_article, response_variable, measured_metrics, measured_unit,
             silvo_mean, silvo_se, silvo_sd, silvo_n,
             control_mean, control_se, control_sd, control_n) %>%
    
    # Step 8: Sort the data by article ID and response variable for consistency
    # - Sorting helps ensure that the data is organized and facilitates easier inspection and analysis.
    arrange(id_article, response_variable)
}
```

```{r}
# Apply the data preparation function to both non-imputed and imputed datasets
# - This creates cleaned and prepared datasets for effect size calculation.
non_imp_data_prep <- prep_dataset_for_rom(non_imp_dataset)
imp_data_prep <- prep_dataset_for_rom(imp_pmm_best)
```

```{r}
# Generic function for effect size calculation using `escalc()`

# This function can be applied to both imputed and non-imputed datasets.

calculate_effect_sizes <- function(data, measure = "ROM") {
  # Check if the required columns are present in the dataset
  required_columns <- c("silvo_mean", "silvo_sd", "silvo_n",
                        "control_mean", "control_sd", "control_n",
                        "id_article", "experiment_year")
  
  if (!all(required_columns %in% names(data))) {
    stop("The dataset is missing one or more required columns.")
  }
  
  # Calculate effect sizes using `escalc()`
  result <- escalc(
    measure = measure,           # Specify the effect size measure (default is "ROM").
    
    # Experimental group (silvo_) parameters:
    m1i = silvo_mean,            # Mean of the experimental (silvo) group.
    sd1i = silvo_sd,             # Standard deviation of the experimental (silvo) group.
    n1i = silvo_n,               # Sample size of the experimental (silvo) group.
    
    # Control group (control_) parameters:
    m2i = control_mean,          # Mean of the control group.
    sd2i = control_sd,           # Standard deviation of the control group.
    n2i = control_n,             # Sample size of the control group.
    
    # Study labels for identification:
    slab = paste(id_article, ", ", experiment_year, sep = ""),
    
    # The input dataset:
    data = data
  ) %>%
    as.data.frame()              # Convert the result to a data frame for easier handling.
  
  # Return the resulting data frame with calculated effect sizes
  return(result)
}
```

```{r}
# Apply the function to both non-imputed and imputed datasets
non_imp_data_rom <- calculate_effect_sizes(non_imp_data_prep)
imp_data_rom <- calculate_effect_sizes(imp_data_prep)
```




Heterogeneity Analysis
Compare the heterogeneity statistics between the meta-analyses conducted on the imputed and non-imputed datasets. This will help you understand if the imputation has influenced the heterogeneity of the studies.

```{r}
# Prepare the original data
original_data_x <- non_imp_data_rom %>%
  select(id_article, id_obs, exp_id, id_obs, response_variable, 
         silvo_se, control_se, yi, vi) |> 
  mutate(data_source = "Original") |> 
  as.data.frame() 

imputed_data_y <- imp_data_rom |> 
  select(id_article, id_obs, exp_id, id_obs, response_variable, 
         silvo_se, control_se, yi, vi) |> 
  mutate(data_source = "Imputed") 

# Combine the original and imputed data
combined_data <- bind_rows(original_data_x, imputed_data_y)

# Join the original and imputed data to directly compare
comparison_data <- original_data_x %>%
  full_join(imputed_data_y, by = c("id_article", "exp_id", "id_obs", "response_variable"), 
            suffix = c("_original", "_imputed")) %>%
  distinct()

# Advarsel: Detected an unexpected many-to-many relationship between `x` and `y`.

comparison_data |> glimpse()
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Fit random-effects models on both datasets
rma_non_imp <- rma(yi = yi_original, vi = vi_original, data = comparison_data)
rma_imp <- rma(yi = yi_imputed, vi = vi_imputed, data = comparison_data)

# Print heterogeneity statistics
cat("Non-Imputed Data - Heterogeneity (I^2):", rma_non_imp$I2, "\n")
cat("Non-Imputed Data - Between-study variance (tau^2):", rma_non_imp$tau2, "\n")

cat("Imputed Data - Heterogeneity (I^2):", rma_imp$I2, "\n")
cat("Imputed Data - Between-study variance (tau^2):", rma_imp$tau2, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 4.055149 secs
# Advarsel: 186 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 10.13463 
# Non-Imputed Data - Between-study variance (tau^2): 1.468685e-06 
# Imputed Data - Heterogeneity (I^2): 99.93572 
# Imputed Data - Between-study variance (tau^2): 0.009922055 
# Time difference of 4.055149 secs

# Last go (01/01-2025)
# Time difference of 5.424392 secs
# Advarsel: 176 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93018 
# Imputed Data - Between-study variance (tau^2): 0.01053153 
# Time difference of 5.424392 secs

# Last go (02/01-2025)
# Time difference of 6.113353 secs
# Advarsel: 177 studies with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Non-Imputed Data - Heterogeneity (I^2): 9.354748 
# Non-Imputed Data - Between-study variance (tau^2): 1.438406e-06 
# Imputed Data - Heterogeneity (I^2): 99.93201 
# Imputed Data - Between-study variance (tau^2): 0.01083873 
# Time difference of 6.113353 secs
```
 
 
 
Kolmogorov-Smirnov Test
The Kolmogorov-Smirnov (KS) test can be used to compare the distributions of effect sizes.

```{r}
# Kolmogorov-Smirnov test
ks_test_result <- ks.test(non_imp_data_rom$yi, imp_data_rom$yi)
print(ks_test_result)

# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.040734, p-value = 0.4179
# alternative hypothesis: two-sided

# Last go (01/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.030426, p-value = 0.7468
# alternative hypothesis: two-sided

# Last go (02/01-2025)
# Advarsel: p-value will be approximate in the presence of ties
# 	Asymptotic two-sample Kolmogorov-Smirnov test
# 
# data:  non_imp_data_rom$yi and imp_data_rom$yi
# D = 0.031966, p-value = 0.6897
# alternative hypothesis: two-sided
```

The Kolmogorov-Smirnov (KS) test was conducted to compare the distributions of effect sizes (ROM) between the imputed and non-imputed datasets. The test statistic D was 0.0407, with a p-value of 0.4179. This high p-value indicates that there is no significant difference between the distributions of the two datasets. Thus, we fail to reject the null hypothesis, suggesting that both datasets share a similar underlying distribution.

A warning about approximate p-values due to ties was noted. This occurs when there are identical values within the data, which can slightly affect the accuracy of the p-value calculation. However, given the large sample size, this effect is minimal and does not alter the overall interpretation.

These results align with earlier findings, such as the identical between-study variance implying that the imputation process did not introduce significant changes. The small D value and non-significant p-value suggest that the imputed values closely resemble the original data.

Overall, the KS test supports the conclusion that the imputation process was effective in preserving the distribution of effect sizes, allowing for a consistent comparison in the meta-analysis without introducing bias or altering the data structure.
 
 
 
```{r}
# Descriptive statistics for non-imputed data
summary_non_imp <- non_imp_data_rom %>%
  summarise(
    mean_yi = mean(yi, na.rm = TRUE),
    median_yi = median(yi, na.rm = TRUE),
    sd_yi = sd(yi, na.rm = TRUE)
  )

# Descriptive statistics for imputed data
summary_imp <- imp_data_rom %>%
  summarise(
    mean_yi = mean(yi, na.rm = TRUE),
    median_yi = median(yi, na.rm = TRUE),
    sd_yi = sd(yi, na.rm = TRUE)
  )

# Print summaries
summary_non_imp
summary_imp

# Last go (01/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02660392	0.002633719	0.2446765	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02660392	0.002633719	0.2446765

# Last go (02/01-2025)
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02767394	0.003205885	0.2501374	
# 
# Description:df [1 × 3]
# mean_yi
# <dbl>
# median_yi
# <dbl>
# sd_yi
# <dbl>
# 0.02369684	0.001916168	0.2386678	
```
 
```{r}
# Density plot for non-imputed data
density_plot_non_imp <- ggplot(non_imp_data_rom, aes(x = yi)) +
  geom_density(fill = "#0072B2", alpha = 0.5) +
  labs(title = "Density Plot: Non-Imputed Data",
       x = "Effect Size (yi)", y = "Density") +
  theme_minimal()

# Density plot for imputed data
density_plot_imp <- ggplot(imp_data_rom, aes(x = yi)) +
  geom_density(fill = "#E69F00", alpha = 0.5) +
  labs(title = "Density Plot: Imputed Data",
       x = "Effect Size (yi)", y = "Density") +
  theme_minimal()

# Boxplot for non-imputed data
boxplot_non_imp <- ggplot(non_imp_data_rom, aes(y = yi)) +
  geom_boxplot(fill = "#0072B2", alpha = 0.5) +
  labs(title = "Boxplot: Non-Imputed Data",
       y = "Effect Size (yi)") +
  theme_minimal()

# Boxplot for imputed data
boxplot_imp <- ggplot(imp_data_rom, aes(y = yi)) +
  geom_boxplot(fill = "#E69F00", alpha = 0.5) +
  labs(title = "Boxplot: Imputed Data",
       y = "Effect Size (yi)") +
  theme_minimal()

# Arrange the plots in a 2x2 layout
(density_plot_non_imp | density_plot_imp) /
(boxplot_non_imp | boxplot_imp)
```








#############
# STEP 8
##########################################################################################################################################
SAVING FINAL PREPROCESSED META-DATASETS (IMPUTED AND NON-IMPUTED) - THESE ARE USED FOR RMA.MV() MODELLING
##########################################################################################################################################


Saving the datasets that is used for the rma.mv() modelling

```{r}
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Ensure the output directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save as RDS files

## NON-IMPUTED
saveRDS(non_imp_data_rom,
        file = here::here(output_dir, "non_imp_data_rom.rds"))

## IMPUTED
saveRDS(imp_data_rom,
        file = here::here(output_dir, "imp_data_rom.rds"))



# Confirmation message
cat("Files saved successfully to:", output_dir, "\n")
```

git fetch origin
git pull origin main
git push origin HEAD:refs/heads/main --force

