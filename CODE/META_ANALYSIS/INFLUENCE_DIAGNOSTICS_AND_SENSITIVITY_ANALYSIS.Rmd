---
title: "INFLUENCE_DIAGNOSTICS_AND_SENSITIVITY_ANALYSIS"
author: "M.K.K. Lindhardt"
date: "2024-11-04"
output: html_document
---


#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    tidygeocoder,     # For geocoding addresses to latitude/longitude
    rnaturalearth,    # For accessing Natural Earth map data
    rnaturalearthdata,# Companion package to rnaturalearth providing the data
    ###################################################################################################################
    # Spatial Data
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    ###################################################################################################################
    # Soil Data
    soilDB,           # For downloading soil data from ISRIC SoilGrids
    aqp,              # For soil profile visualization and analysis
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
```



############################################################################################################################
READING THE PREPROCESSED AND FILTERED METADATA CSV AND VARIANCE-COVARIANCE MATRIX AND ORIGINAL MODEL
############################################################################################################################

```{r}
metdata <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/filtered_meta_data_rom_v5.csv")

V_matrix <- readRDS("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/V_matrix_v5.rds")

# Load the model object from the file
original_model <- readRDS("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/original_model_v5.rds")

##################################################################################
metdata
```











#############
# STEP 1
##########################################################################################################################################
INFLUENCE DIAGNOSTICS AND SENSITIVITY ANALYSIS ON DATA (E.G.: MISSING OUTCOME DATA)
##########################################################################################################################################

```{r, eval = FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################
# Initialize a list to store the results
sensitivity_results <- list()

# Get the list of unique study IDs
unique_studies <- unique(metdata$id_article)

# Loop through each study and fit the model excluding that study
for (study in unique_studies) {
  # Exclude the current study
  data_excluded <- metdata[metdata$id_article != study, ]
  
  # Exclude corresponding rows and columns from the variance-covariance matrix
  V_matrix_excluded <- V_matrix[metdata$id_article != study, metdata$id_article != study]
  
  # Fit the model using the same settings as the original model
  model <- rma.mv(
    yi = yi, 
    V = V_matrix_excluded, 
    random = list(
      ~ 1 | id_article,
      ~ 1 | id_article/response_variable
    ), 
    data = data_excluded,
    method = "ML",  # Consistent with the original model's method
    control = list(
      # Optimizer to be used for fitting the model; 'nlminb' is robust for constrained optimization problems. 
      # Changed to 'optim' (the 08/10-24) with BFGS that can better handle complex, non-linear surfaces and is generally faster in converging. 
      # nlminb might get stuck more easily if the likelihood function is tricky or has flat areas, nlminb more robust when parameter constraints are needed.
      optimizer = "optim",  
      # Use BFGS optimization method, 
      optim.method = "BFGS",
      # Maximum number of iterations for the optimizer; increase if the model is complex or if convergence is slow
      iter.max = 1000,              
      # Relative convergence tolerance; lower values indicate stricter convergence criteria
      rel.tol = 1e-12     # with nlminb I reduced from 1e-8 to 1e-4 (the 08/10-24) to avoid non-convergence      
      # Alternatively, provide initial estimates for variance components to ease convergence
      #sigma2.init = c(0.01, 0.01, 0.01)  
      ),
    verbose = FALSE  # Set to FALSE to avoid excessive output during loop
  )
  
  # Store the results
  sensitivity_results[[as.character(study)]] <- list(
    estimate = model$b,
    se = model$se,
    ci.lb = model$ci.lb,
    ci.ub = model$ci.ub
  )
}

# Convert the results to a data frame
sensitivity_df <- do.call(rbind, lapply(sensitivity_results, function(x) data.frame(
  estimate = x$estimate,
  se = x$se,
  ci.lb = x$ci.lb,
  ci.ub = x$ci.ub
)))

sensitivity_df$id_article <- unique_studies

# sensitivity_df
##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################


#################################################################################
# Last go: (04/11-24)
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 52.45126 mins

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 1.03152 hours
```

```{r, eval = FALSE}
sensitivity_df
```


SAVING THE SENSITIVITY MATRIX

```{r, eval = FALSE}
readr::write_csv(sensitivity_df,
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/sensitivity_df_original_model_database_v5.csv"))
```


Discussion of Model Setup and Trade-offs in this Meta-Analysis




READING THE SAVED SENSITIVITY MATRIX

```{r}
sensitivity_df <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/sensitivity_df_original_model_database_v5.csv")

sensitivity_df
```








##########################################################################
Use the original model fitting
##########################################################################

```{r}
# Extract the original model results
# - 'original_estimate' is the original effect size estimate from the model
# - 'original_ci.lb' is the lower bound of the confidence interval for the original estimate
# - 'original_ci.ub' is the upper bound of the confidence interval for the original estimate

original_estimate <- original_model$b
original_ci.lb <- original_model$ci.lb
original_ci.ub <- original_model$ci.ub

##################################################
##################################################
```


Visualize the changes
Create a plot to visualize the influence of excluding each study

```{r}
# Add the original model results to the sensitivity data frame before plotting
# - 'sensitivity_df' is a data frame containing the results of the sensitivity analysis
# - 'original_estimate' is added to each row of the data frame
# - 'original_ci.lb' is added to each row of the data frame
# - 'original_ci.ub' is added to each row of the data frame

sensitivity_df <- sensitivity_df %>%
  mutate(original_estimate = original_estimate,
         original_ci.lb = original_ci.lb,
         original_ci.ub = original_ci.ub)

# Plot the results
sensitivity_df |> 
  ggplot(aes(x = id_article, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci.lb, ymax = ci.ub), width = 0.2) +
  geom_hline(aes(yintercept = original_estimate), linetype = "dashed", color = "red") +
  geom_hline(aes(yintercept = original_ci.lb), linetype = "dotted", color = "blue") +
  geom_hline(aes(yintercept = original_ci.ub), linetype = "dotted", color = "blue") +
  labs(title = "Sensitivity Analysis",
       x = "Study ID",
       y = "Estimate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_x_continuous(breaks = seq(1, 40, 1))

```

Interpretation of the Sensitivity Analysis Plot
The sensitivity analysis plot shows the effect size estimates (points) and their 95% confidence intervals (error bars) for each study in your meta-analysis. The red dashed line represents the overall effect size estimate from the original model, while the blue dotted lines represent the upper and lower bounds of the 95% confidence interval for the overall estimate. Each point corresponds to the effect size estimate when the respective study is excluded from the analysis.

Consistency of Estimates: Most of the points are centered around the red dashed line (the original estimate), indicating that excluding any single study does not substantially alter the overall effect size estimate.
Confidence Intervals: The confidence intervals are relatively wide, which might suggest variability in the individual study estimates or small sample sizes within the studies.

Influential Studies: If any points lie significantly away from the red dashed line or their confidence intervals do not overlap with the original confidence interval (blue dotted lines), these studies might be considered influential. In this plot, it appears that none of the studies drastically change the overall estimate when excluded.
Suggestions for Further Insights
Leave-One-Out Meta-Analysis: Perform a leave-one-out meta-analysis to quantify the influence of each study on the overall effect size and heterogeneity measures. This involves refitting the model after excluding each study one at a time and comparing the overall results.

Cumulative Meta-Analysis: Conduct a cumulative meta-analysis where studies are added one at a time in a chronological order (or by study size or quality) to observe how the overall effect size estimate evolves as more data is included.

Influence Diagnostics: Calculate and plot various influence diagnostics such as:

Cook's Distance: To measure the influence of each study on the fitted model.
DFBETAS: To assess the influence of each study on individual regression coefficients.
Hat Values: To identify studies with high leverage.
Covariance Ratios: To understand the influence of each study on the precision of the parameter estimates.
Subgroup Analysis: Perform subgroup analyses to explore if certain subsets of studies (e.g., by study design, population characteristics, or intervention type) have a different impact on the overall effect size.

Meta-Regression: Use meta-regression to investigate the relationship between study-level covariates and the effect sizes, which can help identify factors contributing to heterogeneity.

Robustness Checks: Conduct robustness checks by varying the assumptions of the model, such as using different methods for estimating the between-study variance or using alternative effect size measures.





##########################################################################
OTHER DIAGNOSTICS TO GET FURTHER INSIGHTS INTO MODEL STUDY SENSITIVITY
##########################################################################


Studentized Residuals:

Purpose: These residuals are used to detect outliers. They are the residuals divided by an estimate of their standard deviation, adjusted for each observation.
Interpretation: Values beyond ±2 are often considered potential outliers. The plot helps in identifying these observations visually. Points far from the center line (0) indicate observations with larger than expected residuals.




Cook's Distance:

Purpose: Cook's distance assesses the influence of each observation on the overall model. Large values indicate observations that have a significant impact on the model's parameters.
Interpretation: Typically, Cook's distance values greater than 1 may indicate influential points. In large datasets, even smaller values can be significant. Points far above the average line in the plot are considered highly influential.

*OBS! Time for running this code below takes several days*

```{r, eval = FALSE}
# Calculate influence diagnostics

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

##############################################################
# Cook's Distance
##############################################################
# Compute Cook's Distance
cooks_d <- cooks.distance(original_model, 
                          reestimate = TRUE,
                          progbar = TRUE)

# Create a data frame for Cook's Distance
cooks_data <- data.frame(
  Observed_Outcome = seq_along(cooks_d),
  Cooks_Distance = cooks_d
)

# Save to CSV
readr::write_csv(cooks_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cooks_data_5.csv"))

##############################################################
# DFBETAS
##############################################################
# Compute DFBETAS
dfbetas_vals <- dfbetas(original_model, 
                        progbar = TRUE)

# Convert DFBETAS to a data frame
dfbetas_data <- as.data.frame(dfbetas_vals)

# Save to CSV
readr::write_csv(dfbetas_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/dfbetas_data_5.csv"))

##############################################################
# Studentized Residuals
##############################################################
# Compute Studentized Residuals
student_resid <- rstudent(original_model, 
                          progbar = TRUE)

# Convert Studentized Residuals to a data frame
student_resid_data <- data.frame(
  Observed_Outcome = seq_along(student_resid$resid),
  Residuals = student_resid$resid,
  Standard_Errors = student_resid$se,
  Z_Values = student_resid$z
)

# Save to CSV
readr::write_csv(student_resid_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/studentized_residuals_data_5.csv"))

##############################################################
# Hat values
##############################################################
# Compute Hat Values
hat_vals <- hatvalues(original_model)

# Convert Hat values to a data frame
hat_vals_data <- as.data.frame(hat_vals)

# Save to CSV
readr::write_csv(hat_vals_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/hat_vals_data_5.csv"))


##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

#################################################################################
# Last go: (04/11-24)
#   |==================================================| 100% elapsed=04h 03m 25s
#   |==================================================| 100% elapsed=04h 21m 16s
#   |==================================================| 100% elapsed=05h 44m 09s
# Time difference of 14.14954 hours
```



READING THE DIAGNOSTICS DATA

```{r}
# Load Cook's Distance data
cooks_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cooks_data_5.csv")

# Load DFBETAS data
dfbetas_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/dfbetas_data_5.csv")

# Load Studentized Residuals data
student_resid_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/studentized_residuals_data_5.csv")

# Load Hat Values data
hat_vals_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/hat_vals_data_5.csv")
```


##########################################################################################################################################  
PLOTTING INFLUENCE DIAGNOSTICS
##########################################################################################################################################


```{r}
# Plot Studentized Residuals

# Create a data frame for ggplot2
residuals_data <- data.frame(
  Observed_Outcome = seq_along(student_resid_data$Observed_Outcome),
  Studentized_Residuals = student_resid_data$Residuals
)

# Create the ggplot2 plot
residuals_data_plot <- 
residuals_data |> 
ggplot(aes(x = Observed_Outcome, y = Studentized_Residuals)) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  geom_segment(aes(xend = Observed_Outcome, yend = 0), linetype = "solid") +
  geom_point() +
  ylim(-3, 3) +
  labs(title = "Studentized Residuals Plot", 
       x = "Observed Outcome", 
       y = "Studentized Residuals") +
  theme_minimal()

residuals_data_plot
```

```{r, eval = FALSE}
# Saving the residuals data plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "residuals_data_plot_v5.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = residuals_data_plot,
       width = 10, height = 8, dpi = 400)
```

```{r}
# Plot Cook's Distance

# Create a data frame for ggplot2 with appropriate lengths
cooks_data_df <- data.frame(
  Observed_Outcome = seq_along(cooks_data$Cooks_Distance),
  Cooks_Distance = cooks_data$Cooks_Distance
)

# Plot Cook's Distance
cooks_data_df_plot <-
ggplot(cooks_data_df, aes(x = Observed_Outcome, y = Cooks_Distance)) +
  geom_point() +
  geom_hline(yintercept = 4 / nrow(cooks_data_df), linetype = "dashed", color = "red") + 
  labs(
    title = "Cook's Distance Plot",
    x = "Observed Outcome",
    y = "Cook's Distance"
  ) +
  theme_minimal()

cooks_data_df_plot
```

```{r, eval = FALSE}
# Saving the Cook's Distance plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "cooks_data_df_plot_v5.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = cooks_data_df_plot,
       width = 10, height = 8, dpi = 400)
```

A Cook's Distance plot helps identify influential observations in a regression analysis, indicating which data points substantially affect the model's predictions. Observations with high Cook's Distance values suggest that their exclusion would significantly change the model's results. This is crucial in our sensitivity analysis, where we systematically exclude each study to assess the robustness of our findings. By comparing Cook's Distance results with other sensitivity analyses outcomes, we can pinpoint studies or data points that disproportionately influence the meta-analysis. If certain observations are flagged as influential in both analyses, they may require further investigation to check for data entry errors, outliers, or unique characteristics that justify their influence. Based on these insights, we can decide whether to exclude, adjust, or treat these influential observations differently to enhance model reliability. Documenting and addressing these influential points ensures the robustness and credibility of our meta-analysis, providing transparency and enhancing the overall validity of your findings.

In our Cook's Distance plot, each point represents an observation's Cook's Distance, a measure of how much a single observation influences the overall regression analysis. Higher Cook's Distance values indicate that the observation has a more significant impact on the model's predictions, potentially skewing our model results. The red dashed line represents a commonly used threshold, calculated as 4/number of observations, which helps to identify influential observations. Points above this line are considered to have a substantial influence on the model, and we should further investigate their  impact. In our plot, most of the observations have Cook's Distance values near zero, suggesting limited individual influence on the model. However, a few points exceed the threshold, indicating that these observations might be outliers or have unique characteristics that disproportionately affect our model's estimates. These high-influence points needs closer examination to assess whether they should be included, adjusted, or excluded to ensure the robustness and reliability of our meta-analysis regression model.

```{r}
# Plot Hat Values

# Create a data frame for hat values
hat_values_data <- data.frame(
  Observed_Outcome = seq_along(hat_vals),
  Hat_Values = hat_vals
)

# Compute the threshold line
threshold <- 2 * mean(hat_vals)

# Plot Hat Values using ggplot2
hat_values_data_plot <-
ggplot(hat_values_data, aes(x = Observed_Outcome, y = Hat_Values)) +
  geom_segment(aes(xend = Observed_Outcome, yend = 0), color = "blue") +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red") +
  labs(title = "Hat Values Plot", x = "Observed Outcome", y = "Hat Values") +
  theme_minimal()

hat_values_data_plot
```

```{r, eval = FALSE}
# Saving the Hat Values plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "hat_values_data_plot_v5.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = hat_values_data_plot,
       width = 10, height = 8, dpi = 400)
```

A Hat Value plot is a diagnostic tool used in regression analysis to evaluate the influence or leverage of each data point on the fitted model. In this plot, each point represents a hat value, which is derived from the diagonal elements of the hat matrix used in the regression calculation. Hat values measure how far an observation's predictor values are from the mean of all predictor values, indicating its influence on the model's predictions. High hat values suggest that the corresponding observation has substantial leverage, meaning it can significantly affect the regression line or model fit.

In the our plotted results, each blue line represents the hat value for a corresponding observed outcome, with a red dashed line indicating a threshold set at twice the average hat value (2 * mean(hat_vals)). Observations with hat values above this threshold are considered to have high leverage and we should examine these more closely, as they can disproportionately influence the results of our model. Identifying these high-leverage points is crucial for assessing the robustness and reliability of the model, guiding potential adjustments, or making decisions about excluding these points to improve model accuracy and validity.

```{r}
# Plot weights of model fitting - the weights given to the observed effect sizes or outcomes during the model fitting

# Extract weights
weights_vals <- weights(original_model)

# Create a data frame for plotting
weights_data <- data.frame(
  Observed_Outcome = seq_along(weights_vals),
  Weights = weights_vals
)

# Plot the weights using ggplot2
weights_model_fit_plot <- 
ggplot(weights_data, aes(x = Observed_Outcome, y = Weights)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Weights for Observed Outcomes", x = "Observed Outcome", y = "Weight") +
  theme_minimal() +
  theme(panel.grid.major = element_line(colour = "grey90"),
        panel.grid.minor = element_blank())

weights_model_fit_plot
```

```{r, eval = FALSE}
# Saving the Hat Values plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "weights_model_fit_plot_v5.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = weights_model_fit_plot,
       width = 4, height = 2, dpi = 400)
```

The weights plot for model fitting visually represents how much influence each observed outcome has on the overall meta-analysis model. In this context, weights are inversely related to the variance of the observations: smaller variances, indicating more precise estimates, are given higher weights. The plot shows each bar representing an observed outcome, with its height indicating the weight assigned to that outcome. From the provided plot, we can see that a few observations have exceptionally high weights, suggesting they have a significant impact on the model's fit. This disproportionate influence implies that these particular data points are much more precise (or have smaller variances) than others. It will be important to examine these high-weight observations to understand why they are so influential, which might involve checking the underlying study designs or measurement methods. Identifying such influential data points is crucial for ensuring the robustness of our meta-analysis results, potentially leading to adjustments or exclusions for maintaining validity.

#################################################################################
Covariance Ratios plot

*OBS! Time for running this code below is long*

```{r, eval = FALSE}
# Preparing data for Covariance Ratios plot

#################################################################################
# The function calc_cov_r takes a long time to run because it fits a new model by excluding one observation at a time for each observation in the dataset. This process involves re-fitting the model k times (where k is the number of observations), which is computationally expensive, especially for large datasets or complex models.

# Need to create a function to calculate the covariance ratios
calc_cov_r <- function(model) {
  # Extract the variance-covariance matrix of the random effects
  V <- vcov(model)
  
  # Calculate covariance ratio for each observation
  cov_r <- rep(NA, model$k)
  
  for (i in 1:model$k) {
    model_excl <- try(update(model, subset = -i), silent = TRUE)
    if (!inherits(model_excl, "try-error")) {
      V_excl <- vcov(model_excl)
      cov_r[i] <- det(V_excl) / det(V)
    }
  }
  
  return(cov_r)
}

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Calculate covariance ratios
cov_r <- calc_cov_r(original_model)

# Create a data frame for plotting
cov_r_data <- data.frame(
  Observed_Outcome = seq_along(cov_r),
  Covariance_Ratios = cov_r
)

##################################################
##################################################

##################################################
# Save to CSV
readr::write_csv(cov_r_data, 
here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cov_r_data_5.csv"))
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################

##############################################################
# Last go: (08/09-24)
# Time difference of 14.59279 hours

#################################################################################
# Last go: (04/11-24)

```

```{r}
cov_r_data <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cov_r_data_5.csv")
```


```{r}
# Plot Covariance Ratios using ggplot2

cov_var_ratio_plot <- cov_r_data |> 
ggplot(aes(x = Observed_Outcome, y = Covariance_Ratios)) +
  geom_segment(aes(xend = Observed_Outcome, yend = 1), color = "blue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Covariance Ratios Plot", x = "Observed Outcome", y = "Covariance Ratios") +
  theme_minimal() +
  theme(panel.grid.major = element_line(colour = "grey90"),
        panel.grid.minor = element_blank())

cov_var_ratio_plot
```

```{r, eval = FALSE}
# Saving the Covariance Ratios plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "cov_var_ratio_plot_v5.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = cov_var_ratio_plot,
       width = 6, height = 3, dpi = 400)
```



#############
# STEP 2
##########################################################################################################################################
ANALYSING MODERATORS
##########################################################################################################################################


```{r}
metdata |> glimpse()
# metdata |> select(tree_crop_combination) |> 
#   unique()

# Moderators to be investigated:

# "tree_crop_combination_t",
# "season",
# "alley_width",
# "tree_row_orientation",
# "soil_texture",
# "no_tree_per_m", 
# "tree_height",
# "age_system",
# "alley_width",
# "tillage",
# "organic"

```


Performing a moderator analysis using the metafor package in R, can be done by following these steps. Moderator analysis helps to understand how different study characteristics (moderators) influence the effect sizes and whether these moderators can explain some of the heterogeneity observed in the overall meta-analysis. The moderators listed will be included in the moderator assessment of the meta-analysis model to investigate their impact.

Moderator Analysis Step-by-Step 


  1. Data Preparation: Ensure that the variables of interest are used as moderators are correctly formatted and coded in the dataset. 
     Convert categorical variables to factors if they are not already.

  2. Model Fitting with Moderators: Use the rma.mv() function to fit a random-effects meta-analysis model, including the moderators as fixed effects. This will allow to see the influence of each moderator on the effect size.

  3. Interpretation of Results: Assess the coefficients for each moderator to determine their impact on the outcome. Significant moderators can provide insights into factors that influence the effect sizes.
  
  (so for each of the response var. make a model for moderator analysis)
  
  
  
  
  
  
  
**Test each of the moderators if there is a significant effect on the response variables of interest (ES). **

1) Pool/aggregate all the studies that have the same response variable

This model estimates the effect sizes for the moderator variables while accounting for random effects (article-level and response variable level).
moderator_model <- rma.mv(
  # The effect size estimates (dependent variable).
  yi = yi,          
  # The variance-covariance matrix of the effect sizes (weights for the meta-analysis).
  V = V_matrix,          
  # The moderator variables (predictors) included in the model.
  mods = moderator_formula,     
  random = list(
  # Random intercept for each article to capture article-level variability.
    ~ 1 | id_article,           
  # Random intercept for each response variable within each article to account for within-article variability.
    ~ 1 | id_article/response_variable  
  ),
  

```{r}
# Check the reference levels for each moderator
levels(as.factor(metdata$tree_crop_combination))
```
```{r}
# Convert each moderator to a factor with explicit levels

# 1. Tree-Crop Combination
metdata$tree_crop_combination <- factor(metdata$tree_crop_combination, 
                                        levels = c("Biomass-Cereal", "Biomass-Legume", "Biomass-Oilseed", 
                                                   "Fruit-Legume", "Mixed-Cereal", "Mixed-Legume", 
                                                   "Timber-Cereal", "Timber-Legume", "Nut-Root", "Other"))

# 2. Season
metdata$season <- factor(metdata$season, levels = c("Summer", "Winter", "Winter and Summer"))

# 3. Alley Width
# Convert alley_width to numeric if it's not already
metdata$alley_width <- as.numeric(metdata$alley_width)
# Categorize alley width
metdata <- metdata |> 
  mutate(alley_width_category = case_when(
    alley_width <= 10 ~ "Narrow",
    alley_width > 10 & alley_width <= 25 ~ "Moderate",
    alley_width > 25 ~ "Wide"
  ) |> as.factor()
  )

# 4. Tree Row Orientation
metdata$tree_row_orientation <- factor(metdata$tree_row_orientation, 
                                       levels = c("NS", "EW", "Other"))

# 5. Soil Texture
metdata$soil_texture <- factor(metdata$soil_texture, 
                               levels = c("Clay", "Clay loam", "Silty clay", "Sand", "Silt"))

# 6. Tree Density - Number of Trees per Meter (no_tree_per_m)
# Convert no_tree_per_m to numeric if needed
metdata$no_tree_per_m <- as.numeric(metdata$no_tree_per_m)
# Categorize tree density
metdata <- metdata |> 
  mutate(tree_density_category = case_when(
    no_tree_per_m <= 0.5 ~ "Low density",
    no_tree_per_m > 0.5 & no_tree_per_m <= 1.5 ~ "Medium density",
    no_tree_per_m > 1.5 ~ "High density"
  ) |> as.factor()
  )

# 7. Tree Height
# Convert tree_height to numeric if it's not already
metdata$tree_height <- as.numeric(metdata$tree_height)
# Create the categorized tree height variable
metdata <- metdata |> 
  mutate(tree_height_category = case_when(
    tree_height <= 5 ~ "Short",
    tree_height > 5 & tree_height <= 12 ~ "Medium",
    tree_height > 12 ~ "Tall"
  ) |> as.factor()
  )

# 8. Age System
metdata$age_system <- factor(metdata$age_system, 
                             levels = c("Young", "Medium", "Mature"))

# 9. Tillage
metdata$tillage <- factor(metdata$tillage, levels = c("Yes", "No"))

# 10. Organic
metdata$organic <- factor(metdata$organic, levels = c("Yes", "No"))

# Glimpse the updated dataset to confirm changes
# metdata |> glimpse()

```

```{r}
# List of moderators to investigate
moderators <- c("tree_crop_combination",
                "season",
                "alley_width_category",
                "tree_row_orientation",
                "soil_texture",
                "tree_density_category", 
                "tree_height_category",
                "age_system",
                "tillage",
                "organic")
```

```{r}
metdata_moderators <- metdata |> select(yi, 
                                        vi,
                                        id_article,
                                        response_variable,
                                        tree_crop_combination,
                                        season,
                                        alley_width_category,
                                        tree_row_orientation,
                                        soil_texture,
                                        tree_density_category,
                                        tree_height_category,
                                        age_system,
                                        tillage,
                                        organic)

metdata_moderators |> glimpse()
```

```{r}
metdata_moderators |> 
  summarise(across(all_of(moderators), ~ sum(is.na(.))))
```

REML enhances the validity and reliability of meta-analysis findings by providing unbiased variance estimates, reducing overfitting, and ensuring robust hypothesis testing. These features are crucial for understanding how various moderators affect the outcomes and for making informed, generalizable conclusions.

ML was used in the original model to enable hypothesis testing and model comparisons, particularly when assessing the significance of the overall effect size.
REML was chosen for the moderator model to provide more accurate variance component estimates in the presence of complex random effects and multiple moderators. This decision prioritizes reducing bias in the variance components over direct hypothesis testing, aligning better with the goal of exploring the influence of moderators on the effect sizes.
```{r}
# Set explicit levels for each factor
metdata_moderators <- metdata_moderators |> 
  mutate(
    tree_crop_combination = factor(tree_crop_combination, levels = c("Biomass-Cereal", "Biomass-Legume", "Biomass-Oilseed", 
                                                                     "Fruit-Legume", "Mixed-Cereal", "Mixed-Legume", 
                                                                     "Timber-Cereal", "Timber-Legume")),
    season = factor(season, levels = c("Winter", "Summer", "Winter and Summer")),
    alley_width_category = factor(alley_width_category, levels = c("Narrow", "Moderate", "Wide")),
    tree_row_orientation = factor(tree_row_orientation, levels = c("NS", "EW")),
    soil_texture = factor(soil_texture, levels = c("Clay", "Clay loam", "Silty clay")),
    tree_density_category = factor(tree_density_category, levels = c("Low density", "Medium density", "High density")),
    tree_height_category = factor(tree_height_category, levels = c("Short", "Medium", "Tall")),
    age_system = factor(age_system, levels = c("Young", "Medium", "Mature")),
    tillage = factor(tillage, levels = c("Yes", "No")),
    organic = factor(organic, levels = c("Yes", "No"))
  )

# Create a formula for the moderators
# moderator_formula <- as.formula(paste("~", paste(moderators, collapse = " + ")))
# Create a formula for the moderators without an intercept
moderator_formula_no_intercept <- as.formula(paste("~ 0 +", paste(moderators, collapse = " + ")))
```





*OBS! Time for running this code below is slightly long!*

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# List of the moderators to analyze
moderators <- c("tree_crop_combination",
                "season",
                "alley_width_category",
                "tree_row_orientation",
                "soil_texture",
                "tree_density_category", 
                "tree_height_category",
                "age_system",
                "tillage",
                "organic")


# Fit a model

# Initialize a list to store results
moderator_results_list <- list()

# Loop through each moderator
for (moderator in moderators) {
  # Create the formula dynamically (emoving the intercept)
  formula <- as.formula(paste("yi ~ 0 +", moderator))
  
  # Fit the moderator model using rma.uni() without a variance-covariance matrix
  moderator_model <- rma.uni(
    yi = metdata_moderators$yi,
    vi = metdata_moderators$vi,
    mods = formula,
    random = list(~ 1 | id_article, ~ 1 | id_article/response_variable),
    method = "REML",
    control = list(optimizer = "optim", optim.method = "BFGS", iter.max = 1000, rel.tol = 1e-12),
    data = metdata_moderators,
    verbose = TRUE
  )
  
  # Extract the results
  results <- data.frame(
    Moderator = names(coef(moderator_model)),
    Estimate = coef(moderator_model),
    Std_Error = moderator_model$se,
    CI_Lower = moderator_model$ci.lb,
    CI_Upper = moderator_model$ci.ub,
    P_Value = moderator_model$pval
  )
  
  # Store in the list
  moderator_results_list[[moderator]] <- results
}

# Combine all results into a single data frame
moderator_model_results <- do.call(rbind, moderator_results_list)

##############################################################
# Save the results to a CSV file
readr::write_csv(moderator_model_results,
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/moderator_results_v5.csv"))

##############################################################

# Display the extracted results
moderator_model_results

##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
print(time.taken)

##################################################

# NOTES
# In the model analysis above, we conduct individual moderator tests using a univariate random-effects meta-analysis (rma.uni()). 
# - Approach: Instead of a comprehensive multivariate model (as used in the "original_model"), we assess each moderator separately, dynamically creating formulas without an intercept (0 + moderator). Removing the intercept ensures all factor levels are included in the model results, avoiding issues with reference levels. 
# - Model Fitting: The Restricted Maximum Likelihood (REML) is chosen for variance estimation, due to its accuracy in handling nested random effects (study-level and response variable-level variability), and use the BFGS optimizer for efficient convergence. 
# - Random Effects: The model accounts for random intercepts at the study level (~ 1 | id_article) and within-study variability (~ 1 | id_article/response_variable), capturing between- and within-study heterogeneity. 
# - Comparison to Original Model: The original multivariate model (rma.mv()) included a fixed intercept and used a variance-covariance matrix (V), focusing on the correlation structure of effect sizes. Our univariate moderator model however simplifies the analysis by excluding the intercept, making each coefficient directly interpretable for each moderator level. This method provides a clear evaluation of individual moderators but may not capture interactions or correlations between moderators, which could be explored in future comprehensive models.

##################################################
# Last go: (13/11-24)
# Time difference of 23.79412 secs
```



#############
# STEP 3
##########################################################################################################################################
MERGING WITH THE META DATASET 
##########################################################################################################################################
```{r}
moderator_model_results |> glimpse()
```


```{r}
metdata |> select(response_variable,
                  yi, 
                  vi,
                  id_article,
                  response_variable,
                  tree_crop_combination,
                  season,
                  alley_width_category,
                  tree_row_orientation,
                  soil_texture,
                  tree_density_category,
                  tree_height_category,
                  age_system,
                  tillage,
                  organic) |> 
  glimpse()
```

Step 1: Standardize Factor Levels

```{r}
# Standardize factor levels in 'metdata'
metdata_to_merge <- metdata |> 
  select(
    response_variable,
    yi, 
    vi,
    id_article,
    response_variable,
    tree_crop_combination,
    season,
    alley_width_category,
    tree_row_orientation,
    soil_texture,
    tree_density_category,
    tree_height_category,
    age_system,
    tillage,
    organic) |>
  mutate(
    tree_crop_combination = as.factor(tree_crop_combination),
    season = as.factor(season),
    alley_width_category = as.factor(alley_width_category),
    tree_row_orientation = as.factor(tree_row_orientation),
    soil_texture = as.factor(soil_texture),
    tree_density_category = as.factor(tree_density_category),
    tree_height_category = as.factor(tree_height_category),
    age_system = as.factor(age_system),
    tillage = as.factor(tillage),
    organic = as.factor(organic)
  )
```


Step 2: Standardize Moderator Names and Levels

```{r}
# Separate the 'Moderator' column into 'Moderator_Name' and 'Moderator_Level'
moderator_to_merge <- moderator_model_results %>%
  # Split the 'Moderator' column to extract the moderator name and level
  mutate(
    Moderator_Name = str_extract(Moderator, "^[^.]+"),  # Extract everything before the first period
    Moderator_Level = str_remove(Moderator, "^[^.]+\\.")  # Remove everything before and including the first period
  ) %>%
  mutate(
    Moderator_Name = recode(Moderator_Name,
                            "tree_crop_combination" = "tree_crop_combination",
                            "season" = "season",
                            "alley_width_category" = "alley_width_category",
                            "tree_row_orientation" = "tree_row_orientation",
                            "soil_texture" = "soil_texture",
                            "tree_density_category" = "tree_density_category",
                            "tree_height_category" = "tree_height_category",
                            "age_system" = "age_system",
                            "tillage" = "tillage",
                            "organic" = "organic")
  ) |> 
  # Glimpse the transformed dataset
  glimpse()
```












































```{r}
ggplot(moderator_model_results, aes(x = Estimate, y = Moderator, color = Moderator)) +
  geom_point() +
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Forest Plot of Moderator Estimates",
       x = "Effect Size (Estimate)",
       y = "Moderator Level") +
  theme_minimal() +
  theme(legend.position = "none")
```







########################################################
DIAGNOSIS OF THE MODERATOR ANALYSIS
########################################################


```{r}
# Summary of the moderator model
summary(moderator_model_results)
```

```{r}
# ANOVA to test the influence of each moderator on the effect size
anova(moderator_model)
```

INTERPRETATION OF MODERATOR ANALYSIS

*Tree-Crop Combination:*
The analysis evaluates the influence of various tree-crop combinations on the effect size. Most combinations show non-significant results, indicating that the type of tree-crop pairing does not significantly impact the effect size.

 - Apple-Legume: The coefficient (0.0384) is positive but not statistically significant (p = 0.685), suggesting no meaningful effect.
 - Maple-Cereal: Coefficient of 0.0526, with a non-significant p-value (p = 0.5672), showing little influence.
 - Mixed-Legume: Coefficient of 0.0811, non-significant (p = 0.2549), indicating no notable impact.
 - Paulownia-Cereal: Negative coefficient (-0.0747), non-significant (p = 0.7342), suggesting no significant effect.

Overall, none of the tree-crop combinations had a statistically significant influence, highlighting the potential uniformity in response across different agroforestry systems.

*Season:*
The season of measurement plays a significant role in the effect size:

 - Winter: Shows a significant positive effect (coefficient = 0.0374, p = 0.0024**), indicating that outcomes measured in winter tend to have higher effect sizes.
 - Winter and Summer Combined: The effect is not significant (p = 0.8252), suggesting no clear combined seasonal impact.

This suggests a potential seasonal pattern in the data, with winter measurements showing stronger effects.

*Alley Width:*
The influence of different alley widths on effect size is assessed:

 - Width = 48 meters: Significant negative effect (coefficient = -0.0516, p = 0.0012**), indicating that wider alleys reduce the effect size.
 - Other widths (e.g., 10m, 15m): Non-significant results, suggesting that variations in narrower alley widths have no substantial impact.

This finding implies that very wide alleys may diminish the positive effects seen in agroforestry systems, possibly due to reduced tree-crop interactions.

*Soil Texture:*
Different soil types did not show a significant impact on effect sizes:

 - Clay: Coefficient = 0.0093, p = 0.8629 (non-significant).
 - Clay Loam: Coefficient = -0.0524, p = 0.2853 (non-significant).
 - Silty Clay: Coefficient = -0.0006, p = 0.9936 (non-significant).

These results suggest that soil texture is not a major determinant of effect size in the analyzed studies.

*Tree Height and System Age:*
 - Tree Height: Non-significant (coefficient = 0.0011, p = 0.9592), indicating no detectable influence on effect size.

 - Young Age System: Non-significant (coefficient = 0.0194, p = 0.8074), suggesting that younger systems do not show a different trend compared to older systems.

This suggests that structural characteristics like tree height and system age do not explain variation in effect sizes across studies.


*Test of Moderators (ANOVA)*
The ANOVA result for the test of moderators shows a significant effect (QM = 200.6479, df = 20, p < 0.0001), indicating that at least one of the included moderators significantly influences the effect size. This finding justifies the inclusion of moderator variables in the model, highlighting their importance in explaining part of the heterogeneity observed in the meta-analysis.

In summary, the moderator analysis identified season (specifically winter) and wide alley width (48m) as significant moderators, suggesting these factors have the most substantial impact on effect sizes. Other tested moderators, including tree-crop combinations, soil texture, and tree height, did not show significant effects, indicating they do not contribute meaningfully to explaining the variability in effect sizes across studies.


```{r}
metdata |> select(age_system) |> 
  unique()

# Check the reference levels for each moderator
levels(as.factor(metdata$tree_crop_combination))
```



################################################################################################################
VISUALISING OVERALL EFFECT-SIZE MEASURES AS EFFECT OF DIFFERENT MODERATORS
########################################################

Graphs to Include:
 - Forest Plot for Each Moderator: This will show the estimated effect size for each level of the moderator, including confidence intervals.
 - Bubble Plot of Moderator Effects: This will visualize the relationship between the effect size and the continuous moderators (e.g., tree_height).
 - Heatmap of Effect Sizes by Moderator Levels: This will provide an overview of how effect sizes vary across combinations of different moderator levels.
 - Categorical Moderator Comparison: Bar plots for categorical moderators showing effect sizes and their confidence intervals.
 - Interaction Plot (if applicable): Visualize potential interactions between two moderators.

```{r}
# Forest Plot for Each Moderator Level
forest_plot_data <- moderator_results %>%
  filter(!grepl("intrcpt", Moderator))  # Exclude the intercept

forest_plot_graph <- 
  forest_plot_data |> 
  ggplot(aes(x = reorder(Moderator, Coefficient), y = Coefficient)) +
  geom_point(size = 3, color = "darkblue") +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2, color = "darkgray") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(
    title = "Forest Plot of Effect Sizes by Moderator",
    x = "Moderator Level",
    y = "Effect Size (Coefficient)"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 14))

forest_plot_graph
```

```{r}
# Bubble Plot for Continuous Moderators (e.g., Tree Height)
continuous_mod_data <- moderator_results %>%
  filter(Moderator == "tree_height")

continuous_mod_graph <-
  continuous_mod_data |> 
  ggplot(aes(x = Coefficient, y = Std_Error, size = abs(Coefficient))) +
  geom_point(alpha = 0.7, color = "darkorange") +
  scale_size_area(max_size = 10) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Bubble Plot of Effect Size vs. Standard Error (Tree Height)",
    x = "Effect Size (Coefficient)",
    y = "Standard Error"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 14))

continuous_mod_graph
```
```{r}
# Heatmap of Effect Sizes by Moderator Levels
heatmap_data <- moderator_results %>%
  filter(!grepl("intrcpt", Moderator)) %>%
  mutate(Significance = ifelse(P_Value < 0.05, "Significant", "Not Significant"))

heatmap_graph <- 
  heatmap_data |> 
  ggplot(aes(x = Moderator, y = reorder(Moderator, Coefficient), fill = Coefficient)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  geom_text(aes(label = round(Coefficient, 2)), color = "black", size = 4) +
  labs(
    title = "Heatmap of Effect Sizes by Moderator",
    x = "Moderator",
    y = "Level",
    fill = "Effect Size"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

heatmap_graph
```

```{r}
# Bar Plot for Categorical Moderators
bar_plot_data <- moderator_results %>%
  filter(!grepl("intrcpt", Moderator))

bar_plot_data |> 
ggplot(aes(x = reorder(Moderator, Coefficient), y = Coefficient, fill = as.factor(P_Value))) +
  geom_bar(stat = "identity", color = "black", width = 0.7) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  scale_fill_manual(values = c("Significant" = "darkgreen", "Not Significant" = "gray")) +
  coord_flip() +
  labs(
    title = "Bar Plot of Effect Sizes for Categorical Moderators",
    x = "Moderator",
    y = "Effect Size (Coefficient)",
    fill = "Significance"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 14))
```

```{r}
# Interaction Plot (Optional: If there is a notable interaction)
interaction_data <- moderator_results %>%
  filter(Moderator %in% c("seasonWinter", "alley_width48"))

interaction_data |> 
ggplot(aes(x = Moderator, y = Coefficient, group = 1)) +
  geom_line(color = "purple", size = 1) +
  geom_point(size = 3, color = "purple") +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  labs(
    title = "Interaction Plot for Selected Moderators",
    x = "Moderator",
    y = "Effect Size (Coefficient)"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 14))
```




#############
# STEP 3
##########################################################################################################################################
COHERENT DATSET FOR VISUALISATION
##########################################################################################################################################


```{r}
# Clean the column names
moderator_res <- moderator_results |> janitor::clean_names()

# Standardize tree crop combination group names
moderator_res <- moderator_res |> 
  mutate(
    tree_crop_combination_group = case_when(
      str_detect(moderator, "Apple|Fruit") ~ "Fruit-Cereal",
      str_detect(moderator, "Biomass") ~ "Biomass-Cereal",
      str_detect(moderator, "Timber") ~ "Timber-Legume",
      str_detect(moderator, "Mixed-Cereal") ~ "Mixed-Cereal",
      str_detect(moderator, "Mixed-Legume") ~ "Mixed-Legume",
      str_detect(moderator, "Nut") ~ "Nut-Cereal",
      
      str_detect(moderator, "Maple-Legume") ~ "Maple-Legume",
      str_detect(moderator, "Maple-Cereal") ~ "Maple-Cereal",
      str_detect(moderator, "Paulownia-Cereal") ~ "Paulownia-Cereal",
      str_detect(moderator, "Poplar-Legume") ~ "Poplar-Legume",
      str_detect(moderator, "Poplar-Cereal") ~ "Poplar-Cereal",
      TRUE ~ "Other"
    )
  )

# Standardize soil texture group names
moderator_res <- moderator_res |> 
  mutate(
    soil_texture_group = case_when(
      
      #str_detect(moderator, "Silt") ~ "Silt",
      str_detect(moderator, "Clay loam") ~ "Clay loam",
      str_detect(moderator, "Silty") ~ "Silty clay",
      str_detect(moderator, "Clay") ~ "Clay",
      str_detect(moderator, "Sand") ~ "Sand",
      TRUE ~ "Other"
    )
  )

# Standardize alley width group names
moderator_res <- moderator_res |> 
  mutate(
    alley_width_group = case_when(
      str_detect(moderator, "8") ~ "8 m",
      str_detect(moderator, "10") ~ "10 m",
      str_detect(moderator, "15") ~ "15 m",
      str_detect(moderator, "40") ~ "40 m",
      str_detect(moderator, "48") ~ "48 m",
      TRUE ~ "Other"
    )
  )

# Standardize agroforestry age
moderator_res <- moderator_res |> 
  mutate(
    age_group = case_when(
      str_detect(moderator, "Young") ~ "Young (1-10 y)",
      str_detect(moderator, "Old") ~ "Mature (> 11 y)",
      TRUE ~ "Other"
    )
  )

```





































```{r}
# Merge moderator_results with metdata to include response_variable
merged_data_for_vis <- moderator_results |> 
  # Streamlining and cleaning variable column names
  janitor::clean_names() |> 
  left_join(metdata |> 
              select(id_article, response_variable, tree_crop_combination_t, soil_texture, alley_width), 
            by = "tree_crop_combination")

# Glimpse the merged data to verify
merged_data_for_vis |> glimpse()
```




















































```{r}
# Create aggregated high-level categories
moderator_results_transformed <- moderator_results %>%
  mutate(
    # Group tree-crop combinations into broader categories
    tree_crop_combination_group = case_when(
      grepl("Apple|Fruit", Moderator) ~ "Fruit-Cereal",
      grepl("Biomass", Moderator) ~ "Biomass-Cereal",
      grepl("Timber", Moderator) ~ "Timber-Legume",
      grepl("Mixed", Moderator) ~ "Mixed-Cereal",
      grepl("Nut", Moderator) ~ "Nut-Cereal",
      TRUE ~ "Other"
    ),
    # Group soil textures into broader categories
    soil_texture_group = case_when(
      grepl("Clay", Moderator) ~ "Clay",
      grepl("Loam", Moderator) ~ "Loam",
      grepl("Silty", Moderator) ~ "Silty",
      TRUE ~ "Other"
    ),
    # Simplify alley width categories
    alley_width_group = case_when(
      grepl("10|15", Moderator) ~ "Narrow (10-15m)",
      grepl("40|48", Moderator) ~ "Wide (40-48m)",
      TRUE ~ "Other"
    )
  )

# Group the results by new categories and calculate mean effect sizes and confidence intervals
summary_data <- moderator_results_transformed %>%
  group_by(tree_crop_combination_group, soil_texture_group, alley_width_group, Moderator) %>%
  summarize(
    mean_effect = mean(Coefficient, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE)
  ) %>%
  ungroup()

summary_data
```
```{r}
# Filter data for specific ecosystem services and create a panel plot
ggplot(summary_data, aes(x = mean_effect, y = tree_crop_combination_group)) +
  geom_point(size = 3) +
  geom_errorbar(aes(xmin = lower_ci, xmax = upper_ci), width = 0.2) +
  facet_wrap(~ Moderator, scales = "free", ncol = 3) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Effect Size of Tree-Crop Combinations on Ecosystem Services",
    x = "Effect Size (Mean Coefficient)",
    y = "Tree-Crop Combination Group"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 14))
```

