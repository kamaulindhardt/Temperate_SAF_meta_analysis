---
title: "logRR"
author: "Kamau Lindhardt, lbk125"
date: "2024-07-06"
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulae to estimate effects (and their standard errors)?




#############
# STEP 0
#########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
#########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    dlookr,           # Diagnose, explore, and transform data with dlookr
    skimr,            # Provides easy summary statistics about variables in data frames, tibbles, data tables and vectors
    janitor,          # For cleaning and renaming data columns
    readxl,           # To read Excel files
    vroom,            # Fast reading of large datasets from local disk
    missForest,       # Random Forest method for imputing missing data
    mice,             # For dealing with missing data by creating multiple imputations for multivariate missing data
    missRanger,       # Fast missing value imputation by chained random forest
    conflicted,       # An alternative conflict resolution strategy
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    tidygeocoder,     # For geocoding addresses to latitude/longitude
    rnaturalearth,    # For accessing Natural Earth map data
    rnaturalearthdata,# Companion package to rnaturalearth providing the data
    ###################################################################################################################
    # Spatial Data
    raster,           # For spatial data analysis, especially BioClim variables from WorldClim
    sp,               # For spatial data classes and methods
    sf,               # For simple features in R, handling vector data
    ###################################################################################################################
    # Soil Data
    soilDB,           # For downloading soil data from ISRIC SoilGrids
    aqp,              # For soil profile visualization and analysis
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Provides several cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
```


Loading the dataset (main database)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory
# here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/Final_database_2024_05_17.xlsx")
setwd("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/CODE/META_ANALYSIS")


# Suppress warnings to avoid clutter in the console output
suppressWarnings({
  database <- readxl::read_excel(here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/Final_database_2024_05_17_v3.xlsx"), # Final_database_2024_05_17
                         sheet = "Quantatitive data") 
  #%>%
  #  filter(INCLUDED == TRUE) # include only data entries that have been assesed and deemed included
})
```

**Glimpse (taking a look at the data)**
```{r Glimpse the dataset, eval=FALSE}
database %>% dplyr::glimpse() 
```
```{r}
database %>% summary() 
```
```{r}
database |> skim()
```


#############
# STEP 1
#########################################################################################################################################
DATA PREPROCESSING
#########################################################################################################################################

```{r}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Convert standard errors to numeric and handle non-numeric values
database_clean <- database |>
  # Streamlining and cleaning variable column names
  janitor::clean_names() |> 
  # Converting id_article to integer
  mutate(id_article = as.integer(id_article)) |>
  # Converting to numerical data type
  mutate(
    silvo_se = safe_as_numeric(silvo_se),
    control_se = safe_as_numeric(control_se)
  ) %>%
  # Ensure no infinite or NaN values are present in any variable columns
  mutate_all(~ifelse(is.infinite(.), NA, .)) |>
  mutate_all(~ifelse(is.nan(.), NA, .)) |>
  # Relocating variable columns to better get an overview of the data
  relocate(
    id_article, response_variable, measured_metrics, measured_unit, 
    silvo_mean, silvo_se, silvo_sd, silvo_n,
    control_mean, control_se, control_sd, control_n
  )

database_clean
```
```{r}
database_clean %>% glimpse()
```

```{r}
database_clean %>% summary()
```

##########################################################################################################################################
EXPLORATIVE DATA ANALYSIS (EDA)
##########################################################################################################################################

#############
VIEWING DATA
#############

```{r}
database_clean %>% 
  group_by(response_variable) %>%
  summarise(n_silvo = sum(silvo_n),
            n_control = sum(control_n),
            mean_silvo = mean(silvo_mean),
            mean_control = mean(control_mean))
```

###################
EDA WITH DLOOKR
###################

```{r}
# Visualize pareto chart for variables with missing value.

plot_na_pareto(database_clean)

# Indicating a high proportion of NA values in the control_se and silvo_se variables. 
# Might need imputation in order to include them in the meta-analysis
```
Missing values are found in 
- silvo_se
- control_se
- distance_tree_m

```{r}
# Visualize the combinations of missing value across cases.

plot_na_intersect(database_clean)
```
```{r}
# Filter rows with missing values in silvo_se or control_se, and select relevant columns
# omitting distance_tree_m
missing_values <- database_clean |>
  filter(is.na(silvo_se) | is.na(control_se)) |> 
         # is.na(distance_tree_m)) |>
  select(id_article, response_variable, silvo_se, control_se, distance_tree_m)

# Transform data for visualization by pivoting longer and counting missing values
missing_count <- missing_values |>
  pivot_longer(cols = c(silvo_se, control_se), names_to = "missing_type", values_to = "value") |>
  filter(is.na(value)) |>
  count(id_article, response_variable, missing_type)


# Create the bar plot to visualize missing values by article and response variable
ggplot(missing_count, aes(x = factor(id_article), y = n, fill = missing_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ response_variable, scales = "free_x") +
  labs(
    title = "Missing Values by Article and Response Variable",
    x = "Article ID",
    y = "Number of Missing Values",
    fill = "Missing Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
# Identify rows with missing values in control_se or silvo_se
missing_values_se <- missing_values |> 
  filter(is.na(silvo_se) | is.na(control_se)) |>
  select(id_article, response_variable, silvo_se, control_se)

# Prepare data to include missing_type and id_article
missing_val_se_plot <- missing_values_se |>
  mutate(
    missing_silvo_se = ifelse(is.na(silvo_se), 1, 0),
    missing_control_se = ifelse(is.na(control_se), 1, 0)
  ) |>
  pivot_longer(cols = c(missing_silvo_se, missing_control_se), names_to = "missing_type", values_to = "n") |>
  filter(n == 1)  # Keep only the rows where there is a missing value


# Create the stacked bar plot
missing_val_se_plot |>
  ggplot(aes(x = response_variable, y = n, fill = factor(id_article))) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Number of Missing Values per Response Variable",
       x = "Response Variable",
       y = "Number of Missing Values",
       fill = "Article ID") +
  facet_wrap(~missing_type, scales = "free_y") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Firstly, the Crop yield category stands out with the highest number of missing values for both silvo_se and control_se. This is followed by the Pest and Disease and Soil quality categories, which also show significant numbers of missing values, though they are much fewer compared to Crop yield. This indicates a particular problem with data completeness in the Crop yield category.

Secondly, within each response_variable category, the number of missing values for silvo_se and control_se appears to be comparable. This suggests a pattern where if a value is missing in one, it is likely missing in the other. For instance, both silvo_se and control_se have similar heights in the Crop yield and Pest and Disease categories, indicating consistent missing data patterns across these variables.

Furthermore, the plot shows that multiple articles contribute to the missing data, with certain articles, such as IDs 2, 3, 10, and 25, being significant contributors, especially in the Crop yield category. This spread of missing data across different studies suggests that the issue is not confined to a single source but is more widespread.

In summary, the Crop yield category is particularly problematic with a high number of missing values for both silvo_se and control_se. Several articles, particularly IDs 2, 3, and 10, appear frequently in categories with high missing values. The consistency in missing data patterns between silvo_se and control_se suggests that there might be methodological or study-specific issues contributing to the missing data. To address these issues, it is essential to investigate the high-contributing articles, review the methodologies used in the Crop yield category, and develop strategies to handle the missing data, such as imputation, especially for critical categories like Crop yield. This approach will help improve the quality and completeness of the data, leading to more reliable analyses and insights.


```{r}
# Plot density distribution for control_se with log transformation
database_clean |> 
  ggplot(aes(x = control_se)) +
  geom_density() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of control_se (Log-Transformed)") +
  theme_minimal()

# Advarsel i scale_x_log10() :
#   log-10 transformation introduced infinite values.
# Advarsel: Removed 218 rows containing non-finite outside the scale range (`stat_density()`).

# warnings indicate that some of the values in the dataset are either zero or negative, which cause issues when applying a logarithmic transformation
```
```{r}
# Plot density distribution for silvo_se with log transformation
database_clean |> 
  ggplot(aes(x = silvo_se)) +
  geom_density() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of silvo_se (Log-Transformed)") +
  theme_minimal()

# Advarsel i transformation$transform(x) : NaNs produced
# Advarsel i scale_x_log10() :
#   log-10 transformation introduced infinite values.
# Advarsel: Removed 212 rows containing non-finite outside the scale range (`stat_density()`).

# warnings indicate that some of the values in the dataset are either zero or negative, which cause issues when applying a logarithmic transformation
```


```{r, eval = FALSE}
eda_report_meta_analysis_SAF <- database_clean |>
  eda_paged_report(#target = "response_variable", 
                   subtitle = "Temperate SAF EDA for meta-analysis",
                   output_format = "html",
                   output_file = "EDA_SAF.html")

# Remember to open in Microsoft Edge browser. For some reason Google Chrome is not working
eda_report_meta_analysis_SAF
```



##########################################################################################################################################
CONTINUED DATA PREPROCESSING AND HANDELING OF MISSING VALUES IN THE DATASET
##########################################################################################################################################

####################################################################################
Perform imputation using "mice" (Multivariate Imputation by Chained Equations)
####################################################################################

```{r, eval = TRUE}
####################################################################################
# Extract relevant columns for imputation
col_for_impute <- database_clean %>%
  dplyr::select(silvo_se, 
                control_se)
                # omitting imputation of 
                # distance_tree_m) 

####################################################################################
# Set seed for reproducibility
set.seed(1234)

# Perform imputation using mice
# - read about mice() here: https://www.metafor-project.org/doku.php/tips:multiple_imputation_with_mice_and_metafor
# - col_for_impute: the data frame containing the columns to be imputed
# - m = 5: number of multiple imputations to perform
# - maxit = 100: maximum number of iterations to perform for each imputation
# - method = 'pmm': method to use for imputation, 'pmm' stands for predictive mean matching
# - seed = 500: random seed for reproducibility of the imputations
# - printFlag: If TRUE, mice will print history on console. Use print=FALSE for silent computation.

imputed_data <- mice::mice(col_for_impute, 
                           m = 20, 
                           maxit = 100, 
                           method = 'pmm', 
                           seed = 1234,
                           printFlag = FALSE)
```

```{r}
####################################################################################

# Check the summary of imputed data
summary(imputed_data)
```
```{r}
# Evaluate the imputed datasets
# Check convergence diagnostics
plot(imputed_data)
```
```{r}
# Compare the distribution of the observed and imputed data
densityplot(imputed_data)
```

```{r}
# Use stripplot to compare observed and imputed values
stripplot(imputed_data, pch = 20, cex = 1.2)
```

```{r}
# For an evidence-based choice, we may calculate some metrics
# Calculate the mean and standard deviation of each imputed dataset
imputed_summaries <- lapply(1:5, function(i) {
  data <- complete(imputed_data, i)
  summary <- data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE)
    )
  return(summary)
})

# Combine the summaries into a single data frame for comparison
imputed_summaries_df <- bind_rows(imputed_summaries, .id = "imputation")

imputed_summaries_df
```

```{r}
# Choose the imputation based on the diagnostics and summaries
# For simplicity, let's assume we choose the imputation with the median mean_silvo_se
chosen_imputation <- imputed_summaries_df %>%
  filter(mean_silvo_se == median(mean_silvo_se))

chosen_imputation
```

```{r}
# Extract the chosen imputation number
chosen_imputation_number <- chosen_imputation$imputation

# Extract the complete dataset for the chosen imputation
imputed_col_data <- complete(imputed_data, as.integer(chosen_imputation_number))

imputed_col_data
```

```{r}
####################################################################################
# Another way is simply to choose an imputation run from the iterations:
# Choose one of the multiple imputations (e.g., the first imputation)
# imputed_col_data <- complete(imputed_data, 1)
# summary(imputed_col_data)

####################################################################################
# Update the original data with imputed values
imputed_database_clean <- database_clean %>%
  mutate(
    silvo_se = imputed_col_data$silvo_se,
    control_se = imputed_col_data$control_se
    ) 

imputed_database_clean |> glimpse()
```

```{r, eval=FALSE}
imputed_database_clean %>% summary()
```

Visualising the distribution of imputed values for silvo_se and control_se together with the original data

```{r}
# Prepare the original data
original_data_x <- database_clean %>%
  select(id_article, response_variable, silvo_se, control_se) |> 
  mutate(data_source = "Original")

imputed_data_y <- imputed_database_clean |> 
  select(id_article, response_variable, silvo_se, control_se) |> 
  mutate(data_source = "Imputed")

# Combine the original and imputed data
combined_data <- bind_rows(original_data_x, imputed_data_y)

combined_data
```

```{r}
# Create density plots for silvo_se with log transformation
silvo_se_impute_original_plot <- combined_data |> 
ggplot(aes(x = silvo_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of silvo_se (Log-Transformed)") +
  theme_minimal()

# Create density plots for control_se with log transformation
control_se_impute_original_plot <- combined_data |> 
ggplot(aes(x = control_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of silvo_se (Log-Transformed)") +
  theme_minimal()

library(patchwork)

silvo_se_impute_original_plot + control_se_impute_original_plot
```



**Needs to be run from here to STEP 5 "ANALYSING MODERATORS" using the updated database "Final_database_2024_05_17_v3"**







#############
# STEP 3
##########################################################################################################################################
CALCULATING EFFECT SIZE MEASURE AND GENERATING AND BUILDING RANDOM-EFFECTS MODEL
##########################################################################################################################################

SOME PREPROCESSING OF THE META-DATA BEFORE CALCULATING EFFECT SIZE MEASURE

################################
A) The standardized mean difference: It is necessary to standardize the results of the studies to a uniform scale before they can be combined. For effect measure in our case we can use Ratio of Means (RoM) as the ratio of means can be used in either situation, but is appropriate only when outcome measurements are strictly greater than zero.

```{r}
# Create a new dataset for meta-analysis with shifted values for interpretation
meta_data <- imputed_database_clean |>
  # Filtering so that only data with outcome measurements that are greater than zero will be included
  # Including only data with positive standard errors ensures that the subsequent calculations (e.g., variances, effect sizes) are statistically valid and 
  # interpretable. It helps avoid mathematical errors such as division by zero or taking the square root of a negative number.
  # By excluding studies or observations with zero or negative standard errors, you might be systematically excluding certain types of data. For instance,
  # studies with less variability might be excluded, potentially skewing the overall results.
    filter(silvo_se > 0,
           control_se > 0) |>
  # Adjust the signs for specific response variables where lower values for silvo_ vs. control_ is considered 'better'
  mutate(
    silvo_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"), 
                        -silvo_mean, silvo_mean),
    control_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"), 
                          -control_mean, control_mean)
  ) |> 
  # Exclude the variable soil water content due to inconsistent data from only a single measurement
  filter(response_variable != "Soil water content") |> 
  # Filter out rows with missing values in key columns to ensure data completeness
  filter(
    !is.na(silvo_mean) & !is.na(control_mean) & !is.na(silvo_n) & 
    !is.na(control_n) & !is.na(silvo_se) & !is.na(control_se)
  ) |> 
 # Calculate standard deviations from standard errors and sample sizes
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  # Shift values to be positive
  # Shifting values to be positive is a necessary step when using transformations that require positive inputs, like the log-transformed ratio of means. 
  # It enables the inclusion of all studies, avoids mathematical errors, and standardizes data across studies, ensuring the validity and reliability of the
  # meta-analysis. Not performing this step could lead to errors, exclusion of data, and potential biases in the results. However, it's important to
  # interpret the results in the context of the shift and acknowledge that absolute values have been adjusted.
  mutate(
    min_value_shift = abs(min(c(silvo_mean, control_mean), na.rm = TRUE)) + 1,
    silvo_mean = silvo_mean + min_value_shift,
    control_mean = control_mean + min_value_shift
  ) |>
  # Reorder columns for better readability and organization in the output
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n) |> 
  arrange(id_article, response_variable)


meta_data |> glimpse()
```


When shifting values or flipping the sign of the means, it does indeed alter the interpretation of the data and the units they represent. This process can make the original units less meaningful, and it can introduce challenges in interpreting the results. 

```{r}
meta_data |> filter(response_variable == "Water quality") |> skim()
```


################################################################################################
AUTOMATIC CALCULATION OF EFFECT SIZE MEASURE USING escalc() function
################################################################################################

```{r}
# Calculate effect sizes and variances using escalc()
# - measure: Specifies the effect size measure to use, "ROM" for the log transformed ratio of means 
            # Alternative effect size measures: "MD" for the raw mean difference, "RR" for the log risk ratio, 
            # "OR" for the log odds ratio, "RD" for the risk difference, "SMD" stands for Standardized Mean Difference,
            # "AS" for the arcsine square root transformed risk difference, "PETO" for the log odds ratio estimated with Peto's method
# - m1i: Means of the experimental group (silvo_mean)
# - sd1i: Standard deviations of the experimental group (silvo_se)
# - n1i: Sample sizes of the experimental group (silvo_n)
# - m2i: Means of the control group (control_mean)
# - sd2i: Standard deviations of the control group (control_se)
# - n2i: Sample sizes of the control group (control_n)
# - slab: Optional argument to specify study labels, here combining 'id_article' and 'study_year_start'
# - data: The dataset containing all the specified columns

# "ROM" for the log transformed ratio of means
meta_data_ROM <- escalc(measure = "ROM", 
                       # experimental group (silvo_)
                       m1i = silvo_mean, sd1i = silvo_sd, n1i = silvo_n,
                       # control group (control_)
                       m2i = control_mean, sd2i = control_sd, n2i = control_n,
                       # argument to specify study labels for plotting
                       slab = paste(id_article, ", ", study_year_start, sep = ""),
                       # dataset to be used
                       data = meta_data) |> 
  as.data.frame()

# Check if the meta_data_ROM object exists and display its structure
str(meta_data_ROM)

# After shifting the mean values, I don't see this warning below
# Advarsel: Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs
```


The warning message "Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs." means that some of the calculated effect sizes or their variances are turning into infinite values during the computation. This usually happens when the data for the experimental or control groups contain zeros or when there's a big difference between the means and standard deviations that can't be properly captured using the "ROM" (log-transformed ratio of means) effect size measure. To avoid the analysis from failing, the escalc() function replaces these infinite values with NA. Having NA values can be problematic because it means those observations will be excluded from the analysis, which might reduce the study's statistical power. If many observations are affected, it could suggest that the dataset isn't suitable for the "ROM" measure or that there are data quality issues that need addressing. If only a few data points are affected, the impact is less severe, but it's still important to understand why these values are occurring.


```{r}
# Identify rows where 'yi' or 'vi' are NA
problematic_data <- meta_data_ROM %>%
  filter(is.na(yi) | is.na(vi))

# Display the problematic data points
problematic_data |> 
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n) |> 
  arrange(id_article, response_variable)

# Before shifting the mean values, I got these:
# id_article	response_variable	measured_metrics	measured_unit	yi	vi	silvo_mean	silvo_se	silvo_sd	silvo_n
# <int>	<chr>	<chr>	<chr>	<dbl>	<dbl>	<dbl>	<dbl>	<dbl>	<dbl>
# 5	Water quality	NO3- concentration	ppm	NA	NA	0.0	0.006	0.03841875	41
# 6	Pest and Disease	Disease	µg/kg dry barley grain	NA	NA	0.0	3.470	25.96710226	56
# 9	Biodiversity	Plants	Species richness	NA	NA	0.5	0.900	18.70614872	432
# 9	Biodiversity	Plants	Species richness	NA	NA	1.8	0.850	8.32826513	96
# 30	Biodiversity	Airborne arthropods	Abundance	NA	NA	1.0	1.410	8.91762300	40
# 30	Biodiversity	Airborne arthropods	Abundance	NA	NA	1.0	7.300	46.16925384	40
# 30	Biodiversity	Airborne arthropods	Abundance	NA	NA	1.0	11.230	71.02475625	40
# 30	Biodiversity	Airborne arthropods	Abundance	NA	NA	0.0	138.920	878.60722510	40
# 30	Biodiversity	Airborne arthropods	Abundance	NA	NA	4.0	1.290	8.15867636	40
# 30	Pest and Disease	Abunadance of pest insects	Abundance	NA	NA	0.0	1.410	8.91762300	40

# After shifting the mean values I dont get that
# Description:df [0 × 25]
```

The yi and vi values can become NA (or infinite before being recoded to NA) for several reasons when using the escalc() function to calculate effect sizes and variances. Here are some common causes:

Zero or Negative Values in Control or Experimental Group: If the means, standard deviations, or sample sizes in the control or experimental group are zero or negative, it can lead to undefined or infinite values when calculating the log-transformed ratio of means (ROM).



To handle problematic data points by assigning them a small value instead of excluding them, it is possible by following these steps:

1) Identify Problematic Data Points: Identify the data points where yi or vi are NA.
2) Assign Small Values: Replace the problematic values with a small value.
3) Recalculate Effect Sizes and Variances: Recalculate yi and vi after addressing the problematic values.


################################################################################################
RE-CALCULATION OF EFFECT SIZE MEASURE USING escalc() function
################################################################################################


```{r}
# Assign a small value to zero or negative values in the original data
small_value <- 0.001

# Create a new dataset with small values assigned to problematic data points
adjusted_meta_data <- meta_data %>%
  mutate(
    silvo_mean = ifelse(silvo_mean <= 0, small_value, silvo_mean),
    silvo_sd = ifelse(silvo_sd <= 0, small_value, silvo_sd),
    silvo_n = ifelse(silvo_n <= 0, small_value, silvo_n),
    control_mean = ifelse(control_mean <= 0, small_value, control_mean),
    control_sd = ifelse(control_sd <= 0, small_value, control_sd),
    control_n = ifelse(control_n <= 0, small_value, control_n)
  )

# Recalculate effect sizes and variances

# "ROM" for the log transformed ratio of means
meta_data_ROM_adjusted <- escalc(measure = "ROM",
                                 # experimental group (silvo_)
                                 m1i = silvo_mean, sd1i = silvo_sd, n1i = silvo_n,
                                 # control group (control_)
                                 m2i = control_mean, sd2i = control_sd, n2i = control_n,
                                 # argument to specify study labels for plotting
                                 slab = paste(id_article, ", ", study_year_start, sep = ""),
                                 # dataset to be used
                                 data = adjusted_meta_data) |> 
  as.data.frame()

# Check if the meta_data_ROM object exists and display its structure
str(meta_data_ROM_adjusted)
```


```{r, eval = FALSE}
# Saving the metadata as csv

# Storing the now adjusted dataset with calculated effect sizes as the old name'meta_data_ROM' (before adjustment)

meta_data_ROM <- meta_data_ROM_adjusted

##############################################################
# Save to CSV
readr::write_csv(meta_data_ROM, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/meta_data_ROM_database_v3.csv"))
```




############################################################################################################################
READING THE METADATA CSV
############################################################################################################################

```{r}
meta_data_ROM <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/meta_data_ROM_database_v3.csv")
```

```{r}
# Display the data with re-calculated effect size measure 
meta_data_ROM <- as.data.frame(meta_data_ROM) |> 
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n) |> 
  arrange(id_article, response_variable)

meta_data_ROM
```


################################################################################################
VISUAL ASSESSMENT
################################################################################################

```{r}
# Prepare the data
# Separate the data for silvo_ and control_
silvo_data <- meta_data_ROM %>%
  select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  mutate(group = "silvo_")|> 
  as.data.frame()

control_data <- meta_data_ROM %>%
  select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  mutate(group = "control_") |> 
  as.data.frame()

#####################################

silvo_data <- meta_data_ROM %>%
  mutate(yi = log(silvo_mean / control_mean),
         vi = (silvo_se^2 / silvo_mean^2) + (control_se^2 / control_mean^2)) %>%
  select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  mutate(group = "silvo_")

control_data <- meta_data_ROM %>%
  mutate(yi = log(control_mean / silvo_mean),
         vi = (control_se^2 / control_mean^2) + (silvo_se^2 / silvo_mean^2)) %>%
  select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  mutate(group = "control_")



# Combine the data
combined_data <- bind_rows(silvo_data, 
                           control_data)
```

Visual assessment of yi (effect sizes) and vi (variances) for both the silvo_ and control_ groups, faceted by each response_variable
```{r}
# Prepare the data for visualization without reversing effect size
combined_long_data <- combined_data %>% # meta_data_ROM
  pivot_longer(cols = c(yi, vi), names_to = "measure", values_to = "value")

# Create the density plot
combined_long_data |> 
  ggplot(aes(x = value, fill = group, color = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(
    title = "Density Plot of Effect Sizes (yi) and Variances (vi) by Group",
    x = "Value",
    y = "Density",
    fill = "Group",
    color = "Group"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
combined_long_data |> summary()

combined_long_data %>%
  group_by(group, measure) %>%
  summarise(min_value = min(value, na.rm = TRUE), max_value = max(value, na.rm = TRUE)) %>%
  print()
```
```{r}
# Inspect the data
table(combined_long_data$group)
summary(combined_long_data$value)
```
```{r}
head(silvo_data)
head(control_data)
```
```{r}
summary(silvo_data$mean)
summary(control_data$mean)
```
```{r}
# Apply skim to your data
silvo_data$mean |> skim()
control_data$mean |> skim()
```

Visualizing with log-transformed values

```{r}
# Create the density plot with log scale
combined_long_data |> 
  ggplot(aes(x = value, fill = group, color = group)) +
  geom_density(alpha = 0.3) +
  scale_x_log10() +  # Applying log scale to x-axis
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(title = "Density Plot of Effect Sizes and Variances by Group (Log Scale)",
       x = "Value (Log Scale)",
       y = "Density",
       fill = "Group") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Advarsel i transformation$transform(x) : NaNs produced
# Advarsel i scale_x_log10() :
#   log-10 transformation introduced infinite values.
# Advarsel: Removed 1080 rows containing non-finite outside the scale range (`stat_density()`).
```
```{r}
# Identify rows with non-positive values in 'value'
problematic_values <- combined_long_data %>%
  filter(value <= 0)
```

```{r}
# Define a small constant for shifting values
shift_constant <- abs(min(combined_long_data$value, na.rm = TRUE)) + 1

# Shift values to be positive
combined_long_data_shifted <- combined_long_data %>%
  mutate(
    value_shifted = value + shift_constant,
    shift_info = "Values shifted by a constant for log-transformed visualization"
  )

# Create the density plot with log scale using shifted values
combined_long_data_shifted |> 
  ggplot(aes(x = value_shifted, fill = group, color = group)) +
  geom_density(alpha = 0.3) +
  scale_x_log10() +  # Applying log scale to x-axis
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(
    title = "Density Plot of Effect Sizes and Variances by Group (Log Scale)",
    x = "Value (Log Scale, Shifted for Visualization)",
    y = "Density",
    fill = "Group",
    caption = "Note: Values shifted by a constant purely for visualization"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
# Create the density plot for silvo_ group
combined_long_data |> 
  filter(group == "silvo_") |> 
ggplot(aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(title = "Density Plot of Effect Sizes and Variances by Response Variable (Silvo)",
       x = "Value",
       y = "Density",
       fill = "Group") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Create the density plot for control_ group
combined_long_data |> 
  filter(group == "control_") |> 
  ggplot(aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(title = "Density Plot of Effect Sizes and Variances by Response Variable (Control)",
       x = "Value",
       y = "Density",
       fill = "Group") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
combined_long_data |> filter(response_variable == "Product quality")
combined_long_data |> filter(response_variable == "Water quality")
```

```{r}
# Get the unique response variables
response_variables <- unique(combined_long_data$response_variable)

# Create individual plots for each response variable and combine them
plots <- list()
for (rv in response_variables) {
  # Filter data for the current response variable
  data_rv <- combined_long_data %>% filter(response_variable == rv)
  
  # Determine the fixed scales for yi and vi
  yi_range <- range(data_rv %>% filter(measure == "yi") %>% pull(value), na.rm = TRUE)
  vi_range <- range(data_rv %>% filter(measure == "vi") %>% pull(value), na.rm = TRUE)
  
  # Plot for yi
  p_yi <- ggplot(data_rv %>% filter(measure == "yi"), aes(x = value, colour = group)) +
    geom_density(alpha = 0.5, linetype = "dotted", linewidth = 0.8) +
    scale_x_continuous(limits = yi_range) +
    labs(title = paste("Density Plot of Effect Sizes (yi) for", rv),
         x = "Effect Size (yi)",
         y = "Density",
         fill = "Group") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Plot for vi
  p_vi <- ggplot(data_rv %>% filter(measure == "vi"), aes(x = value, colour = group)) +
    geom_density(alpha = 0.5, linetype = "dotted", linewidth = 0.8) +
    scale_x_continuous(limits = vi_range) +
    labs(title = paste("Density Plot of Variances (vi) for", rv),
         x = "Variance (vi)",
         y = "Density",
         fill = "Group") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Combine the plots using patchwork
  combined_plot <- p_yi + p_vi + plot_layout(ncol = 1)
  
  # Store the combined plot in the list
  plots[[rv]] <- combined_plot
}

# Print the combined plots
for (rv in names(plots)) {
  print(plots[[rv]])
}
```


Interpretation of the yi and vi density plots

The visual assessment from the density plots indicates that there are very high variances (vi) in several response variables, particularly in product quality, water quality, crop yield, pest and diseases, and soil quality. This high spread in variances can significantly influence the results of your meta-analysis, leading to potential biases and less reliable estimates. Dealing with extreme variances (vi) in a meta-analysis is crucial because these 'outliers' can disproportionately influence the results, leading to unreliable conclusions. 

1. Identify Observations with Extreme Variances

To identify observations with extreme variances, it is possible to use the following approach:
  a) Calculate summary statistics for the vi values to understand their distribution.
  b) Identify outliers based on these statistics, typically using a threshold like the top 1% of vi values or using statistical measures like IQR
  (Interquartile Range).






These density plots illustrate the distributions of the effect sizes (yi) and their variances (vi) across different response variables for two groups: the silvo group and the control group. Here's a detailed interpretation:

Interpretation of the Plots
Effect Sizes (yi):

The density plots for effect sizes (yi) indicate how the observed effect sizes are distributed across different response variables.
The majority of the effect sizes seem to be centered around zero, which is typical in meta-analysis when combining results from multiple studies.
Variances (vi):

The density plots for variances (vi) show the spread of the sampling variances of the effect sizes.
For some response variables like "Biodiversity," "Crop yield," and "Pest and Disease," the variances are highly skewed with extreme values.
This broad variance indicates high variability in the effect size estimates, suggesting that the studies contributing to these effect sizes may have differing levels of precision or sample sizes.
Reasons for Broad Variance
Heterogeneity: The broad variances can be due to substantial heterogeneity among the included studies. Differences in study designs, populations, interventions, and outcomes can contribute to this.
Measurement Error: Some studies might have higher measurement errors or smaller sample sizes, leading to larger variances in their effect size estimates.
Outliers: Extreme values or outliers in the data can inflate the variance.
Addressing Broad Variance
Transformations: Consider applying transformations to stabilize the variances. For example, log transformation can sometimes help in reducing skewness.
Robust Meta-Analysis Methods: Use robust statistical methods that are less sensitive to outliers and extreme values.
Subgroup Analysis: Perform subgroup analyses to explore sources of heterogeneity. Group studies based on characteristics like study design, population, or intervention.
Meta-Regression: Use meta-regression to account for potential moderators that explain variability in effect sizes.
Trim-and-Fill Method: Use the trim-and-fill method to identify and adjust for publication bias, which can contribute to variance.



Standardizing the variance (vi) data can be helpful in addressing some of the issues with broad variance. However, it's important to understand that standardizing vi will not necessarily address the underlying heterogeneity or outliers in the data. It can make the scale of variances more comparable, but it won't change the fact that some studies have higher uncertainty than others.

Regarding the effect size measure yi, the ROM (Ratio of Means) is already a standardized effect size measure that accounts for differences in scales between studies to some extent. The broad variance you are observing is likely due to the inherent heterogeneity and differences in study quality.

Standardizing Variance (vi)
You can standardize the variances by transforming them into a common scale. One way to do this is to divide each variance by the mean variance or by some measure of central tendency (e.g., the median):


Standardizing Effect Sizes (yi) 
While the ROM effect size measure does help in standardizing the effect sizes across studies, you might still consider standardizing yi for further analyses, especially if you have different scales or units across studies:




Alternative

##########################################################################################################################################
EXCLUDE HIGH-VARIANCE OBSERVATIONS 
##########################################################################################################################################

Identify and Exclude High Variance Observations from the dataset before making the variance-covariance matrix and 
fitting the random-effects model

To avoid the previous error:
Error: Fejl: Optimizer (nlminb) did not achieve convergence (convergence = 1).

Warning: Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
The warning "Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results" indicates that there is a large disparity between the variances of your effect sizes, which can lead to instability in the model results.

```{r}
# Inspect the variances (vi)
meta_data_ROM |>
  filter(!is.na(vi)) |> 
  ggplot(aes(x = vi)) +
  geom_histogram(bins = 100) +
  #scale_y_log10() + 
  scale_x_log10() +
  ggtitle("Distribution of Sampling Variances (vi)") +
  theme_minimal() 
```
```{r}
# Identify rows with high variances

high_variance_obs <- meta_data_ROM |> 
  filter(vi > quantile(vi, 0.950)) |> # 0.995
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n) |> 
  arrange(id_article, response_variable)

skim(high_variance_obs)
```

```{r}
# Exclude high variance observations from the dataset

# Add a row number column to meta_data_ROM
meta_data_ROM <- meta_data_ROM |> 
  as.data.frame() |> 
  mutate(row_id = row_number())

# Identify rows with high variances
high_variance_obs <- meta_data_ROM |> 
  filter(vi > quantile(vi, 0.950, # 0.995 # 0.950
                       na.rm = TRUE))
##################################################################################################################

# Exclude high variance observations from the dataset
filtered_meta_data_ROM <- meta_data_ROM |> 
  filter(!row_id %in% high_variance_obs$row_id)

filtered_meta_data_ROM
filtered_meta_data_ROM |> glimpse()
filtered_meta_data_ROM |> skim()

###############################################################################
```
```{r, eval = FALSE}
##############################################################
# Save to CSV
readr::write_csv(filtered_meta_data_ROM, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/filtered_meta_data_ROM_database_v3.csv"))
```




############################################################################################################################
READING THE FILTERED METADATA CSV
############################################################################################################################

```{r}
filtered_meta_data_ROM <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/filtered_meta_data_ROM_database_v3.csv")

filtered_meta_data_ROM
```

##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

Creating a variance-covariance matrix is crucial in multivariate meta-analysis because it captures the dependencies among the effect sizes from different outcomes measured within the same study. Without accounting for these dependencies, the analysis could be biased and less efficient.

Why a Variance-Covariance Matrix is Needed
- Account for Within-Study Correlations: When multiple outcomes are reported within the same study, they are often correlated. Ignoring these correlations can lead to inaccurate estimates of the overall effect size and its variance.
- Borrowing Strength: The variance-covariance matrix allows the analysis to borrow strength across different outcomes, leading to more precise estimates.
- Improve Model Accuracy: Including the correct variance-covariance structure improves the accuracy of the random-effects model, leading to better inference.

```{r}
# Create a variance-covariance matrix for each study
V_list <- list() # Initialize an empty list to store variance-covariance matrices for each study

# Loop through each unique study ID in the dataset
for (study in unique(filtered_meta_data_ROM$id_article)) {
  # Subset the data for the current study
  study_data <- filtered_meta_data_ROM[filtered_meta_data_ROM$id_article == study, ]
  
  # Check if the current study has more than one outcome
  if (nrow(study_data) > 1) {
    # Create a diagonal matrix with the variances (vi) of the outcomes
    V <- diag(study_data$vi)
    
    # Assume a constant correlation of 0.5 between outcomes within the same study
    corr <- 0.5
    
    # Loop through the rows of the matrix to set the off-diagonal elements
    for (i in 1:nrow(V)) {
      for (j in 1:nrow(V)) {
        # Set the off-diagonal elements to the product of the correlation and the square root of the product of the corresponding variances
        if (i != j) {
          V[i, j] <- corr * sqrt(V[i, i] * V[j, j])
        }
      }
    }
    # Add the variance-covariance matrix to the list for the current study
    V_list[[as.character(study)]] <- V
  } else {
    # If there is only one outcome, the variance is just the variance of the single outcome
    V_list[[as.character(study)]] <- study_data$vi
  }
}

# Combine the matrices into a block-diagonal matrix
V_matrix <- bldiag(V_list)
```

```{r}
str(V_matrix)
```




##########################################################################################################################################
Quality Assessment of the filtered_meta_data_ROM and V_matrix data 
##########################################################################################################################################

```{r}
# Check for missing values in filtered_meta_data_ROM
missing_values <- sapply(filtered_meta_data_ROM, function(x) sum(is.na(x)))
print("Missing Values in filtered_meta_data_ROM:")
print(missing_values)

# [1] "Missing Values in filtered_meta_data_ROM:"
#              id_article       response_variable        measured_metrics           measured_unit                      yi                      vi 
#                       0                       0                       0                       0                       0                       0 
#              silvo_mean                silvo_se                silvo_sd                 silvo_n            control_mean              control_se 
#                       0                       0                       0                       0                       0                       0 
#              control_sd               control_n                  id_obs     experimental_design              comparator   tree_crop_combination 
#                       0                       0                       0                     854                       0                       0 
# tree_crop_combination_t                  season    tree_row_orientation            soil_texture           no_tree_per_m             tree_height 
#                       0                       0                     314                      42                     114                     441 
#             alley_width                 tillage                 organic   sub_response_variable         distance_tree_m        study_year_start 
#                      83                       0                       0                       0                     200                       0 
#          study_year_end                comments         min_value_shift                  row_id 
#                       0                       0                       0                       0 
```
```{r}
# Distribution of vi
filtered_meta_data_ROM |> 
  ggplot(aes(x = vi)) +
  geom_histogram(binwidth = 0.1) +
  ggtitle("Distribution of Sampling Variances (vi)") +
  scale_x_log10() +
  theme_minimal()
```

```{r}
# Check for positive definiteness of V_matrix
is_positive_definite <- function(mat) {
  eigenvalues <- eigen(mat)$values
  all(eigenvalues > 0)
}

# Summary statistics for key columns
summary_stats <- filtered_meta_data_ROM %>%
  select(yi, vi, id_article, response_variable) %>%
  summary()
print("Summary Statistics for filtered_meta_data_ROM:")
print(summary_stats)

# Check for positive definiteness of V_matrix
is_positive_definite <- function(mat) {
  eigenvalues <- eigen(mat)$values
  all(eigenvalues > 0)
}
```

```{r}
positive_definite_check <- is_positive_definite(V_matrix)
print(paste("V_matrix is positive definite:", positive_definite_check))
```

The code snippet provided is checking if the variance-covariance matrix (V_matrix) used in your meta-analysis model is positive definite. 

Positive Definiteness of a Matrix:
A matrix is positive definite if all its eigenvalues are positive.
Positive definiteness is a desirable property for variance-covariance matrices, ensuring that the matrix can be used in statistical models and that the calculations involving the matrix (e.g., inversions) are stable.

Ensuring the positive definiteness of the variance-covariance matrix is crucial for the integrity of statistical models, including those used in meta-analysis. A positive definite matrix guarantees that the matrix is invertible, a necessary condition for various statistical calculations, such as estimating model parameters. This property also ensures numerical stability during computations, reducing the likelihood of errors and ensuring accurate results, which is fundamental for the reliability of the model fitting process and subsequent statistical inference.

Excluding missing values from the dataset is essential to maintain data integrity and model accuracy. Missing values can lead to biased estimates and incorrect inferences if not handled appropriately. By removing rows with missing values in critical columns, we ensure that all observations used in the analysis are complete and reliable. This process simplifies the dataset, making it easier to apply statistical methods and interpret the results accurately.







##########################################################################
EXCLUDIDNG MISSING VALUES FROM THE DATASET - in case of any missing values. However, in this case there is no missing values
##########################################################################



##########################################################################################################################################
PERFORM THE META-ANALYSIS MODELLING WITH A RANDOM-EFFECTS MODEL
##########################################################################################################################################

##########################################################################
NOW RUNNING THE MODEL FITTING
##########################################################################

TEST

```{r}
# rma.mv(yi = yi, 
#        V = V_matrix_clean, 
#        random = ~ 1 | id_article/response_variable, 
#        data = filtered_meta_data_ROM_clean,
#        verbose = TRUE)

# Iteration 71    ll = -5945.9314   sigma2 = 0.0077  0.0190  
# 
# 
# Multivariate Meta-Analysis Model (k = 985; method: REML)
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0077  0.0876     37     no                    id_article 
# sigma^2.2  0.0190  0.1377     50     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 984) = 16532.7158, p-val < .0001
# 
# Model Results:
# 
# estimate      se    zval    pval    ci.lb   ci.ub    
#   0.0156  0.0282  0.5530  0.5803  -0.0396  0.0708    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```


```{r}
filtered_meta_data_ROM
```

```{r}
# Count the number of unique response variables for each study
study_response_count <- filtered_meta_data_ROM %>%
  group_by(id_article) %>%
  summarize(num_response_vars = n_distinct(response_variable))

# Plot the number of response variables for each study

ggplot(study_response_count, aes(x = factor(id_article), y = num_response_vars)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Response Variables per Study",
       x = "Study (id_article)",
       y = "Number of Response Variables") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotate x-axis labels for readability

```

```{r}
model_with_response_var <- rma.mv(
  # Effect size estimates (dependent variable)
  yi = yi,              
  # Variance-covariance matrix of the effect sizes
  V = V_matrix,         
  random = list(
    # Random intercept for each article to account for between-study variability
    ~ 1 | id_article,                   
    # Random intercept for each response variable within each article to account for within-study correlation
    ~ 1 | id_article/response_variable
  ),
  data = filtered_meta_data_ROM,  # The dataset for the meta-analysis
  verbose = TRUE,                 # Enable verbose output to track convergence
  control = list(
    optimizer = "optim",          # Use 'optim' optimizer
    optim.method = "BFGS",        # Use BFGS optimization method
    iter.max = 1000,              # Maximum iterations
    rel.tol = 1e-8                # Normal convergence tolerance
  ),
  method = "ML"                   # Use Maximum Likelihood for model fitting
)


model_without_response_var <- rma.mv(
  # Effect size estimates (dependent variable)
  yi = yi,              
  # Variance-covariance matrix of the effect sizes
  V = V_matrix,         
  # Only random intercept for each article
  random = ~ 1 | id_article,  
  data = filtered_meta_data_ROM,  # The dataset for the meta-analysis
  verbose = TRUE,                 # Enable verbose output to track convergence
  control = list(
    optimizer = "optim",          # Use 'optim' optimizer
    optim.method = "BFGS",        # Use BFGS optimization method
    iter.max = 1000,              # Maximum iterations
    rel.tol = 1e-8                # Normal convergence tolerance
  ),
  method = "ML"                   # Use Maximum Likelihood for model fitting
)

```

```{r}
AIC(model_with_response_var, model_without_response_var)
```

```{r}
anova(model_with_response_var, model_without_response_var)
```


Full Model (with both random intercepts for article and response variable within article) is better than the Reduced Model (with only random intercept for article).



#######################################################################################################################################
ACTUAL MODEL FITTING
#######################################################################################################################################

```{r, eval = FALSE}
# Fit a random-effects model using rma.mv
# By setting "verbose = TRUE", we can obtain information on the progress of the optimization algorithm:

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Adjust the model to include study-level variance and within-study correlation

#######################################################################################################################################
# Fit the original multivariate random-effects meta-analysis model using the 'rma.mv' function
# This model accounts for study-level variance and within-study correlation

original_model <- rma.mv(
  # Effect size estimates (dependent variable)
  yi = yi,             
  # Variance-covariance matrix of the effect sizes (providing information on within-study sampling variances and covariances)
  V = V_matrix,                                      
  random = list(
    # Random intercept for each article to account for between-study variability
    ~ 1 | id_article,                   
    # Random intercept for each response variable within each article to account for within-study correlation
    ~ 1 | id_article/response_variable               
  ),
  # Data frame containing the meta-analysis data, including effect sizes and their variances 
  # The filtered data (using the filter(vi > quantile(vi, 0.950) has rows/obs: 1,007)
  data = filtered_meta_data_ROM,    
  # Enable verbose output to print progress and convergence information
  verbose = TRUE,                                    
  control = list(
    # Optimizer to be used for fitting the model; 'nlminb' is robust for constrained optimization problems. 
    # Changed to 'optim' (the 08/10-24) with BFGS that can better handle complex, non-linear surfaces and is generally faster in converging. 
    # nlminb might get stuck more easily if the likelihood function is tricky or has flat areas, but it’s more robust when parameter constraints are needed.
    optimizer = "optim",  
    # Use BFGS optimization method,   
    optim.method = "BFGS",
     # Maximum number of iterations for the optimizer; increase if the model is complex or if convergence is slow
    iter.max = 1000,              
    # Relative convergence tolerance; lower values indicate stricter convergence criteria
    rel.tol = 1e-12     # with nlminb I reduced from 1e-8 to 1e-4 (the 08/10-24) to avoid non-convergence       
    # Alternatively, provide initial estimates for variance components to ease convergence
    #sigma2.init = c(0.01, 0.01, 0.01)  
  ),
  # Use Maximum Likelihood (ML) estimation for fitting the model (alternatively, REML could be used for restricted maximum likelihood)
  # Justification for using Maximum Likelihood (ML) over REML:
  # - ML allows for straightforward model comparisons, making it suitable for hypothesis testing 
  #   and model selection based on different fixed effects (e.g., adding or removing moderators).
  # - ML provides consistent estimates of fixed effects, which is essential for accurately assessing 
  #   overall effect sizes and moderator impacts in the meta-analysis.
  # - For complex models with multiple levels of random effects, ML can be more computationally 
  #   efficient and is more versatile, making it a practical choice for this analysis.
  # - Although ML might slightly underestimate variance components in small sample sizes, the primary 
  #   focus here is on hypothesis testing and the fixed effects, for which ML is well-suited.
  method = "ML"                                      
)

# Notes on the model:
# - 'yi': Represents the vector of observed effect sizes, calculated previously using escalc() function.
# - 'V': The variance-covariance matrix representing the sampling variances of effect sizes and covariances, allowing for dependencies within studies.
# - 'random': Specifies the random-effects structure. 
#     - '~ 1 | id_article': Random effect at the study level to capture between-study variance.
#     - '~ 1 | id_article/response_variable': Nested random effects to account for within-study variability across different response variables.
# - 'data': Specifies the dataset being used, which has been preprocessed to include only relevant and cleaned data points.
# - 'verbose': Provides output on the optimization process, helpful for diagnosing convergence issues.
# - 'control': Contains settings for the optimization process, including the choice of optimizer, maximum iterations, and convergence tolerance.
# - 'method': Specifies the estimation method; ML is chosen here to estimate the fixed and random effects.
#             The choice between ML and REML should be guided by the specific goals of the analysis, with ML being suitable for hypothesis testing and REML
#             offering better variance estimation properties. 

# Running this model will provide insight into both the fixed effect sizes and the random effects that account for variability within and between studies.
# It will also help in understanding how much of the total variability is due to differences between studies and how much is due to within-study correlation among response variables.
#######################################################################################################################################
##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##############################################################
# Last go: (25/08-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Variance Components in Model:
# 
# Iteration 0     ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# Iteration 1     ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# Iteration 2     ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106
# ...
# Iteration 212   ll = -16178.1646   sigma2 = 0.0006  0.0000  0.0041  
# Iteration 213   ll = -16178.1648   sigma2 = 0.0006  0.0000  0.0041  
# Iteration 214   ll = -16178.1646   sigma2 = 0.0006  0.0000  0.0041

# Time difference of 1.47106 mins

##############################################################
# Last go: (04/09-24)

# Iteration 68    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 69    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 70    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 71    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 72    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# 
# Time difference of 54.30005 secs

##############################################################
# Last go: (08/10-24)

# Iteration 70    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 71    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# Iteration 72    ll = -3044.1332   sigma2 = 0.0014  0.0014  0.0011  
# 
# Time difference of 1.071125 mins

# Iteration 26    ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# Iteration 27    ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# Iteration 28    ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# Iteration 29    ll = -16196.8585   sigma2 = 0.0106  0.0106  0.0106  
# 
# Time difference of 13.45971 secs

##############################################################
# Last go: (08/10-24)

# Iteration 105   ll = -16178.2376   sigma2 = 0.0000  0.0000  0.0047  
# Iteration 106   ll = -16178.2376   sigma2 = 0.0000  0.0000  0.0047  
# Iteration 107   ll = -16178.2396   sigma2 = 0.0000  0.0000  0.0047  
# Iteration 108   ll = -16178.2377   sigma2 = 0.0000  0.0000  0.0047  
# Iteration 109   ll = -16178.2376   sigma2 = 0.0000  0.0000  0.0047  
# Iteration 110   ll = -16178.2376   sigma2 = 0.0000  0.0000  0.0047  
# 
# Time difference of 46.44449 secs

```




TAKING A LOOK AT THE FITTED MODEL

```{r, eval = FALSE}
# Inspect the model summary to get more details
summary(original_model)

# Multivariate Meta-Analysis Model (k = 1007; method: REML)
# 
#      logLik     Deviance          AIC          BIC         AICc   
# -16564.4318   33128.8636   33136.8636   33156.5186   33136.9036   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0006  0.0245     37     no                    id_article 
# sigma^2.2  0.0047  0.0683     37     no                    id_article 
# sigma^2.3  0.0280  0.1674     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1006) = 98341.7572, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0227  0.0303  -0.7489  0.4539  -0.0821  0.0367    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#################################################################################
# Last go: (25/08-24)

# Multivariate Meta-Analysis Model (k = 1007; method: ML)
# 
#      logLik     Deviance          AIC          BIC         AICc   
# -16178.1646   37227.3676   32364.3291   32383.9881   32364.3691   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0006  0.0253     37     no                    id_article 
# sigma^2.2  0.0000  0.0000     37     no                    id_article 
# sigma^2.3  0.0041  0.0637     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1006) = 264771.5488, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0098  0.0109  -0.8998  0.3682  -0.0311  0.0115    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##############################################################
# Last go: (04/09-24)

# Multivariate Meta-Analysis Model (k = 1017; method: ML)
# 
#     logLik    Deviance         AIC         BIC        AICc   
# -3044.1332  10969.9452   6096.2665   6115.9649   6096.3060   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0014  0.0380     37     no                    id_article 
# sigma^2.2  0.0014  0.0380     37     no                    id_article 
# sigma^2.3  0.0011  0.0334     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1016) = 1919085.9233, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0065  0.0107  -0.6103  0.5417  -0.0276  0.0145    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##############################################################
# Last go: (05/09-24)

# Multivariate Meta-Analysis Model (k = 1017; method: ML)
# 
#     logLik    Deviance         AIC         BIC        AICc   
# -3044.1332  10969.9452   6096.2665   6115.9649   6096.3060   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0014  0.0380     37     no                    id_article 
# sigma^2.2  0.0014  0.0380     37     no                    id_article 
# sigma^2.3  0.0011  0.0334     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1016) = 1919085.9233, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0065  0.0107  -0.6103  0.5417  -0.0276  0.0145    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##############################################################
# Last go: (08/10-24)

# Multivariate Meta-Analysis Model (k = 1017; method: ML)
# 
#     logLik    Deviance         AIC         BIC        AICc   
# -3044.1332  10969.9452   6096.2665   6115.9649   6096.3060   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0014  0.0380     37     no                    id_article 
# sigma^2.2  0.0014  0.0380     37     no                    id_article 
# sigma^2.3  0.0011  0.0334     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1016) = 1919085.9233, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0065  0.0107  -0.6103  0.5417  -0.0276  0.0145    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##############################################################
# Last go: (08/10-24)

# Multivariate Meta-Analysis Model (k = 1007; method: ML)
# 
#      logLik     Deviance          AIC          BIC         AICc   
# -16196.8585   37264.7554   32401.7170   32421.3759   32401.7569   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0106  0.1032     37     no                    id_article 
# sigma^2.2  0.0106  0.1032     37     no                    id_article 
# sigma^2.3  0.0106  0.1032     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1006) = 264771.5488, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0112  0.0291  -0.3826  0.7020  -0.0683  0.0460    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##############################################################
# Last go: (08/10-24)

# Multivariate Meta-Analysis Model (k = 1007; method: ML)
# 
#      logLik     Deviance          AIC          BIC         AICc   
# -16178.2376   37227.5136   32364.4752   32384.1341   32364.5151   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0000  0.0037     37     no                    id_article 
# sigma^2.2  0.0000  0.0037     37     no                    id_article 
# sigma^2.3  0.0047  0.0682     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1006) = 264771.5488, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0095  0.0106  -0.9008  0.3677  -0.0302  0.0112    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```


INTERPRETATION OF MODEL SUMMARY

**Older model from earlier database**
The multivariate meta-analysis model includes 1007 effect sizes from various studies and is implemented using the maximum likelihood (ML) method. The log-likelihood value is -16178.1646, which indicates how well the model fits the data. The model's fit statistics, including the AIC (32364.3291), BIC (32383.9881), and AICc (32364.3691), are measures of the model's goodness of fit, with lower values suggesting a better fit. These values are essential for comparing different models and selecting the most appropriate one.

The model accounts for variability at different levels using random effects. The variance components for the id_article level are as follows:

- sigma^2.1: 0.0006 (with a standard deviation of 0.0253), indicating the variance attributed to individual articles.
- sigma^2.2: 0.0000 (with a standard deviation of 0.0000), suggesting negligible or no variance at this level.
- sigma^2.3: 0.0041 (with a standard deviation of 0.0637), representing the variance attributed to different response variables within each article.

These variance components reflect the hierarchical structure of the data, emphasizing the importance of considering both article-level and response variable-level variability.

The Q-test for heterogeneity shows significant variability among the effect sizes, with a Q-value of 264771.5488 and a p-value of less than 0.0001. This significant heterogeneity implies that the effect sizes vary more than would be expected by chance alone, indicating underlying differences between the studies that must be accounted for in the analysis. The presence of such substantial heterogeneity justifies the use of a random-effects model to capture the between-study variability.

Despite the significant heterogeneity, the overall effect size estimate is -0.0098, with a standard error of 0.0109. This estimate is not statistically significant, as indicated by a p-value of 0.3682. The 95% confidence interval for the effect size ranges from -0.0311 to 0.0115, which includes zero, further indicating the lack of a significant overall effect. This suggests that the effect sizes across the studies do not show a consistent trend or significant overall impact, highlighting the complexity and variability inherent in the data.

**Updated model from database v3**
The multivariate meta-analysis model, based on 1007 effect sizes from different studies, was fitted using the Maximum Likelihood (ML) estimation method. The log-likelihood value of -16178.2376 reflects the fit of the model, while fit indices like AIC (32364.4752), BIC (32384.1341), and AICc (32364.5151) provide measures of the model's goodness of fit. These values are crucial for model comparison, with lower values indicating a better fit.

The random-effects model accounts for variability at two levels: study-level (id_article) and within-study (response variables). The variance components are as follows:

- sigma^2.1 = 0.0000 (standard deviation = 0.0037): Indicates near-zero variance between studies, suggesting minimal between-study variability.
- sigma^2.2 = 0.0000 (standard deviation = 0.0037): Also reflects negligible variance at the article level, implying little variation between the articles.
- sigma^2.3 = 0.0047 (standard deviation = 0.0682): Captures within-study variability across different response variables, suggesting more substantial within-study variability.

The model also reports a highly significant Q-test for heterogeneity (Q = 264771.5488, p < 0.0001), showing substantial variability among the effect sizes across studies. This indicates that the observed differences in effect sizes are not due to chance alone and supports the need for a random-effects model to account for this heterogeneity.

Despite the high heterogeneity, the overall effect size estimate is -0.0095 with a standard error of 0.0106, which is not statistically significant (p = 0.3677). The 95% confidence interval, ranging from -0.0302 to 0.0112, includes zero, indicating no significant overall effect across the studies.

################################################################################################
SUMMARISED:

**Older model from earlier database**
Understanding the Variance Components
The model accounts for variability at multiple levels:

- sigma^2.1 (id_article level): A small variance (0.0006) indicates limited variability between articles, suggesting that article-level differences are not a major source of variability.

- sigma^2.2 (id_article level): No variance (0.0000) at this level suggests negligible additional article-level influences on the outcomes.

- sigma^2.3 (id_article/response_variable level): A higher variance (0.0041) indicates more variability due to differences in response variables within each article, highlighting the influence of specific metrics or outcomes.

Significant Heterogeneity
The Q-test shows substantial heterogeneity among the effect sizes (Q = 264771.5488, p < 0.0001), indicating that the observed differences are not due to chance alone but are likely influenced by factors such as study design and measurement methods. This significant variability supports the use of a random-effects model to appropriately account for the complexity of the data.

Non-Significant Overall Effect Size
The overall effect size (-0.0098) is not statistically significant (p = 0.3682), with the confidence interval (-0.0311 to 0.0115) crossing zero. This suggests that the combined studies do not show a consistent trend or significant effect, which may be due to inherent variability across studies or a genuine lack of a consistent overall effect.

**Updated model from database v3**
Understanding the Variance Components
The model highlights variability at multiple levels:

- sigma^2.1 (id_article level): Near-zero variance (0.0000) suggests limited variability between studies, indicating that differences between articles do not contribute much to overall variability.
- sigma^2.2 (id_article level): Also near-zero variance (0.0000), indicating no significant additional study-level variance.
- sigma^2.3 (id_article/response_variable level): Higher variance (0.0047) suggests more substantial variability within studies, driven by differences in response variables, showing the influence of study-specific outcomes.

Significant Heterogeneity
The Q-test indicates significant heterogeneity among effect sizes (Q = 264771.5488, p < 0.0001). This heterogeneity points to substantial variability that may arise from differences in study designs, populations, or measurement methods, supporting the use of random-effects modeling to handle this complexity.

Non-Significant Overall Effect Size
The overall effect size of -0.0095 is not statistically significant (p = 0.3677), with a confidence interval ranging from -0.0302 to 0.0112, indicating that the effect size across studies is small and not significantly different from zero. This suggests that, on average, the studies do not show a consistent trend or significant effect, likely due to variability between and within studies.

################################################################################################
COMPARING 'OLD' AND NEW (DATABASE v3) MODELS

The updated model shows slightly different variance components, particularly a reduction in between-study variance (sigma^2.1) and a slight increase in within-study variance (sigma^2.3). However, the overall conclusions remain the same: there is no significant overall effect across studies, and the heterogeneity between studies remains high, justifying the use of a random-effects model in both cases.


```{r, eval = FALSE}
# Extract variance-covariance components
vcov_matrix <- vcov(original_model)
vcov_matrix

# Extract additional parameters if they are modeled
tau2_value <- original_model$tau2
rho_value <- original_model$rho
gamma2_value <- original_model$gamma2
phi_value <- original_model$phi

# Print additional parameters
cat("tau2:", tau2_value, "\n")
cat("rho:", rho_value, "\n")
cat("gamma2:", gamma2_value, "\n")
cat("phi:", phi_value, "\n")
```


################################################################################################
FORMULATING THE NEXT STEPS IN THE MEAT-ANALYSIS
################################################

Next Steps:

1. Sensitivity Analysis:
    - Assess robustness of results by checking influence of individual studies.
      Evaluate the influence of individual studies on the overall results.
    - Check if the results change when certain studies are removed.
      Use leave-one-out analysis, detect outliers, and examine impact of large sample sizes.
    
2. Moderator Analysis:
    - Investigate if certain study characteristics (moderators) explain the heterogeneity.
      Identify sources of heterogeneity.
    - Include moderators in the model to see if they account for some of the variability.
      Conduct meta-regression using study-level covariates, analyze categorical and continuous moderators.

3. Visualization:
    - Create forest plots to visualize the effect sizes and their confidence intervals across studies.
      Enhance interpretation through visual representation.
    - Use bubble plots to show the relationship between effect sizes and study characteristics.
      Create forest plots for effect sizes, bubble plots for study characteristics, and distribution plots for effect sizes.

4. Diagnostic Plots:
    - Examine funnel plots to assess publication bias.
      Check for model assumptions and biases.
    - Use residual plots to check model assumptions.
      Use funnel plots for publication bias and residual plots for model assumptions.

Alternatives:
  Reconsider the Use of REML
    - Improve variance component estimation.
      Use REML for final variance estimation, compare with ML results to ensure robustness.
  
  Data Quality and Consistency Checks
    - Maintain reliability of data.
      Standardize measurement techniques and harmonize study protocols to reduce variability.
  
  Explore Potential Unobserved Moderators
    - Identify other factors affecting results.
      Perform qualitative review of studies and consult experts to hypothesize potential moderators.



#############
# STEP 4
##########################################################################################################################################
INFLUENCE DIAGNOSTICS AND SENSITIVITY ANALYSIS ON DATA (E.G.: MISSING OUTCOME DATA)
##########################################################################################################################################

original_model <- rma.mv(
  yi = yi,             
  V = V_matrix,                                      
  random = list(
    ~ 1 | id_article,                   
    ~ 1 | id_article/response_variable               
  ),
  data = filtered_meta_data_ROM,    
  verbose = TRUE,                                    
  control = list(
    optimizer = "nlminb",   
    iter.max = 1000,              
    rel.tol = 1e-8),  
    method = "ML"                                      
)

```{r, eval = FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################
# Initialize a list to store the results
sensitivity_results <- list()

# Get the list of unique study IDs
unique_studies <- unique(filtered_meta_data_ROM$id_article)

# Loop through each study and fit the model excluding that study
for (study in unique_studies) {
  # Exclude the current study
  data_excluded <- filtered_meta_data_ROM[filtered_meta_data_ROM$id_article != study, ]
  
  # Exclude corresponding rows and columns from the variance-covariance matrix
  V_matrix_excluded <- V_matrix[filtered_meta_data_ROM$id_article != study, filtered_meta_data_ROM$id_article != study]
  
  # Fit the model using the same settings as the original model
  model <- rma.mv(
    yi = yi, 
    V = V_matrix_excluded, 
    random = list(
      ~ 1 | id_article,
      ~ 1 | id_article/response_variable
    ), 
    data = data_excluded,
    method = "ML",  # Consistent with the original model's method
    control = list(
      # Optimizer to be used for fitting the model; 'nlminb' is robust for constrained optimization problems. 
      # Changed to 'optim' (the 08/10-24) with BFGS that can better handle complex, non-linear surfaces and is generally faster in converging. 
      # nlminb might get stuck more easily if the likelihood function is tricky or has flat areas, nlminb more robust when parameter constraints are needed.
      optimizer = "optim",  
      # Use BFGS optimization method, 
      optim.method = "BFGS",
      # Maximum number of iterations for the optimizer; increase if the model is complex or if convergence is slow
      iter.max = 1000,              
      # Relative convergence tolerance; lower values indicate stricter convergence criteria
      rel.tol = 1e-12     # with nlminb I reduced from 1e-8 to 1e-4 (the 08/10-24) to avoid non-convergence      
      # Alternatively, provide initial estimates for variance components to ease convergence
      #sigma2.init = c(0.01, 0.01, 0.01)  
      ),
    verbose = FALSE  # Set to FALSE to avoid excessive output during loop
  )
  
  # Store the results
  sensitivity_results[[as.character(study)]] <- list(
    estimate = model$b,
    se = model$se,
    ci.lb = model$ci.lb,
    ci.ub = model$ci.ub
  )
}

# Convert the results to a data frame
sensitivity_df <- do.call(rbind, lapply(sensitivity_results, function(x) data.frame(
  estimate = x$estimate,
  se = x$se,
  ci.lb = x$ci.lb,
  ci.ub = x$ci.ub
)))

sensitivity_df$id_article <- unique_studies

# sensitivity_df
##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################

# (first run)
# Time difference of 17.05628 mins

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.

# (second run)
# Time difference of 36.92614 mins
# 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.

#################################################################################
# Last go: (25/08-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 7.94416 mins

#################################################################################
# Last go: (03/09-24)
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 4.106577 mins

#################################################################################
# Last go: (04/09-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 7.815606 mins

#################################################################################
# Last go: (05/09-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 5.464035 mins

#################################################################################
# Last go: (08/10-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 11.23059 mins

#################################################################################
# Last go: (08/10-24)

# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Time difference of 35.15741 mins
```

```{r, eval = FALSE}
sensitivity_df
```

SAVING THE SENSITIVITY MATRIX

```{r, eval = FALSE}
readr::write_csv(sensitivity_df,
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/sensitivity_df_original_model_database_v3.csv"))
```


Discussion of Model Setup and Trade-offs in this Meta-Analysis

Addressing Non-Convergence and Model Adaptations

Handling non-convergence is crucial in meta-analysis to ensure reliable results. A series of strategies can be employed to mitigate these issues, including adjusting initial values for model parameters, simplifying the model by reducing the number of random effects, and using different optimizers such as "optim" instead of the default "nlminb" in the rma.mv function. Diagnostic measures like Cook's distance or leverage statistics can help identify studies with high influence, allowing for robust variance estimation methods that are less sensitive to these influential points. Additionally, modifying the analysis loop to handle non-convergence gracefully—by skipping problematic studies or trying alternative optimization methods—ensures the robustness of the findings.

The warning about the “Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results” highlights significant disparities in the variances among study estimates. This disparity can lead to instability, as studies with larger variances may disproportionately influence the pooled effect size, potentially skewing results. Addressing this requires a thorough examination of variance distribution to understand and mitigate the impact of outliers. Using robust variance estimators or adjusting study weights based on variance can help stabilize results. Continuing with sensitivity analysis is essential to assess the robustness of findings, and persistent instability may warrant consulting with a statistician to ensure the integrity of the analysis.

Modifications in Sensitivity Analysis Compared to the Original Model

The sensitivity analysis involved key modifications to the original model settings to handle convergence issues effectively. While the original model used the optimizer nlminb with a maximum of 1000 iterations and a strict relative tolerance of 1e-8 for high precision, the sensitivity analysis increased the maximum iterations to 5000, relaxed the relative tolerance to 1e-2, set the step size to 100, and introduced initial estimates for variance components (sigma2.init = c(0.001, 0.001, 0.005)). These adjustments were necessary to enhance the model’s ability to find stable solutions, particularly when excluding specific studies.

The relaxation of convergence criteria in the sensitivity analysis highlights a trade-off: it prioritizes consistent convergence over precision, which is essential for handling data variability. While the original model’s strict settings were geared towards obtaining precise solutions, they were more prone to convergence failures, especially during iterative fitting across different subsets. Introducing initial variance component estimates helped guide the optimizer towards reasonable starting points, improving the chances of convergence in complex models.

Impact and Interpretation of Model Adjustments

The relaxed settings in the sensitivity analysis model, though essential for achieving convergence across multiple iterations, may lead to slightly less stable and precise estimates. This could result in increased variability in the results, potentially masking subtle effects that the original model detected. Therefore, findings from the sensitivity analysis should be interpreted with caution and seen as a measure of robustness rather than definitive conclusions.

These adjustments underscore the complexity of the meta-analysis model and the need for further refinement to ensure reliable results. Exploring alternative modeling strategies or data transformation methods may be required to reduce variability in sampling variance and improve the stability of estimates. Ultimately, while sensitivity analyses are crucial for understanding the robustness of findings, they also highlight the importance of continually refining analytical methods to enhance the validity and reliability of meta-analytic conclusions.








READING THE SAVED SENSITIVITY MATRIX

```{r}
sensitivity_df <- readr::read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/sensitivity_df_original_model_database_v3.csv")

sensitivity_df
```









##########################################################################
Use the original model fitting
##########################################################################
```{r}
# Extract the original model results
# - 'original_estimate' is the original effect size estimate from the model
# - 'original_ci.lb' is the lower bound of the confidence interval for the original estimate
# - 'original_ci.ub' is the upper bound of the confidence interval for the original estimate

original_estimate <- original_model$b
original_ci.lb <- original_model$ci.lb
original_ci.ub <- original_model$ci.ub

##################################################
##################################################
```


Visualize the changes
Create a plot to visualize the influence of excluding each study

```{r}
# Add the original model results to the sensitivity data frame before plotting
# - 'sensitivity_df' is a data frame containing the results of the sensitivity analysis
# - 'original_estimate' is added to each row of the data frame
# - 'original_ci.lb' is added to each row of the data frame
# - 'original_ci.ub' is added to each row of the data frame

sensitivity_df <- sensitivity_df %>%
  mutate(original_estimate = original_estimate,
         original_ci.lb = original_ci.lb,
         original_ci.ub = original_ci.ub)

# Plot the results
sensitivity_df |> 
  ggplot(aes(x = id_article, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci.lb, ymax = ci.ub), width = 0.2) +
  geom_hline(aes(yintercept = original_estimate), linetype = "dashed", color = "red") +
  geom_hline(aes(yintercept = original_ci.lb), linetype = "dotted", color = "blue") +
  geom_hline(aes(yintercept = original_ci.ub), linetype = "dotted", color = "blue") +
  labs(title = "Sensitivity Analysis",
       x = "Study ID",
       y = "Estimate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  scale_x_continuous(breaks = seq(1, 40, 1))

```

Interpretation of the Sensitivity Analysis Plot
The sensitivity analysis plot shows the effect size estimates (points) and their 95% confidence intervals (error bars) for each study in your meta-analysis. The red dashed line represents the overall effect size estimate from the original model, while the blue dotted lines represent the upper and lower bounds of the 95% confidence interval for the overall estimate. Each point corresponds to the effect size estimate when the respective study is excluded from the analysis.

Consistency of Estimates: Most of the points are centered around the red dashed line (the original estimate), indicating that excluding any single study does not substantially alter the overall effect size estimate.
Confidence Intervals: The confidence intervals are relatively wide, which might suggest variability in the individual study estimates or small sample sizes within the studies.

Influential Studies: If any points lie significantly away from the red dashed line or their confidence intervals do not overlap with the original confidence interval (blue dotted lines), these studies might be considered influential. In this plot, it appears that none of the studies drastically change the overall estimate when excluded.
Suggestions for Further Insights
Leave-One-Out Meta-Analysis: Perform a leave-one-out meta-analysis to quantify the influence of each study on the overall effect size and heterogeneity measures. This involves refitting the model after excluding each study one at a time and comparing the overall results.

Cumulative Meta-Analysis: Conduct a cumulative meta-analysis where studies are added one at a time in a chronological order (or by study size or quality) to observe how the overall effect size estimate evolves as more data is included.

Influence Diagnostics: Calculate and plot various influence diagnostics such as:

Cook's Distance: To measure the influence of each study on the fitted model.
DFBETAS: To assess the influence of each study on individual regression coefficients.
Hat Values: To identify studies with high leverage.
Covariance Ratios: To understand the influence of each study on the precision of the parameter estimates.
Subgroup Analysis: Perform subgroup analyses to explore if certain subsets of studies (e.g., by study design, population characteristics, or intervention type) have a different impact on the overall effect size.

Meta-Regression: Use meta-regression to investigate the relationship between study-level covariates and the effect sizes, which can help identify factors contributing to heterogeneity.

Robustness Checks: Conduct robustness checks by varying the assumptions of the model, such as using different methods for estimating the between-study variance or using alternative effect size measures.







##########################################################################
OTHER DIAGNOSTICS TO GET FURTHER INSIGHTS INTO MODEL STUDY SENSITIVITY
##########################################################################


Studentized Residuals:

Purpose: These residuals are used to detect outliers. They are the residuals divided by an estimate of their standard deviation, adjusted for each observation.
Interpretation: Values beyond ±2 are often considered potential outliers. The plot helps in identifying these observations visually. Points far from the center line (0) indicate observations with larger than expected residuals.




Cook's Distance:

Purpose: Cook's distance assesses the influence of each observation on the overall model. Large values indicate observations that have a significant impact on the model's parameters.
Interpretation: Typically, Cook's distance values greater than 1 may indicate influential points. In large datasets, even smaller values can be significant. Points far above the average line in the plot are considered highly influential.

*OBS! Time for running this code below takes several days*

```{r, eval = FALSE}
# Calculate influence diagnostics

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

##############################################################
# Cook's Distance
##############################################################
# Compute Cook's Distance
cooks_d <- cooks.distance(original_model, 
                          reestimate = TRUE,
                          progbar = TRUE)

# Create a data frame for Cook's Distance
cooks_data <- data.frame(
  Observed_Outcome = seq_along(cooks_d),
  Cooks_Distance = cooks_d
)

# Save to CSV
readr::write_csv(cooks_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cooks_data_3.csv"))

##############################################################
# DFBETAS
##############################################################
# Compute DFBETAS
dfbetas_vals <- dfbetas(original_model, 
                        progbar = TRUE)

# Convert DFBETAS to a data frame
dfbetas_data <- as.data.frame(dfbetas_vals)

# Save to CSV
readr::write_csv(dfbetas_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/dfbetas_data_3.csv"))

##############################################################
# Studentized Residuals
##############################################################
# Compute Studentized Residuals
student_resid <- rstudent(original_model, 
                          progbar = TRUE)

# Convert Studentized Residuals to a data frame
student_resid_data <- data.frame(
  Observed_Outcome = seq_along(student_resid$resid),
  Residuals = student_resid$resid,
  Standard_Errors = student_resid$se,
  Z_Values = student_resid$z
)

# Save to CSV
readr::write_csv(student_resid_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/studentized_residuals_data_3.csv"))

##############################################################
# Hat values
##############################################################
# Compute Hat Values
hat_vals <- hatvalues(original_model)

# Convert Hat values to a data frame
hat_vals_data <- as.data.frame(hat_vals)

# Save to CSV
readr::write_csv(hat_vals_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/hat_vals_data_3.csv"))


##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################

# Time difference of 3 hours!

# <!-- #   |==================================================| 100% elapsed=01h 53m 02s -->
# <!-- #   |==================================================| 100% elapsed=03h 25m 55s -->
# <!-- #   |==================================================| 100% elapsed=02h 26m 29s -->
# Time difference of 7.757072 hours

#################################################################################
# Last go: (26/08-24)

# Time difference of 23.78939 hours
# 
# > #   |==================================================| 100% elapsed=08h 10m 24s
# > #   |==================================================| 100% elapsed=07h 56m 07s
# > #   |==================================================| 100% elapsed=07h 40m 51s

#################################################################################
# Last go: (07/09-24)

#  cooks_d <- cooks.distance(original_model, 
# +                           reestimate = TRUE,
# +                           progbar = TRUE)
#   |==================================================| 100% elapsed=08h 53m 22s
# dfbetas_vals <- dfbetas(original_model, 
# +                         progbar = TRUE)
#   |==================================================| 100% elapsed=07h 22m 26s
# student_resid <- rstudent(original_model, 
# +                           progbar = TRUE)
#   |==================================================| 100% elapsed=04h 09m 34s
# Time difference of 20.42284 hours
```



READING THE DIAGNOSTICS DATA

```{r}
# Load Cook's Distance data
cooks_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cooks_distance_3.csv")

# Load DFBETAS data
dfbetas_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/dfbetas_3.csv")

# Load Studentized Residuals data
student_resid_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/studentized_residuals_3.csv")

# Load Hat Values data
hat_vals_data <- 
  read_csv("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/hat_vals_data_3.csv")
```


##########################################################################################################################################  
PLOTTING INFLUENCE DIAGNOSTICS
##########################################################################################################################################

```{r}
# Plot Studentized Residuals

# Create a data frame for ggplot2
residuals_data <- data.frame(
  Observed_Outcome = seq_along(student_resid_data$Observed_Outcome),
  Studentized_Residuals = student_resid_data$Residuals
)

# Create the ggplot2 plot
residuals_data_plot <- 
residuals_data |> 
ggplot(aes(x = Observed_Outcome, y = Studentized_Residuals)) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  geom_segment(aes(xend = Observed_Outcome, yend = 0), linetype = "solid") +
  geom_point() +
  ylim(-3, 3) +
  labs(title = "Studentized Residuals Plot", 
       x = "Observed Outcome", 
       y = "Studentized Residuals") +
  theme_minimal()

residuals_data_plot
```

```{r, eval = FALSE}
# Saving the residuals data plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "residuals_data_plot.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = residuals_data_plot,
       width = 10, height = 8, dpi = 400)
```

```{r}
# Plot Cook's Distance

# Create a data frame for ggplot2 with appropriate lengths
cooks_data_df <- data.frame(
  Observed_Outcome = seq_along(cooks_data$Cooks_Distance),
  Cooks_Distance = cooks_data$Cooks_Distance
)

# Plot Cook's Distance
cooks_data_df_plot <-
ggplot(cooks_data_df, aes(x = Observed_Outcome, y = Cooks_Distance)) +
  geom_point() +
  geom_hline(yintercept = 4 / nrow(cooks_data_df), linetype = "dashed", color = "red") + 
  labs(
    title = "Cook's Distance Plot",
    x = "Observed Outcome",
    y = "Cook's Distance"
  ) +
  theme_minimal()

cooks_data_df_plot
```

```{r, eval = FALSE}
# Saving the Cook's Distance plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "cooks_data_df_plot.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = cooks_data_df_plot,
       width = 10, height = 8, dpi = 400)
```

A Cook's Distance plot helps identify influential observations in a regression analysis, indicating which data points substantially affect the model's predictions. Observations with high Cook's Distance values suggest that their exclusion would significantly change the model's results. This is crucial in our sensitivity analysis, where we systematically exclude each study to assess the robustness of our findings. By comparing Cook's Distance results with other sensitivity analyses outcomes, we can pinpoint studies or data points that disproportionately influence the meta-analysis. If certain observations are flagged as influential in both analyses, they may require further investigation to check for data entry errors, outliers, or unique characteristics that justify their influence. Based on these insights, we can decide whether to exclude, adjust, or treat these influential observations differently to enhance model reliability. Documenting and addressing these influential points ensures the robustness and credibility of our meta-analysis, providing transparency and enhancing the overall validity of your findings.

In our Cook's Distance plot, each point represents an observation's Cook's Distance, a measure of how much a single observation influences the overall regression analysis. Higher Cook's Distance values indicate that the observation has a more significant impact on the model's predictions, potentially skewing our model results. The red dashed line represents a commonly used threshold, calculated as 4/number of observations, which helps to identify influential observations. Points above this line are considered to have a substantial influence on the model, and we should further investigate their  impact. In our plot, most of the observations have Cook's Distance values near zero, suggesting limited individual influence on the model. However, a few points exceed the threshold, indicating that these observations might be outliers or have unique characteristics that disproportionately affect our model's estimates. These high-influence points needs closer examination to assess whether they should be included, adjusted, or excluded to ensure the robustness and reliability of our meta-analysis regression model.

```{r}
# Plot Hat Values

# Create a data frame for hat values
hat_values_data <- data.frame(
  Observed_Outcome = seq_along(hat_vals),
  Hat_Values = hat_vals
)

# Compute the threshold line
threshold <- 2 * mean(hat_vals)

# Plot Hat Values using ggplot2
hat_values_data_plot <-
ggplot(hat_values_data, aes(x = Observed_Outcome, y = Hat_Values)) +
  geom_segment(aes(xend = Observed_Outcome, yend = 0), color = "blue") +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red") +
  labs(title = "Hat Values Plot", x = "Observed Outcome", y = "Hat Values") +
  theme_minimal()

hat_values_data_plot
```

```{r, eval = FALSE}
# Saving the Hat Values plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "hat_values_data_plot.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = hat_values_data_plot,
       width = 10, height = 8, dpi = 400)
```

A Hat Value plot is a diagnostic tool used in regression analysis to evaluate the influence or leverage of each data point on the fitted model. In this plot, each point represents a hat value, which is derived from the diagonal elements of the hat matrix used in the regression calculation. Hat values measure how far an observation's predictor values are from the mean of all predictor values, indicating its influence on the model's predictions. High hat values suggest that the corresponding observation has substantial leverage, meaning it can significantly affect the regression line or model fit.

In the our plotted results, each blue line represents the hat value for a corresponding observed outcome, with a red dashed line indicating a threshold set at twice the average hat value (2 * mean(hat_vals)). Observations with hat values above this threshold are considered to have high leverage and we should examine these more closely, as they can disproportionately influence the results of our model. Identifying these high-leverage points is crucial for assessing the robustness and reliability of the model, guiding potential adjustments, or making decisions about excluding these points to improve model accuracy and validity.

```{r}
# Plot weights of model fitting - the weights given to the observed effect sizes or outcomes during the model fitting

# Extract weights
weights_vals <- weights(original_model)

# Create a data frame for plotting
weights_data <- data.frame(
  Observed_Outcome = seq_along(weights_vals),
  Weights = weights_vals
)

# Plot the weights using ggplot2
weights_model_fit_plot <- 
ggplot(weights_data, aes(x = Observed_Outcome, y = Weights)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Weights for Observed Outcomes", x = "Observed Outcome", y = "Weight") +
  theme_minimal() +
  theme(panel.grid.major = element_line(colour = "grey90"),
        panel.grid.minor = element_blank())
```

```{r, eval = FALSE}
# Saving the Hat Values plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "weights_model_fit_plot.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = weights_model_fit_plot,
       width = 4, height = 2, dpi = 400)
```

The weights plot for model fitting visually represents how much influence each observed outcome has on the overall meta-analysis model. In this context, weights are inversely related to the variance of the observations: smaller variances, indicating more precise estimates, are given higher weights. The plot shows each bar representing an observed outcome, with its height indicating the weight assigned to that outcome. From the provided plot, we can see that a few observations have exceptionally high weights, suggesting they have a significant impact on the model's fit. This disproportionate influence implies that these particular data points are much more precise (or have smaller variances) than others. It will be important to examine these high-weight observations to understand why they are so influential, which might involve checking the underlying study designs or measurement methods. Identifying such influential data points is crucial for ensuring the robustness of our meta-analysis results, potentially leading to adjustments or exclusions for maintaining validity.

*OBS! Time for running this code below is long*

```{r, eval = FALSE}
# Preparing data for Covariance Ratios plot

#################################################################################
# The function calc_cov_r takes a long time to run because it fits a new model by excluding one observation at a time for each observation in the dataset. This process involves re-fitting the model k times (where k is the number of observations), which is computationally expensive, especially for large datasets or complex models.

# Need to create a function to calculate the covariance ratios
calc_cov_r <- function(model) {
  # Extract the variance-covariance matrix of the random effects
  V <- vcov(model)
  
  # Calculate covariance ratio for each observation
  cov_r <- rep(NA, model$k)
  
  for (i in 1:model$k) {
    model_excl <- try(update(model, subset = -i), silent = TRUE)
    if (!inherits(model_excl, "try-error")) {
      V_excl <- vcov(model_excl)
      cov_r[i] <- det(V_excl) / det(V)
    }
  }
  
  return(cov_r)
}

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Calculate covariance ratios
cov_r <- calc_cov_r(original_model)

# Create a data frame for plotting
cov_r_data <- data.frame(
  Observed_Outcome = seq_along(cov_r),
  Covariance_Ratios = cov_r
)

##################################################
##################################################

##################################################
# Save to CSV
readr::write_csv(cov_r_data, 
here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cov_r_data_3.csv"))
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################

##############################################################
# Last go: (08/09-24)
# Time difference of 14.59279 hours
```

```{r}
# Plot Covariance Ratios using ggplot2

cov_var_ratio_plot <- cov_r_data |> 
ggplot(aes(x = Observed_Outcome, y = Covariance_Ratios)) +
  geom_segment(aes(xend = Observed_Outcome, yend = 1), color = "blue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Covariance Ratios Plot", x = "Observed Outcome", y = "Covariance Ratios") +
  theme_minimal() +
  theme(panel.grid.major = element_line(colour = "grey90"),
        panel.grid.minor = element_blank())

cov_var_ratio_plot
```

```{r, eval = FALSE}
# Saving the Covariance Ratios plot
# Specify the folder path and file name
folder_path <- "C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/FIGURES/"
file_name <- "cov_var_ratio_plot.jpg"
output_path <- file.path(folder_path, file_name)


ggsave(output_path, 
       plot = cov_var_ratio_plot,
       width = 6, height = 3, dpi = 400)
```











#############
# STEP 5
##########################################################################################################################################
ANALYSING MODERATORS
##########################################################################################################################################


```{r}
imputed_database_clean |> glimpse()

# Moderators to be investigated:
# 
# Tree_crop_combination_T
# Season	
# Tree_row _orientation	
# Soil_texture	
# No_tree_per_m	
# Tree_height	
# Alley_width	
# Tillage	
# Organic
```

Performing a moderator analysis using the metafor package in R, can be done by following these steps. Moderator analysis helps to understand how different study characteristics (moderators) influence the effect sizes and whether these moderators can explain some of the heterogeneity observed in the overall meta-analysis. The moderators listed will be included in the moderatroer assessment of the meta-analysis model to investigate their impact.

The moderator analysis

Step-by-Step Moderator Analysis
  1. Data Preparation: Ensure that the variables of interest are used as moderators are correctly formatted and coded in the dataset. 
     Convert categorical variables to factors if they are not already.

  2. Model Fitting with Moderators: Use the rma.mv() function to fit a random-effects meta-analysis model, including the moderators as fixed      effects. This will allow to see the influence of each moderator on the effect size.

  3. Interpretation of Results: Assess the coefficients for each moderator to determine their impact on the outcome. Significant moderators       can provide insights into factors that influence the effect sizes.
  
  (so for each of the response var. make a model for moderator analysis)
  
**Test each of the moderators if there is a significant effect on the response variables of interest (ES). **

1) Pool/aggregate all the studies that have the same response variable

This model estimates the effect sizes for the moderator variables while accounting for random effects (article-level and response variable level).
moderator_model <- rma.mv(
  # The effect size estimates (dependent variable).
  yi = yi,          
  # The variance-covariance matrix of the effect sizes (weights for the meta-analysis).
  V = V_matrix,          
  # The moderator variables (predictors) included in the model.
  mods = moderator_formula,     
  random = list(
  # Random intercept for each article to capture article-level variability.
    ~ 1 | id_article,           
  # Random intercept for each response variable within each article to account for within-article variability.
    ~ 1 | id_article/response_variable  
  ),
  
  

```{r}
# Unfortunately, the details of moderators were specified in another updated datasheet under another tab called "descriptive data" -- which is not the same tab used for the quantitative meta-analysis modelling and diagnostics. Hence, it is necessary to first import that new sheet and merge the details with the previous version (as I have ran all other models and diagnostics with the previous version of the dataset.),  then make sure information from the "descriptive data" sheet match 1:1 with the qualitative, on a study-observation level.  

# List of moderators to investigate
moderators <- c("tree_crop_combination_t",
                "season",
                "alley_width",
                "tree_row_orientation",
                "soil_texture",
                "no_tree_per_m", 
                "tree_height",
                "alley_width",
                "tillage",
                "organic")

# All moderators
# c("tree_crop_combination_t",
#   "season",
#   "alley_width",
#   "tree_row_orientation",
#   "soil_texture",
#   "no_tree_per_m", 
#   "tree_height",
#   "alley_width",
#   "tillage",
#   "organic")

```

REML enhances the validity and reliability of meta-analysis findings by providing unbiased variance estimates, reducing overfitting, and ensuring robust hypothesis testing. These features are crucial for understanding how various moderators affect the outcomes and for making informed, generalizable conclusions.

*OBS! Time for running this code below is slightly long!*

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Create a formula for the moderators
moderator_formula <- as.formula(paste("~", paste(moderators, collapse = " + ")))

# Fit a multivariate meta-analysis model including moderators using the rma.mv() function from the metafor package.

# This model estimates the effect sizes for the moderator variables while accounting for random effects (article-level and response variable level).
moderator_model <- rma.mv(
  # The effect size estimates (dependent variable).
  yi = yi,             
  # The variance-covariance matrix of the effect sizes (weights for the meta-analysis).
  V = V_matrix,               
  # The moderator variables (predictors) included in the model.
  mods = moderator_formula,     
  random = list(
    # Random intercept for each article to capture article-level variability.
    ~ 1 | id_article,          
    # Random intercept for each response variable within each article to account for within-article variability.
    ~ 1 | id_article/response_variable  
  ),
  # The dataset that contains the effect sizes and moderator variables.
  data = filtered_meta_data_ROM,   
  # Use Restricted Maximum Likelihood (REML) instead of ML, for estimating variance components, reducing bias.
  method = "REML",               
  control = list(
    # Optimizer to be used for fitting the model; changed to 'optim' with BFGS that can better handle complex models.
    optimizer = "optim",  
    # Use the BFGS optimization method for more efficient convergence.
    optim.method = "BFGS",       
    # Set maximum number of iterations for the optimizer to ensure convergence.
    iter.max = 1000,              
    # Set relative tolerance to ensure convergence with sufficient precision.
    rel.tol = 1e-12              
  ),
  # Enable verbose output for progress information and convergence diagnostics.
  verbose = TRUE                
)

# Extract relevant results from the model
# Coefficients (b), standard errors (se), confidence intervals (ci.lb, ci.ub), and p-values (pval)
# The coefficients (b), standard errors (se), confidence intervals (ci.lb, ci.ub), and p-values (pval) for the moderators are stored as a data frame.
moderator_results <- data.frame(
  Coefficient = moderator_model$b,   # Extracts the estimated effect sizes (regression coefficients) for each moderator.
  Std_Error = moderator_model$se,    # Extracts the standard errors associated with each moderator's coefficient.
  CI_Lower = moderator_model$ci.lb,  # Extracts the lower bounds of the 95% confidence intervals for each coefficient.
  CI_Upper = moderator_model$ci.ub,  # Extracts the upper bounds of the 95% confidence intervals for each coefficient.
  P_Value = moderator_model$pval     # Extracts the p-values to assess the statistical significance of each moderator.
)

# Add row names (moderators) as a column
moderator_results$Moderator <- rownames(moderator_results)
rownames(moderator_results) <- NULL

##############################################################
# Save the results to a CSV file
readr::write_csv(moderator_results, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/moderator_results_database_v3.csv"))

##############################################################

# Display the extracted results
moderator_results

##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
print(time.taken)

##################################################

# NOTES

# In this code, we first create a formula for the moderators by concatenating the list of moderator variables into a single formula. This formula is passed into the mods argument in the rma.mv() function, allowing the inclusion of moderators in the meta-analysis model. The model itself is fitted using the rma.mv() function from the metafor package, which estimates the effect sizes for each moderator while accounting for random effects at the article level and response variable level.

# The model employs Restricted Maximum Likelihood (REML) for estimating variance components. REML is known for reducing bias in random effect estimates, especially in smaller sample sizes or when the random effects structure is complex. By focusing on estimating the random effects first, REML ensures that variance components are accurately reflected, providing more reliable and less biased results.

# The optimizer has been set to "optim" with the BFGS method. BFGS is well-suited for handling complex, non-linear optimization problems like those encountered in meta-analysis models with multiple levels of random effects. The optimizer is configured with a maximum iteration count of 1000 and a relative tolerance of 1e-12, providing stricter convergence criteria while ensuring computational efficiency.

##############################################################
# Last go: (08/09-24)

# Advarsel: 83 rows with NAs omitted from model fitting.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Redundant predictors dropped from the model.
# Iteration 48    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 49    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 50    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 51    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 52    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# 
# Time difference of 20.4787 secs

# Sometimes I get this error message "Fejl: uventet numerisk konstant in "Iteration 48"" - as well as the other warnings above
```

########################################################
DIAGNOSIS OF THE MODERATOR ANALYSIS
########################################################

```{r}
# Summary of the moderator model
summary(moderator_model)
```

```{r}
# ANOVA to test the influence of each moderator on the effect size
anova(moderator_model)
```

INTERPRETATION OF MODERATOR ANALYSIS

*Tree-Crop Combination:*
The tree-crop combinations represent different agroforestry systems where specific tree species are combined with either cereal or legume crops. 

Apple-Legume:
- Estimate: 0.1411, not statistically significant (p = 0.6257).

Maple-Cereal:
Estimate: 0.1061, not statistically significant (p = 0.5792).

Maple-Legume:
Estimate: 0.1871, not statistically significant (p = 0.3554).

Mixed-Cereal:
Estimate: 0.0492, not statistically significant (p = 0.7314).

Mixed-Legume:
Estimate: 0.0535, not statistically significant (p = 0.7089).

Paulownia-Cereal:
Estimate: -0.2062, not statistically significant (p = 0.5704). This negative estimate suggests a potential decrease in effect size, but it is not statistically significant.

Poplar-Cereal:
Estimate: 0.0163, not statistically significant (p = 0.9094).

Poplar-Legume:
Estimate: 0.0543, not statistically significant (p = 0.7049).

Hence, none of the tree-crop combinations show a statistically significant effect on the overall effect size in this analysis, as their confidence intervals all cross zero and p-values are well above 0.05. However, there might be significant variations within each response variable (ecosystem services). This is assessed below.


*Test of Moderators (ANOVA)*
The ANOVA result shows the overall test for the significance of all moderator variables: QM(df = 20) = 216.3594, p-val < .0001
This test indicates that the set of moderator variables, as a whole, is statistically significant. The low p-value (<0.0001) suggests that the moderators, collectively, explain a significant portion of the variance in the effect sizes. While individual moderators might not be significant on their own, the model as a whole benefits from the inclusion of moderators, improving the fit.




################################################################################################################
VISUALISING OVERALL EFFECT-SIZE MEASURES AS EFFECT OF DIFFERENT MODERATORS
########################################################

This is visualized and assessed by pooling all response variables and showing the general effect-size measure, hence not accounting for the potential variations in effect-size measures for the different response variables separately. 

Overall Forest Plot 
With all response variables to see effect of SAF against mono


```{r}
# Forest plot to visualize the effect sizes for each study
forest(moderator_model, slab = paste(id_article, response_variable, sep = ", "), cex = 0.6)
```

```{r}
# Function to create a bubble plot for a given moderator with additional plot information
bubble_plot <- function(data, moderator, title_info) {
  
  # Ensure the moderator variable is numeric
  data[[moderator]] <- as.numeric(data[[moderator]])
  
  # Remove rows with NA values in the moderator variable
  data <- data[!is.na(data[[moderator]]), ]
  
  # Generate the bubble plot
  data %>%
    ggplot(aes_string(x = moderator, y = "yi", size = "vi")) +  # `vi` represents variance of effect sizes
    geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
    labs(
      title = paste("Bubble Plot of Effect Sizes vs", title_info),  # Custom title based on the moderator
      x = paste(moderator, "(in meters)"),  # Custom x-axis label with units
      y = "Effect Size (yi)",  # Descriptive y-axis label
      size = "Variance (vi)"   # Label explaining the size of bubbles
    ) +  
    scale_x_continuous(breaks = seq(0, max(data[[moderator]], na.rm = TRUE), by = 5),  # Custom x-axis intervals
                       limits = c(0, max(data[[moderator]], na.rm = TRUE))) +  # Set range for x-axis
    theme_minimal() +  # Clean theme for visualization
    theme(plot.title = element_text(hjust = 0.5))  # Center the title
}

# Example bubble plot for the "Alley_width" moderator with title
bubble_plot(filtered_meta_data_ROM, "alley_width", "Alley Width")
```

```{r}
# Interaction plot between "tree_crop_combination_t" and another moderator
interaction_plot <- function(data, moderator1, moderator2) {
  data %>%
    ggplot(aes_string(x = moderator1, y = "yi", color = moderator2, group = moderator2)) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
      title = paste("Interaction Plot of", moderator1, "and", moderator2, "on Effect Size"),
      x = moderator1,
      y = "Effect Size (yi)",
      color = moderator2
    ) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
}

# Example interaction plot for "tree_crop_combination_t" and "alley_width"
interaction_plot(filtered_meta_data_ROM, "tree_crop_combination_t", "alley_width")
```

```{r}
# Box plot for the categorical moderator "tree_crop_combination_t"
box_plot_categorical <- function(data, moderator) {
  data %>%
    ggplot(aes_string(x = moderator, y = "yi")) +
    geom_boxplot() +
    labs(
      title = paste("Box Plot of Effect Sizes by", moderator),
      x = moderator,
      y = "Effect Size (yi)"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))
}

# Example box plot for "tree_crop_combination_t"
box_plot_categorical(filtered_meta_data_ROM, "tree_crop_combination_t")
```



#############
# STEP 6
##########################################################################################################################################
VISAUALISING (E.G. FOREST PLOT) EFFECT-SIZE MEASURES WITHIN THE DIFFERENT COMBINATIONS OF TREE-CROP
##########################################################################################################################################


A)
2nd Forrest Plot
Within the SAF there is different combination of tree-crop: Compare different tree-crop combinations on different ES

First, creating a systematic dataset where moderators are pooled and the forest plot only distinguish for response variables (ecosystem services)

```{r}
# Function to create a systematic dataset by pooling moderators and aggregating by response variables
create_systematic_dataset_pooled <- function(systematic_data, effect_size_col, variance_col, response_variable_col) {
  
  # Ensure necessary columns are present in the dataset
  if (!effect_size_col %in% colnames(systematic_data) | 
      !variance_col %in% colnames(systematic_data) | 
      !response_variable_col %in% colnames(systematic_data)) {
    stop("Necessary columns (effect size, variance, response variable) are missing from the dataset")
  }
  
  # Group data by response variable (ecosystem services) and summarize the effect sizes and variances
  pooled_data <- systematic_data %>%
    group_by(.data[[response_variable_col]]) %>%
    summarise(
      mean_effect_size = mean(.data[[effect_size_col]], na.rm = TRUE),    # Average effect size for each response variable
      total_variance = mean(.data[[variance_col]], na.rm = TRUE)          # Average variance for each response variable
    ) %>%
    ungroup()
  
  return(pooled_data)
}

systematic_dataset_pooled <- create_systematic_dataset_pooled(filtered_meta_data_ROM, "yi", "vi", "response_variable")
```


```{r}
# Create a forest plot for each response variable in the pooled dataset
forest(systematic_dataset_pooled$mean_effect_size, 
       vi = systematic_dataset_pooled$total_variance,
       slab = systematic_dataset_pooled$response_variable,  # Label each row with response variable names
       xlab = "Effect Size", 
       main = "Forest Plot of Pooled Effect Sizes Across Ecosystem Services",
       cex = 0.8)  # Text size adjustment
```





Making a generic systematic dataset to visualize effect-sizes for different moderators and response variables (ecosystem services) 

```{r}
# Function to generate a systematic dataset for any moderator and response variable (ecosystem service)
create_systematic_dataset <- function(data, moderator, response_var) {
  
  # Ensure the moderator and response variable are present in the data
  if (!moderator %in% colnames(data) | !response_var %in% colnames(data)) {
    stop("Moderator or response variable not found in the dataset")
  }
  
  # Create a dataset grouped by the moderator and response variable
  systematic_dataset <- data %>%
    select(id_article, response_variable = !!sym(response_var), yi, vi, moderator = !!sym(moderator)) %>%  # Dynamically select columns
    filter(!is.na(moderator), !is.na(response_variable)) %>%  # Remove rows with missing values
    group_by(moderator, response_variable) %>%  # Group by the moderator and response variable
    summarise(
      mean_effect_size = mean(yi, na.rm = TRUE),   # Calculate the mean effect size
      total_variance = sum(vi, na.rm = TRUE),      # Calculate the total variance
      n_studies = n()                              # Count the number of studies per group
    ) %>%
    ungroup()  # Ungroup the dataset
  
  return(systematic_dataset)
}
```

```{r}
# Tree-crop combination
systematic_dataset_tree_crop <- create_systematic_dataset(filtered_meta_data_ROM, "tree_crop_combination_t", "response_variable")

systematic_dataset_tree_crop 
```

```{r}
# Season
systematic_dataset_season <- create_systematic_dataset(filtered_meta_data_ROM, "season", "response_variable")

systematic_dataset_season
```

```{r}
# Tillage
systematic_dataset_tillage <- create_systematic_dataset(filtered_meta_data_ROM, "tillage", "response_variable")

systematic_dataset_tillage
```

```{r}
# Organic vs conventional
systematic_dataset_organic <- create_systematic_dataset(filtered_meta_data_ROM, "organic", "response_variable")

systematic_dataset_organic
```




```{r}
# Function to create forest plots for different moderator groups
create_forest_plots_individual_es <- function(systematic_data, moderator) {
  
  # Ensure that the moderator column is present in the data
  if (!moderator %in% colnames(systematic_data)) {
    stop("Moderator not found in the dataset")
  }
  
  # Loop through each unique moderator value and create a forest plot
  unique_moderators <- unique(systematic_data[[moderator]])
  
  for (mod_value in unique_moderators) {
    
    # Subset the data for the current moderator value
    subset_data <- systematic_data[systematic_data[[moderator]] == mod_value, ]
    
    # Create the forest plot using the metafor package
    forest_plot <- forest(subset_data$mean_effect_size, 
                          subset_data$total_variance, 
                          slab = paste(subset_data$response_variable, sep = ", "), 
                          xlab = "Effect Size", 
                          main = paste("Forest Plot for", moderator, "=", mod_value),
                          cex = 0.7)  # Adjust text size
    
    print(forest_plot)  # Print the plot for each moderator value
  }
}
```

```{r}
# Tree-crop on each response variable (ecosystem service)
create_forest_plots_individual_es(systematic_dataset_tree_crop, "moderator")
```
```{r}
# Season on each response variable (ecosystem service)
create_forest_plots_individual_es(systematic_dataset_season, "moderator")
```

```{r}
# Tollage on each response variable (ecosystem service)
create_forest_plots_individual_es(systematic_dataset_tillage, "moderator")
```
```{r}
# Organic vs. conventional on each response variable (ecosystem service)
create_forest_plots_individual_es(systematic_dataset_organic, "moderator")
```

```{r}
ggplot(systematic_dataset_pooled, aes(x = response_variable, y = mean_effect_size)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_errorbar(aes(ymin = mean_effect_size - sqrt(total_variance), 
                    ymax = mean_effect_size + sqrt(total_variance)), 
                width = 0.2) +
  labs(
    title = "Bar Plot of Pooled Effect Sizes with Variance Across Ecosystem Services",
    x = "Ecosystem Services (Response Variables)",
    y = "Mean Effect Size"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
```







#############
# STEP 7
##########################################################################################################################################
TEST AND VISAUALISE THE ORIGINAL MODEL AGAINST MORE ADVANCED A MODELS WHERE SITE LOCATION, EXPERIEMENTAL YEAR AND THE INTERACTION IS INCLUDED
##########################################################################################################################################


B)

Test original model against a model where we include site location and experimental year and a model with interaction (site location * experimental year )

Evaluate on BIC AIC (select the lowest one)

mod_1 against mod_2 and mod_3


#############
# STEP 8
##########################################################################################################################################
TESTING THE MODERATOR' EFFECTS ON THE RESPONSE VARIABLES (ECOSYSTEM SERVICES)
##########################################################################################################################################


C)

Moderator with aggregated/pooled 

Testing for the moderator effect on response variable


#############
# STEP 9
##########################################################################################################################################
MAP THE STUDY SITES AND EXPERIMENTAL INVESTIGATIONS GEOGRAPHICALLY
##########################################################################################################################################

D) Map study sites

Number of studies
Some information on the agro-ES studied in the different places!!
With number of exp. fields or effect size



Tree-crop combinations? Now we have many. We should reduce. Make numbered categories. And change the name of the general things. 

























#############
# STEP 10
##########################################################################################################################################
SENSITIVITY ANALYSIS ON DATA (E.G.: MISSING OUTCOME DATA)
##########################################################################################################################################



CHOOSING RESPONSE VARIABLES AND PERFORM 



#############
# STEP 11
##########################################################################################################################################
ASSESS THE META-ANALYSIS RESULTS
##########################################################################################################################################




#############
# STEP 12
##########################################################################################################################################
VISUALISE THE META-ANALYSIS RESULTS
##########################################################################################################################################