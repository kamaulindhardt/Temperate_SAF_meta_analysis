---
title: "old_code_notes_to be_saved_but_not_used"
output: html_document
date: "2024-07-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#############
# STEP 2
##########################################################################################################################################
CALCULATING INITIAL EFFECT SIZES AND EVALUATING NUMBER OF STUDIES TO INCLUDE (SENSITIVITY ANALYSIS)
##########################################################################################################################################

continuous data, where each individual’s outcome is a measurement of a numerical quantity;

```{r}
## Calculate Effect Sizes and Variances for Multiple Parameters
# Log-transformed response ratios (lnRR) and corresponding variances
data_lnRR <- database_clean %>%
  filter(
    !is.na(silvo_mean) & !is.na(control_mean) & !is.na(silvo_n) & 
    !is.na(control_n) & !is.na(silvo_se) & !is.na(control_se)
  ) %>%
  mutate(
    lnRR = log(silvo_mean / control_mean),
    var_lnRR = (silvo_se^2 / (silvo_n * silvo_mean^2)) + 
               (control_se^2 / (control_n * control_mean^2)),
    slab = paste(Id_article, ", ", Study_Year_Start)
  ) %>%
  filter(
    !is.nan(lnRR) & !is.infinite(lnRR) & 
    !is.nan(var_lnRR) & !is.infinite(var_lnRR) & 
    var_lnRR > 0
  ) %>%
  relocate(Id_article, Response_variable, Sub_response_variable, silvo_mean, silvo_se, control_mean, control_se, lnRR, var_lnRR)

# Check the structure of the meta_data
glimpse(meta_data)
```

```{r}
meta_data %>% dplyr::glimpse() 
meta_data %>% View()
```


```{r}
## Meta-Analysis for Each Response Variable
results <- list()  # List to store results for each response variable
response_vars <- unique(meta_data$Response_variable)  # Get unique response variables

# Loop through each response variable
for (response in response_vars) {
  # Filter data for the current response variable and remove rows with non-positive or missing variances
  data_response <- filter(meta_data, Response_variable == response & var_lnRR > 0 & !is.na(var_lnRR))
  
  # Check if there's enough data to perform meta-analysis
  if (nrow(data_response) > 1) {
    # Fit random-effects model using log-transformed response ratios and variances
    res <- rma(yi = lnRR, vi = var_lnRR, data = data_response, method = "REML")
    
    # Store the results in the list
    results[[response]] <- res
    
    # Print summary of the meta-analysis
    cat("\nResponse Variable:", response, "\n")
    print(summary(res))
    
    # Print predicted pooled effect size and corresponding CI/PI
    print(predict(res, transf = exp, digits = 2))
    
    # Generate forest plot
    forest(res, xlab = "Log Response Ratio (lnRR)", slab = data_response$slab, main = response, cex = 0.8, cex.lab = 1.2)
    
    # Generate funnel plot to check for publication bias
    funnel(res)
  } else {
    cat("\nResponse Variable:", response, "\n")
    cat("Not enough data to perform meta-analysis.\n")
  }
}

## Sensitivity Analysis for Each Response Variable
# Loop through each response variable to perform influence diagnostics
for (response in response_vars) {
  res <- results[[response]]  # Get results for the current response variable
  
  if (!is.null(res)) {
    # Perform influence diagnostics
    inf <- influence(res)
    
    # Plot influence diagnostics
    plot(inf, main = paste("Influence Diagnostics for", response))
    
    # Print summary statistics for interpretation
    cat("\nResponse Variable:", response, "\n")
    cat("Overall Effect Size (lnRR):", res$b, "\n")
    cat("95% Confidence Interval:", confint(res)$ci.lb, "to", confint(res)$ci.ub, "\n")
    cat("Heterogeneity (Q):", res$QE, "\n")
    cat("I^2:", res$I2, "%\n")
  }
}
```







################################
MANUAL CALCULATION OF EFFECT SIZE MEASURE
################################

log-transformed response ratio (lnRR)
```{r}
# Calculate Effect Sizes (lnRR) and Variances for Multiple Parameters
meta_data_lnRR <- meta_data |> 
  # Group by article ID and response variable
  group_by(id_article, response_variable) |> 
  # Summarise to get means per group and include study_year_start
  summarise(
    silvo_mean = mean(silvo_mean, na.rm = TRUE),   # Mean of silvo_mean per group
    control_mean = mean(control_mean, na.rm = TRUE), # Mean of control_mean per group
    silvo_se = mean(silvo_se, na.rm = TRUE),         # Mean of silvo_se per group
    control_se = mean(control_se, na.rm = TRUE),     # Mean of control_se per group
    silvo_n = mean(silvo_n, na.rm = TRUE),           # Mean of silvo_n per group
    control_n = mean(control_n, na.rm = TRUE),       # Mean of control_n per group
    study_year_start = first(study_year_start),      # Include study_year_start
    .groups = 'drop'
  ) |> 
  # Calculate log response ratio (lnRR) and its variance (var_lnRR)
  mutate(
    # Compute log response ratio
    lnRR = log(silvo_mean / control_mean), 
    # Variance of lnRR combining silvo and control variances
    var_lnRR = (silvo_se^2 / (silvo_n * silvo_mean^2)) +  
               (control_se^2 / (control_n * control_mean^2)),  
    # Create label for plotting
    slab = paste(id_article, ", ", study_year_start)  
  ) |> 
  # Filter out rows with NaN, infinite, or non-positive variance values
  filter(
    !is.nan(lnRR) & !is.infinite(lnRR) & 
    !is.nan(var_lnRR) & !is.infinite(var_lnRR) & 
    var_lnRR > 0
  ) |> 
  # Reorder columns for better readability and organization
  relocate(id_article, response_variable, silvo_mean, control_mean, lnRR, var_lnRR, slab)
```

```{r}
# Display a glimpse of the final dataset for verification
meta_data_lnRR |> glimpse()
```


Step 1: Inspect the Variances

```{r}
# Remove rows with missing values in filtered_meta_data_ROM
filtered_meta_data_ROM_clean <- filtered_meta_data_ROM[complete.cases(filtered_meta_data_ROM), ]

# Check for missing values again to confirm
missing_values_updated <- sapply(filtered_meta_data_ROM_clean, function(x) sum(is.na(x)))
print("Missing Values in updated filtered_meta_data_ROM:")
print(missing_values_updated)

filtered_meta_data_ROM_clean
```


```{r}
# Check for missing values
meta_data_RR |> as.data.frame() |> summary()

# Check for extreme values in variances
hist(meta_data_RR$vi, breaks = 10, main = "Histogram of Variances", xlab = "Variance of yi")
```
```{r}
# Simplify the model by removing the multilevel structure
simple_res <- rma.mv(yi = yi, 
                     V = V_matrix, 
                     random = ~ 1 | id_article, 
                     data = meta_data_RR)
```


```{r}
# List of optimizers to try
optimizers <- c("nlminb", "optim", "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent")

# Function to fit the random-effects model with different optimizers
fit_model <- function() {
  for (opt in optimizers) {
    res <- tryCatch({
      message(paste("Trying optimizer:", opt))
      rma.mv(yi = yi, 
             V = V_matrix, 
             random = ~ 1 | id_article/response_variable, 
             data = meta_data_RR,
             verbose = TRUE,
             control = list(iter.max = 1000, rel.tol = 1e-8, optimizer = opt))
    }, error = function(e) {
      message(paste("Optimizer", opt, "failed with error:", e$message))
      NULL
    })
    if (!is.null(res)) {
      if (res$convergence == 0) {
        message(paste("Optimizer", opt, "converged successfully."))
        return(res)
      }
    }
  }
  stop("None of the optimizers converged.")
}

```


```{r}
# Fit the random-effects model using the filtered data
res_filtered <- rma.mv(yi = yi, 
                       V = V_matrix, 
                       random = ~ 1 | id_article/response_variable, 
                       data = filtered_meta_data_ROM,
                       verbose = TRUE,
                       control = list(iter.max = 1000, rel.tol = 1e-8))


```



```{r}
# Plot Cook's Distance
plot(cooks_d, type="h", ylab="Cook's Distance", xlab="Observed Outcome",
     main="Cook's Distance Plot")
abline(h=4/(nrow(filtered_meta_data_ROM_clean)-length(coef(res))), lty=2, col="red")

# Plot Studentized Residuals
plot(student_resid$resid, type="h", ylim=c(-3, 3),
     ylab="Studentized Residuals", xlab="Observed Outcome",
     main="Studentized Residuals Plot")
abline(h=c(-2, 2), lty=2, col="red")

# Plot DFBETAS for each coefficient
dfbetas_df <- as.data.frame(dfbetas_vals)
for (i in 1:ncol(dfbetas_df)) {
  plot(dfbetas_df[, i], type="h", ylab=paste("DFBETAS for Coef", i), xlab="Observed Outcome",
       main=paste("DFBETAS for Coefficient", i))
  abline(h=c(-2, 2), lty=2, col="red")
}

# Plot Hat Values
plot(hat_vals, type="h", ylab="Hat Values", xlab="Observed Outcome",
     main="Hat Values Plot")
abline(h=2*mean(hat_vals), lty=2, col="red")
```



























```{r}
# Step 1: Inspect the Variances
# Inspect the distribution of var_lnRR
summary(meta_data_lnRR$var_lnRR)
hist(meta_data_lnRR$var_lnRR, 
     breaks = 30, 
     main = "Histogram of Variances", 
     xlab = "Variance of lnRR")
```
```{r}
# Create the density plot for var_lnRR
ggplot(meta_data_lnRR, aes(x = var_lnRR)) +
  geom_density(fill = "blue", alpha = 0.5) +
  labs(title = "Density Plot of Variances", 
       x = "Variance of lnRR", 
       y = "Density") +
  theme_minimal()
```














































##########################################################################################################################################
Using the build-inn metafor function "escalc" to calculate RoM" for the log transformed ratio of means
##########################################################################################################################################

```{r}
###################################################################
# Ensure standard deviations are valid for escalc
meta_data <- meta_data %>%
   # Ensure standard deviations are valid for escalc
    filter(silvo_se > 0 & control_se > 0) 
    # Ensure means are greater than 0 to avoid infinite values
    #filter(silvo_mean > 0 & control_mean > 0) 

###################################################################
# Use the escalc function to compute effect sizes and variances
meta_data_es <- metafor::escalc(
  # "ROM" for the log transformed ratio of means
  measure = "ROM", 
  m1i = silvo_mean, m2i = control_mean, 
  sd1i = silvo_se, sd2i = control_se, 
  n1i = silvo_n, n2i = control_n, 
  data = meta_data, 
  slab = paste(id_article, response_variable, sep = ", ")) %>% 
  # Reorder columns for better readability and organization
  relocate(id_article, response_variable, silvo_mean, control_mean, yi, vi)

###################################################################
# Summarize effect sizes and variances per study and response variable
summary_meta_es <- meta_data_es %>%
  group_by(id_article, response_variable) %>%
  summarize(
    mean_yi = mean(yi, na.rm = TRUE),  # Average effect size
    mean_vi = mean(vi, na.rm = TRUE),  # Average variance
    n = n()  # Number of observations
  ) %>%
  dplyr::ungroup()
###################################################################
# Join the summarized data back to the original data
meta_data_es <- meta_data_es %>%
  left_join(summary_meta_es, by = c("id_article", "response_variable"))

###################################################################
# View the computed effect sizes and variances
meta_data_es %>% glimpse()
```

##############################################################################
Building generic function to derive ROM, to be used for each response variable 
##########################################################################################################################################


```{r}
# Define a generic function to compute effect sizes and variances
compute_effect_sizes <- function(data, response_var) {
  # Filter data for the specified response variable
  filtered_data <- data %>%
    filter(response_variable == response_var)
  
  # Ensure standard deviations are valid for escalc
  filtered_data <- filtered_data %>%
    # Ensure standard deviations are valid for escalc
    filter(silvo_se > 0 & control_se > 0) 
    # Ensure means are greater than 0 to avoid infinite values
    #filter(silvo_mean > 0 & control_mean > 0) 
  
  # Use the escalc function to compute effect sizes and variances
  effect_sizes <- metafor::escalc(
    # "ROM" specifies the log-transformed ratio of means as the effect size measure
    measure = "ROM",  
    # Mean of the silvoarable treatment
    m1i = silvo_mean,  
    # Mean of the control treatment
    m2i = control_mean,  
    # Standard error of the silvoarable treatment
    sd1i = silvo_se,  
    # Standard error of the control treatment
    sd2i = control_se,  
    # Sample size of the silvoarable treatment
    n1i = silvo_n,  
    # Sample size of the control treatment
    n2i = control_n,  
     # Data frame containing the relevant columns
    data = filtered_data, 
    # Create a label combining id_article and response_variable
    slab = paste(id_article, response_variable, sep = ", ")  
  ) %>% 
  # Reorder columns for better readability and organization
  relocate(id_article, response_variable, silvo_mean, control_mean, yi, vi)
  
  # Summarize effect sizes and variances per study and response variable
  summarized_effect_sizes <- effect_sizes %>%
    group_by(id_article, response_variable) %>%
    summarize(
      mean_yi = mean(yi, na.rm = TRUE),  # Average effect size
      mean_vi = mean(vi, na.rm = TRUE),  # Average variance
      n = n()  # Number of observations
    ) %>%
    ungroup()
  
  # Join summarized data back to the original data
  combined_data <- effect_sizes %>%
    left_join(summarized_effect_sizes, by = c("id_article", "response_variable"))
  
  # Return the combined data with summarized effect sizes and variances
  return(combined_data)
}
```

```{r}
# Define the list of response variables
response_variables <- c(
  "Biodiversity",
  "Crop yield",
  "Greenhouse gas emission",
  "Pest and Disease",
  "Product quality",
  "Soil quality",
  "Soil water content",
  "Water quality"
)

# Initialize an empty list to store results
results <- list()

# Loop through each response variable and compute effect sizes
for (response_var in response_variables) {
  cat("Computing effect sizes for:", response_var, "\n")
  results[[response_var]] <- compute_effect_sizes(meta_data, response_var)
}

# Combine all results into a single data frame for easier analysis and visualization
combined_results <- bind_rows(results)
# Warnings: Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs --> happens when either control_mean or solvi_mean is 0
# Computing effect sizes for: Biodiversity 
# Advarsel: Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs.
# Computing effect sizes for: Crop yield 
# Computing effect sizes for: Greenhouse gas emission 
# Computing effect sizes for: Pest and Disease 
# Advarsel: Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs.
# Computing effect sizes for: Product quality 
# Computing effect sizes for: Soil quality 
# Computing effect sizes for: Soil water content 
# Computing effect sizes for: Water quality 
# Advarsel: Some 'yi' and/or 'vi' values equal to +-Inf. Recoded to NAs.

combined_results %>% glimpse()
```

```{r}
# Access the computed effect sizes for a specific response variable
# Convert results for a specific response variable to tibble and view
biodiversity_results <- as_tibble(results[["Biodiversity"]]) %>% 
  # Reorder columns for better readability and organization
  relocate(id_article, response_variable, silvo_mean, control_mean, mean_yi, mean_vi)

biodiversity_results
```



################################
B) Sensitivity Analyses to assess the potential impact of missing outcome data.

```{r}

```


#############
# STEP 4
##########################################################################################################################################
PERFORMING META-ANALYSIS USING THE GENERAL FUNCTION rma IN metafor
##########################################################################################################################################

Linear (Mixed-Effects) Models

```{r}
# Remove duplicates to keep one row per combination of id_article and response_variable
biodiversity_results_forrest <- biodiversity_results %>%
  distinct(id_article, response_variable, 
           .keep_all = TRUE)
```

```{r}
# Biodiversity

res_meta_biodiversity <- rma(mean_yi, mean_vi, 
                             data = biodiversity_results_forrest) 


```
```{r}
# Create a forest plot for the meta-analysis of Biodiversity
forest(res_meta_biodiversity, 
       xlab = "Log Response Ratio (lnRR)",  # Label for the x-axis
       slab = biodiversity_results_forrest$id_article,  # Study labels
       main = "Forest Plot for Biodiversity",  # Title of the plot
       cex = 0.8,  # Size of the text
       cex.lab = 1.2)  # Size of the axis labels

# Create a funnel plot for the meta-analysis of Biodiversity
funnel(res_meta_biodiversity, 
  main = "Funnel Plot for Biodiversity")  # Title of the plot
```

```{r}
positive_definite_check <- is_positive_definite(V_matrix)
print(paste("V_matrix is positive definite:", positive_definite_check))

# Error
# The error encountered, "uendelig eller manglende værdier i 'x'" (infinite or missing values in 'x'), indicates that V_matrix contains infinite or missing values, which can cause issues with positive definiteness checks and model fitting. To address this, we need to clean V_matrix by handling these infinite or missing values.

# But I will do this on the original filtered_meta_data_ROM data!
```



```{r}
# Check for missing values in specified columns and remove those rows
filtered_meta_data_ROM_clean <- filtered_meta_data_ROM %>%
  filter(!is.na(silvo_mean) & !is.na(silvo_se) & !is.na(silvo_sd) &
         !is.na(control_mean) & !is.na(control_se) & !is.na(control_sd) &
         !is.na(yi) & !is.na(vi))
```

Proportion of excluded studies:

```{r}
filtered_meta_data_ROM_clean vs filtered_meta_data_ROM
```



##########################################################################
UPDATING THE V_matrix
##########################################################################

```{r}
# Create a new V_matrix corresponding to the updated filtered_meta_data_ROM aka. filtered_meta_data_ROM_clean
# Assuming the original V_matrix has row and column names that match id_obs or another unique identifier
# of filtered_meta_data_ROM

# Create a list to store variance-covariance matrices for each study
V_list <- list()

# Loop through each unique study ID in the dataset
for (study in unique(filtered_meta_data_ROM_clean$id_article)) {
  # Subset the data for the current study
  study_data <- filtered_meta_data_ROM_clean[filtered_meta_data_ROM_clean$id_article == study, ]
  
  # Check if the current study has more than one outcome
  if (nrow(study_data) > 1) {
    # Create a diagonal matrix with the variances (vi) of the outcomes
    V <- diag(study_data$vi)
    
    # Assume a constant correlation of 0.5 between outcomes within the same study
    corr <- 0.5
    
    # Loop through the rows of the matrix to set the off-diagonal elements
    for (i in 1:nrow(V)) {
      for (j in 1:nrow(V)) {
        # Set the off-diagonal elements to the product of the correlation and the square root of the product of the corresponding variances
        if (i != j) {
          V[i, j] <- corr * sqrt(V[i, i] * V[j, j])
        }
      }
    }
    # Add the variance-covariance matrix to the list for the current study
    V_list[[as.character(study)]] <- V
  } else {
    # If there is only one outcome, the variance is just the variance of the single outcome
    V_list[[as.character(study)]] <- study_data$vi
  }
}

# Combine the matrices into a block-diagonal matrix
V_matrix_clean <- bldiag(V_list)
```

```{r}
# Ensure the V_matrix_clean is still valid and clean
positive_definite_check <- is_positive_definite(V_matrix_clean)
print(paste("V_matrix_clean is positive definite:", positive_definite_check))
```

```{r}
# Prepare the data
# Separate the data for silvo_ and control_
silvo_data <- meta_data_ROM %>%
  select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  mutate(group = "silvo_")|> 
  as.data.frame()

control_data <- meta_data_ROM %>%
  select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  mutate(group = "control_") |> 
  as.data.frame()

#####################################

silvo_data <- meta_data_ROM %>%
  mutate(yi = log(silvo_mean / control_mean),
         vi = (silvo_se^2 / silvo_mean^2) + (control_se^2 / control_mean^2)) %>%
  select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  mutate(group = "silvo_")

control_data <- meta_data_ROM %>%
  mutate(yi = log(control_mean / silvo_mean),
         vi = (control_se^2 / control_mean^2) + (silvo_se^2 / silvo_mean^2)) %>%
  select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  mutate(group = "control_")



# Combine the data
combined_data <- bind_rows(silvo_data, 
                           control_data)

# Prepare data for plotting
combined_long_data <- combined_data %>%
  pivot_longer(cols = c(yi, vi), names_to = "measure", values_to = "value")














silvo_data_manual <- meta_data_ROM %>%
  mutate(yi = log(silvo_mean / control_mean),
         vi = (silvo_se^2 / silvo_mean^2) + (control_se^2 / control_mean^2)) %>%
  select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  mutate(group = "silvo_")

control_data_manual <- meta_data_ROM %>%
  mutate(yi = log(control_mean / silvo_mean),
         vi = (control_se^2 / control_mean^2) + (silvo_se^2 / silvo_mean^2)) %>%
  select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  mutate(group = "control_")

# Merge silvo_data and silvo_data_manual
silvo_comparison <- silvo_data %>%
  select(id_article, response_variable, yi, vi) %>%
  rename(yi_escalc = yi, vi_escalc = vi) %>%
  inner_join(
    silvo_data_manual %>%
      select(id_article, response_variable, yi, vi) %>%
      rename(yi_manual = yi, vi_manual = vi),
    by = c("id_article", "response_variable")
  )

# Calculate differences
silvo_comparison <- silvo_comparison %>%
  mutate(
    yi_diff = yi_escalc - yi_manual,
    vi_diff = vi_escalc - vi_manual
  )

# View differences
print(silvo_comparison)

```

```{r}
silvo_data <- meta_data_ROM %>%
  dplyr::select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  dplyr::rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  dplyr::mutate(group = "silvo_")|>
  as.data.frame()

control_data <- meta_data_ROM %>%
  dplyr::select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  dplyr::rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  dplyr::mutate(group = "control_") |>
  as.data.frame()


combined_data <- bind_rows(silvo_data, control_data)

# Prepare data for plotting
combined_long_data <- combined_data %>%
  pivot_longer(cols = c(yi, vi), names_to = "measure", values_to = "value")

# Check the prepared data
glimpse(combined_long_data)

# Create the density plot
combined_long_data |> 
  ggplot(aes(x = value, fill = group, color = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(
    title = "Density Plot of Effect Sizes (yi) and Variances (vi) by Group",
    x = "Value",
    y = "Density",
    fill = "Group",
    color = "Group"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# New dataset to be used for mata-analysis
meta_data <- imputed_database_clean |> 
  # Filtering so that only data with outcome measurements that are greater than zero will be included
  filter(silvo_se > 0,
         control_se > 0) |>
  # Adjust the signs for specific response variables where lower values for silvo_ vs. control_ is actually better
  # Manually changing the signs for greenhouse gas emissions, pests and diseases and water quality, as lower values in the silvo_ treatment group are 
  # considered better. This ensures consistency in the interpretation of effect sizes measures across all outcomes. 
  mutate(
    silvo_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"), 
                        -silvo_mean, silvo_mean),
    control_mean = ifelse(response_variable %in% c("Greenhouse gas emissions", "Pests and Diseases", "Water quality"), 
                          -control_mean, control_mean)
  ) |> 
  # Exclude the variable soil water content due to inconsistent data from only a single measurement
  filter(response_variable != "Soil water content") |> 
  # To be 100 % sure. Filter out rows with missing values in key columns
  filter(
    !is.na(silvo_mean) & !is.na(control_mean) & !is.na(silvo_n) & 
    !is.na(control_n) & !is.na(silvo_se) & !is.na(control_se)
  ) |> 
  # Calculate standard deviations from standard errors and sample sizes
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  # Reorder columns for better readability and organization
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n) |> 
  arrange(id_article, response_variable) 

  # The calculation of effect size requires all values to be positive, 
  # I apply here a transformation to ensure that all standard deviation values are positive. 
  mutate(
    shift_value = min(silvo_mean, control_mean, na.rm = TRUE) - 1,  # Shift to ensure positivity
    silvo_mean_transformed = log(silvo_mean - shift_value),
    control_mean_transformed = log(control_mean - shift_value)) |> 
  # ) |> 
  # mutate(
  #   silvo_sd = abs(silvo_sd),
  #   control_sd = abs(control_sd)
  # ) |>

    
    
     # Shift all values to the positive range if necessary by adding a constant
  mutate(
    # Find the minimum value across silvo_mean and control_mean and shift it to the positive range
    min_value_shift = abs(min(c(silvo_mean, control_mean), na.rm = TRUE)) + 1,
    # Adjust silvo_mean by adding the shift value if it's negative
    silvo_mean = ifelse(silvo_mean < 0, silvo_mean + min_value_shift, silvo_mean),
    # Adjust control_mean by adding the shift value if it's negative
    control_mean = ifelse(control_mean < 0, control_mean + min_value_shift, control_mean)
  ) |> 
  # Calculate standard deviations from standard errors and sample sizes for both groups
  mutate(
    # Use absolute values for standard errors to prevent negative standard deviations
    silvo_sd = abs(silvo_se * sqrt(silvo_n)),
    control_sd = abs(control_se * sqrt(control_n))
  ) |> 
```

```{r}
# Preparing the data for plotting

# Create a dataset for the silvo_ group (directly using the calculated effect sizes)
silvo_data <- meta_data_ROM %>%
  dplyr::select(id_article, response_variable, yi, vi, silvo_mean, silvo_se, silvo_sd, silvo_n) %>%
  dplyr::rename(mean = silvo_mean, se = silvo_se, sd = silvo_sd, n = silvo_n) %>%
  dplyr::mutate(group = "silvo_")

# Create a dataset for the control_ group
# Note: For control, reverse the effect size (yi) and use the same variance (vi)
control_data <- meta_data_ROM %>%
  dplyr::select(id_article, response_variable, yi, vi, control_mean, control_se, control_sd, control_n) %>%
  # Reverse the effect size for the control group
  dplyr::mutate(yi = -yi) %>%  
  dplyr::rename(mean = control_mean, se = control_se, sd = control_sd, n = control_n) %>%
  dplyr::mutate(group = "control_")

# Combine the data
combined_data <- bind_rows(silvo_data, control_data)

# Prepare data for plotting
combined_long_data <- combined_data %>%
  pivot_longer(cols = c(yi, vi), names_to = "measure", values_to = "value")

# Check the prepared data
glimpse(combined_long_data)

# Create the density plot
combined_long_data |> 
  ggplot(aes(x = value, fill = group, color = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ response_variable + measure, scales = "free") +
  labs(
    title = "Density Plot of Effect Sizes (yi) and Variances (vi) by Group",
    x = "Value",
    y = "Density",
    fill = "Group",
    color = "Group"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# Fit a random-effects model using rma.mv
# By setting "verbose = TRUE", we can obtain information on the progress of the optimization algorithm:

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Adjust the model to include study-level variance and within-study correlation

#######################################################################################################################################
# Fit the original multivariate random-effects meta-analysis model using the 'rma.mv' function
# This model accounts for study-level variance and within-study correlation

original_model <- rma.mv(
  # Effect size estimates (dependent variable)
  yi = yi,             
  # Variance-covariance matrix of the effect sizes (providing information on within-study sampling variances and covariances)
  V = V_matrix,                                      
  random = list(
    # Random intercept for each article to model between-study variability
    ~ 1 | id_article,                   
    # Random intercept for each response variable within each article to model within-study correlation
    ~ 1 | id_article/response_variable               
  ),
  # Data frame containing the meta-analysis data, including effect sizes and their variances
  data = filtered_meta_data_ROM,    
  # Enable verbose output to print progress and convergence information
  verbose = TRUE,                                    
  control = list(
    # Optimizer to be used for fitting the model; 'nlminb' is robust for constrained optimization problems
    optimizer = "nlminb",   
     # Maximum number of iterations for the optimizer; increase if the model is complex or if convergence is slow
    iter.max = 1000,              
    # Relative convergence tolerance; lower values indicate stricter convergence criteria
    rel.tol = 1e-8                                   
  ),
  # Use Maximum Likelihood (ML) estimation for fitting the model (alternatively, REML could be used for restricted maximum likelihood)
  # Justification for using Maximum Likelihood (ML) over REML:
  # - ML allows for straightforward model comparisons, making it suitable for hypothesis testing 
  #   and model selection based on different fixed effects (e.g., adding or removing moderators).
  # - ML provides consistent estimates of fixed effects, which is essential for accurately assessing 
  #   overall effect sizes and moderator impacts in the meta-analysis.
  # - For complex models with multiple levels of random effects, ML can be more computationally 
  #   efficient and is more versatile, making it a practical choice for this analysis.
  # - Although ML might slightly underestimate variance components in small sample sizes, the primary 
  #   focus here is on hypothesis testing and the fixed effects, for which ML is well-suited.
  method = "ML"                                      
)

# Notes on the model:
# - 'yi': Represents the vector of observed effect sizes, calculated previously using escalc() function.
# - 'V': The variance-covariance matrix representing the sampling variances of effect sizes and covariances, allowing for dependencies within studies.
# - 'random': Specifies the random-effects structure. 
#     - '~ 1 | id_article': Random effect at the study level to capture between-study variance.
#     - '~ 1 | id_article/response_variable': Nested random effects to account for within-study variability across different response variables.
# - 'data': Specifies the dataset being used, which has been preprocessed to include only relevant and cleaned data points.
# - 'verbose': Provides output on the optimization process, helpful for diagnosing convergence issues.
# - 'control': Contains settings for the optimization process, including the choice of optimizer, maximum iterations, and convergence tolerance.
# - 'method': Specifies the estimation method; ML is chosen here to estimate the fixed and random effects.

# Running this model will provide insight into both the fixed effect sizes and the random effects that account for variability within and between studies.
# It will also help in understanding how much of the total variability is due to differences between studies and how much is due to within-study correlation among response variables.
#######################################################################################################################################
##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

# Time difference of 4.314359 mins - too simple! And still failing :-(
# Time difference of 1.77205 hours - with all model optimizes! And still failing :-(
# Time difference of 29.00199 secs - very simplified - but works :-)
# Time difference of 39.1189 secs

# Iteration 100   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 101   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 102   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 103   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 104   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 105   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 106   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# Iteration 107   ll = -5966.3000   sigma2 = 0.0067  0.0067  0.0237  
# 
# Time difference of 52.40915 secs
# Time difference of 27.06277 secs

```

```{r}
I get the error:

Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.

Variance Components in Model:


Iteration 0     ll = -16196.0178   sigma2 = 0.0106  0.0106  0.0106  
Iteration 1     ll = -16196.0178   sigma2 = 0.0106  0.0106  0.0106  
Iteration 2     ll = -16196.0178   sigma2 = 0.0106  0.0106  0.0106  
..  
Iteration 66    ll = -16196.0178   sigma2 = 0.0106  0.0106  0.0106  
Iteration 67    ll = -16196.0178   sigma2 = 0.0106  0.0106  0.0106  
Fejl: Optimizer (nlminb) did not achieve convergence (convergence = 1).

3.
stop(mstyle$stop(paste0("Optimizer (", optimizer, ") did not achieve convergence (convergence = ",
opt.res$convergence, ").")), call. = FALSE)

2.
.chkconv(optimizer = optimizer, opt.res = opt.res, optcontrol = optcontrol,
fun = "rma.mv", verbose = verbose)

1.
rma.mv(yi = yi, V = V_matrix, random = list(~1 | id_article,
~1 | id_article/response_variable), data = filtered_meta_data_ROM,
verbose = TRUE, control = list(iter.max = 1000, rel.tol = 1e-06))



```

##########################################################################
IDENTIFY OBSERVATIONS WITH EXTREME VARIACE AND EXCLUDE
##########################################################################


```{r}
# Identify observations with extreme variances
extreme_variances <- filtered_meta_data_ROM %>%
  filter(vi == max(vi) | vi == min(vi))

extreme_variances

# Optionally, remove observations with extremely high or low variances
filtered_meta_data_ROM_less_extreme_vi <- filtered_meta_data_ROM %>%
  filter(vi < quantile(vi, 0.99) & vi > quantile(vi, 0.01))  # Adjust the quantiles as needed

```

TAKING A LOOK AT THE FITTED MODEL

```{r}
# Inspect the model summary to get more details
summary(original_model)

# Multivariate Meta-Analysis Model (k = 1007; method: REML)
# 
#      logLik     Deviance          AIC          BIC         AICc   
# -16564.4318   33128.8636   33136.8636   33156.5186   33136.9036   
# 
# Variance Components:
# 
#             estim    sqrt  nlvls  fixed                        factor 
# sigma^2.1  0.0006  0.0245     37     no                    id_article 
# sigma^2.2  0.0047  0.0683     37     no                    id_article 
# sigma^2.3  0.0280  0.1674     48     no  id_article/response_variable 
# 
# Test for Heterogeneity:
# Q(df = 1006) = 98341.7572, p-val < .0001
# 
# Model Results:
# 
# estimate      se     zval    pval    ci.lb   ci.ub    
#  -0.0227  0.0303  -0.7489  0.4539  -0.0821  0.0367    
# 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```


Older intepretation:

INTERPRETATION OF MODEL SUMMARY

The multivariate meta-analysis model includes 1032 effect sizes from various studies and is implemented using the restricted maximum likelihood (REML) method. The log-likelihood value is -5966.3000, indicating the fit of the model to the data. The model's fit statistics, such as the AIC (11940.6000), BIC (11960.3531), and AICc (11940.6390), provide measures of the model's goodness of fit, with lower values indicating a better fit. These values help in comparing different models and selecting the best one.

The model accounts for variability at different levels using random effects. The variance components for the id_article level are 0.0067 (with a standard deviation of 0.0820) for both sigma^2.1 and sigma^2.2, indicating the variance attributed to individual articles. Additionally, the variance component for the id_article/response_variable level is 0.0237 (with a standard deviation of 0.1539), capturing the variance attributed to different response variables within each article. These variance components highlight the hierarchical structure of the data and the importance of considering both article-level and response variable-level variability.

The Q-test for heterogeneity reveals significant variability among the effect sizes, with a Q-value of 112909.0493 and a p-value of less than 0.0001. This significant heterogeneity indicates that the effect sizes differ more than would be expected by chance alone, suggesting that there are underlying differences between the studies that need to be accounted for in the analysis. The presence of such heterogeneity underscores the necessity of using a random-effects model to capture the between-study variability.

Despite the significant heterogeneity, the overall effect size estimate is 0.0046, with a standard error of 0.0321. However, this estimate is not statistically significant, as indicated by a p-value of 0.8865. The 95% confidence interval for the effect size ranges from -0.0583 to 0.0674, encompassing zero and further indicating the lack of a significant effect. This suggests that the effect sizes across the studies do not show a consistent trend or significant overall effect, highlighting the complexity and variability inherent in the data.



With the updated model and summary intepretation:

EXTENDED INTERPRETATION AND NEXT STEPS IN META-ANALYSIS

Overview of Data and Model Results
This multivariate meta-analysis model incorporates 1007 effect sizes derived from various studies, implementing a maximum likelihood (ML) method to estimate the parameters. The significant values from the model include:

Log-Likelihood: -16178.1646
AIC (Akaike Information Criterion): 32364.3291
BIC (Bayesian Information Criterion): 32383.9881
AICc (Corrected Akaike Information Criterion): 32364.3691
These metrics are crucial for evaluating model fit, with lower values indicating a better fit. In this context, while the model provides a statistical framework for integrating the different studies, its effectiveness in capturing the true underlying relationships is guided by these fit statistics.

Understanding the Variance Components
The model accounts for random effects at different levels:

sigma^2.1 (id_article level): This small variance component (0.0006, SD = 0.0253) suggests limited variability attributable to individual articles, indicating that the majority of variability is not due to differences between articles.

sigma^2.2 (id_article level): The value is zero (0.0000, SD = 0.0000), implying negligible or no detectable variance at this level, suggesting that there might not be additional article-level factors influencing the outcomes.

sigma^2.3 (id_article/response_variable level): The higher variance component (0.0041, SD = 0.0637) highlights that variability exists between different response variables within each article. This suggests that the specific metrics or outcomes measured might influence the variability more than article-level factors alone.

Significant Heterogeneity
The Q-test for heterogeneity is a critical component of this analysis, revealing significant variability among the effect sizes (Q = 264771.5488, p < 0.0001). This result indicates that the differences between the studies are not solely due to random sampling variability but may also be influenced by other factors such as study design, population characteristics, measurement methods, or other contextual variables. The presence of substantial heterogeneity justifies the use of a random-effects model to account for this variability, underscoring the model's appropriateness in handling the complex structure of the data.

Non-Significant Overall Effect Size
The overall effect size estimate of -0.0098, with a p-value of 0.3682, indicates that the combined studies do not show a consistent, statistically significant effect. The confidence interval (-0.0311 to 0.0115) encompasses zero, suggesting that the effect sizes across the studies vary without a clear direction or magnitude. This non-significance might reflect a true lack of effect or could be due to variability introduced by study differences.

Implications for Data Validity and Meta-Analysis
Model Validity: The significant heterogeneity and variance components observed in this model suggest that while the random-effects model is appropriate, it might still not capture all sources of variability. This impacts the validity of the meta-analysis, as unaccounted variability can lead to biased or less reliable estimates. Careful interpretation is required, recognizing that the results might be influenced by unobserved moderators or confounding variables.

Data Quality and Consistency: The substantial heterogeneity indicates that studies included in the meta-analysis vary considerably in their findings. This variation can stem from differences in study design, population demographics, measurement techniques, or other contextual factors. Ensuring consistency in data quality, such as using standardized measurement techniques and harmonizing study protocols, could enhance the reliability of future analyses.

Exploring Sources of Heterogeneity: To improve the understanding of the observed heterogeneity, further analyses could investigate potential moderators. Meta-regression can be employed to examine how study-level covariates (e.g., study design, geographic location, sample size) influence effect sizes. Identifying significant moderators can help explain the variability and guide the refinement of future studies.

Subgroup Analyses: Performing subgroup analyses can further elucidate the effects within more homogeneous groups, potentially revealing trends or significant effects masked in the overall analysis. Subgroups could be based on study characteristics such as year, location, population type, or specific response variables measured.

Sensitivity Analyses: Conducting sensitivity analyses to test the robustness of the results is critical. By systematically removing studies with extreme effect sizes or high variances, the stability of the overall conclusions can be evaluated. This process helps to identify influential studies or outliers that may disproportionately affect the results.

Publication Bias Assessment: Considering the possibility of publication bias is essential, as it could skew the meta-analysis results. Using tools like funnel plots or statistical tests such as Egger's test can help detect asymmetry, indicating potential publication bias.

Use of REML: Although ML was used in this analysis, considering REML (Restricted Maximum Likelihood) could be beneficial in future iterations. REML is generally preferred in random-effects meta-analyses as it provides unbiased estimates of variance components by accounting for the degrees of freedom used in estimating fixed effects. However, ML is often used for comparing nested models and is useful for hypothesis testing, as was necessary in this analysis. The choice between ML and REML should be guided by the specific goals of the analysis, with ML being suitable for hypothesis testing and REML offering better variance estimation properties.

Conclusion
The results of this meta-analysis model reveal significant heterogeneity among the included studies, necessitating a careful and nuanced interpretation of the findings. The absence of a statistically significant overall effect size does not diminish the value of the analysis but highlights the complexity of integrating diverse studies. Future research should focus on identifying sources of heterogeneity, refining inclusion criteria, and standardizing study methodologies to enhance the robustness and validity of meta-analytic conclusions. By addressing these areas, the reliability and applicability of meta-analysis as a tool for evidence synthesis can be significantly improved.



1. Sensitivity Analysis
* Objective: To determine how robust the meta-analysis results are to the inclusion or exclusion of specific studies.
* Approach: 
  - Influence Diagnostics: Use leave-one-out analysis to see how the removal of each study individually affects the overall effect size estimate and
    heterogeneity measures.
  - Outlier Detection: Identify studies with extreme effect sizes or variances. These outliers can disproportionately influence the overall results. Removing
    these studies can provide insights into their impact.
  - Impact of Large Sample Sizes: Check if studies with large sample sizes dominate the overall findings. You can perform analyses with and without these        studies to assess their influence.

2. Moderator Analysis
* Objective: To explore and account for the sources of heterogeneity observed in your model.
* Approach: 
  - Meta-Regression: Include study-level covariates (e.g., year of study, geographic location, sample size, measurement methods) as moderators in the model      to see if they explain some of the variance.
  - Categorical Moderators: Group studies based on categorical variables (e.g., region, population type) and assess whether these groups show different          effect sizes.
  - Continuous Moderators: Investigate continuous variables (e.g., year of publication, age of subjects) to see if they correlate with the effect sizes.

3. Visualization
* Objective: To provide a visual representation of the data and model outcomes for better interpretation and communication.
* Approach:
  - Forest Plots: Create forest plots to show the effect sizes and confidence intervals for each study. This will help visualize the variation across studies     and the overall effect size.
  - Bubble Plots: Use bubble plots to illustrate the relationship between effect sizes and study characteristics, with bubble size representing sample size      or variance.
  - Effect Size Distribution: Plot the distribution of effect sizes to see the spread and central tendency, which can help in identifying any skewness or        extreme values.


4. Diagnostic Plots
* Objective: To assess model assumptions and detect potential biases.
* Approach:
  - Funnel Plots: Generate funnel plots to visualize potential publication bias. Asymmetry in the plot could indicate bias, which can be further tested using     Egger's test or trim-and-fill methods.
  - Residual Plots: Plot residuals to check if the assumptions of normality and homoscedasticity are met. Patterns in residuals might indicate model             misspecification or the presence of influential studies.

5. Reconsider the Use of REML
* Objective: To obtain more accurate variance component estimates.
* Approach:
  - Consider running the analysis using REML, especially for the final model, to compare the results with those obtained using ML. REML can provide more         reliable estimates of variance components by accounting for the uncertainty in estimating fixed effects.
  - Use ML for initial model fitting and hypothesis testing (e.g., comparing nested models) and REML for the final variance estimation to ensure robustness.

6. Further Data Quality and Consistency Checks
* Objective: To ensure the reliability of the data used in the meta-analysis.
* Approach:
  - Standardization: Ensure that all included studies use standardized measurement techniques. If not, consider transforming or harmonizing data to reduce       variability.
  - Protocol Harmonization: Evaluate the consistency of study protocols. Large deviations can introduce heterogeneity that may not be accounted for by           statistical methods alone.

7. Explore Potential Unobserved Moderators
* Objective: To identify any potential factors influencing the results that were not initially considered.
* Approach:
  - Qualitative Review: Review the studies to identify any commonalities or differences that might serve as unobserved moderators.
  - Collaboration with Experts: Consult domain experts to hypothesize potential unobserved moderators that could explain some of the heterogeneity.





Strategies to Address Non-Convergence
Check and Adjust Initial Values: Using different starting values for the model parameters can sometimes help the optimizer find a solution. In the rma.mv function, you can specify control parameters to set initial values.

Simplify the Model: Try fitting a simpler model (e.g., reducing the number of random effects) when excluding certain studies to see if this leads to convergence.

Use a Different Optimizer: In some cases, using a different optimizer may help. The rma.mv function allows you to specify alternative optimizers like "optim" instead of the default "nlminb".

Diagnostics and Robust Estimation: Before fitting the sensitivity models, check which studies have a high influence using diagnostics like Cook's distance or leverage statistics. Consider robust variance estimation methods that are less sensitive to influential points.

Handling Convergence Failures in Loop: Modify the loop to handle non-convergence gracefully, for instance, by skipping the study that leads to non-convergence or trying a different optimization method for that iteration.




The warning message you’re encountering, “Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results,” highlights a significant issue with the variances associated with your study estimates in the meta-analysis. In meta-analysis, each study's effect size estimate comes with a sampling variance, which is crucial for determining how much weight that study should carry in the overall analysis. When the ratio between the largest and smallest variances is extremely high, it suggests a substantial disparity in the uncertainty of the estimates across studies.

This large variance ratio can lead to instability in your meta-analysis results. Studies with higher sampling variances might disproportionately influence the pooled effect size, potentially skewing the results and making them less reliable. Essentially, when some studies have much higher uncertainty compared to others, it can undermine the stability of the meta-analysis.

In your sensitivity analysis, where you iteratively exclude each study to assess the robustness of your findings, this warning suggests that the results might be particularly sensitive to the inclusion or exclusion of studies with high variances. You should carefully examine the distribution of variances to understand which studies contribute most to this disparity and whether they are outliers or have special characteristics that could be influencing the analysis.

To address this issue, you might need to explore methods to stabilize the results, such as using robust variance estimators or adjusting the weights based on the variances. Additionally, it is important to investigate the studies with extreme variances to determine if there are underlying reasons for their high uncertainty, which might inform whether they should be treated differently in your analysis.

While the warning suggests caution, continuing with your sensitivity analysis is appropriate. It will help you gauge how the exclusion of each study affects the overall results and provide insights into the robustness of your meta-analysis. If the results remain unstable or if you're unsure about the implications of the warning, consulting with a statistician can offer further guidance and ensure that your analysis is conducted appropriately.



Differences Between the Original Model and the Sensitivity Analysis Model
In the sensitivity analysis, there were modifications made to the model settings compared to the original model. These changes were necessary to address convergence issues and ensure that the model could fit successfully for each subset of the data when specific studies were excluded. Below are the key differences between the original model and the sensitivity analysis model:

Justification for Modifying Model Settings
The adjustments were necessary due to warnings about the large ratio of the largest to smallest sampling variances, which indicated potential instability in the results. By relaxing the convergence criteria and providing initial estimates, the sensitivity analysis was able to proceed without errors, thus allowing a thorough investigation into the influence of individual studies. This approach, while introducing some compromise on precision, ensures that the sensitivity analysis remains informative and can highlight potential issues or influential studies within the meta-analysis dataset.

Saving the data of the influence diagnostics

SAVING THE DIAGNOSTICS DATA

```{r, eval = FALSE}
##############################################################
# Cook's Distance
##############################################################
# Create a data frame for Cook's Distance
cooks_data <- data.frame(
  Observed_Outcome = seq_along(cooks_d),
  Cooks_Distance = cooks_d
)

# Save to CSV
readr::write_csv(cooks_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/cooks_distance_2.csv"))

##############################################################
# DFBETAS
##############################################################
# Convert DFBETAS to a data frame
dfbetas_data <- as.data.frame(dfbetas_vals)

# Save to CSV
readr::write_csv(dfbetas_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/dfbetas_2.csv"))

##############################################################
# Studentized Residuals
##############################################################
# Convert Studentized Residuals to a data frame
student_resid_data <- data.frame(
  Observed_Outcome = seq_along(student_resid$resid),
  Residuals = student_resid$resid,
  Standard_Errors = student_resid$se,
  Z_Values = student_resid$z
)

# Save to CSV
readr::write_csv(student_resid_data, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/studentized_residuals_2.csv"))
```

 | 17% ~07h 34m 36s 



```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################


# Create a formula for the moderators
moderator_formula <- as.formula(paste("~", paste(moderators, collapse = " + ")))

# Fit a multivariate meta-analysis model including moderators
moderator_model <- rma.mv(
  # Effect size estimates
  yi = yi,                     
  # Variance-covariance matrix of the effect sizes
  V = V_matrix,                
  # Moderator variables
  mods = moderator_formula,    
  random = list(
  # Random intercept for each article
    ~ 1 | id_article,          
  # Random intercept for each response variable within each article
    ~ 1 | id_article/response_variable  
  ),
  # Data frame containing the meta-analysis data with effect sizes
  data = filtered_meta_data_ROM,  
  # Use REML for estimation with moderators
  method = "REML",             
  control = list(
    optimizer = "nlminb",
    iter.max = 5000,
    rel.tol = 1e-5
  ),
  # Print progress and convergence information
  verbose = TRUE               
)

# Extract coefficients as a data frame in order to effectively save the model results. 
# $b extracts the regression coefficients (or fixed-effect estimates) from that model. 
# These coefficients represent the effect sizes for the moderator variables included in the model.
moderator_results <- as.data.frame(moderator_model$b)

# Notes:
# Using Restricted Maximum Likelihood (REML) for estimating models with moderators in a meta-analysis is beneficial due to its more accurate handling of variance components. Unlike Maximum Likelihood (ML), which can underestimate these components, REML focuses on estimating random effects first, reducing bias and leading to more reliable estimates. This is particularly important when the number of studies is small or when the data structure is complex, involving nested or hierarchical relationships. By adjusting for the degrees of freedom lost in estimating fixed effects, REML provides a better fit for models with random effects, ensuring the residual variance is accurately represented.

# REML is also less prone to overfitting compared to ML. Overfitting can occur when a model is too tailored to the specific dataset, particularly with many parameters relative to observations. REML's more conservative estimation approach mitigates this risk, enhancing the generalizability of the findings. This robustness is crucial in meta-analyses, where the goal is to draw broader conclusions across multiple studies, not just fit the model to the peculiarities of the sample data.

# Furthermore, REML is preferable for hypothesis testing involving random effects because it provides more accurate p-values and confidence intervals. This accuracy is vital when examining the influence of moderators, as it ensures the significance and strength of these moderators are assessed reliably. In meta-analysis contexts, using REML helps maintain consistency across studies, accurately reflecting true between-study variability and improving the validity of conclusions about how moderators impact outcomes.

# In summary, REML enhances the validity and reliability of meta-analysis findings by providing unbiased variance estimates, reducing overfitting, and ensuring robust hypothesis testing. These features are crucial for understanding how various moderators affect the outcomes and for making informed, generalizable conclusions.

##############################################################
# Save to CSV
readr::write_csv(moderator_model, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/moderator_model_3.csv"))

##############################################################

# Display the moderator model
moderator_model

##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
time.taken

##################################################
```

```{r}
summary(moderator_model)
```

```{r}
# Examine the influence of each moderator
anova(moderator_model)
```

```{r}
# Forest plot to visualize the results
forest(moderator_model, slab = paste(id_article, response_variable, sep = ", "), cex = 0.6)

# Bubble plot for visualizing relationship between effect sizes and selected moderator
bubble_plot <- function(data, moderator) {
  data %>%
    ggplot(aes_string(x = moderator, y = "yi", size = "vi")) +
    geom_point(alpha = 0.5) +
    labs(x = moderator, y = "Effect Size (yi)", size = "Variance (vi)") +
    theme_minimal()
}
```


```{r}
# Example bubble plot for one moderator
bubble_plot(filtered_meta_data_ROM, "Alley_width")
```




```{r}
# Function to create a bubble plot for a given moderator with continuous, custom x-axis
bubble_plot <- function(data, moderator) {
  
  # Ensure the moderator variable is numeric
  data[[moderator]] <- as.numeric(data[[moderator]])
  
  # Remove rows with NA values in the moderator variable
  data <- data[!is.na(data[[moderator]]), ]
  
  # Generate the bubble plot
  data %>%
    ggplot(aes_string(x = moderator, y = "yi", size = "vi")) +  # `vi` represents variance of effect sizes
    geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
    labs(x = moderator, y = "Effect Size (yi)", size = "Variance (vi)") +  # Axis labels
    scale_x_continuous(breaks = seq(0, max(data[[moderator]], na.rm = TRUE), by = 5),  # Custom x-axis intervals
                       limits = c(0, max(data[[moderator]], na.rm = TRUE))) +  # Set range for x-axis
    theme_minimal()  # Clean theme for visualization
}

# Example bubble plot for the "Alley_width" moderator
bubble_plot(filtered_meta_data_ROM, "alley_width")

```



```{r}
# Function to create a bubble plot for a given moderator
bubble_plot <- function(data, moderator) {
  data %>%
    ggplot(aes_string(x = moderator, y = "yi", size = "vi")) +  # `vi` represents variance of effect sizes
    geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
    labs(x = moderator, y = "Effect Size (yi)", size = "Variance (vi)") +  # Axis labels
    theme_minimal()  # Clean theme for visualization
}

# Example bubble plot for the "Alley_width" moderator
bubble_plot(filtered_meta_data_ROM, "alley_width")

```





**Older model from earlier database**
The multivariate meta-analysis model includes 1007 effect sizes from various studies and is implemented using the maximum likelihood (ML) method. The log-likelihood value is -16178.1646, which indicates how well the model fits the data. The model's fit statistics, including the AIC (32364.3291), BIC (32383.9881), and AICc (32364.3691), are measures of the model's goodness of fit, with lower values suggesting a better fit. These values are essential for comparing different models and selecting the most appropriate one.

The model accounts for variability at different levels using random effects. The variance components for the id_article level are as follows:

- sigma^2.1: 0.0006 (with a standard deviation of 0.0253), indicating the variance attributed to individual articles.
- sigma^2.2: 0.0000 (with a standard deviation of 0.0000), suggesting negligible or no variance at this level.
- sigma^2.3: 0.0041 (with a standard deviation of 0.0637), representing the variance attributed to different response variables within each article.

These variance components reflect the hierarchical structure of the data, emphasizing the importance of considering both article-level and response variable-level variability.

The Q-test for heterogeneity shows significant variability among the effect sizes, with a Q-value of 264771.5488 and a p-value of less than 0.0001. This significant heterogeneity implies that the effect sizes vary more than would be expected by chance alone, indicating underlying differences between the studies that must be accounted for in the analysis. The presence of such substantial heterogeneity justifies the use of a random-effects model to capture the between-study variability.

Despite the significant heterogeneity, the overall effect size estimate is -0.0098, with a standard error of 0.0109. This estimate is not statistically significant, as indicated by a p-value of 0.3682. The 95% confidence interval for the effect size ranges from -0.0311 to 0.0115, which includes zero, further indicating the lack of a significant overall effect. This suggests that the effect sizes across the studies do not show a consistent trend or significant overall impact, highlighting the complexity and variability inherent in the data.

**Updated model from database v3**
The multivariate meta-analysis model, based on 1007 effect sizes from different studies, was fitted using the Maximum Likelihood (ML) estimation method. The log-likelihood value of -16178.2376 reflects the fit of the model, while fit indices like AIC (32364.4752), BIC (32384.1341), and AICc (32364.5151) provide measures of the model's goodness of fit. These values are crucial for model comparison, with lower values indicating a better fit.

The random-effects model accounts for variability at two levels: study-level (id_article) and within-study (response variables). The variance components are as follows:

- sigma^2.1 = 0.0000 (standard deviation = 0.0037): Indicates near-zero variance between studies, suggesting minimal between-study variability.
- sigma^2.2 = 0.0000 (standard deviation = 0.0037): Also reflects negligible variance at the article level, implying little variation between the articles.
- sigma^2.3 = 0.0047 (standard deviation = 0.0682): Captures within-study variability across different response variables, suggesting more substantial within-study variability.

The model also reports a highly significant Q-test for heterogeneity (Q = 264771.5488, p < 0.0001), showing substantial variability among the effect sizes across studies. This indicates that the observed differences in effect sizes are not due to chance alone and supports the need for a random-effects model to account for this heterogeneity.

Despite the high heterogeneity, the overall effect size estimate is -0.0095 with a standard error of 0.0106, which is not statistically significant (p = 0.3677). The 95% confidence interval, ranging from -0.0302 to 0.0112, includes zero, indicating no significant overall effect across the studies.

################################################################################################
SUMMARISED:

**Older model from earlier database**
Understanding the Variance Components
The model accounts for variability at multiple levels:

- sigma^2.1 (id_article level): A small variance (0.0006) indicates limited variability between articles, suggesting that article-level differences are not a major source of variability.

- sigma^2.2 (id_article level): No variance (0.0000) at this level suggests negligible additional article-level influences on the outcomes.

- sigma^2.3 (id_article/response_variable level): A higher variance (0.0041) indicates more variability due to differences in response variables within each article, highlighting the influence of specific metrics or outcomes.

Significant Heterogeneity
The Q-test shows substantial heterogeneity among the effect sizes (Q = 264771.5488, p < 0.0001), indicating that the observed differences are not due to chance alone but are likely influenced by factors such as study design and measurement methods. This significant variability supports the use of a random-effects model to appropriately account for the complexity of the data.

Non-Significant Overall Effect Size
The overall effect size (-0.0098) is not statistically significant (p = 0.3682), with the confidence interval (-0.0311 to 0.0115) crossing zero. This suggests that the combined studies do not show a consistent trend or significant effect, which may be due to inherent variability across studies or a genuine lack of a consistent overall effect.

**Updated model from database v3**
Understanding the Variance Components
The model highlights variability at multiple levels:

- sigma^2.1 (id_article level): Near-zero variance (0.0000) suggests limited variability between studies, indicating that differences between articles do not contribute much to overall variability.
- sigma^2.2 (id_article level): Also near-zero variance (0.0000), indicating no significant additional study-level variance.
- sigma^2.3 (id_article/response_variable level): Higher variance (0.0047) suggests more substantial variability within studies, driven by differences in response variables, showing the influence of study-specific outcomes.

Significant Heterogeneity
The Q-test indicates significant heterogeneity among effect sizes (Q = 264771.5488, p < 0.0001). This heterogeneity points to substantial variability that may arise from differences in study designs, populations, or measurement methods, supporting the use of random-effects modeling to handle this complexity.

Non-Significant Overall Effect Size
The overall effect size of -0.0095 is not statistically significant (p = 0.3677), with a confidence interval ranging from -0.0302 to 0.0112, indicating that the effect size across studies is small and not significantly different from zero. This suggests that, on average, the studies do not show a consistent trend or significant effect, likely due to variability between and within studies.

################################################################################################
COMPARING 'OLD' AND NEW (DATABASE v3) MODELS

The updated model shows slightly different variance components, particularly a reduction in between-study variance (sigma^2.1) and a slight increase in within-study variance (sigma^2.3). However, the overall conclusions remain the same: there is no significant overall effect across studies, and the heterogeneity between studies remains high, justifying the use of a random-effects model in both cases.




Discussion of Model Setup and Trade-offs in this Meta-Analysis

Addressing Non-Convergence and Model Adaptations

Handling non-convergence is crucial in meta-analysis to ensure reliable results. A series of strategies can be employed to mitigate these issues, including adjusting initial values for model parameters, simplifying the model by reducing the number of random effects, and using different optimizers such as "optim" instead of the default "nlminb" in the rma.mv function. Diagnostic measures like Cook's distance or leverage statistics can help identify studies with high influence, allowing for robust variance estimation methods that are less sensitive to these influential points. Additionally, modifying the analysis loop to handle non-convergence gracefully—by skipping problematic studies or trying alternative optimization methods—ensures the robustness of the findings.

The warning about the “Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results” highlights significant disparities in the variances among study estimates. This disparity can lead to instability, as studies with larger variances may disproportionately influence the pooled effect size, potentially skewing results. Addressing this requires a thorough examination of variance distribution to understand and mitigate the impact of outliers. Using robust variance estimators or adjusting study weights based on variance can help stabilize results. Continuing with sensitivity analysis is essential to assess the robustness of findings, and persistent instability may warrant consulting with a statistician to ensure the integrity of the analysis.

Modifications in Sensitivity Analysis Compared to the Original Model

The sensitivity analysis involved key modifications to the original model settings to handle convergence issues effectively. While the original model used the optimizer nlminb with a maximum of 1000 iterations and a strict relative tolerance of 1e-8 for high precision, the sensitivity analysis increased the maximum iterations to 5000, relaxed the relative tolerance to 1e-2, set the step size to 100, and introduced initial estimates for variance components (sigma2.init = c(0.001, 0.001, 0.005)). These adjustments were necessary to enhance the model’s ability to find stable solutions, particularly when excluding specific studies.

The relaxation of convergence criteria in the sensitivity analysis highlights a trade-off: it prioritizes consistent convergence over precision, which is essential for handling data variability. While the original model’s strict settings were geared towards obtaining precise solutions, they were more prone to convergence failures, especially during iterative fitting across different subsets. Introducing initial variance component estimates helped guide the optimizer towards reasonable starting points, improving the chances of convergence in complex models.

Impact and Interpretation of Model Adjustments

The relaxed settings in the sensitivity analysis model, though essential for achieving convergence across multiple iterations, may lead to slightly less stable and precise estimates. This could result in increased variability in the results, potentially masking subtle effects that the original model detected. Therefore, findings from the sensitivity analysis should be interpreted with caution and seen as a measure of robustness rather than definitive conclusions.

These adjustments underscore the complexity of the meta-analysis model and the need for further refinement to ensure reliable results. Exploring alternative modeling strategies or data transformation methods may be required to reduce variability in sampling variance and improve the stability of estimates. Ultimately, while sensitivity analyses are crucial for understanding the robustness of findings, they also highlight the importance of continually refining analytical methods to enhance the validity and reliability of meta-analytic conclusions.




INTERPRETATION OF MODERATOR ANALYSIS

*Tree-Crop Combination:*
The tree-crop combinations represent different agroforestry systems where specific tree species are combined with either cereal or legume crops. 

Apple-Legume:
- Estimate: 0.1411, not statistically significant (p = 0.6257).

Maple-Cereal:
Estimate: 0.1061, not statistically significant (p = 0.5792).

Maple-Legume:
Estimate: 0.1871, not statistically significant (p = 0.3554).

Mixed-Cereal:
Estimate: 0.0492, not statistically significant (p = 0.7314).

Mixed-Legume:
Estimate: 0.0535, not statistically significant (p = 0.7089).

Paulownia-Cereal:
Estimate: -0.2062, not statistically significant (p = 0.5704). This negative estimate suggests a potential decrease in effect size, but it is not statistically significant.

Poplar-Cereal:
Estimate: 0.0163, not statistically significant (p = 0.9094).

Poplar-Legume:
Estimate: 0.0543, not statistically significant (p = 0.7049).

Hence, none of the tree-crop combinations show a statistically significant effect on the overall effect size in this analysis, as their confidence intervals all cross zero and p-values are well above 0.05. However, there might be significant variations within each response variable (ecosystem services). This is assessed below.


*Test of Moderators (ANOVA)*
The ANOVA result shows the overall test for the significance of all moderator variables: QM(df = 20) = 216.3594, p-val < .0001
This test indicates that the set of moderator variables, as a whole, is statistically significant. The low p-value (<0.0001) suggests that the moderators, collectively, explain a significant portion of the variance in the effect sizes. While individual moderators might not be significant on their own, the model as a whole benefits from the inclusion of moderators, improving the fit.


```{r}
# Ensure all moderators are factors with explicitly defined levels
metdata_moderators <- metdata_moderators |> 
  mutate(
    tree_crop_combination = factor(tree_crop_combination, 
                                   levels = c("Biomass-Cereal", "Biomass-Legume", "Biomass-Oilseed", 
                                              "Fruit-Legume", "Mixed-Cereal", "Mixed-Legume", 
                                              "Timber-Cereal", "Timber-Legume")),
    season = factor(season, levels = c("Winter", "Summer", "Winter and Summer")),
    alley_width_category = factor(alley_width_category, levels = c("Narrow", "Moderate", "Wide")),
    tree_row_orientation = factor(tree_row_orientation, levels = c("NS", "EW")),
    soil_texture = factor(soil_texture, levels = c("Clay", "Clay loam", "Silty clay")),
    tree_density_category = factor(tree_density_category, levels = c("Low density", "Medium density", "High density")),
    tree_height_category = factor(tree_height_category, levels = c("Short", "Medium", "Tall")),
    age_system = factor(age_system, levels = c("Young", "Medium", "Mature")),
    tillage = factor(tillage, levels = c("Yes", "No")),
    organic = factor(organic, levels = c("Yes", "No"))
  )

# Add a dummy row with all factor levels
dummy_row <- data.frame(
  yi = 0, vi = 1e6,  # Neutral effect size with very high variance (negligible weight)
  id_article = NA,
  response_variable = NA,
  tree_crop_combination = factor("Biomass-Cereal", levels = levels(metdata_moderators$tree_crop_combination)),
  season = factor("Winter", levels = levels(metdata_moderators$season)),
  alley_width_category = factor("Narrow", levels = levels(metdata_moderators$alley_width_category)),
  tree_row_orientation = factor("NS", levels = levels(metdata_moderators$tree_row_orientation)),
  soil_texture = factor("Clay", levels = levels(metdata_moderators$soil_texture)),
  tree_density_category = factor("Low density", levels = levels(metdata_moderators$tree_density_category)),
  tree_height_category = factor("Short", levels = levels(metdata_moderators$tree_height_category)),
  age_system = factor("Young", levels = levels(metdata_moderators$age_system)),
  tillage = factor("Yes", levels = levels(metdata_moderators$tillage)),
  organic = factor("Yes", levels = levels(metdata_moderators$organic))
)

# Combine the dummy row with the actual data
metdata_moderators <- bind_rows(metdata_moderators, dummy_row)

# Check the factor levels again to ensure all are present
lapply(metdata_moderators, levels)

# Create the design matrix using model.matrix() with drop = FALSE
design_matrix <- model.matrix(~ 0 + tree_crop_combination + season + alley_width_category +
                              tree_row_orientation + soil_texture + tree_density_category +
                              tree_height_category + age_system + tillage + organic, 
                              data = metdata_moderators, 
                              drop.unused.levels = FALSE)

```



*OBS! Time for running this code below is slightly long!*

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################

# Start time tracking
start.time <- Sys.time()

##################################################
##################################################

# Fit a multivariate meta-analysis model including moderators using the rma.mv() function from the metafor package.

# This model estimates the effect sizes for the moderator variables while accounting for random effects (article-level and response variable level).
moderator_model <- rma.mv(
  # The effect size estimates (dependent variable).
  yi = yi,             
  # The variance-covariance matrix of the effect sizes (weights for the meta-analysis).
  V = V_matrix,               
  # The moderator variables (predictors) included in the model.
  mods = moderator_formula_no_intercept,     
  random = list(
    # Random intercept for each article to capture article-level variability.
    ~ 1 | id_article,          
    # Random intercept for each response variable within each article to account for within-article variability.
    ~ 1 | id_article/response_variable  
  ),
  # The dataset that contains the effect sizes and moderator variables.
  data = metdata_moderators,   
  # Use Restricted Maximum Likelihood (REML) instead of ML, for estimating variance components, reducing bias. In the original_model I've used ML for estimating           variance components. 
  # Justification for using Restricted Maximum Likelihood (REML) instead of Maximum Likelihood (ML):
  # - Enhanced Precision for Moderator Analysis: REML improves the accuracy of variance estimates, which is crucial when evaluating the impact of moderators, as it          helps isolate the true effects of these variables.
  # - Effective Handling of Complex Random Effects: The use of REML is better suited for the nested random effects structure (study-level and within-study variation)        in moderator analysis, accounting for both between- and within-study variability.
  # - Minimizes Bias in Variance Components: By focusing on estimating variance components first, REML reduces bias, ensuring that the effects of each moderator are         not distorted by inaccurate variance estimates.
  # - Improved Robustness in Heterogeneous Data: REML provides more stable and reliable estimates when analyzing heterogeneous data with multiple moderators, helping        to accurately detect their influence on effect sizes.
  # - Downsides: REML cannot compare models with different fixed effects, is computationally slower, and may underestimate variance components with very small sample        sizes.
  method = "REML",               
  control = list(
    # Optimizer to be used for fitting the model; changed to 'optim' with BFGS that can better handle complex models.
    optimizer = "optim",  
    # Use the BFGS optimization method for more efficient convergence.
    optim.method = "BFGS",       
    # Set maximum number of iterations for the optimizer to ensure convergence.
    iter.max = 1000,              
    # Set relative tolerance to ensure convergence with sufficient precision.
    rel.tol = 1e-12              
  ),
  # Enable verbose output for progress information and convergence diagnostics.
  verbose = TRUE                
)

# Extract relevant results from the model
# Coefficients (b), standard errors (se), confidence intervals (ci.lb, ci.ub), and p-values (pval)
# The coefficients (b), standard errors (se), confidence intervals (ci.lb, ci.ub), and p-values (pval) for the moderators are stored as a data frame.
# moderator_results <- data.frame(
#   Coefficient = moderator_model$b,   # Extracts the estimated effect sizes (regression coefficients) for each moderator.
#   Std_Error = moderator_model$se,    # Extracts the standard errors associated with each moderator's coefficient.
#   CI_Lower = moderator_model$ci.lb,  # Extracts the lower bounds of the 95% confidence intervals for each coefficient.
#   CI_Upper = moderator_model$ci.ub,  # Extracts the upper bounds of the 95% confidence intervals for each coefficient.
#   P_Value = moderator_model$pval     # Extracts the p-values to assess the statistical significance of each moderator.
# )

# Extract the coefficients for all levels
moderator_results <- data.frame(
  Moderator = rownames(moderator_model$b),
  Estimate = moderator_model$b,
  Std_Error = moderator_model$se,
  CI_Lower = moderator_model$ci.lb,
  CI_Upper = moderator_model$ci.ub,
  P_Value = moderator_model$pval
)


# Add row names (moderators) as a column
moderator_results$Moderator <- rownames(moderator_results)
rownames(moderator_results) <- NULL

##############################################################
# Save the results to a CSV file
readr::write_csv(moderator_results, 
                 here::here("C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/META_ANALYSIS/FROM_R/moderator_results_database_v5.csv"))

##############################################################

# Display the extracted results
moderator_results

##################################################
##################################################

# End time tracking
end.time <- Sys.time()

##############################################################
# Calculate time taken
time.taken <- end.time - start.time
print(time.taken)

##################################################

# NOTES

# In this code, we first create a formula for the moderators by concatenating the list of moderator variables into a single formula. This formula is passed into the mods argument in the rma.mv() function, allowing the inclusion of moderators in the meta-analysis model. The model itself is fitted using the rma.mv() function from the metafor package, which estimates the effect sizes for each moderator while accounting for random effects at the article level and response variable level.

# The model employs Restricted Maximum Likelihood (REML) for estimating variance components. REML is known for reducing bias in random effect estimates, especially in smaller sample sizes or when the random effects structure is complex. By focusing on estimating the random effects first, REML ensures that variance components are accurately reflected, providing more reliable and less biased results.

# The optimizer has been set to "optim" with the BFGS method. BFGS is well-suited for handling complex, non-linear optimization problems like those encountered in meta-analysis models with multiple levels of random effects. The optimizer is configured with a maximum iteration count of 1000 and a relative tolerance of 1e-12, providing stricter convergence criteria while ensuring computational efficiency.

# OBS: Be aware that removing the intercept changes the interpretation of the coefficients: 
# In the original model, the intercept represented the effect size for the reference level of each moderator (e.g., "Summer" for season, "Clay" for soil_texture).
# Without the intercept, each coefficient stands alone, which can be more intuitive but might complicate comparisons if you're used to reference-based interpretation.

##############################################################
# Last go: (08/09-24)

# Advarsel: 83 rows with NAs omitted from model fitting.
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Advarsel: Redundant predictors dropped from the model.
# Iteration 48    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 49    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 50    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 51    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# Iteration 52    ll = -1614.1311   sigma2 = 0.0032  0.0032  0.0010  
# 
# Time difference of 20.4787 secs

# Sometimes I get this error message "Fejl: uventet numerisk konstant in "Iteration 48"" - as well as the other warnings above

##############################################################
# Last go: (08/11-24)

# Iteration 238   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# Iteration 239   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# Iteration 240   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# Iteration 241   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# 
# Time difference of 5.843096 secs

# Last go: (13/11-24)
# Iteration 240   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# Iteration 241   ll = -1859.7286   sigma2 = 0.0000  0.0000  0.0010  
# 
# Time difference of 15.90985 secs
```
```{r}
# Extract the intercept and coefficients
intercept_value <- moderator_model$b["intrcpt"]
intercept_ci_lb <- moderator_model$ci.lb["intrcpt"]
intercept_ci_ub <- moderator_model$ci.ub["intrcpt"]
```

```{r}
# ANOVA to test the influence of each moderator on the effect size
anova(moderator_model)
```

INTERPRETATION OF MODERATOR ANALYSIS

*Tree-Crop Combination:*
The analysis evaluates the influence of various tree-crop combinations on the effect size. Most combinations show non-significant results, indicating that the type of tree-crop pairing does not significantly impact the effect size.

 - Apple-Legume: The coefficient (0.0384) is positive but not statistically significant (p = 0.685), suggesting no meaningful effect.
 - Maple-Cereal: Coefficient of 0.0526, with a non-significant p-value (p = 0.5672), showing little influence.
 - Mixed-Legume: Coefficient of 0.0811, non-significant (p = 0.2549), indicating no notable impact.
 - Paulownia-Cereal: Negative coefficient (-0.0747), non-significant (p = 0.7342), suggesting no significant effect.

Overall, none of the tree-crop combinations had a statistically significant influence, highlighting the potential uniformity in response across different agroforestry systems.

*Season:*
The season of measurement plays a significant role in the effect size:

 - Winter: Shows a significant positive effect (coefficient = 0.0374, p = 0.0024**), indicating that outcomes measured in winter tend to have higher effect sizes.
 - Winter and Summer Combined: The effect is not significant (p = 0.8252), suggesting no clear combined seasonal impact.

This suggests a potential seasonal pattern in the data, with winter measurements showing stronger effects.

*Alley Width:*
The influence of different alley widths on effect size is assessed:

 - Width = 48 meters: Significant negative effect (coefficient = -0.0516, p = 0.0012**), indicating that wider alleys reduce the effect size.
 - Other widths (e.g., 10m, 15m): Non-significant results, suggesting that variations in narrower alley widths have no substantial impact.

This finding implies that very wide alleys may diminish the positive effects seen in agroforestry systems, possibly due to reduced tree-crop interactions.

*Soil Texture:*
Different soil types did not show a significant impact on effect sizes:

 - Clay: Coefficient = 0.0093, p = 0.8629 (non-significant).
 - Clay Loam: Coefficient = -0.0524, p = 0.2853 (non-significant).
 - Silty Clay: Coefficient = -0.0006, p = 0.9936 (non-significant).

These results suggest that soil texture is not a major determinant of effect size in the analyzed studies.

*Tree Height and System Age:*
 - Tree Height: Non-significant (coefficient = 0.0011, p = 0.9592), indicating no detectable influence on effect size.

 - Young Age System: Non-significant (coefficient = 0.0194, p = 0.8074), suggesting that younger systems do not show a different trend compared to older systems.

This suggests that structural characteristics like tree height and system age do not explain variation in effect sizes across studies.


*Test of Moderators (ANOVA)*
The ANOVA result for the test of moderators shows a significant effect (QM = 200.6479, df = 20, p < 0.0001), indicating that at least one of the included moderators significantly influences the effect size. This finding justifies the inclusion of moderator variables in the model, highlighting their importance in explaining part of the heterogeneity observed in the meta-analysis.

In summary, the moderator analysis identified season (specifically winter) and wide alley width (48m) as significant moderators, suggesting these factors have the most substantial impact on effect sizes. Other tested moderators, including tree-crop combinations, soil texture, and tree height, did not show significant effects, indicating they do not contribute meaningfully to explaining the variability in effect sizes across studies.
