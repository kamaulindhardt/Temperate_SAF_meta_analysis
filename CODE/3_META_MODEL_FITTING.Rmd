---
title: "3_META_MODEL_FITTING"
author: "M.K.K. Lindhardt"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Load multiple add-on packages using pacman::p_load for efficiency
# pacman::p_load automatically installs missing packages and loads them
pacman::p_load(
  
  # Conflict Resolution
  conflicted,        # Resolves conflicts when functions with the same name exist in multiple packages
  
  # Data Manipulation / Transformation
  tidyverse,         # Comprehensive collection of R packages for data science (e.g., ggplot2, dplyr, readr)
  readr,             # Simplifies reading and writing of delimited text files (e.g., CSV)
  dplyr,             # A grammar of data manipulation (e.g., filter, mutate, summarise, etc.)
  skimr,             # Provides summary statistics with a more user-friendly output
  future,            # Supports parallel processing for speeding up computations
  future.apply,      # Extends the future package for parallelized versions of base R apply functions
  furrr,             # Provides a simple and consistent way to work with futures and parallel processing
  readxl,            # Read excel files
  
  ###################################################################################################################
  # Data Visualization
  ggplot2,           # A data visualization package for creating static and interactive graphics (part of tidyverse)
  patchwork,         # Extends ggplot2 by providing tools to combine multiple plots into one
  gridExtra,         # Arranges multiple grid-based plots (e.g., from ggplot2) into a single display
  scales,            # Adds tools for handling scale transformations and labels in visualizations
  gt,                # Stylish tables
  ggbreak,           # Breaks on axis for bar charts etc.
  ggpubr,            # Working with plots and legends library
  forcats,           # Tools for Working with Categorical Variables (Factors)
  
  ###################################################################################################################
  # Meta-Analysis
  metafor,           # For conducting meta-analyses, including calculating effect sizes and response ratios
  orchaRd,           # Provides tools for conducting and visualising meta-analyses and meta-regressions
  clubSandwich,      # Provides cluster-robust variance estimators for meta-analysis models
  mice,              # Multivariate Imputation by Chained Equations for handling missing data
  
  ###################################################################################################################
  # Exploratory Data Analysis (EDA)
  DataExplorer,      # Automates exploratory data analysis with summary statistics and visualizations
  SmartEDA,          # Automates exploratory data analysis with summary reports and visualizations
  naniar,            # Provides tools for handling and visualizing missing data
  VIM,               # Visualization and Imputation of Missing Data
  Hmisc,             # Miscellaneous functions including data summary, analysis, and visualization
  BaylorEdPsych,     # Provides tools for reliability analysis and missing data imputation
  openxlsx,          # Simplifies the the process of writing and styling Excel xlsx
  
  ###################################################################################################################
  # Project Management and Code Styling
  here,              # Simplifies file referencing by locating the root of a project directory
  styler             # Formats and styles R code to improve readability and consistency
)
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("summarize", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
conflict_prefer("intersect", "base")
```



#############
# STEP 1
##########################################################################################################################################
LOADING PREPARED META-DATA
##########################################################################################################################################


Loading the two datasets (imputed and non-imputed)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Define file paths
non_imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom.rds"))
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))

# non_imp_data_rom_dummy <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom_dummy.rds"))

# Read in the non-imputed dataset
non_imp_dataset <- non_imp_data_rom %>%
  as.data.frame()|> 
  # select(-geometry) |> 
  relocate(
    # Overall ID info
    id_study, id_article, id_obs, treat_id, exp_id,
    # Effect size calculation and variance
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative meta-analysis variables - mean and no. of observations
    silvo_mean, control_mean, silvo_n, control_n,
    # Quantitative meta-analysis variables - variance info
    silvo_se, silvo_sd, silvo_sd_from_se, silvo_sd_final, 
    control_se, control_sd, control_sd_from_se, control_sd_final
  )

# Read in the imputed dataset
imp_dataset <- imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_study, id_article, id_obs, treat_id, exp_id,
    # Effect size calculation and variance
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative meta-analysis variables - mean and no. of observations
    silvo_mean, control_mean, silvo_n, control_n,
    # Quantitative meta-analysis variables - variance info
    silvo_se, silvo_sd, silvo_sd_from_se, silvo_sd_final, 
    control_se, control_sd, control_sd_from_se, control_sd_final
  )
})
```

```{r}
# Checking high observations with extreme high variance
high_variance_obs <- 
  imp_dataset|> 
  filter(vi > quantile(vi, 0.95)) |> # 0.995
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           # Quantitative meta-analysis variables - mean and no. of observations
           silvo_mean, control_mean, silvo_n, control_n,
           # Quantitative meta-analysis variables - variance info
           silvo_se, silvo_sd_from_se, silvo_sd_final, 
           control_se, control_sd_from_se, control_sd_final,
           tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width) |> 
  arrange(id_article, response_variable)

skim(high_variance_obs)
```
```{r}
non_imp_dataset |> glimpse()
imp_dataset |> glimpse()
```

```{r}
# Select distinct id_article entries
distinct_articles <- imp_dataset %>%
  select(id_article) %>%
  distinct()

# Get the number of unique id_article entries
num_distinct_articles <- nrow(distinct_articles)

# Print the result
cat("Number of distinct id_article entries:", num_distinct_articles, "\n")
# Number of distinct id_article entries: 37 
```




##########################################################################################################################################
LISTING RESPONSE VARIABLES AND SETTING UP COSTUM COLORS
##########################################################################################################################################

```{r}
# Custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Response variables to analyze
response_variables <- names(custom_colors)
```

```{r}
# Filter dataset for specified response variables
filtered_dataset <- imp_dataset %>%
  filter(response_variable %in% response_variables)

# Calculate the Response Ratio
filtered_dataset <- filtered_dataset %>%
  mutate(
    ResponseRatio = silvo_mean / control_mean,
    log_ResponseRatio = log(ResponseRatio) # Log transformation for better interpretation
  ) |> 
  # Recalculate log ResponseRatio and add a small constant to avoid log(0)
  mutate(
    log_ResponseRatio = log(ResponseRatio + 1e-6) # Add a tiny constant for log safety
  )


# Bootstrap data for violin plot
bootstrapped_data <- filtered_dataset %>%
  group_by(response_variable) %>%
  summarise(
    bootstrapped_rr = list(boot::boot(data = log_ResponseRatio, statistic = function(x, i) mean(x[i]), R = 25000)$t)
  ) %>%
  unnest(bootstrapped_rr)

# Summary statistics for plotting
summary_data <- filtered_dataset %>%
  group_by(response_variable) %>%
  summarize(
    WeightedMeanRR = mean(ResponseRatio, na.rm = TRUE),
    LowerCI = mean(ResponseRatio, na.rm = TRUE) - 1.96 * sd(ResponseRatio, na.rm = TRUE) / sqrt(n()),
    UpperCI = mean(ResponseRatio, na.rm = TRUE) + 1.96 * sd(ResponseRatio, na.rm = TRUE) / sqrt(n()),
    RR_Less_1 = mean(ResponseRatio < 1) * 100,
    RR_Greater_1 = mean(ResponseRatio > 1) * 100,
    Studies = n_distinct(id_article),
    Observations = n(),
    .groups = "drop"
  )
```

```{r}
# Ensure consistent factor levels for response_variable
common_levels <- filtered_dataset$response_variable %>% unique() %>% sort()

filtered_dataset <- filtered_dataset %>%
  mutate(response_variable = factor(response_variable, levels = common_levels))

bootstrapped_data <- bootstrapped_data %>%
  mutate(response_variable = factor(response_variable, levels = common_levels))

summary_data <- summary_data %>%
  mutate(response_variable = factor(response_variable, levels = common_levels))

# Recreate the violin plot with proper alignment
violin_plot_response_var_lnrr <- 
  ggplot() +
  # Violin plot for bootstrapped data
  geom_violin(data = bootstrapped_data, aes(y = response_variable, x = exp(bootstrapped_rr), fill = response_variable), 
              alpha = 0.5, scale = "area") + # Use 'area' scaling for better proportional representation
  # Overlay mean and confidence intervals
  # geom_point(data = summary_data, aes(y = response_variable, x = exp(WeightedMeanRR)), color = "black", size = 3) +
  # geom_errorbarh(data = summary_data, aes(y = response_variable, xmin = exp(LowerCI), xmax = exp(UpperCI)), 
  #               height = 0.2, color = "black") +
  # Add a red vertical dotted line at x = 1
  geom_vline(xintercept = 1, linetype = "dotted", color = "red", size = 0.8) +
    # Add annotations for proportions and study counts
  geom_text(data = summary_data, aes(
    y = response_variable, x = max(summary_data$UpperCI) * 0.825, 
    label = paste0("RR<1: ", round(RR_Less_1), "%\nRR>1: ", round(RR_Greater_1), "%\n[N=", Studies, ", NO=", Observations, "]")
  ), size = 3, hjust = 0) +
  # Apply custom colors
  scale_fill_manual(values = custom_colors) +
  # Customize plot
  scale_x_continuous(limits = c(0.8, 1.6), trans = "identity", breaks = scales::pretty_breaks()) +
  labs(
    title = "Weighted Mean Response Ratio",
    subtitle = "Agroforestry vs. Non-Agroforestry Effects by Response Variable",
    x = "Response Ratio",
    y = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    legend.position = "none"
  )

# Print the plot
violin_plot_response_var_lnrr
```




 
##########################################################################################################################################
PUBLICATION-READY RIDGELINE PLOTS OF EFFECT SIZE DISTRIBUTIONS PER RESPONSE VARIABLE
##########################################################################################################################################

Ridge Plot: Shows the distribution of effect sizes for each response variable.
 
```{r}
# meta_data |> glimpse()
```
```{r}
# Extract relevant columns, including vi
# effect_size_data <- meta_data %>%
#   select(response_variable, yi, vi) %>%
#   drop_na()  # Remove missing values
```

 
 








#############
# STEP 2
##########################################################################################################################################
PERFORMING MULTIVARIATE/MULTILEVEL LINEAR (MIXED-EFFECTS) MODELLING 
##########################################################################################################################################

Assessment of Missing Data for Moderators
Imputation of Missing Values for Moderators Using mice()
Post-Imputation Assessment of Moderators
Selection of Moderators for Analysis
Fitting the Multivariate Random-Effects Model with Selected Moderators

```{r}
# Define the function for missing data assessment
assess_missing_data <- function(dataset, moderators, dataset_name = "Dataset") {
  
  cat("\nStarting missing data assessment for", dataset_name, "...\n")
  
  # Step 1: Calculate the proportion of missing values for each moderator
  missing_summary <- dataset %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_proportion")

  # Print the missing summary table
  cat("\nProportion of Missing Values for Each Moderator:\n")
  print(missing_summary)

  # Step 2: Create a basic bar chart of missing proportions
  missing_plot <- ggplot(missing_summary, aes(x = reorder(variable, -missing_proportion), y = missing_proportion)) +
    geom_bar(stat = "identity", fill = "#0072B2") +
    labs(
      title = paste("Proportion of Missing Data for Moderator Variables -", dataset_name),
      x = "Moderator Variable",
      y = "Missing Proportion"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Step 3: Calculate missingness for each moderator by response_variable
  missing_by_response <- dataset %>%
    group_by(response_variable) %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = -response_variable, names_to = "moderator", values_to = "missing_proportion")

  # Print the summary table for missingness by response_variable
  cat("\nMissing Proportion by Response Variable for Each Moderator:\n")
  print(missing_by_response)

  # Step 4: Create a heatmap for missingness by response_variable
  missing_heatmap <- ggplot(missing_by_response, aes(x = moderator, y = response_variable, fill = missing_proportion)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "#56B1F7", high = "#132B43", na.value = "gray90", labels = scales::percent) +
    labs(
      title = paste("Heatmap of Missing Data by Moderator and Response Variable -", dataset_name),
      x = "Moderator Variable",
      y = "Response Variable",
      fill = "Missing Proportion"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Display the plots
  print(missing_plot)
  print(missing_heatmap)
  
  cat("\nMissing data assessment completed for", dataset_name, ".\n")
}
```

```{r}
# Assessing Moderator missingness

moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Assess missing data for non-imputed dataset
assess_missing_data(non_imp_dataset, moderators, "Non-Imputed Dataset")

# Assess missing data for imputed dataset
assess_missing_data(imp_dataset, moderators, "Imputed Dataset")
```






##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################


```{r}
# Variance-Covariance Matrix Calculation Function
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  v_list <- list()
  for (study in unique(data$id_article)) {
    study_data <- data[data$id_article == study, ]
    
    if (nrow(study_data) > 1) {
      v <- diag(study_data$vi)
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      v_list[[as.character(study)]] <- v
    } else {
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }

  v_matrix <- bldiag(v_list)
  cat("\nGenerated Variance-Covariance Matrix:\n")
  print(v_matrix)
  
  return(v_matrix)
}
```

```{r, results = 'hide'}
#########################################################################
###############################################################################
###################################################################################
########################################################################################
#############################################################################################
####################################################################################################

# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset

####################################################################################################
#############################################################################################
########################################################################################
###################################################################################
###############################################################################
#########################################################################


# Directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Generate and save v_matrices for each response variable
v_matrices <- list()

for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Calculate the variance-covariance matrix
  v_matrix <- invisible(calculate_v_matrix(data_subset, correlation = 0.5))
  
  # Store the matrix in the list
  v_matrices[[response]] <- v_matrix
  
  # Save the matrix to an individual RDS file
  file_name <- paste0("v_matrix_", tolower(gsub(" ", "_", response)), ".rds")
  saveRDS(v_matrix, file = file.path(output_dir, file_name))
  
  cat("Saved v_matrix for response variable:", response, "to", file.path(output_dir, file_name), "\n")
}

# Also, save the entire list of v_matrices as a single file
saveRDS(v_matrices, file = file.path(output_dir, "v_matrices_by_response_variable.rds"))
cat("\nAll v_matrices saved to:", output_dir, "\n")
```




#############
# STEP 3
##########################################################################################################################################
MODEL FITTING ON EACH SUBSET DATA USING ASSOCIATED VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

```{r}
# Load the saved v_matrices
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(output_dir, "v_matrices_by_response_variable.rds"))
```


```{r}
imp_dataset |> glimpse()
```
```{r}
# Summary of missing data by column
missing_summary <- imp_dataset %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "MissingCount")

# View variables with missing data
missing_summary <- missing_summary %>%
  filter(MissingCount > 0) %>%
  arrange(desc(MissingCount))

# Visualize missingness
gg_miss_var(imp_dataset) +
  labs(title = "Missing Data Across Variables")

# View rows with missing data
rows_with_missing <- imp_dataset %>%
  filter(!complete.cases(.))
```
```{r}
# Summary statistics for effect size (yi)
summary(imp_dataset$yi)

# Summary statistics for variance (vi)
summary(imp_dataset$vi)

# Identify rows with extreme variances on a sub_response_variable level
extreme_variance_rows <- imp_dataset %>%
  group_by(sub_response_variable) %>%
  filter(vi > quantile(vi, 0.95, na.rm = TRUE) | vi < quantile(vi, 0.05, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(desc(vi))

extreme_variance_rows |> glimpse()

# Last go (24/01-2025)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
#  0.00000  0.00019  0.00102  0.09496  0.00620 40.29043 
# Rows: 131
# Columns: 64
```


##########################################################################################################################################
FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

```{r}
##########################################################################################################################################
# HIERARCHICAL COMPLEXITY APPROACH ALIGNED WITH THE CABBAGE APPROACH
##########################################################################################################################################

# Start time tracking
start.time <- Sys.time()
##########################################################################
# Set up parallel processing
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)


# Function to fit models incrementally for a response variable
fit_models_all <- function(data_subset, response_variable, v_matrix, moderators, random_effects) {
  results <- list()

  cat("\nProcessing response variable:", response_variable, "\n")

  #############################################################################################
  # Null model: Global average without moderators
  results$null_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      # random = random_effects, # Remove 'random = random_effects to resemple a true null model
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in null model:", e$message, "\n")
    return(NULL)
  })
  ############################################################################################# <-------------- ! (chosen model) !
  # Minimal random effects model: Intercept-only model
  results$minimal_random_effects <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = ~ 1,  # Intercept-only model, explicitly estimates an intercept
      random = random_effects,  # Including a random effects structure
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in minimal random effects model:", e$message, "\n")
    return(NULL)
  })
  
  #############################################################################################
  # Incremental models for each moderator (no interaction):
  results$moderator_models <- map(moderators, ~ {
    moderator <- .x
    tryCatch({
      rma.mv(
        yi = yi,
        V = v_matrix,
        mods = as.formula(paste("~", moderator)),
        random = random_effects,
        data = data_subset,
        method = "REML",
        control = control_params
      )
    }, error = function(e) {
      cat("Error in moderator model for", moderator, ":", e$message, "\n")
      return(NULL)
    })
  })
  names(results$moderator_models) <- moderators
  
  ############################################################################################# 
  # Full model with all moderators (no interaction):
  results$full_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = as.formula(paste("~", paste(moderators, collapse = " + "))),
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in full model:", e$message, "\n")
    return(NULL)
  })

  ############################################################################################# 
  # Full interaction model with all moderators (with interaction): 
  results$interaction_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = as.formula(paste("~", paste(moderators, collapse = " * "))),
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in interaction model:", e$message, "\n")
    return(NULL)
  })

  return(results)
}

##########################################################################
# Fit Models for Each Response Variable
##########################################################################

# Initialize an empty list to store model results
model_results <- list()

# Loop through each response variable to fit models
for (response in names(v_matrices)) {
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Extract the variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]

  # Define the moderators to include in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture", "no_tree_per_m", "tree_height", "alley_width")
  # moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Define random effects structure 
  # Previously defined as [~ 1 | exp_id], 
  # but now defined as ~ 1 | id_article/location*experiment_year where location-year is included as a crossed random effect
  # Actually the random effects structure is now [~ 1 | id_article/location]
  
  random_effects <- list(~ 1 | id_article/exp_id)
  
  # random_effects <- list(~ 1 | id_article)
  
  # random_effects <- list(~ 1 | exp_id)
  
  # random_effects <- list(~ 1 | id_article/location * experiment_year)
  
#   random_effects <- list(
#   ~ 1 | id_article,                  # Study-level variance
#   ~ 1 | location,                    # Location variance
#   ~ 1 | experiment_year,             # Year variance
#   ~ 1 | location*experiment_year     # Interaction variance
# )
#   random_effects <- list(
#   ~ 1 | id_article,                  # Study-level variance
#   ~ 1 | location,                    # Location variance
#   ~ 1 | experiment_year              # Year variance
# )

  # Fit models incrementally using the cabbage approach
  model_results[[response]] <- fit_models_all(
    data_subset = data_subset,
    response_variable = response,
    v_matrix = v_matrix,
    moderators = moderators,
    random_effects = random_effects
  )
}

##########################################################################
# Save All Fitted Models
##########################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in a combined file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all_new.rds"))

# Save individual model results
for (response in names(model_results)) {
  saveRDS(model_results[[response]], file = file.path(output_dir, paste0("fitted_models_", response, "_new.rds")))
}

cat("\nAll models have been saved successfully!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, units(time.taken), "\n")
##########################################################################
# Last go (25/01-2025) 
# Total time taken: 31.73879 secs 

# Last go (01/03-2025) 
# Total time taken: 49.73181 secs

# Processing response variable: Biodiversity 
# Advarsel: Redundant predictors dropped from the model.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Greenhouse gas emission 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Product quality 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Crop yield 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Pest and Disease 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Soil quality 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Water quality 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.
# All models have been saved successfully!
# 
# Total time taken: 49.73181 secs
```

```{r}
# Cross-checking data
# table(meta_data$id_article, meta_data$location, meta_data$experiment_year)
# table(meta_data$location, meta_data$experiment_year)
```


Explanations of the above Multi-Model Fitting 



```{r}
# Load and assess the model results from the saved file
# This script allows evaluating both higher-level and lower-level effects of moderators for each response variable and model.

# Load the combined model results
model_results <- readRDS(file = here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "fitted_models_all_new.rds"))

# Define a function to summarize and evaluate model results
evaluate_model <- function(models, response_variable) {
  cat("\nEvaluating models for response variable:", response_variable, "\n")
  
  # Initialize a list to store evaluation results
  evaluation_results <- list()
  
  # Evaluate the null model
  if (!is.null(models$null_model)) {
    evaluation_results$null_model <- summary(models$null_model)
    cat("\nNull model:\n", paste(capture.output(evaluation_results$null_model), collapse = "\n"), "\n")
  }
  
  # Evaluate the minimal random effects model
  if (!is.null(models$minimal_random_effects)) {
    evaluation_results$minimal_random_effects <- summary(models$minimal_random_effects)
    cat("\nMinimal random effects model:\n", paste(capture.output(evaluation_results$minimal_random_effects), collapse = "\n"), "\n")
  }
  
  # Evaluate individual moderator models
  if (!is.null(models$moderator_models)) {
    evaluation_results$moderator_models <- lapply(models$moderator_models, summary)
    cat("\nModerator models:\n")
    for (moderator in names(models$moderator_models)) {
      cat("\nModerator:", moderator, "\n")
      cat(paste(capture.output(evaluation_results$moderator_models[[moderator]]), collapse = "\n"), "\n")
    }
  }
  
  # Evaluate the full model (no interaction)
  if (!is.null(models$full_model)) {
    evaluation_results$full_model <- summary(models$full_model)
    cat("\nFull model (no interaction):\n", paste(capture.output(evaluation_results$full_model), collapse = "\n"), "\n")
  }
  
  # Evaluate the full interaction model
  if (!is.null(models$interaction_model)) {
    evaluation_results$interaction_model <- summary(models$interaction_model)
    cat("\nFull interaction model:\n", paste(capture.output(evaluation_results$interaction_model), collapse = "\n"), "\n")
  }
  
  return(evaluation_results)
}

# Loop through each response variable and evaluate model results
evaluation_results <- list()

for (response in names(model_results)) {
  evaluation_results[[response]] <- evaluate_model(models = model_results[[response]], response_variable = response)
}

cat("\nModel evaluation completed.\n")


##########################################################################
# Save evaluation results for further analysis or reporting
##########################################################################

# Save combined evaluation results
saveRDS(evaluation_results, file = here::here("DATA", "OUTPUT_FROM_R", 
                                              "MULTI_MODEL_EVALUATION_RESULTS", "evaluation_results_combined.rds"))

# Save individual evaluation results
for (response in names(evaluation_results)) {
  saveRDS(evaluation_results[[response]], file = here::here("DATA", "OUTPUT_FROM_R", 
                                                            "MULTI_MODEL_EVALUATION_RESULTS", paste0("evaluation_results_", response, ".rds")))
}

cat("\nEvaluation results of the multi-model fitting have been saved successfully!\n")
```





#############
# STEP 4
##########################################################################################################################################
MODEL EVALUATION AND SELECTION
##########################################################################################################################################


```{r}
# evaluation_results$Biodiversity |> str()

# evaluation_results$Biodiversity

evaluation_results$`Crop yield`$minimal_random_effects
evaluation_results$Biodiversity$minimal_random_effects
```



```{r}
##########################################################################
# Assess AIC, BIC, and LogLik from all models
##########################################################################
##########################################################################
# HIERARCHICAL COMPLEXITY APPROACH - MODEL STATISTICS EXTRACTION
##########################################################################

# Load the combined model results
model_results <- readRDS(file = here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "fitted_models_all_new.rds"))

##########################################################################
# Function to extract AIC, BIC, and LogLik from all models
##########################################################################
extract_model_stats <- function(models, response_variable) {
  stats <- data.frame(
    Model = character(),
    AIC = numeric(),
    BIC = numeric(),
    LogLik = numeric(),
    ResponseVariable = character(),
    stringsAsFactors = FALSE
  )

  # Helper function to safely extract stats
  get_stats <- function(model) {
    if (!is.null(model)) {
      return(c(
        AIC = tryCatch(AIC(model), error = function(e) NA),
        BIC = tryCatch(BIC(model), error = function(e) NA),
        LogLik = tryCatch(logLik(model), error = function(e) NA)
      ))
    } else {
      return(c(AIC = NA, BIC = NA, LogLik = NA))
    }
  }

  # Add model statistics if available
  if (!is.null(models$null_model)) {
    stats <- rbind(stats, data.frame(
      Model = "Null Model",
      t(get_stats(models$null_model)),
      ResponseVariable = response_variable
    ))
  }

  if (!is.null(models$minimal_random_effects)) {
    stats <- rbind(stats, data.frame(
      Model = "Minimal Random Effects Model",
      t(get_stats(models$minimal_random_effects)),
      ResponseVariable = response_variable
    ))
  }

  if (!is.null(models$moderator_models)) {
    for (moderator in names(models$moderator_models)) {
      mod <- models$moderator_models[[moderator]]
      stats <- rbind(stats, data.frame(
        Model = paste("Moderator -", moderator),
        t(get_stats(mod)),
        ResponseVariable = response_variable
      ))
    }
  }

  if (!is.null(models$full_model)) {
    stats <- rbind(stats, data.frame(
      Model = "Full Model",
      t(get_stats(models$full_model)),
      ResponseVariable = response_variable
    ))
  }

  if (!is.null(models$interaction_model)) {
    stats <- rbind(stats, data.frame(
      Model = "Interaction Model",
      t(get_stats(models$interaction_model)),
      ResponseVariable = response_variable
    ))
  }

  return(stats)
}

##########################################################################
# Extract model statistics for all response variables
##########################################################################
all_model_stats <- do.call(rbind, lapply(names(model_results), function(response) {
  extract_model_stats(models = model_results[[response]], response_variable = response)
}))

all_model_stats |> glimpse()

# Excluding the incremental models for each moderator
all_model_stats_models <- all_model_stats |> 
  filter(!str_detect(Model, "Moderator"))

all_model_stats_models |> glimpse()
```

```{r}
##########################################################################
# Calculate relative AIC difference compared to Null Model for each response variable
# Function to compute adjusted relative AIC and BIC for all cases
calculate_relative_metrics <- function(model_stats) {
  model_stats %>%
    group_by(ResponseVariable) %>%
    mutate(
      # Compute relative AIC and BIC using the Null Model as a baseline
      RelativeAIC = AIC - first(AIC[Model == "Null Model"]),
      RelativeBIC = BIC - first(BIC[Model == "Null Model"]),
      
      # Determine adjustment dynamically: if AIC or BIC decreases, use absolute value transformation
      AdjustedRelativeAIC = if_else(abs(RelativeAIC) < abs(first(AIC[Model == "Null Model"])), 
                                    abs(RelativeAIC), RelativeAIC),
      AdjustedRelativeBIC = if_else(abs(RelativeBIC) < abs(first(BIC[Model == "Null Model"])), 
                                    abs(RelativeBIC), RelativeBIC)
    ) %>%
    ungroup()
}

# Apply function to compute adjusted AIC/BIC metrics
relative_aic_bic_adjusted <- calculate_relative_metrics(all_model_stats_models) |> 
  relocate(ResponseVariable, Model, AIC, BIC, RelativeAIC, RelativeBIC)

# Display adjusted AIC/BIC metrics dataset
# relative_aic_bic_adjusted |> glimpse()

# Define the file path for saving
output_file <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "relative_aic_bic_adjusted.xlsx")
# Save the dataframe to an Excel file
write.xlsx(relative_aic_bic_adjusted, file = output_file, row.names = FALSE)
cat("The relative AIC data has been saved to:", output_file, "\n")

##########################################################################
# Display adjusted AIC/BIC metrics dataset
relative_aic_bic_adjusted
```

```{r}
##########################################################################
# Create publication-ready visualizations for AIC with faceting - focusing on comparing across models
##########################################################################

# Plot AIC values for all models with faceting
plot_aic <- relative_aic_bic_adjusted |> 
  ggplot(aes(x = ResponseVariable, y = AIC, fill = ResponseVariable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  facet_wrap(~ Model, scales = "free_y") +
  coord_flip() +
  labs(
    title = "Model AIC Values Across Response Variables",
    x = "Response Variable",
    y = "AIC",
    fill = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 16),
    strip.text = element_text(face = "bold")
  )

plot_aic

##########################################################################
# Modify plot to use pseudo-log-scale on the y-axis with faceting
##########################################################################

plot_aic_log <- relative_aic_bic_adjusted |> 
  ggplot(aes(x = ResponseVariable, y = log10(AIC), fill = ResponseVariable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  facet_wrap(~ Model, scales = "free_y") +
  coord_flip() +
  labs(
    title = "Model AIC Values Across Response Variables (Log Scale)",
    x = "Response Variable",
    y = "Log10(AIC)",
    fill = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 16),
    strip.text = element_text(face = "bold")
  )

plot_aic_log
```

```{r}
##########################################################################
# Create publication-ready visualizations for AIC - focusing on comparing across response variables
##########################################################################

# Plot AIC values for all models
plot_aic_models <- relative_aic_bic_adjusted |> 
  ggplot(aes(x = fct_reorder(Model, AIC), y = AIC, fill = ResponseVariable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  coord_flip() +
  facet_wrap(~ ResponseVariable, scales = "free_x") +
  labs(
    title = "Model AIC Values Across Response Variables",
    x = "Models",
    y = "AIC",
    fill = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 16)
  )

plot_aic_models

# Modify plot to use pseudo-log-scale on the x-axis (log-transformed AIC)
plot_aic_log_models <- relative_aic_bic_adjusted |> 
  ggplot(aes(x = fct_reorder(Model, AIC), y = log10(AIC), fill = ResponseVariable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  coord_flip() +
  facet_wrap(~ ResponseVariable, scales = "free_x") +
  labs(
    title = "Model AIC Values Across Response Variables (Log Scale)",
    x = "Models",
    y = "Log10(AIC)",
    fill = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 16)
  )

plot_aic_log_models
```


```{r}
##########################################################################
# Create publication-ready visualizations - focusing specifically on relative AIC difference
##########################################################################

# Plot relative AIC values for all models
plot_relative_aic <- relative_aic_bic_adjusted |> 
  ggplot(aes(x = Model, y = RelativeAIC, fill = ResponseVariable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  coord_flip() +
  facet_wrap(~ResponseVariable, scales = "free_x") +
  labs(
    title = "Relative AIC Difference Across Models (Compared to Null Model)",
    x = "Models",
    y = "Relative AIC (Difference from Null Model)",
    fill = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 16)
  )

plot_relative_aic
```

```{r}
# Calculate mean adjusted relative AIC and BIC across all response variables for each model
model_performance_adjusted <- relative_aic_bic_adjusted |> 
  group_by(Model) |> 
  summarise(
    MeanAdjustedRelativeAIC = mean(AdjustedRelativeAIC, na.rm = TRUE),
    MeanAdjustedRelativeBIC = mean(AdjustedRelativeBIC, na.rm = TRUE)
  ) |> 
  # Sorting models based on mean adjusted relative AIC and BIC
  arrange(MeanAdjustedRelativeAIC, MeanAdjustedRelativeBIC)

# Calculate mean adjusted absolute AIC and BIC across all response variables for each model
model_performance_adjusted_absolute_all <- relative_aic_bic_adjusted |> 
  group_by(ResponseVariable) |> 
  summarise(
    AIC_absolute_all = mean(AIC, na.rm = TRUE),
    BIC_absolute_all = mean(BIC, na.rm = TRUE)
  ) |> 
  # Sorting models based on mean adjusted absolute AIC and BIC
  arrange(AIC_absolute_all, BIC_absolute_all)


# -------------------------------------------------------------------------------------------------------
# Identify the overall best-performing model based on relative adjusted criteria
best_model_adjusted <- model_performance_adjusted |> 
  filter(Model != "Null Model") |> 
  slice(1)

# Display the adjusted performance summary
model_performance_adjusted

# Highlight the adjusted best-performing model
best_model_adjusted
```


Streamlined and Transparent Interpretation of The Model Selection Based on AIC and BIC Values


The relative AIC and BIC values provide a comparison of model performance against the Null Model for each response variable. Negative relative values indicate better model performance, while positive values indicate poorer performance. Additionally, absolute AIC and BIC values provide context for the overall model fit and complexity, allowing a balanced evaluation of performance.

---

**Key Insights by Response Variable:**

1. **Biodiversity:**
   - The `Full Model` and `Interaction Model` demonstrate improved performance over the Null Model, with **relative AIC values of -3087** and **-3087**, respectively.
   - **Relative BIC values** suggest a similar improvement (-3057 for the Full Model and -3053 for the Interaction Model), reinforcing that while both models perform better, their added complexity does not drastically improve fit.
   - The `Minimal Random Effects Model` shows improvement over the Null Model, with **relative AIC of -2442**, indicating that adding random effects enhances performance.

2. **Crop Yield:**
   - The `Interaction Model` and the `Full Model` show substantial improvements, with **relative AIC values of -110037** and **-109744**, respectively.
   - The **relative BIC values (-109905 for the Interaction Model and -109705 for the Full Model)** suggest that while the Interaction Model provides better AIC performance, the BIC penalty for additional complexity is higher.
   - The `Minimal Random Effects Model` improves performance over the Null Model with **relative AIC of -108849**, showing that random effects contribute significantly.

3. **Greenhouse Gas Emissions:**
   - The `Full Model` and `Interaction Model` exhibit **exceptional improvements**, with **relative AIC values of -3152** and **-3147**, respectively.
   - **Relative BIC values (-3137 and -3129, respectively)** confirm that these models offer meaningful improvements over the Null Model while keeping model complexity in check.
   - The `Minimal Random Effects Model` shows a slight increase in AIC (+4) compared to the Null Model, suggesting no major improvement.

4. **Pest and Disease:**
   - Both the `Full Model` and the `Interaction Model` improve performance, with **relative AIC values of -5548**, but interactions do not further improve the model.
   - The **relative BIC values (-5536 for both models)** reinforce that adding interactions does not provide meaningful gains in model fit.
   - The `Minimal Random Effects Model` improves performance with **relative AIC of -5159**, indicating that random effects are important.

5. **Product Quality:**
   - The `Full Model` and `Interaction Model` show increasing AIC values compared to the Null Model, with relative AIC values of **+34** and **+77**, respectively.
   - **Relative BIC values (+60 for the Full Model and +119 for the Interaction Model)** further confirm that added complexity does not improve model fit.
   - This indicates that these models perform worse than the Null Model in explaining product quality outcomes.
   - The `Minimal Random Effects Model` improves performance with **relative AIC of -9**, making it the most parsimonious choice.

6. **Soil Quality:**
   - The `Full Model` significantly improves performance with **relative AIC value of -1788167**.
   - The `Interaction Model` further improves it to **-1788183**, but the improvement is marginal.
   - The **relative BIC values (-1788132 for the Full Model and -1788138 for the Interaction Model)** reinforce that additional complexity in interactions does not meaningfully improve performance.
   - The `Minimal Random Effects Model` improves performance significantly, with **relative AIC of -1787220**, showing that random effects are crucial.

7. **Water Quality:**
   - The `Full Model` and the `Interaction Model` exhibit **higher AIC values** compared to the Null Model, with a relative increase of **-192**.
   - **Relative BIC values (-184 for both models)** further indicate that additional complexity does not improve performance.
   - The `Minimal Random Effects Model` improves performance with **relative AIC of -208**, making it the most efficient choice.

---

**Justification for Selecting the Minimal Random Effects Model**

1. **Overall Model Performance Across Response Variables:**
   - While some response variables, such as `Crop Yield`, `Soil Quality`, and `Greenhouse Gas Emissions`, show significantly better fits using the `Full Model` and `Interaction Model`, the improvements are **not substantial enough to justify the added complexity**.
   - The `Minimal Random Effects Model` **still outperforms the Null Model** across all response variables while maintaining a much simpler structure.

2. **Balancing Complexity and Fit:**
   - The **relative AIC and BIC values demonstrate that the `Minimal Random Effects Model` captures the essential variance in the data** without overcomplicating the model.
   - The `Full Model` and `Interaction Model` introduce numerous moderator effects and interactions, which can lead to overfitting or reduced interpretability, particularly when data is scarce for certain moderator-response combinations.
   - The **BIC penalty is more severe for the `Full Model` and `Interaction Model`**, indicating that their slight improvements in AIC do not outweigh the cost of added complexity.

3. **Avoiding Overfitting and Data Limitations:**
   - Some moderator-response variable combinations have **limited data availability**, making interactions less reliable and increasing the risk of model instability.
   - The `Minimal Random Effects Model` captures **overall variance across studies while avoiding spurious interactions** that may be driven by noise rather than true underlying effects.

4. **Consistency and Interpretability:**
   - The `Minimal Random Effects Model` offers **the most stable performance across all response variables**, ensuring consistent and interpretable results.
   - Unlike the `Full Model` or `Interaction Model`, which require extensive parameter estimation, the `Minimal Random Effects Model` provides **a clear baseline understanding of effect sizes across studies**.

---

**Conclusion: Choosing the Minimal Random Effects Model**

Despite the `Full Model` and `Interaction Model` showing superior performance for some response variables, their improvements in AIC and BIC are **not substantial enough to justify the additional complexity and risk of overfitting**. The **Minimal Random Effects Model emerges as the best choice** because it provides:
   - **Substantial improvements over the Null Model**
   - **Balanced complexity and model fit**
   - **Avoidance of overfitting due to sparse data for some moderators**
   - **Stable and interpretable estimates across all response variables**

Thus, the **Minimal Random Effects Model is selected as the optimal model for this study**, offering a robust, consistent, and interpretable framework for understanding the effects across multiple response variables.



#############
# STEP 5
##########################################################################################################################################
TESTING DIFFERENT RANDOM EFFECTS STRUCTURES FOR THE CHOSEN MODEL (MINIMAL RANDOM EFFECTS MODEL)
##########################################################################################################################################

```{r}
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Set up parallel processing
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset

# Define control parameters for optimization
control_params <- list(optimizer = "optim", 
                       method = "BFGS", 
                       iter.max = 10000, 
                       rel.tol = 1e-12)

# Define different random effects structures manually

# Simple models
model_study <- rma.mv(yi = meta_data$yi, 
                      V = vi, 
                      mods = ~ 1, 
                      random = ~ 1 | id_article, 
                      data = meta_data, 
                      method = "REML", 
                      control = control_params)

model_year <- rma.mv(yi = meta_data$yi, 
                     V = vi, 
                     mods = ~ 1,
                     random = ~ 1 | experiment_year, 
                     data = meta_data, 
                     method = "REML", 
                     control = control_params)

model_location <- rma.mv(yi = meta_data$yi, 
                         V = vi, 
                         mods = ~ 1,
                         random = ~ 1 | location, 
                         data = meta_data, 
                         method = "REML", 
                         control = control_params)

model_experiment <- rma.mv(yi = meta_data$yi, 
                           V = vi,  
                           mods = ~ 1, 
                           random = ~ 1 | exp_id, 
                           data = meta_data, 
                           method = "REML", 
                           control = control_params)

# Nested models 
model_study_experiment <- rma.mv(yi = meta_data$yi,   # <-------------- ! (chosen model) !
                                 V = vi,  
                                 mods = ~ 1, 
                                 random = ~ 1 | id_article/exp_id, 
                                 data = meta_data, 
                                 method = "REML", 
                                 control = control_params)

model_study_location <- rma.mv(yi = meta_data$yi,
                               V = vi,  
                               mods = ~ 1, 
                               random = ~ 1 | id_article/location, 
                               data = meta_data, 
                               method = "REML", 
                               control = control_params)

model_location_year <- rma.mv(yi = meta_data$yi,
                              V = vi, 
                              mods = ~ 1,
                              random = ~ 1 | location/experiment_year, 
                              data = meta_data, 
                              method = "REML", 
                              control = control_params)

# Crossed models
model_study_location_year_crossed <- rma.mv(yi = meta_data$yi, 
                                            V = vi, 
                                            mods = ~ 1,
                                            random = list(~ 1 | id_article, 
                                                          ~ 1 | location, 
                                                          ~ 1 | experiment_year),  
                                            data = meta_data, 
                                            method = "REML", 
                                            control = control_params)

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, units(time.taken), "\n")
##########################################################################
# Last go (04/02-2025) 
# Total time taken: 6.676552 mins 
```

```{r}
# Function to extract AIC.REML values safely
extract_aic <- function(model) {
  if (!is.null(model) && "AIC" %in% rownames(model$fit.stats)) {
    return(model$fit.stats["AIC", "REML"])  # Ensure we extract REML AIC correctly
  } else {
    return(NA)
  }
}

# Create a data frame to store AIC values correctly
aic_values <- data.frame(
  Model = c("Simple Study-Level",               # model_study
            "Experiment Only",                  # model_experiment
            "Location Only",                    # model_location
            "Year Only",                        # model_year
            "Study and Experiment",             # model_study_experiment - like the originally chosen model setup 'minimal random effects model'
            "Study and Location",               # model_location
            "Location and Year",                # model_year
            "Study, Location, Year (Crossed)"), # model_study_location_year_crossed
  AIC_REML = c(
    extract_aic(model_study),
    extract_aic(model_experiment),
    extract_aic(model_location),
    extract_aic(model_year),
    extract_aic(model_study_experiment),
    extract_aic(model_study_location),
    extract_aic(model_location_year),
    extract_aic(model_study_location_year_crossed)
  )
) %>%
  arrange(AIC_REML)

# Print model comparison
aic_values
```

```{r}
model_study_experiment$fit.stats
model_experiment$fit.stats

model_study_location_year_crossed$fit.stats
```










#############
# STEP 6
##########################################################################################################################################
BACK-TRANSFORM MODEL RESULTS FROM LOG-ROM TO PERCENTAGE FOR ALL RESPONSE VARIABLES
##########################################################################################################################################


```{r}
##########################################################################################################################################
# Function to Back-Transform log-ROM to ROM in Percentage for All Response Variables
##########################################################################################################################################

# Define the function
back_transform_logROM <- function(evaluation_results) {
  # Initialize an empty list to store back-transformed results
  back_transformed_results <- list()

  # Iterate through each response variable in evaluation_results
  for (response in names(evaluation_results)) {
    cat("Processing response variable:", response, "\n")

    # Extract models for the response variable
    models <- evaluation_results[[response]]

    # Initialize a list to store back-transformed values for this response variable
    response_results <- list()

    # Define a helper function to back-transform log-ROM values
    back_transform <- function(estimate, ci.lb, ci.ub) {
      list(
        ROM_percent = (exp(estimate) - 1) * 100,  # Convert to percentage
        ROM_lower_percent = (exp(ci.lb) - 1) * 100,
        ROM_upper_percent = (exp(ci.ub) - 1) * 100
      )
    }

    # Iterate through all models (null, minimal, moderators, full, interaction)
    for (model_name in names(models)) {
      model <- models[[model_name]]

      # Skip if the model is NULL
      if (is.null(model)) {
        response_results[[model_name]] <- NULL
        next
      }

      # Extract estimates and confidence intervals
      if (!is.null(model$b)) {
        response_results[[model_name]] <- back_transform(
          estimate = model$b[1],
          ci.lb = model$ci.lb[1],
          ci.ub = model$ci.ub[1]
        )
      } else {
        response_results[[model_name]] <- NULL
      }
    }

    # Store the back-transformed results for this response variable
    back_transformed_results[[response]] <- response_results
  }

  return(back_transformed_results)
}

##########################################################################################################################################
# Example: Apply the function to evaluation_results
##########################################################################################################################################

# Assuming evaluation_results is already loaded
back_transformed_results <- back_transform_logROM(evaluation_results)

# Inspect the back-transformed results for a specific response variable
print(back_transformed_results$Biodiversity)

##########################################################################################################################################
```

Interpretation of Back-Transformed Values (ROM in Percentages) for All Response Variables

The back-transformed Ratio of Means (ROM) values provide an interpretable measure of effect sizes across different response variables. These values indicate how the studied system influences each response variable compared to controls, with associated confidence intervals (CIs) reflecting uncertainty.

Key Differences Between the Minimal Random Effects Model and the Interaction Model

**Crop Yield**
- Minimal Random Effects Model: -2.36% [CI: -5.50%, 0.89%]
- Full Model: -1.38% [CI: -6.44%, 3.95%]
- Interaction Model: -24.47% [CI: -36.27%, -10.48%]
- The minimal model suggests a small yield reduction (-2.36%), while the full model slightly reduces this effect. However, the interaction model indicates a significantly larger decline (-24.47%), suggesting that moderator interactions strongly amplify the negative impact on crop yield.

**Biodiversity**
- Minimal Random Effects Model: +3.89% [CI: -1.68%, 9.78%]
- Full Model: -8.35% [CI: -62.21%, 122.24%]
- Interaction Model: -2.78% [CI: -64.38%, 165.35%]
- The minimal model suggests a slight positive impact (+3.89%), while the full model shifts to a stronger negative effect (-8.35%), though with a highly uncertain CI. The interaction model further complicates interpretation, with a very wide CI indicating substantial uncertainty.

**Soil Quality**
- Minimal Random Effects Model: +3.32% [CI: 0.36%, 6.37%]
- Full Model: -33.94% [CI: -54.82%, -3.42%]
- Interaction Model: -10.80% [CI: -16.00%, -5.28%]
- The minimal model suggests a slight positive effect, while the full model reverses this trend and shows a significant decline (-33.94%). The interaction model indicates a similar negative trend, but with less extreme values. This suggests that moderators collectively contribute to a decline in soil quality, though interactions slightly mitigate the extent.

**Greenhouse Gas Emissions**
- Minimal Random Effects Model: +0.94% [CI: 0.69%, 1.20%]
- Full Model: +25.73% [CI: 12.47%, 40.54%]
- Interaction Model: +25.58% [CI: 12.26%, 40.48%]
- The minimal model estimates only a minor increase (+0.94%), while both the full and interaction models produce a substantially larger increase (+25.73% and +25.58%). The nearly identical estimates suggest that interaction terms do not provide additional explanatory power beyond what is already captured by the full model.

**Product Quality**
- Minimal Random Effects Model: -2.15% [CI: -4.26%, 0.01%]
- Full Model: -1.58% [CI: -7.77%, 5.03%]
- Interaction Model: -0.80% [CI: -13.13%, 13.29%]
- The minimal model suggests a slight decrease in product quality (-2.15%), while the full model provides a smaller negative effect (-1.58%). The interaction model shifts further toward neutrality (-0.80%) but with greater uncertainty, indicating that interaction effects do not strongly influence product quality but introduce additional variability.

**Pest and Disease**
- Minimal Random Effects Model: -11.35% [CI: -36.92%, 24.59%]
- Full Model: -40.47% [CI: -61.94%, -6.90%]
- Interaction Model: -40.47% [CI: -61.94%, -6.90%]
- The minimal model suggests a modest reduction in pest and disease (-11.35%), while both the full and interaction models show a much stronger decrease (-40.47%). This indicates that including moderators substantially improves explanatory power, but interactions do not provide additional benefits beyond the full model.

**Water Quality**
- Minimal Random Effects Model: +1.56% [CI: -1.35%, 4.56%]
- Full Model: +3.08% [CI: -1.27%, 7.62%]
- Interaction Model: +3.08% [CI: -1.27%, 7.62%]
- The minimal model suggests a small positive effect, and both the full and interaction models slightly increase this estimate. The similar confidence intervals suggest that interactions do not strongly affect water quality but introduce minor variability.

*General Observations*
1. Significant Amplification in Some Response Variables:
Greenhouse gas emissions, pest and disease, and crop yield see the most substantial changes under the interaction model, suggesting that interactions between moderators strongly influence these variables.

2. Uncertainty and Variability:
Biodiversity, product quality, and soil quality show directional shifts, where the minimal model suggests a mild impact, but the interaction model introduces uncertainty or reverses the effect.
Water quality remains relatively stable, with minimal effects from moderator interactions.

3. Trade-off Between Complexity and Interpretability:
While the interaction model provides deeper insights into how different factors interact, it introduces substantial uncertainty in some cases.
The minimal random effects model provides more stable estimates with narrower confidence intervals, making it a more reliable choice in cases where data limitations exist.

*Conclusion: Selecting the Minimal Random Effects Model*
- The **Minimal Random Effects Model** provides a more balanced trade-off between stability and interpretability.
- For certain response variables (greenhouse gas emissions, crop yield, pest and disease), interactions among moderators play a crucial role, but for others, their effects remain uncertain.
- Given the increased uncertainty in the interaction model, the **Minimal Random Effects Model** remains the preferred approach, as it captures key moderator effects without adding unnecessary complexity.
- Future analyses could explore specific interactions in a more targeted manner rather than including all potential interactions at once.



```{r}
##########################################################################################################################################
# Combine Raw Effect Sizes and Back-Transformed Values into a Structured Data Frame
##########################################################################################################################################

# Function to create a structured data frame
combine_effect_sizes <- function(evaluation_results, back_transformed_results) {
  combined_results <- data.frame(
    ResponseVariable = character(),
    Model = character(),
    Estimate = numeric(),
    CI_Lower = numeric(),
    CI_Upper = numeric(),
    P_Value = character(),
    Significance = character(),
    ROM_Percent = numeric(),
    ROM_Lower_Percent = numeric(),
    ROM_Upper_Percent = numeric(),
    stringsAsFactors = FALSE
  )

  for (response in names(evaluation_results)) {
    models <- evaluation_results[[response]]
    back_transformed <- back_transformed_results[[response]]

    for (model_name in names(models)) {
      model <- models[[model_name]]
      if (!is.null(model) && !is.null(model$b) && !is.null(model$ci.lb) && !is.null(model$pval)) {
        p_value <- ifelse(model$pval[1] < 0.001, "<0.001", formatC(model$pval[1], format = "f", digits = 3))
        significance <- if (model$pval[1] < 0.001) {
          "***"
        } else if (model$pval[1] < 0.01) {
          "**"
        } else if (model$pval[1] < 0.05) {
          "*"
        } else if (model$pval[1] < 0.1) {
          "."
        } else {
          " "
        }

        combined_results <- rbind(combined_results, data.frame(
          ResponseVariable = response,
          Model = model_name,
          Estimate = model$b[1],
          CI_Lower = model$ci.lb[1],
          CI_Upper = model$ci.ub[1],
          P_Value = p_value,
          Significance = significance,
          ROM_Percent = back_transformed[[model_name]]$ROM_percent,
          ROM_Lower_Percent = back_transformed[[model_name]]$ROM_lower_percent,
          ROM_Upper_Percent = back_transformed[[model_name]]$ROM_upper_percent
        ))
      }
    }
  }

  return(combined_results)
}


# Apply the function
structured_results <- combine_effect_sizes(evaluation_results, back_transformed_results)


output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# Save structured_results in a combined file
saveRDS(structured_results, file = file.path(output_dir, "structured_results_all_effect_sizes.rds"))



# Inspect the structured data frame
structured_results |> glimpse()
```


```{r}
##########################################################################################################################################
# Create a Publication-Ready gt Table for Back-Transformed Values
##########################################################################################################################################


# Function to create a gt table for ROM values
create_gt_table <- function(data) {
  data |> 
    dplyr::select(ResponseVariable, Model, ROM_Percent, ROM_Lower_Percent, ROM_Upper_Percent, P_Value, Significance) |> 
    gt() |> 
    tab_header(
      title = "Back-Transformed Values (ROM in Percentages)",
      subtitle = "Includes Estimates, Confidence Intervals, P-Values, and Significance Levels"
    ) |> 
    cols_label(
      ResponseVariable = "Response Variable",
      Model = "Model",
      ROM_Percent = "ROM (%)",
      ROM_Lower_Percent = "Lower CI (%)",
      ROM_Upper_Percent = "Upper CI (%)",
      P_Value = "P-Value",
      Significance = "Significance"
    ) |> 
    fmt_number(
      columns = c(ROM_Percent, ROM_Lower_Percent, ROM_Upper_Percent),
      decimals = 2
    ) |> 
    tab_options(
      table.font.size = "small",
      heading.align = "center"
    )
}

# Create the gt table
back_transformed_table <- create_gt_table(structured_results)

# Render the table
back_transformed_table
```


```{r}
##########################################################################################################################################
# Filter for the Best-Performing Model (Interaction Model) and Create a gt Table
##########################################################################################################################################

# Choose the best-performing model based on adjusted relative AIC and BIC
chosen_model <- "minimal_random_effects"                                   # <-------------- ! (chosen model) !

# Filter structured results for the Interaction Model
best_model_results <- structured_results |> 
  dplyr::filter(Model == chosen_model)

# Create the gt table for the Interaction Model
best_model_gt_table <- create_gt_table(best_model_results) |> 
  cols_hide("Model")

# Render the table
best_model_gt_table
```


#############
# STEP 7
##########################################################################################################################################
META-ANALYSIS MODEL DIAGNOSTICS - HETEROGENEITY STATISTICS
##########################################################################################################################################

```{r}
# model_results |> str()
# model_results$`Crop yield` |> str()

model_results$`Crop yield`
```



```{r}
##########################################################################################################################################
# A: Extract heterogeneity statistics for each response variable
##########################################################################################################################################
extract_heterogeneity_stats <- function(model_results) {
  heterogeneity_stats <- lapply(names(model_results), function(response) {
    
    model <- model_results[[response]]$minimal_random_effects    # <------------------------------- !(chosen model)!
    if (is.null(model)) return(NULL)

    # Calculate I2 manually if tau2 is 0
    QE <- model$QE
    df <- model$QEdf
    I2 <- if (QE > df) (QE - df) / QE * 100 else 0

    data.frame(
      Response_Variable = response,
      I2 = I2,
      Tau2 = model$tau2,
      QE = QE,
      QE_pval = model$QEp,
      stringsAsFactors = FALSE
    )
  })

  # Combine results into a single data frame
  do.call(rbind, heterogeneity_stats)
}

# Extract heterogeneity statistics
heterogeneity_stats <- extract_heterogeneity_stats(model_results)

# Inspect the results
heterogeneity_stats
```

Interpretation of Heterogeneity Statistics

The heterogeneity results reveal key insights into how variability in effect sizes is distributed across response variables. This analysis is crucial for assessing the degree of unexplained variability and determining whether additional moderators or model refinements are necessary.
It is common to use the I² statistic to report the between-study heterogeneity in meta-analyses, and I² is included by default in the output we get from {meta}. The popularity of this statistic may be associated with the fact that there is a “rule of thumb” on how we can interpret it (J. P. Higgins and Thompson 2002):
I² = 25%: low heterogeneity, I² = 50%: moderate heterogeneity, I² = 75%: substantial heterogeneity.
Tau² quantifies the variance of the true effect sizes underlying our data. When we take the square root of Tau², we obtain τ, which is the standard deviation of the true effect sizes. A great asset of τ is that it is expressed on the same scale as the effect size metric. This means that we can interpret it in the same as one would interpret, for example, the mean and standard deviation of the sample’s age in a primary study. The value of τ tells us something about the range of the true effect sizes. If τ is large, the true effect sizes are spread out over a wide range, and if τ is small, the true effect sizes are clustered around the mean effect size.

**1. I² (Proportion of Total Variability Due to Heterogeneity)**
Extremely High Heterogeneity: Biodiversity (99.98%), Crop Yield (99.86%), Pest and Disease (99.94%), and Soil Quality (99.99%) show nearly complete heterogeneity, indicating that most of the observed variation in effect sizes is not due to random sampling error but due to real between-study differences.
Greenhouse Gas Emissions (98.42%) and Water Quality (94.23%) also display substantial heterogeneity, though slightly lower than the highest group.
Moderate Heterogeneity: Product Quality (38.11%) exhibits considerably lower heterogeneity compared to other response variables. This suggests that much of the variation in Product Quality effects is due to sampling error rather than between-study differences.
Implication:

Response variables with I² values close to 100% suggest that substantial heterogeneity remains unexplained, meaning the random-effects variance component does not fully capture it. For Product Quality, the moderate I² suggests the model is capturing a fair amount of variation, reducing the need for additional moderators.

**2. Tau² (Estimated Between-Study Variance)**
Tau² = 0 for All Response Variables: The fact that Tau² is zero for all response variables suggests that the model does not attribute any of the heterogeneity to true between-study variance. This could mean one of two things:
- The actual between-study variance is minimal, meaning the observed variation is due to moderators or within-study variation.
- The model lacks the statistical power to detect true between-study variance, which may be improved by incorporating additional moderators.

Implication:
The zero Tau² suggests that random effects alone are not sufficient to explain heterogeneity, reinforcing the need for additional fixed effects (moderators) or more flexible modeling approaches.

**3. Cochran's Q (QE) and Its p-value**
The QE statistic tests for residual heterogeneity after accounting for random effects. A large QE with a significant p-value suggests that heterogeneity remains unexplained. Findings: Extremely Large QE values for Biodiversity (881,409), Crop Yield (203,353), and Soil Quality (1,926,528) strongly indicate that residual heterogeneity is highly significant. All p-values are < 0.001, confirming that residual heterogeneity is not due to chance.
Even for Product Quality, which has the lowest I², the QE value (171.28) is significant (p = 6.13e-05), reinforcing that heterogeneity is present, albeit at a lower level.

Implication:
The model (minimal_random_effects), in its current form, fail to explain much of the heterogeneity, suggesting that additional factors (such as environmental, management, or study design-related moderators) should be explored. The significance of QE across all response variables confirms that residual heterogeneity must be addressed.

**4. Practical Implications and Next Steps**
For Biodiversity, Crop Yield, Pest and Disease, and Soil Quality: These variables exhibit extreme heterogeneity, suggesting that additional moderators, interaction terms, or alternative model structures should be considered. The fact that Tau² is zero despite extreme I² and significant QE suggests that the random-effects model alone does not sufficiently capture variability.
Potential solutions: Exploring additional study-level covariates (e.g., environmental conditions, study duration). Testing alternative variance structures to better account for heterogeneity. 
For Greenhouse Gas Emissions and Water Quality: Despite being slightly lower than the extreme group, these response variables still exhibit very high heterogeneity. The same strategies as above (additional moderators or interaction terms) should be considered.
For Product Quality: With a lower I² and a significant but more moderate QE, the current model explains variability better here than for other response variables.
This suggests that existing moderators may already be capturing much of the relevant variation.




#############
# STEP 8
##########################################################################################################################################
FOREST PLOTS FOR THE MINIMAL RANDOM EFFECTS MODEL (NOT THE FINAL FOREST PLOT)
##########################################################################################################################################


```{r}
# Initialize a list to store summaries
model_summaries <- list()
 
# Loop over each response variable to get the summaries
for (response in names(model_results)) {
  model_summaries[[response]] <- list(
    null_model_summary = summary(model_results[[response]]$null_model),
    minimal_random_effects_summary = summary(model_results[[response]]$minimal_random_effects),
    full_model_summary = summary(model_results[[response]]$full_model),
    interaction_model_summary = summary(model_results[[response]]$interaction_model),
    moderator_model_summaries = lapply(model_results[[response]]$moderator_models, summary)
  )
}
 
# Now model_summaries contains summaries of all the models
```

```{r}
# Loop over each response variable and print the summary for the minimal random effects model
for (response in names(model_summaries)) {
  cat("\nModel Summaries for", response, ":\n")
  # Print the minimal random effects model summary
  cat("\nMinimal Random Effects Model Summary:\n")
  print(model_summaries[[response]]$minimal_random_effects_summary)
}
```

```{r}
# Function to compute prediction intervals
compute_prediction_intervals <- function(model_results, conf_level = 0.95) {
  pi_list <- lapply(names(model_results), function(response) {
    model <- model_results[[response]]$minimal_random_effects  # Selected model
    if (is.null(model)) return(NULL)  # Skip NULL models
    
    # Extract key values
    estimate <- model$b[1]
    tau2 <- ifelse(length(model$tau2) > 0, model$tau2, 0)  # Handle missing tau2
    se <- ifelse(length(model$se) > 0, model$se, 0)  # Handle missing SE
    
    # Calculate prediction interval
    lower_PI <- estimate - qnorm(1 - (1 - conf_level) / 2) * sqrt(se^2 + tau2)
    upper_PI <- estimate + qnorm(1 - (1 - conf_level) / 2) * sqrt(se^2 + tau2)
    
    data.frame(
      Response_Variable = response,
      Estimate = estimate,
      Lower_PI = lower_PI,
      Upper_PI = upper_PI
    )
  })

  # Remove NULL values and combine into a data frame
  pi_list <- Filter(Negate(is.null), pi_list)
  if (length(pi_list) == 0) return(NULL)  # Return NULL if all responses were NULL

  return(do.call(rbind, pi_list))
}

# Compute prediction intervals
prediction_intervals <- compute_prediction_intervals(model_results)

# Print results
print(prediction_intervals)

```

```{r}
# Example using metafor
predict_intervals_metafor <- function(model) {
  preds <- predict(model, transf = exp)  # If using log-transformed effects, back-transform
  data.frame(
    Estimate = preds$pred,
    Lower_PI = preds$pi.lb,
    Upper_PI = preds$pi.ub
  )
}

# Apply to a metafor model (e.g., minimal random effects model)
pi_results <- predict_intervals_metafor(model_results$Biodiversity$minimal_random_effects)
print(pi_results)
```


```{r}
# 1. Generalized Function to Extract and Summarize Data

# Function to prepare and summarize forest plot data
prepare_forest_data <- function(model) {
  if (is.null(model)) return(NULL)

  # Extract study-level effect sizes, variances, and study labels
  study_effects <- model$yi
  study_variances <- model$vi
  study_labels <- model$data$id_study  

  # Create study-level data frame
  study_data <- data.frame(
    Study = as.character(study_labels),
    ES = study_effects,
    SE = sqrt(study_variances),
    Type = "Study"
  ) |> 
    group_by(Study) |> 
    summarise(
      Mean_ES = mean(ES, na.rm = TRUE),  # Mean effect size per study
      Pooled_SE = sqrt(sum(SE^2, na.rm = TRUE) / n()),  # Pooled standard error
      .groups = "drop"
    ) |> 
    mutate(
      Lower_CI = Mean_ES - 1.96 * Pooled_SE,
      Upper_CI = Mean_ES + 1.96 * Pooled_SE,
      Type = "Study"
    )

  # Extract overall summary estimate from the model
  summary_data <- data.frame(
    Study = "Summary",
    Mean_ES = model$b[1],  # Summary effect size from model
    Pooled_SE = model$se,  # Model SE
    Type = "Summary"
  ) |> 
    mutate(
      Lower_CI = Mean_ES - 1.96 * Pooled_SE,
      Upper_CI = Mean_ES + 1.96 * Pooled_SE
    )

  # Combine study-level and summary data
  forest_data <- bind_rows(study_data, summary_data)

  # Ensure the ordering of the study factor (so that Summary appears last)
  forest_data$Study <- factor(forest_data$Study, levels = rev(unique(forest_data$Study)))

  return(forest_data)
}

# 2. Back-Transformation of Effect Sizes

# Function to apply back-transformation
apply_back_transformation <- function(forest_data) {
  forest_data |> 
    mutate(
      Mean_ES = (exp(Mean_ES) - 1) * 100,  # Convert log-ROM to percentage change
      Lower_CI = (exp(Lower_CI) - 1) * 100,
      Upper_CI = (exp(Upper_CI) - 1) * 100
    )
}

# 3. Forest Plot Function (ggplot)

# Function to create back-transformed forest plot
plot_forest <- function(forest_data, response_variable) {
  ggplot(forest_data, aes(x = Study, y = Mean_ES, ymin = Lower_CI, ymax = Upper_CI, color = Type)) +
    geom_pointrange(size = 0.7) +  
    geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +  
    coord_flip() +  
    scale_color_manual(values = c("Study" = "grey50", "Summary" = "black")) +  
    labs(
      title = paste("Forest Plot:", response_variable, "(Back-Transformed)"),
      x = "Study",
      y = "Effect Size (% Change from Control)",
      color = "Type"
    ) +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text.y = element_text(size = 10),
      axis.title.y = element_text(size = 12, face = "bold"),
      axis.title.x = element_text(size = 12, face = "bold")
    )
}
```

 
```{r}
# 4. Applying to Multiple Response Variables

# Iterate over all response variables
response_variables <- names(model_results)

# Generate forest plots for each response variable
forest_plots <- lapply(response_variables, function(response) {
  model <- model_results[[response]]$minimal_random_effects  # Select model
  forest_data <- prepare_forest_data(model)  # Extract data
  forest_data <- apply_back_transformation(forest_data)  # Apply back-transformation
  plot_forest(forest_data, response)  # Create plot
})

# forest_plots |> str()
# Display a specific forest plot, e.g., for Biodiversity
forest_plots
```



 
 
 
#############
# STEP 9
##########################################################################################################################################
EXTRACT MODEL WEIGHTS PER RESPONSE VARIABLE 
##########################################################################################################################################

 
 
##########################################################################################################################################
SAVING DATASETS AND MODEL OBJECTS
##########################################################################################################################################

 
 

Improvements and Adjustments
To address collinearity, pre-analysis checks such as variance inflation factors (VIF) or pairwise correlation matrices should be performed. Moderators with high collinearity or sparse data should be excluded or merged. Missing data can be handled using imputation methods or treated as a separate category. Log-transforming or standardizing `yi` and moderators could stabilize models with high variance ratios, while robust variance estimators, such as Knapp-Hartung adjustments, may improve reliability. Reducing the complexity of interaction models by focusing on key moderator pairs (e.g., `tree_type * crop_type`) rather than all interactions could improve convergence.

Model Diagnostics and Reporting
For each response variable, heterogeneity was assessed using statistics such as `I²` and `τ²`, and model fits were compared using AIC and BIC. Funnel plots, Egger’s test, and leave-one-out sensitivity analyses were conducted to evaluate publication bias and model robustness. Failed models were documented to ensure transparency, and systematic adjustments were reported to improve reproducibility. This approach balances explanatory power and interpretability, ensuring that the hierarchical complexity of moderators is appropriately captured.








