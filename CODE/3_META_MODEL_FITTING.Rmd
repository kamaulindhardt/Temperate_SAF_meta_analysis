---
title: "3_META_MODEL_FITTING"
author: "M.K.K. Lindhardt"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

  # Load multiple add-on packages using pacman::p_load for efficiency
  pacman::p_load(
    conflicted,
    # Data Manipulation / Transformation
    tidyverse,        # Comprehensive collection of R packages for data science
    readr,            # Read and write csv 
    future,           # Parallel processing
    future.apply,     # Parallel processing
    ###################################################################################################################
    # Data Visualization
    ggplot2,          # Data visualization package (part of tidyverse)
    patchwork,        # ggplot2 API for sequentially building up a plot
    gridExtra,
    scales,
    ###################################################################################################################
    # Meta-Analysis
    metafor,          # For conducting meta-analysis, effect sizes, and response ratios
    clubSandwich,     # Cluster-robust variance estimators for ordinary and weighted least squares linear regression models
    ###################################################################################################################
    # Exploratory Data Analysis (EDA)
    DataExplorer,     # For exploratory data analysis
    SmartEDA,         # For smart exploratory data analysis
    skimr,
    naniar,
    VIM,
    Hmisc,
    BaylorEdPsych,
    ###################################################################################################################
    # Project Management and Code Styling
    here,             # Easy file referencing using the top-level directory of a file project
    styler            # For code formatting and styling
  )
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
```



#############
# STEP 1
##########################################################################################################################################
LOADING PREPARED META-DATA
##########################################################################################################################################


Loading the two datasets (imputed and non-imputed)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Define file paths
non_imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom.rds"))
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))

non_imp_data_rom_dummy <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom_dummy.rds"))

# Read in the non-imputed dataset
non_imp_dataset <- non_imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Read in the imputed dataset
imp_dataset <- imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


# Read in the dummy version of non-imputed dataset
non_imp_data_dummy <- non_imp_data_rom_dummy %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    #location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
})
```

```{r}
# Checking high observations with extreme high variance
high_variance_obs <- 
  imp_dataset|> 
  filter(vi > quantile(vi, 0.995)) |> # 0.995
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n,
           tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width) |> 
  arrange(id_article, response_variable)

skim(high_variance_obs)
```
```{r}
non_imp_dataset |> glimpse()
imp_dataset |> glimpse()
```

```{r}
# Select distinct id_article entries
distinct_articles <- imp_dataset %>%
  select(id_article) %>%
  distinct()

# Get the number of unique id_article entries
num_distinct_articles <- nrow(distinct_articles)

# Print the result
cat("Number of distinct id_article entries:", num_distinct_articles, "\n")
```


*Points to address after the Tshering meeting with her supervisor (26/11-2024)*

* Subgroup Analysis:
Keep subgroup analyses for each response_variable (e.g., biodiversity, crop yield) to respect conceptual differences. This ensures tailored insights into unique trends and moderators like tree_type or crop_type. Account for reduced statistical power in smaller subgroups by adjusting for multiple comparisons (e.g., Bonferroni). Compare heterogeneity metrics (Q, I², τ²) across subgroups to identify variability differences.

* Meta-Regression:
Use meta-regression as a complementary step to capture overarching patterns. Include response_variable as a moderator and test interactions (e.g., tree_type * response_variable) to detect shared vs. outcome-specific effects. Start with a simpler model and add key moderators incrementally to maintain interpretability.

* Heterogeneity Comparison:
Compare heterogeneity metrics (I², τ²) between original and imputed datasets to assess the impact of imputation. Highlight changes in variability and their implications.

* Visualizations:
Simplify forest plots, focusing on clarity (e.g., one plot per response variable). Use ggplot for clean annotations, log-scaled axes (if relevant), and consistent aesthetics. Add funnel plots for individual response variable models and meta-regression to assess bias and precision.

*Points to address after the meeting with Maarit (03/12-2024*

#### **1. Imputation**
   - **Evaluate Missingness:** 
     - Check patterns of missingness in the original data and verify study reliability.
     - Assess missingness in moderator-response variable combinations.
   - **Non-Random Missingness:** 
     - Test for non-random missingness in standard deviations for moderator-response combinations.
   - **Imputation Methodology:**
     - Use robust methods (e.g., upper quartile) for imputation if data quality is uncertain.
     - Avoid imputing response variables, moderators, or fixed-effect variables (especially continuous ones).
   - **Model Comparison:**
     - Compare model results using imputed vs. non-imputed datasets to assess imputation impact.

---

#### **2. Moderators**
   - Ensure a minimum of 10 studies per sublevel of moderators for sufficient statistical power.

---

#### **3. Random Effects**
   - **Revise `exp_id`:**
     - Aggregate location data within `exp_id` to simplify its structure.
   - **Simplify Random Effects:**
     - Remove unnecessary random effects (`id_article/response_variable`) if `exp_id` sufficiently captures random intercepts/slopes.

---

#### **4. Year as a Fixed Effect**
   - Add standardized year as a fixed-effect, continuous variable to capture potential slope effects.
   - Validate the structure and missingness of the `exp_id` variable before inclusion.
   - "This morning, I worked on our database to determine which level we can use for the location. Since we are already focusing on the temperate climatic zone, I thought it would be good to use the country as the location."
---

#### **5. Multilevel Modelling vs. Subgroup Analysis**
   - Use **multilevel modelling** to account for incomplete combinations of moderators and response variables.
   - Multilevel modelling is valid for disentangling hierarchical structures but, for transparency and communication purposes, subgroup meta-analysis will remain the primary approach.
   - Consider specific interaction terms between moderators after including additive effects in the model.

---

#### **6. Bias Assessment**
   - Test for publication bias and variance error bias for each response variable and moderator.

---

#### **7. Global Mean Comparisons**
   - Include a global mean to compare overall responses, particularly in cases of sign changes across responses.

---

#### **8. Model Diagnostics**
   - **Delta AIC:**
     - Use delta AIC to compare model performance between alternative specifications.
   - **Influence Diagnostics:**
     - Perform diagnostics on a study level using the `nlme` package with `id_article` as the grouping variable.


Key Action Points
	1. Check and Address Missing Data:
		○ Assess patterns of missingness and trustworthiness of studies.
		○ Implement robust imputation for error values only.
	2. Aggregate and Simplify:
		○ Aggregate soil texture levels and location data within exp_id.
	3. Review Model Structure:
		○ Simplify random effects.
		○ Include standardized year as a fixed effect.
	4. Evaluate Bias:
		○ Perform publication and variance error bias assessments.
	5. Use Multilevel Models:
		○ Implement multilevel modeling to manage hierarchical structures.
	6. Conduct Diagnostics:
		○ Apply influence diagnostics at the study level (nlme).
	7. Include Interaction Terms:
		○ Incorporate interactions only after main effects in the model.
	8. Global Response Comparisons:
Use a global mean for overarching response evaluations.

##########################################################################################################################################
LISTING RESPONSE VARIABLES AND SETTING UP COSTUM COLORS
##########################################################################################################################################

```{r}
# Custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Response variables to analyze
response_variables <- names(custom_colors)
```




#############
# STEP 2
##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################


```{r}
# Load required package
library(here)

# Variance-Covariance Matrix Calculation Function
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  v_list <- list()
  for (study in unique(data$id_article)) {
    study_data <- data[data$id_article == study, ]
    
    if (nrow(study_data) > 1) {
      v <- diag(study_data$vi)
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      v_list[[as.character(study)]] <- v
    } else {
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }

  v_matrix <- bldiag(v_list)
  cat("\nGenerated Variance-Covariance Matrix:\n")
  print(v_matrix)
  
  return(v_matrix)
}
```

```{r}
meta_data <- imp_dataset

# Directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Generate and save v_matrices for each response variable
v_matrices <- list()

for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Calculate the variance-covariance matrix
  v_matrix <- calculate_v_matrix(data_subset, correlation = 0.5)
  
  # Store the matrix in the list
  v_matrices[[response]] <- v_matrix
  
  # Save the matrix to an individual RDS file
  file_name <- paste0("v_matrix_", tolower(gsub(" ", "_", response)), ".rds")
  saveRDS(v_matrix, file = file.path(output_dir, file_name))
  
  cat("Saved v_matrix for response variable:", response, "to", file.path(output_dir, file_name), "\n")
}

# Also, save the entire list of v_matrices as a single file
saveRDS(v_matrices, file = file.path(output_dir, "v_matrices_by_response_variable.rds"))
cat("\nAll v_matrices saved to:", output_dir, "\n")
```

#############
# STEP 3
##########################################################################################################################################
MODEL FITTING ON EACH SUBSET DATA USING ASSOCIATED VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

```{r}
# Load the saved v_matrices
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(output_dir, "v_matrices_by_response_variable.rds"))
```

(1)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
  # the experiment_year (the year the experiment was conducted)
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  
(2)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), 
  # the experiment_year (the year the experiment was conducted), and study duration (the number of years the study has been completed)
  group_by(id_article, location, experiment_year, study duration) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 

A)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article,                           # Random intercept for each article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
B)      
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
C)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),


```{r}
# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }

# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 + Year | id_article,                           # Random intercept for each article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      # Data: the dataset used for the analysis
      data = data,
      # Method: optimization method for model fitting
      method = "ML",                                # Maximum likelihood estimation
      # Control settings: additional options for the optimization algorithm
      control = list(
        optimizer = "optim",                        # Specify the optimization function to use
        optim.method = "BFGS",                      # Use the Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                            # Maximum number of iterations allowed
        rel.tol = 1e-8                              # Convergence tolerance (stopping criterion for optimization)
      )
    )
}, error = function(e) {                            # Catch any errors during model fitting
    cat("Error in model fitting:", e$message, "\n") # Print the error message
    return(NULL)                                    # Return NULL if an error occurs
})


  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators = c("tree_type", "crop_type", "age_system", "season", "soil_texture"))
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 11.96204 secs
```

#############
# STEP 4
##########################################################################################################################################
MODEL DIAGNOSTICS ON EACH SUBSET MODEL FITTING 
##########################################################################################################################################

```{r}
# Load the saved models
model_results <- readRDS(file.path(output_dir, "fitted_models_by_response_variable.rds"))
```

```{r}
summary(model_results$Biodiversity)
```


```{r}
# Function to extract key diagnostics from a fitted model
extract_model_diagnostics <- function(model, response_variable) {
  if (is.null(model)) {
    return(data.frame(
      ResponseVariable = response_variable,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      Tau2 = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract diagnostics
  aic <- AIC(model)
  bic <- BIC(model)
  log_likelihood <- as.numeric(logLik(model))
  tau2 <- sum(model$sigma2)
  i2 <- round((tau2 / (tau2 + mean(model$vi))) * 100, 1)
  qm <- model$QM
  qmp <- model$QMp
  
  data.frame(
    ResponseVariable = response_variable,
    AIC = aic,
    BIC = bic,
    LogLikelihood = log_likelihood,
    Tau2 = tau2,
    I2 = i2,
    QM = qm,
    QMp = qmp
  )
}
```

```{r}
# Extract diagnostics for all models
model_diagnostics <- bind_rows(
  lapply(names(model_results), function(response) {
    extract_model_diagnostics(model_results[[response]], response)
  })
)
```

```{r}
# Save diagnostics table
write.csv(model_diagnostics, file.path(output_dir, "model_diagnostics_summary.csv"), row.names = FALSE)
```

```{r}
# Visualize AIC, BIC, and Log-Likelihood
diagnostics_plot <- model_diagnostics %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood), names_to = "Metric", values_to = "Value") %>%
  ggplot(aes(x = ResponseVariable, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Model Fit Comparison",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(diagnostics_plot)
```

```{r}
# Visualize Variance Components (Tau2) and Heterogeneity (I²)
variance_plot <- model_diagnostics %>%
  ggplot(aes(x = ResponseVariable)) +
  geom_bar(aes(y = Tau2, fill = "Tau2 (Variance Components)"), stat = "identity", position = "dodge") +
  geom_point(aes(y = I2 / 100, color = "I² (Heterogeneity)"), size = 4) +
  scale_y_continuous(
    name = "Variance Components (Tau2)",
    sec.axis = sec_axis(~.*100, name = "Heterogeneity (I² %)"),
    limits = c(0, 0.02, na.rm = TRUE)
  ) +
  labs(
    title = "Variance Components and Heterogeneity",
    x = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

variance_plot
```

```{r}
# Create a formatted summary table using `gt`
diagnostics_table <- model_diagnostics %>%
  gt() %>%
  tab_header(
    title = "Model Diagnostics Summary",
    subtitle = "Comparison of Key Metrics Across Models"
  ) %>%
  fmt_number(
    columns = c(AIC, BIC, LogLikelihood, Tau2, I2, QM, QMp),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = "small",
    column_labels.font.size = "medium"
  )

# the table
model_diagnostics

# the gt table
diagnostics_table
```
```{r}
# Save table to an HTML file
gtsave(diagnostics_table, file.path(output_dir, "model_diagnostics_summary.html"))
```



#############
# STEP 5
##########################################################################################################################################
KEY INFLUENCE DIAGNOSTICS ON EACH SUBSET - SIMPLIFIED MODEL FITTING 
##########################################################################################################################################

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Fit models for each response variable
model_results_infdia <- list()

for (response in unique(meta_data$response_variable)) {
  cat("\nProcessing response variable:", response, "...\n")
  
  # Subset data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the model with random effects for `id_article`
  res <- tryCatch({
    rma(yi = yi, 
        vi = vi, 
        # Add study-level random effect
        random = ~ 1 | id_article,  
        data = data_subset, 
        # Restricted ML
        method = "REML")
  }, error = function(e) {
    cat("Model fitting failed for", response, ":", e$message, "\n")
    return(NULL)
  })
  
  # Save the fitted model
  model_results_infdia[[response]] <- res
}

# Recompute influence diagnostics
influence_diagnostics <- list()

for (response in names(model_results_infdia)) {
  cat("\nComputing influence diagnostics for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    inf <- tryCatch({
      influence(model)
    }, error = function(e) {
      cat("Influence diagnostics failed for:", response, ":", e$message, "\n")
      return(NULL)
    })
    influence_diagnostics[[response]] <- inf
  } else {
    cat("Skipping influence diagnostics for:", response, "due to missing model.\n")
  }
}


# Perform leave-one-out analysis
leave1out_results <- list()

for (response in names(model_results_infdia)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 1.117442 mins
# Time difference of 3.097009 mins
# Time difference of 2.744736 mins
```


```{r}
# Save the results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(model_results_infdia, file = file.path(output_dir, "simplified_fitted_models.rds"))
saveRDS(influence_diagnostics, file = file.path(output_dir, "simplified_influence_diagnostics.rds"))
cat("\nModels and influence diagnostics saved to:", output_dir, "\n")
```
```{r}
influence_diagnostics |> str()
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Generate plots for influence diagnostics
plot_dir <- file.path(output_dir, "Influence_Diagnostics_Plots")
if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE)

for (response in names(influence_diagnostics)) {
  cat("\nGenerating influence diagnostic plots for:", response, "...\n")
  
  inf <- influence_diagnostics[[response]]
  
  if (!is.null(inf)) {
    tryCatch({
      par(mfrow = c(8, 1))
      plot(inf)
      dev.copy(jpeg, file = file.path(plot_dir, paste0("influence_plot_", gsub(" ", "_", response), ".jpg")))
      dev.off()
    }, error = function(e) {
      cat("Plotting failed for", response, ":", e$message, "\n")
    })
  } else {
    cat("No influence diagnostics available for:", response, "\n")
  }
}
cat("\nInfluence diagnostic plots saved to:", plot_dir, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 49.21399 secs
```
```{r}
# Perform leave-one-out analysis at the study level
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    # Perform LOO only for studies (id_article)
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

```

```{r}
# Function to extract influence diagnostic data
extract_diagnostics <- function(inf_obj, response_variable) {
  if (is.null(inf_obj)) {
    cat("No influence diagnostics available for", response_variable, "\n")
    return(NULL)
  }
  
  # Extract diagnostics safely and pad missing elements with NA
  tryCatch({
    n_studies <- length(inf_obj$inf$rstudent)  # Total number of studies
    
    diagnostics <- data.frame(
      Study = if (!is.null(rownames(inf_obj$inf$rstudent))) rownames(inf_obj$inf$rstudent) else seq_len(n_studies),
      rstudent = inf_obj$inf$rstudent,
      dffits = inf_obj$inf$dffits,
      cook.d = inf_obj$inf$cook.d,
      cov.r = inf_obj$inf$cov.r,
      tau2.del = inf_obj$inf$tau2.del,
      QE.del = inf_obj$inf$QE.del,
      hat = inf_obj$inf$hat,
      weight = inf_obj$inf$weight,
      ResponseVariable = response_variable
    )
    
    return(diagnostics)
  }, error = function(e) {
    cat("Error extracting diagnostics for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
}
```

```{r}
# Combine diagnostics into a single data frame
# Combine diagnostics into a single data frame
diagnostics_list <- lapply(names(influence_diagnostics), function(response) {
  inf <- influence_diagnostics[[response]]
  extract_diagnostics(inf, response)
})

# Filter out NULL entries
diagnostics_data <- do.call(rbind, diagnostics_list[!sapply(diagnostics_list, is.null)])

# Check the resulting data frame
diagnostics_data |> glimpse()
```
```{r}
# Save diagnostics dataset
# diagnostics_list |> str()

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save the diagnostics list as an RDS file
saveRDS(diagnostics_list, file = file.path(output_dir, "diagnostics_list.rds"))
cat("Diagnostics list saved as RDS file to:", file.path(output_dir, "diagnostics_list.rds"), "\n")
```


```{r}
# Check the distribution of diagnostics by response variable
table(diagnostics_data$ResponseVariable)

# Summarize key diagnostics for each response variable
summary_stats <- diagnostics_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    Mean_rstudent = mean(rstudent, na.rm = TRUE),
    Max_rstudent = max(rstudent, na.rm = TRUE),
    Mean_dffits = mean(dffits, na.rm = TRUE),
    Max_dffits = max(dffits, na.rm = TRUE),
    Mean_cook.d = mean(cook.d, na.rm = TRUE),
    Max_cook.d = max(cook.d, na.rm = TRUE),
    .groups = "drop"
  )

summary_stats
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = rstudent, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Standardized Residuals by Study",
    x = "Study ID",
    y = "Standardized Residuals"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = cook.d, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Cook's Distance by Study",
    x = "Study ID",
    y = "Cook's Distance"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```

```{r}
# Define thresholds
thresholds <- diagnostics_data %>%
  mutate(
    IsInfluential = (abs(rstudent) > 2) | (cook.d > 0.5)
  )

# Check how many studies are flagged as influential
table(thresholds$ResponseVariable, thresholds$IsInfluential)

# Save flagged data for review
output_dir <- here::here("DATA", "OUTPUT_FROM_R")
write.csv(thresholds, file.path(output_dir, "flagged_influential_studies.csv"), row.names = FALSE)
```



```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################





##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 2.056713 mins
```


```{r}
# Define the output directory an file path for the plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
output_file <- file.path(output_dir, "leave_one_out_effect_sizes.png")

# Save the plot to the output directory
ggsave(output_file, plot = loo_plot, width = 12, height = 8, dpi = 300)
cat("Plot saved to:", output_file, "\n")
```


```{r}
# Inspect the diagnostics data structure for a single response variable
str(diagnostics_list[[1]])
```


```{r}
# Define a threshold for Cook's Distance
cooks_threshold <- 0.8  # Adjust as necessary

# Identify influential studies
influential_studies <- diagnostics_data %>%
  filter(cook.d > cooks_threshold) %>%
  distinct(Study, ResponseVariable)

# Map back to `meta_data`
mapped_influential <- meta_data %>%
  semi_join(influential_studies, by = c("id_article" = "Study"))

# View the result
print(mapped_influential)
```


#############
# STEP 6
##########################################################################################################################################
PUBLICATION-READY PLOTS OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################

Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.

```{r}
model_results |> str()
model_results |> glimpse()
```


```{r}
# Combine results from all response variables into a single data frame
# Prepare data for visualization
forest_plot_data <- bind_rows(
  lapply(names(model_results), function(response) {
    model <- model_results[[response]]
    
    if (!is.null(model) && length(model$yi) > 0) {
      data.frame(
        Study = model$data$id_article,  # Use the unique id_obs for studies
        EffectSize = model$yi,      # Effect sizes
        CI_Lower = model$yi - 1.96 * sqrt(model$vi),  # Lower CI
        CI_Upper = model$yi + 1.96 * sqrt(model$vi),  # Upper CI
        ResponseVariable = response # Response variable
      )
    } else {
      NULL  # Skip if no valid data
    }
  })
)

# Check the prepared data
forest_plot_data |> glimpse()
```
```{r}
# Prepare Aggregated Data
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

aggregated_data
```


```{r}
# Create the forest plot with custom colors
forest_plot <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank))) +
  # Add points for effect sizes
  geom_point(aes(size = size_category, color = ResponseVariable)) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = ResponseVariable), height = 0.2) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot of Response Variables with Custom Colors",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Number of Studies"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Display the plot
forest_plot
```























