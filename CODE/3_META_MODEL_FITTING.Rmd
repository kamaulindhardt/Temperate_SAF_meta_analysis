---
title: "3_META_MODEL_FITTING"
author: "M.K.K. Lindhardt"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Load multiple add-on packages using pacman::p_load for efficiency
# pacman::p_load automatically installs missing packages and loads them
pacman::p_load(
  
  # Conflict Resolution
  conflicted,        # Resolves conflicts when functions with the same name exist in multiple packages
  
  # Data Manipulation / Transformation
  tidyverse,         # Comprehensive collection of R packages for data science (e.g., ggplot2, dplyr, readr)
  readr,             # Simplifies reading and writing of delimited text files (e.g., CSV)
  dplyr,             # A grammar of data manipulation (e.g., filter, mutate, summarise, etc.)
  skimr,             # Provides summary statistics with a more user-friendly output
  future,            # Supports parallel processing for speeding up computations
  future.apply,      # Extends the future package for parallelized versions of base R apply functions
  
  ###################################################################################################################
  # Data Visualization
  ggplot2,           # A data visualization package for creating static and interactive graphics (part of tidyverse)
  patchwork,         # Extends ggplot2 by providing tools to combine multiple plots into one
  gridExtra,         # Arranges multiple grid-based plots (e.g., from ggplot2) into a single display
  scales,            # Adds tools for handling scale transformations and labels in visualizations
  
  ###################################################################################################################
  # Meta-Analysis
  metafor,           # For conducting meta-analyses, including calculating effect sizes and response ratios
  clubSandwich,      # Provides cluster-robust variance estimators for meta-analysis models
  mice,              # Multivariate Imputation by Chained Equations for handling missing data
  
  ###################################################################################################################
  # Exploratory Data Analysis (EDA)
  DataExplorer,      # Automates exploratory data analysis with summary statistics and visualizations
  SmartEDA,          # Automates exploratory data analysis with summary reports and visualizations
  naniar,            # Provides tools for handling and visualizing missing data
  VIM,               # Visualization and Imputation of Missing Data
  Hmisc,             # Miscellaneous functions including data summary, analysis, and visualization
  BaylorEdPsych,     # Provides tools for reliability analysis and missing data imputation
  
  ###################################################################################################################
  # Project Management and Code Styling
  here,              # Simplifies file referencing by locating the root of a project directory
  styler             # Formats and styles R code to improve readability and consistency
)
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
```



#############
# STEP 1
##########################################################################################################################################
LOADING PREPARED META-DATA
##########################################################################################################################################


Loading the two datasets (imputed and non-imputed)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Define file paths
non_imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom.rds"))
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))

# non_imp_data_rom_dummy <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom_dummy.rds"))

# Read in the non-imputed dataset
non_imp_dataset <- non_imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Read in the imputed dataset
imp_dataset <- imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
})
```

```{r}
# Checking high observations with extreme high variance
high_variance_obs <- 
  imp_dataset|> 
  filter(vi > quantile(vi, 0.995)) |> # 0.995
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n,
           tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width) |> 
  arrange(id_article, response_variable)

skim(high_variance_obs)
```
```{r}
non_imp_dataset |> glimpse()
imp_dataset |> glimpse()
```

```{r}
# Select distinct id_article entries
distinct_articles <- imp_dataset %>%
  select(id_article) %>%
  distinct()

# Get the number of unique id_article entries
num_distinct_articles <- nrow(distinct_articles)

# Print the result
cat("Number of distinct id_article entries:", num_distinct_articles, "\n")
# Number of distinct id_article entries: 37 
```




##########################################################################################################################################
LISTING RESPONSE VARIABLES AND SETTING UP COSTUM COLORS
##########################################################################################################################################

```{r}
# Custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Response variables to analyze
response_variables <- names(custom_colors)
```




#############
# STEP 2
##########################################################################################################################################
PERFORMING MULTIVARIATE/MULTILEVEL LINEAR (MIXED-EFFECTS) MODELLING 
##########################################################################################################################################

Assessment of Missing Data for Moderators
Imputation of Missing Values for Moderators Using mice()
Post-Imputation Assessment of Moderators
Selection of Moderators for Analysis
Fitting the Multivariate Random-Effects Model with Selected Moderators

```{r}
# Define the function for missing data assessment
assess_missing_data <- function(dataset, moderators, dataset_name = "Dataset") {
  
  cat("\nStarting missing data assessment for", dataset_name, "...\n")
  
  # Step 1: Calculate the proportion of missing values for each moderator
  missing_summary <- dataset %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_proportion")

  # Print the missing summary table
  cat("\nProportion of Missing Values for Each Moderator:\n")
  print(missing_summary)

  # Step 2: Create a basic bar chart of missing proportions
  missing_plot <- ggplot(missing_summary, aes(x = reorder(variable, -missing_proportion), y = missing_proportion)) +
    geom_bar(stat = "identity", fill = "#0072B2") +
    labs(
      title = paste("Proportion of Missing Data for Moderator Variables -", dataset_name),
      x = "Moderator Variable",
      y = "Missing Proportion"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Step 3: Calculate missingness for each moderator by response_variable
  missing_by_response <- dataset %>%
    group_by(response_variable) %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = -response_variable, names_to = "moderator", values_to = "missing_proportion")

  # Print the summary table for missingness by response_variable
  cat("\nMissing Proportion by Response Variable for Each Moderator:\n")
  print(missing_by_response)

  # Step 4: Create a heatmap for missingness by response_variable
  missing_heatmap <- ggplot(missing_by_response, aes(x = moderator, y = response_variable, fill = missing_proportion)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "#56B1F7", high = "#132B43", na.value = "gray90", labels = percent) +
    labs(
      title = paste("Heatmap of Missing Data by Moderator and Response Variable -", dataset_name),
      x = "Moderator Variable",
      y = "Response Variable",
      fill = "Missing Proportion"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Display the plots
  print(missing_plot)
  print(missing_heatmap)
  
  cat("\nMissing data assessment completed for", dataset_name, ".\n")
}
```

```{r}
# Assessing Moderator missingness

moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Assess missing data for non-imputed dataset
assess_missing_data(non_imp_dataset, moderators, "Non-Imputed Dataset")

# Assess missing data for imputed dataset
assess_missing_data(imp_dataset, moderators, "Imputed Dataset")
```






##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################


```{r}
# Variance-Covariance Matrix Calculation Function
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  v_list <- list()
  for (study in unique(data$id_article)) {
    study_data <- data[data$id_article == study, ]
    
    if (nrow(study_data) > 1) {
      v <- diag(study_data$vi)
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      v_list[[as.character(study)]] <- v
    } else {
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }

  v_matrix <- bldiag(v_list)
  cat("\nGenerated Variance-Covariance Matrix:\n")
  print(v_matrix)
  
  return(v_matrix)
}
```

```{r}
# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset




# Directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Generate and save v_matrices for each response variable
v_matrices <- list()

for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Calculate the variance-covariance matrix
  v_matrix <- calculate_v_matrix(data_subset, correlation = 0.5)
  
  # Store the matrix in the list
  v_matrices[[response]] <- v_matrix
  
  # Save the matrix to an individual RDS file
  file_name <- paste0("v_matrix_", tolower(gsub(" ", "_", response)), ".rds")
  saveRDS(v_matrix, file = file.path(output_dir, file_name))
  
  cat("Saved v_matrix for response variable:", response, "to", file.path(output_dir, file_name), "\n")
}

# Also, save the entire list of v_matrices as a single file
saveRDS(v_matrices, file = file.path(output_dir, "v_matrices_by_response_variable.rds"))
cat("\nAll v_matrices saved to:", output_dir, "\n")
```




#############
# STEP 3
##########################################################################################################################################
MODEL FITTING ON EACH SUBSET DATA USING ASSOCIATED VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

```{r}
# Load the saved v_matrices
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(output_dir, "v_matrices_by_response_variable.rds"))
```

(1)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
  # the experiment_year (the year the experiment was conducted)
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  
(2)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), 
  # the experiment_year (the year the experiment was conducted), and study duration (the number of years the study has been completed)
  group_by(id_article, location, experiment_year, study duration) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 

A)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article,                           # Random intercept for each article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
B)      
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
C)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),

D)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 + Year | id_article,                    # Random intercept for each year and article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),


##########################################################################################################################################
FITTING MODELS FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

Original Model
This model incorporates two levels of random effects: one nested within articles and response variables, and another at the experiment level.

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }

  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()

  # Fit the model
  model <- tryCatch({
   rma.mv(
    # Dependent variable: effect size
    yi = yi,
    # Variance-covariance matrix for within-study variability
    V = v_matrix,
    # Moderator formula: relationship between the effect size and moderators
    mods = moderator_formula,
    # Random effects structure
     random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random effect: accounts for variability at the experiment level
      ),
    # Data used for model fitting
    data = data_subset,
    # Method for model fitting
    method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
    # Optimization settings
    control = list(
      optimizer = "optim",                  # Optimizer function to use for fitting
      optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
      iter.max = 1000,                      # Maximum number of iterations allowed
      rel.tol = 1e-8                        # Convergence tolerance for optimization
    )
  )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 11.96204 secs

# Last go (01/01-2025)
# Time difference of 8.034523 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 8.034523 secs

# Last go (02/12-2024)
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 9.78518 secs
```

Alternative Model 1: Simplified Random Effects
This model simplifies the random effects structure, focusing only on variability at the experiment level.
```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }

  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()

  # Fit the model
  model <- tryCatch({
   rma.mv(
    # Dependent variable: effect size
    yi = yi,
    # Variance-covariance matrix for within-study variability
    V = v_matrix,
    # Moderator formula: relationship between the effect size and moderators
    mods = moderator_formula,
    # Random effects structure
    random = ~ 1 | exp_id,                 # Random effect: accounts for variability at the experiment level
    # Data used for model fitting
    data = data_subset,
    # Method for model fitting
    method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
    # Optimization settings
    control = list(
      optimizer = "optim",                  # Optimizer function to use for fitting
      optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
      iter.max = 1000,                      # Maximum number of iterations allowed
      rel.tol = 1e-8                        # Convergence tolerance for optimization
    )
  )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable_simplified.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 4.593977 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 4.593977 secs

# Last go (02/12-2024)
# Time difference of 4.643164 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 4.643164 secs
```

Minimally Reduced Model:
This model further simplifies the random effects structure by removing all nested random effects,
focusing only on the experiment level and an intercept-only model.

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Start time tracking
start.time <- Sys.time()

##########################################################################
# Define the function to fit minimally reduced models for each response variable
fit_minimal_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting minimally reduced model for response variable:", response_variable, "...\n")

  # Define the intercept-only formula
  minimal_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")
  }

  # Ensure all moderators are treated as factors
  if (!is.null(moderators)) {
    data_subset <- data_subset %>%
      mutate(across(all_of(moderators), as.factor)) %>%
      as.data.frame()
  }

  # Fit the model
  model <- tryCatch({
    rma.mv(
      # Dependent variable: effect size
      yi = yi,
      # Variance-covariance matrix for within-study variability
      V = v_matrix,
      # Moderator formula: Intercept-only model
      mods = minimal_formula,
      # Random effects structure
      random = ~ 1 | exp_id,                 # Random effect: accounts for variability at the experiment level
      # Data used for model fitting
      data = data_subset,
      # Method for model fitting
      method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
      # Optimization settings
      control = list(
        optimizer = "optim",                  # Optimizer function to use for fitting
        optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                      # Maximum number of iterations allowed
        rel.tol = 1e-8                        # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Fit minimally reduced models for each response variable using the precomputed v_matrices
minimal_model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")

  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]

  # Define moderators (if applicable)
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit the minimally reduced model for the current response variable
  model <- fit_minimal_model(data_subset, response, v_matrix, moderators)

  # Save the model result
  minimal_model_results[[response]] <- model
}

##########################################################################
# Save the fitted minimally reduced models to a file
saveRDS(minimal_model_results, file = file.path(output_dir, "fitted_models_by_response_variable_minimal.rds"))
cat("\nAll minimally reduced models fitted and saved to:", output_dir, "\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (02/12-2024)
# Total time taken: 4.232735 
# Processing response variable: Biodiversity 
# Fitting minimally reduced model for response variable: Biodiversity ...
# Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting minimally reduced model for response variable: Greenhouse gas emission ...
# Model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting minimally reduced model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting minimally reduced model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting minimally reduced model for response variable: Pest and Disease ...
# Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting minimally reduced model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting minimally reduced model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Water quality .
# All minimally reduced models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Total time taken: 4.232735 
```

#############
# STEP 4
##########################################################################################################################################
MODEL COMPARISONS, EVALUATION AND DIAGNOSTICS
##########################################################################################################################################

```{r}
# Compare models for each response variable
model_comparisons <- list()

for (response in names(original_model_results)) {
  cat("\nComparing models for response variable:", response, "\n")
  
  original_model <- original_model_results[[response]]
  alternative_model <- simplified_model_results[[response]]
  minimal_model <- minimal_model_results[[response]]
  
  if (!is.null(original_model) & !is.null(alternative_model) & !is.null(minimal_model)) {
    # Extract AIC for each model, handle NULL cases
    aic_values <- c(
      "Original" = if (!is.null(original_model)) original_model$aic else NA,
      "Alternative" = if (!is.null(alternative_model)) alternative_model$aic else NA,
      "Minimal" = if (!is.null(minimal_model)) minimal_model$aic else NA
    )
    
    # Perform likelihood ratio tests, handle potential errors
    lr_original_vs_alternative <- tryCatch(
      anova(original_model, alternative_model),
      error = function(e) { cat("LR test failed for Original vs. Alternative:", e$message, "\n"); NULL }
    )
    lr_original_vs_minimal <- tryCatch(
      anova(original_model, minimal_model),
      error = function(e) { cat("LR test failed for Original vs. Minimal:", e$message, "\n"); NULL }
    )
    
    # Store results
    model_comparisons[[response]] <- list(
      AIC = aic_values,
      LR_Original_vs_Alternative = lr_original_vs_alternative,
      LR_Original_vs_Minimal = lr_original_vs_minimal
    )
    
    # Print AIC values
    cat("AIC Values:\n", aic_values, "\n")
  } else {
    cat("Model fitting failed for one or more models for", response, "\n")
  }
}

# Display results for all response variables
model_comparisons |> glimpse() |> head()
```


```{r}
# Visualize Model Comparisons
# Create visualizations for the AIC and likelihood ratio test (LRT) results from `model_comparisons`.

# Extract AIC values and LRT results
comparison_results <- lapply(names(model_comparisons), function(response) {
  comparison <- model_comparisons[[response]]
  if (!is.null(comparison)) {
    tibble(
      Response = response,
      AIC_Original = comparison$AIC[["Original"]],
      AIC_Alternative = comparison$AIC[["Alternative"]],
      AIC_Minimal = comparison$AIC[["Minimal"]],
      LRT_Original_vs_Alternative = comparison$LR_Original_vs_Alternative$pval,
      LRT_Original_vs_Minimal = comparison$LR_Original_vs_Minimal$pval
    )
  } else {
    tibble(
      Response = response,
      AIC_Original = NA,
      AIC_Alternative = NA,
      AIC_Minimal = NA,
      LRT_Original_vs_Alternative = NA,
      LRT_Original_vs_Minimal = NA
    )
  }
}) %>% bind_rows()

comparison_results |> str()

# Melt the data for AIC visualization
aic_data <- comparison_results %>%
  select(Response, 
         AIC_Original, 
         AIC_Alternative, 
         AIC_Minimal) %>%
  pivot_longer(cols = starts_with("AIC"), names_to = "Model", values_to = "AIC") %>%
  mutate(Model = factor(Model, levels = c("AIC_Original", "AIC_Alternative", "AIC_Minimal"),
                        labels = c("Original", "Alternative", "Minimal")))

# Plot AIC values
plot_aic <- ggplot(aic_data, aes(x = Response, y = AIC, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AIC Comparisons Across Models",
       x = "Response Variable",
       y = "AIC",
       fill = "Model") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Prepare LRT data
lrt_data <- comparison_results %>%
  select(Response, LRT_Original_vs_Alternative, LRT_Original_vs_Minimal) %>%
  pivot_longer(cols = starts_with("LRT"), names_to = "Comparison", values_to = "p_value") %>%
  mutate(Comparison = factor(Comparison, 
                              levels = c("LRT_Original_vs_Alternative", "LRT_Original_vs_Minimal"),
                              labels = c("Original vs. Alternative", "Original vs. Minimal")))

# Plot LRT p-values
plot_lrt <- ggplot(lrt_data, aes(x = Response, y = p_value, color = Comparison)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  labs(title = "LRT P-Values for Model Comparisons",
       x = "Response Variable",
       y = "P-Value",
       color = "Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the plots
print(plot_aic)
print(plot_lrt)

```





























```{r}
# Load the saved models

original_model_results <- readRDS(file.path(output_dir, "fitted_models_by_response_variable.rds"))
simplified_model_results <- readRDS(file.path(output_dir, "fitted_models_by_response_variable_simplified.rds"))
minimally_reduced_model <- readRDS(file.path(output_dir, "fitted_models_by_response_variable_minimal.rds"))

names(original_model_results)
names(simplified_model_results)
names(minimally_reduced_model)

# [1] "Biodiversity"     "Product quality"  "Crop yield"       "Pest and Disease" "Soil quality"    
# [1] "Biodiversity"     "Product quality"  "Crop yield"       "Pest and Disease" "Soil quality"   

# Not able to fitt models (output = Errors: "Error in model fitting for Greenhouse gas emission or Water quality : contrasts can be applied
# only to factors with 2 or more levels ", for Greenhouse gas emission and Water quality)
```



```{r}
### Visual Evaluation Protocol: AIC and BIC Comparison

# Function to create a performance comparison table for individual response variables
compare_models <- function(original_model_list, simplified_model_list, response_variable) {
  original_model <- original_model_list[[response_variable]]
  simplified_model <- simplified_model_list[[response_variable]]

  if (is.null(original_model) || is.null(simplified_model)) {
    cat("Model missing for response variable:", response_variable, "\n")
    return(NULL)
  }

  performance_metrics <- tibble(
    ResponseVariable = response_variable,
    Model = c("Original", "Simplified"),
    AIC = c(AIC(original_model), AIC(simplified_model)),
    BIC = c(BIC(original_model), BIC(simplified_model))
  )
  return(performance_metrics)
}

# Visualize AIC and BIC side-by-side for all response variables
plot_model_comparison_by_response <- function(performance_comparison) {
  performance_long <- performance_comparison %>%
    pivot_longer(cols = c(AIC, BIC), names_to = "Metric", values_to = "Value") %>%
    mutate(ResponseVariable = as.factor(ResponseVariable))

  ggplot(performance_long, aes(x = Model, y = Value, fill = Metric)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    facet_wrap(~ResponseVariable, scales = "free_y") +
    labs(
      title = "Model Performance Comparison by Response Variable",
      x = "Model",
      y = "Value",
      fill = "Metric"
    ) +
    theme_minimal() +
    scale_fill_brewer(palette = "Set2")
}

# Example Usage
# Assuming `original_model_results` and `simplified_model_results` are lists of model objects by response variable
response_variables <- names(original_model_results)

# Generate performance metrics for all response variables
all_performance_metrics <- lapply(response_variables, function(rv) {
  compare_models(original_model_results, simplified_model_results, rv)
})

# Combine all metrics into a single data frame
all_performance_metrics <- bind_rows(all_performance_metrics)

# Print the performance metrics
glimpse(all_performance_metrics)
```
```{r}

# Generate the visualization
plot_model_comparison_by_response(all_performance_metrics)
```

```{r}
summary(original_model)$random
summary(simplified_model)$random
```

#############
# STEP 5
##########################################################################################################################################
MODEL DIAGNOSTICS ON EACH SUBSET MODEL FITTING 
##########################################################################################################################################

```{r}
# Load the saved models
model_results <- readRDS(file.path(output_dir, "fitted_models_by_response_variable.rds"))
```

```{r}
summary(model_results$Biodiversity)
```


```{r}
# Function to extract key diagnostics from a fitted model
extract_model_diagnostics <- function(model, response_variable) {
  if (is.null(model)) {
    return(data.frame(
      ResponseVariable = response_variable,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      Tau2 = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract diagnostics
  aic <- AIC(model)
  bic <- BIC(model)
  log_likelihood <- as.numeric(logLik(model))
  tau2 <- sum(model$sigma2)
  i2 <- round((tau2 / (tau2 + mean(model$vi))) * 100, 1)
  qm <- model$QM
  qmp <- model$QMp
  
  data.frame(
    ResponseVariable = response_variable,
    AIC = aic,
    BIC = bic,
    LogLikelihood = log_likelihood,
    Tau2 = tau2,
    I2 = i2,
    QM = qm,
    QMp = qmp
  )
}
```

```{r}
# Extract diagnostics for all models
model_diagnostics <- bind_rows(
  lapply(names(model_results), function(response) {
    extract_model_diagnostics(model_results[[response]], response)
  })
)
```

```{r}
# Save diagnostics table
write.csv(model_diagnostics, file.path(output_dir, "model_diagnostics_summary.csv"), row.names = FALSE)
```

```{r}
# Visualize AIC, BIC, and Log-Likelihood
diagnostics_plot <- model_diagnostics %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood), names_to = "Metric", values_to = "Value") %>%
  ggplot(aes(x = ResponseVariable, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Model Fit Comparison",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(diagnostics_plot)
```



Visualization 2: Fixed Effects Estimates Comparison

```{r}
# Plot Fixed Effects Estimates with Confidence Intervals
coef_plot <- ggplot(fixed_effects_data, aes(x = Term, y = Estimate, color = Dataset)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Comparison of Fixed Effects Estimates Across Models",
       x = "Fixed Effect Term",
       y = "Estimate",
       color = "Dataset") +
  theme_minimal()

print(coef_plot)
```


Visualization 3: Heterogeneity (I²) Comparison

```{r}
# Heterogeneity Comparison Plot
I2_plot <- ggplot(model_summaries, aes(x = Dataset, y = I2, fill = Dataset)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(I2, 2)), vjust = -0.5) +
  labs(title = "Comparison of I² (Heterogeneity) Across Models",
       x = "Dataset",
       y = "I² (%)") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))

print(I2_plot)

```

Comparison table of key model statistics

```{r}
colnames(model_summaries)
```

```{r}
# Visualize Variance Components (Tau2) and Heterogeneity (I²)
variance_plot <- model_diagnostics %>%
  ggplot(aes(x = ResponseVariable)) +
  geom_bar(aes(y = Tau2, fill = "Tau2 (Variance Components)"), stat = "identity", position = "dodge") +
  geom_point(aes(y = I2 / 100, color = "I² (Heterogeneity)"), size = 4) +
  scale_y_continuous(
    name = "Variance Components (Tau2)",
    sec.axis = sec_axis(~.*100, name = "Heterogeneity (I² %)"),
    limits = c(0, 0.02, na.rm = TRUE)
  ) +
  labs(
    title = "Variance Components and Heterogeneity",
    x = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

variance_plot
```

```{r}
# Create a formatted summary table using `gt`
diagnostics_table <- model_diagnostics %>%
  gt() %>%
  tab_header(
    title = "Model Diagnostics Summary",
    subtitle = "Comparison of Key Metrics Across Models"
  ) %>%
  fmt_number(
    columns = c(AIC, BIC, LogLikelihood, Tau2, I2, QM, QMp),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = "small",
    column_labels.font.size = "medium"
  )

# the table
model_diagnostics

# the gt table
diagnostics_table
```
```{r}
# Save table to an HTML file
gtsave(diagnostics_table, file.path(output_dir, "model_diagnostics_summary.html"))
```



#############
# STEP 5
##########################################################################################################################################
KEY INFLUENCE DIAGNOSTICS ON EACH SUBSET - SIMPLIFIED MODEL FITTING 
##########################################################################################################################################

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Fit models for each response variable
model_results_infdia <- list()

for (response in unique(meta_data$response_variable)) {
  cat("\nProcessing response variable:", response, "...\n")
  
  # Subset data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the model with random effects for `id_article`
  res <- tryCatch({
    rma(yi = yi, 
        vi = vi, 
        # Add study-level random effect
        random = ~ 1 | id_article,  
        data = data_subset, 
        # Restricted ML
        method = "REML")
  }, error = function(e) {
    cat("Model fitting failed for", response, ":", e$message, "\n")
    return(NULL)
  })
  
  # Save the fitted model
  model_results_infdia[[response]] <- res
}

# Recompute influence diagnostics
influence_diagnostics <- list()

for (response in names(model_results_infdia)) {
  cat("\nComputing influence diagnostics for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    inf <- tryCatch({
      influence(model)
    }, error = function(e) {
      cat("Influence diagnostics failed for:", response, ":", e$message, "\n")
      return(NULL)
    })
    influence_diagnostics[[response]] <- inf
  } else {
    cat("Skipping influence diagnostics for:", response, "due to missing model.\n")
  }
}


# Perform leave-one-out analysis
leave1out_results <- list()

for (response in names(model_results_infdia)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################

# Last go (02/12-2024)
# Time difference of 1.117442 mins
# Time difference of 3.097009 mins
# Time difference of 2.744736 mins

# Last go (01/01-2025)
# Time difference of 2.148423 mins
# Processing response variable: Biodiversity ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Crop yield ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Water quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Pest and Disease ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Soil quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Greenhouse gas emission ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Product quality ...
# Advarsel: Extra argument ('random') disregarded.
# Computing influence diagnostics for: Biodiversity ...
# Computing influence diagnostics for: Crop yield ...
# Computing influence diagnostics for: Water quality ...
# Computing influence diagnostics for: Pest and Disease ...
# Computing influence diagnostics for: Soil quality ...
# Computing influence diagnostics for: Greenhouse gas emission ...
# Computing influence diagnostics for: Product quality ...
# Running Leave-One-Out for: Biodiversity ...
# Running Leave-One-Out for: Crop yield ...
# Running Leave-One-Out for: Water quality ...
# Running Leave-One-Out for: Pest and Disease ...
# Running Leave-One-Out for: Soil quality ...
# Running Leave-One-Out for: Greenhouse gas emission ...
# Running Leave-One-Out for: Product quality ...
# Time difference of 2.148423 mins
```


=======
# Last go: (17/11-24)
# Time difference of 25.3915 secs
# 
# Calculating Bootstrap Confidence Intervals for Non-Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Dataset .
# 
# Forest Plot for Non-Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Dataset .
# 
# Forest Plot for Imputed Dataset :
# Advarsel: longer object length is not a multiple of shorter object length
# Calculating Bootstrap Confidence Intervals for Non-Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Imputed Dataset .
# 
# Forest Plot for Non-Imputed Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Imputed Dataset .
# 
# Forest Plot for Imputed Imputed Dataset :

Saving plots


Fit the simplified multivariate random-effects model for diagnostics

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Helper function to run the multivariate random-effects model
run_meta_analysis <- function(data, dataset_name) {
  cat("Running analysis for", dataset_name, "\n")
  
  # Step 1: Calculate the variance-covariance matrix if needed
  V_matrix <- as.matrix(data$vi) # Example assumes `vi` is already correct
  
  # Step 2: Fit the simplified multivariate random-effects model
  model_meta_diagnostics <- tryCatch({
    rma(
      yi = data$yi,
      vi = data$vi,
      random = list(
        ~ 1 | id_article,
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data,
      method = "ML"
    )
  }, error = function(e) {
    stop("Error in model fitting: ", e$message)
  })
  
  # Step 3: Compute influence diagnostics
  inf <- influence(model_meta_diagnostics)
  
  # Return the model and diagnostics
  return(list(model = model_meta_diagnostics, influence = inf))
}

# Step 1: Apply meta-analysis function to each dataset
results <- lapply(names(datasets), function(dataset_name) {
  tryCatch({
    run_meta_analysis(datasets[[dataset_name]], dataset_name)
  }, error = function(e) {
    cat("Error in dataset", dataset_name, ":", e$message, "\n")
    return(NULL)
  })
})

# Step 2: Name the results list
names(results) <- names(datasets)


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (27/11-24)
# Time difference of 2.942727 hours
```


```{r}
saveRDS(results, file = "meta_analysis_diagnosis_on_datasets_results.rds")
```


```{r}
# Step 3: Plot influence diagnostics
par(mfrow = c(8, 1), oma = c(2, 2, 2, 2)) # Set up plotting area
for (dataset_name in names(results)) {
  if (!is.null(results[[dataset_name]])) {
    plot(results[[dataset_name]]$influence) # Default plot
    mtext(paste("Influence Diagnostics for", dataset_name), side = 3, line = 0.5, outer = FALSE)
  }
}


# Optional: Inspect the results
# str(results)
```

```{r}
datasets_effect_sizes$non_imp_dataset %>% glimpse()
```


















##########################################################################################################################################
SAVING DATASETS AND MODEL OBJECTS
##########################################################################################################################################

```{r}
# Save the results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(model_results_infdia, file = file.path(output_dir, "simplified_fitted_models.rds"))
saveRDS(influence_diagnostics, file = file.path(output_dir, "simplified_influence_diagnostics.rds"))
cat("\nModels and influence diagnostics saved to:", output_dir, "\n")
```
```{r}
influence_diagnostics |> str()
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Generate plots for influence diagnostics
plot_dir <- file.path(output_dir, "Influence_Diagnostics_Plots")
if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE)

for (response in names(influence_diagnostics)) {
  cat("\nGenerating influence diagnostic plots for:", response, "...\n")
  
  inf <- influence_diagnostics[[response]]
  
  if (!is.null(inf)) {
    tryCatch({
      par(mfrow = c(8, 1))
      plot(inf)
      dev.copy(jpeg, file = file.path(plot_dir, paste0("influence_plot_", gsub(" ", "_", response), ".jpg")))
      dev.off()
    }, error = function(e) {
      cat("Plotting failed for", response, ":", e$message, "\n")
    })
  } else {
    cat("No influence diagnostics available for:", response, "\n")
  }
}
cat("\nInfluence diagnostic plots saved to:", plot_dir, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 49.21399 secs
```
```{r}
model_results |> str()
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################



# Custom Leave-One-Out for rma.mv models
leave1out_mv <- function(model, data, study_id_col) {
  unique_studies <- unique(data[[study_id_col]])
  results <- list()
  
  for (study in unique_studies) {
    cat("Leaving out study:", study, "...\n")
    
    # Subset data to exclude the current study
    data_subset <- data[data[[study_id_col]] != study, ]
    
    # Refit the model without the excluded study
    tryCatch({
      refit_model <- rma.mv(yi = model$yi,
                            V = model$V,
                            mods = model$X,
                            random = model$s.names,
                            data = data_subset,
                            method = "ML")
      
      # Store refit model's coefficients
      results[[as.character(study)]] <- list(
        coefficients = refit_model$b,
        fit_stats = refit_model$fit.stats
      )
    }, error = function(e) {
      cat("Error with study:", study, "->", e$message, "\n")
      results[[as.character(study)]] <- NULL
    })
  }
  
  return(results)
}

# Perform Leave-One-Out for all response variables
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nPerforming Leave-One-Out for:", response, "...\n")
  
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out_mv(
      model = model,
      data = model$data,
      study_id_col = "id_article"
    )
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}




##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (01/01-2025)
# Time difference of  mins
```

```{r}
# Function to extract influence diagnostic data
extract_diagnostics <- function(inf_obj, response_variable) {
  if (is.null(inf_obj)) {
    cat("No influence diagnostics available for", response_variable, "\n")
    return(NULL)
  }
  
  # Extract diagnostics safely and pad missing elements with NA
  tryCatch({
    n_studies <- length(inf_obj$inf$rstudent)  # Total number of studies
    
    diagnostics <- data.frame(
      Study = if (!is.null(rownames(inf_obj$inf$rstudent))) rownames(inf_obj$inf$rstudent) else seq_len(n_studies),
      rstudent = inf_obj$inf$rstudent,
      dffits = inf_obj$inf$dffits,
      cook.d = inf_obj$inf$cook.d,
      cov.r = inf_obj$inf$cov.r,
      tau2.del = inf_obj$inf$tau2.del,
      QE.del = inf_obj$inf$QE.del,
      hat = inf_obj$inf$hat,
      weight = inf_obj$inf$weight,
      ResponseVariable = response_variable
    )
    
    return(diagnostics)
  }, error = function(e) {
    cat("Error extracting diagnostics for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
}
```

```{r}
# Combine diagnostics into a single data frame
# Combine diagnostics into a single data frame
diagnostics_list <- lapply(names(influence_diagnostics), function(response) {
  inf <- influence_diagnostics[[response]]
  extract_diagnostics(inf, response)
})

# Filter out NULL entries
diagnostics_data <- do.call(rbind, diagnostics_list[!sapply(diagnostics_list, is.null)])

# Check the resulting data frame
diagnostics_data |> glimpse()
```

```{r}
# Save diagnostics dataset
# diagnostics_list |> str()

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save the diagnostics list as an RDS file
saveRDS(diagnostics_list, file = file.path(output_dir, "diagnostics_list.rds"))
cat("Diagnostics list saved as RDS file to:", file.path(output_dir, "diagnostics_list.rds"), "\n")
```


```{r}
# Check the distribution of diagnostics by response variable
table(diagnostics_data$ResponseVariable)

# Summarize key diagnostics for each response variable
summary_stats <- diagnostics_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    Mean_rstudent = mean(rstudent, na.rm = TRUE),
    Max_rstudent = max(rstudent, na.rm = TRUE),
    Mean_dffits = mean(dffits, na.rm = TRUE),
    Max_dffits = max(dffits, na.rm = TRUE),
    Mean_cook.d = mean(cook.d, na.rm = TRUE),
    Max_cook.d = max(cook.d, na.rm = TRUE),
    .groups = "drop"
  )

summary_stats
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = rstudent, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Standardized Residuals by Study",
    x = "Study ID",
    y = "Standardized Residuals"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = cook.d, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Cook's Distance by Study",
    x = "Study ID",
    y = "Cook's Distance"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```

```{r}
# Define thresholds
thresholds <- diagnostics_data %>%
  mutate(
    IsInfluential = (abs(rstudent) > 2) | (cook.d > 0.5)
  )

# Check how many studies are flagged as influential
table(thresholds$ResponseVariable, thresholds$IsInfluential)

# Save flagged data for review
output_dir <- here::here("DATA", "OUTPUT_FROM_R")
write.csv(thresholds, file.path(output_dir, "flagged_influential_studies.csv"), row.names = FALSE)
```






```{r}
# Define the output directory an file path for the plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
output_file <- file.path(output_dir, "leave_one_out_effect_sizes.png")

# Save the plot to the output directory
ggsave(output_file, plot = loo_plot, width = 12, height = 8, dpi = 300)
cat("Plot saved to:", output_file, "\n")
```


```{r}
# Inspect the diagnostics data structure for a single response variable
str(diagnostics_list[[1]])
```


```{r}
# Define a threshold for Cook's Distance
cooks_threshold <- 0.8  # Adjust as necessary

# Identify influential studies
influential_studies <- diagnostics_data %>%
  filter(cook.d > cooks_threshold) %>%
  distinct(Study, ResponseVariable)

# Map back to `meta_data`
mapped_influential <- meta_data %>%
  semi_join(influential_studies, by = c("id_article" = "Study"))

# View the result
print(mapped_influential)
```


#############
# STEP 6
##########################################################################################################################################
PUBLICATION-READY PLOTS OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################

Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.

```{r}
model_results |> str()
model_results |> glimpse()
```


```{r}
# Combine results from all response variables into a single data frame
forest_plot_data <- bind_rows(
  lapply(names(model_results), function(response) {
    model <- model_results[[response]]
    
    if (!is.null(model) && !is.null(model$data)) {
      n_effects <- length(model$yi)
      n_studies <- nrow(model$data)
      
      # Ensure the lengths match or skip inconsistent data
      if (n_effects == n_studies) {
        data.frame(
          Study = model$data$id_article,                 # Study IDs
          EffectSize = model$yi,                        # Effect sizes
          CI_Lower = model$yi - 1.96 * sqrt(model$vi),  # Lower CI
          CI_Upper = model$yi + 1.96 * sqrt(model$vi),  # Upper CI
          ResponseVariable = response                   # Response variable
        )
      } else {
        warning(sprintf(
          "Skipping response variable '%s': Mismatched lengths between model$yi (%d) and model$data (%d).",
          response, n_effects, n_studies
        ))
        NULL
      }
    } else {
      warning(sprintf("Skipping response variable '%s': Missing model or data.", response))
      NULL
    }
  })
)

# Check the prepared data
forest_plot_data |> glimpse()
```
```{r}
# Prepare Aggregated Data
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

aggregated_data
```



```{r}
# Create the forest plot with custom colors
forest_plot <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank))) +
  # Add points for effect sizes
  geom_point(aes(size = size_category, color = ResponseVariable)) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = ResponseVariable), height = 0.2) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot of Response Variables with Custom Colors",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Number of Studies"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Display the plot
forest_plot
```























