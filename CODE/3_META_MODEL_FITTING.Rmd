---
title: "3_META_MODEL_FITTING"
author: "M.K.K. Lindhardt"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


################################################################################
Sources for developing approaches in meta-analysis: 

https://wviechtb.github.io/metafor/reference/rma.mv.html 
https://www.taylorfrancis.com/chapters/edit/10.1201/9780429061240-8/multivariate-meta-analysis-ding-geng-din-chen-karl-peace 
https://training.cochrane.org/handbook/current/chapter-06 
https://training.cochrane.org/handbook/current/chapter-10


It is not uncommon for studies to have multiple outcome measures which lead to statistical multivariate analysis. The results of studies with multiple outcomes and/or endpoints are typically synthesized via conventional univariate meta-analysis (UMA) on each outcome separately, ignoring the correlations between the outcomes. The impact of ignoring the within-study correlation has been explored extensively in the statistical literature, with issues including overestimated variance of the pooled eﬀect size and biased estimates, which in turn may inﬂuence statistical inferences. In this case, multivariate meta-analysis should be used to synthesize multiple outcomes while taking into account their correlations, often resulting in superior parameter estimation. With study-level moderators or predictors, multivariate meta-regression can also be developed in parallel with multivariate regression techniques.

Meta-Analysis with R Package metafor. Rearrange the Data Format. In order to use the metafor package for multivariate meta-analysis, meta-data
should be rearranged accordingly. With this rearranged data format, we then construct a list of the variance-covariance matrices of the observed outcomes for the ﬁve studies to create a block diagonal matrix, V, for metafor.
With the rearranged data and variance-covariance matrix, V, we now ﬁt the ﬁxed-eﬀects meta-analysis model using metafor with the option of
method="FE". To ﬁt the random-eﬀects multivariate meta-analysis model using the metafor package, we simply change the option to method="REML". We note that this reproduces the results, with this random-effects model, we can also test the difference between

#####################################################

Response Variables are measured on a continuous scale, where each individual outcome is a measurement of a numerical quantity
Chosen Effect Size:  

Formulas to estimate effects (and their standard errors)?

#####################################################
Step-by-Step Framework for Meta-Analysis

1) Data Preparation
Clean and transform the data.
Standardize location information and create unique identifiers.
Classify locations by continent.
--- DESCRIPTIVE_VIZ: Exploratory data analysis and visualization.
Standardize measures of variation and convert SE to SD (save this version).
Impute missing values (silvo_se, control_se, silvo_n, control_n).
Convert SE to SD in the imputed dataset (save this version).
Calculate effect sizes (ROM) for both non-imputed and imputed datasets.
Compare the imputed and non-imputed datasets using descriptive statistics (mean, SD, median) and tests (density plots, boxplots, t-tests, Kolmogorov-Smirnov test).

2) Meta-Analysis Model Fitting
Use multivariate/multilevel mixed-effects models (rma.mv).
Fit models on both non-imputed and imputed datasets.
Compare results side by side, including effect sizes, confidence intervals, and heterogeneity statistics.

3) Sensitivity Analysis and Diagnostics
Perform sensitivity analysis, including leave-one-out analysis.
Conduct trim-and-fill analysis to check for publication bias.

4) Moderator Analysis
Split the dataset by moderators and conduct analyses on both imputed and non-imputed data.
Consider meta-regression to formally test for differences in moderator effects between the datasets.

5) Visualizations
Create comprehensive visual summaries, including caterpillar plots, forest plots, and geographical maps of study locations.

#############
# STEP 0
##########################################################################################################################################
PREPARING SCRIPT AND READ IN THE DATA
##########################################################################################################################################

```{r}
# Clean workspace
rm(list = ls())
```


## Loading required packages and libraries

```{r Loading other needed packages, warning=FALSE}
# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Load multiple add-on packages using pacman::p_load for efficiency
# pacman::p_load automatically installs missing packages and loads them
pacman::p_load(
  
  # Conflict Resolution
  conflicted,        # Resolves conflicts when functions with the same name exist in multiple packages
  
  # Data Manipulation / Transformation
  tidyverse,         # Comprehensive collection of R packages for data science (e.g., ggplot2, dplyr, readr)
  readr,             # Simplifies reading and writing of delimited text files (e.g., CSV)
  dplyr,             # A grammar of data manipulation (e.g., filter, mutate, summarise, etc.)
  skimr,             # Provides summary statistics with a more user-friendly output
  future,            # Supports parallel processing for speeding up computations
  future.apply,      # Extends the future package for parallelized versions of base R apply functions
  
  ###################################################################################################################
  # Data Visualization
  ggplot2,           # A data visualization package for creating static and interactive graphics (part of tidyverse)
  patchwork,         # Extends ggplot2 by providing tools to combine multiple plots into one
  gridExtra,         # Arranges multiple grid-based plots (e.g., from ggplot2) into a single display
  scales,            # Adds tools for handling scale transformations and labels in visualizations
  
  ###################################################################################################################
  # Meta-Analysis
  metafor,           # For conducting meta-analyses, including calculating effect sizes and response ratios
  clubSandwich,      # Provides cluster-robust variance estimators for meta-analysis models
  mice,              # Multivariate Imputation by Chained Equations for handling missing data
  
  ###################################################################################################################
  # Exploratory Data Analysis (EDA)
  DataExplorer,      # Automates exploratory data analysis with summary statistics and visualizations
  SmartEDA,          # Automates exploratory data analysis with summary reports and visualizations
  naniar,            # Provides tools for handling and visualizing missing data
  VIM,               # Visualization and Imputation of Missing Data
  Hmisc,             # Miscellaneous functions including data summary, analysis, and visualization
  BaylorEdPsych,     # Provides tools for reliability analysis and missing data imputation
  
  ###################################################################################################################
  # Project Management and Code Styling
  here,              # Simplifies file referencing by locating the root of a project directory
  styler             # Formats and styles R code to improve readability and consistency
)
  
  # If encountering issues with raster::getData(), consider using geodata::worldclim_global() from the geodata package
})

###################################################################################################################
# Set preferences for conflicting functions
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("group_by", "dplyr") 
conflict_prefer("summarise", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("arrange", "dplyr")
conflict_prefer("locale", "readr")
```



#############
# STEP 1
##########################################################################################################################################
LOADING PREPARED META-DATA
##########################################################################################################################################


Loading the two datasets (imputed and non-imputed)

```{r Loading database, warning=FALSE, message=FALSE}
# Set the working directory automatically using 'here'
setwd(here::here())

# Suppress warnings to avoid clutter in the console output
suppressWarnings({

# Define file paths
non_imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom.rds"))
imp_data_rom <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "imp_data_rom.rds"))

# non_imp_data_rom_dummy <- readRDS(here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "non_imp_data_rom_dummy.rds"))

# Read in the non-imputed dataset
non_imp_dataset <- non_imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Read in the imputed dataset
imp_dataset <- imp_data_rom %>%
  as.data.frame()|> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
})
```

```{r}
# Checking high observations with extreme high variance
high_variance_obs <- 
  imp_dataset|> 
  filter(vi > quantile(vi, 0.995)) |> # 0.995
  # Reorder columns for better readability
  relocate(id_article, response_variable, measured_metrics, measured_unit, 
           yi, vi,
           silvo_mean, silvo_se, silvo_sd, silvo_n, 
           control_mean, control_se, control_sd, control_n,
           tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width) |> 
  arrange(id_article, response_variable)

skim(high_variance_obs)
```
```{r}
non_imp_dataset |> glimpse()
imp_dataset |> glimpse()
```

```{r}
# Select distinct id_article entries
distinct_articles <- imp_dataset %>%
  select(id_article) %>%
  distinct()

# Get the number of unique id_article entries
num_distinct_articles <- nrow(distinct_articles)

# Print the result
cat("Number of distinct id_article entries:", num_distinct_articles, "\n")
# Number of distinct id_article entries: 37 
```


*Points to address after the Tshering meeting with her supervisor (26/11-2024)*

* Subgroup Analysis:
Keep subgroup analyses for each response_variable (e.g., biodiversity, crop yield) to respect conceptual differences. This ensures tailored insights into unique trends and moderators like tree_type or crop_type. Account for reduced statistical power in smaller subgroups by adjusting for multiple comparisons (e.g., Bonferroni). Compare heterogeneity metrics (Q, I², τ²) across subgroups to identify variability differences.

* Meta-Regression:
Use meta-regression as a complementary step to capture overarching patterns. Include response_variable as a moderator and test interactions (e.g., tree_type * response_variable) to detect shared vs. outcome-specific effects. Start with a simpler model and add key moderators incrementally to maintain interpretability.

* Heterogeneity Comparison:
Compare heterogeneity metrics (I², τ²) between original and imputed datasets to assess the impact of imputation. Highlight changes in variability and their implications.

* Visualizations:
Simplify forest plots, focusing on clarity (e.g., one plot per response variable). Use ggplot for clean annotations, log-scaled axes (if relevant), and consistent aesthetics. Add funnel plots for individual response variable models and meta-regression to assess bias and precision.

*Points to address after the meeting with Maarit (03/12-2024*

#### **1. Imputation**
   - **Evaluate Missingness:** 
     - Check patterns of missingness in the original data and verify study reliability.
     - Assess missingness in moderator-response variable combinations.
   - **Non-Random Missingness:** 
     - Test for non-random missingness in standard deviations for moderator-response combinations.
   - **Imputation Methodology:**
     - Use robust methods (e.g., upper quartile) for imputation if data quality is uncertain.
     - Avoid imputing response variables, moderators, or fixed-effect variables (especially continuous ones).
   - **Model Comparison:**
     - Compare model results using imputed vs. non-imputed datasets to assess imputation impact.

---

#### **2. Moderators**
   - Ensure a minimum of 10 studies per sublevel of moderators for sufficient statistical power.

---

#### **3. Random Effects**
   - **Revise `exp_id`:**
     - Aggregate location data within `exp_id` to simplify its structure.
   - **Simplify Random Effects:**
     - Remove unnecessary random effects (`id_article/response_variable`) if `exp_id` sufficiently captures random intercepts/slopes.

---

#### **4. Year as a Fixed Effect**
   - Add standardized year as a fixed-effect, continuous variable to capture potential slope effects.
   - Validate the structure and missingness of the `exp_id` variable before inclusion.
   - "This morning, I worked on our database to determine which level we can use for the location. Since we are already focusing on the temperate climatic zone, I thought it would be good to use the country as the location."
---

#### **5. Multilevel Modelling vs. Subgroup Analysis**
   - Use **multilevel modelling** to account for incomplete combinations of moderators and response variables.
   - Multilevel modelling is valid for disentangling hierarchical structures but, for transparency and communication purposes, subgroup meta-analysis will remain the primary approach.
   - Consider specific interaction terms between moderators after including additive effects in the model.

---

#### **6. Bias Assessment**
   - Test for publication bias and variance error bias for each response variable and moderator.

---

#### **7. Global Mean Comparisons**
   - Include a global mean to compare overall responses, particularly in cases of sign changes across responses.

---

#### **8. Model Diagnostics**
   - **Delta AIC:**
     - Use delta AIC to compare model performance between alternative specifications.
   - **Influence Diagnostics:**
     - Perform diagnostics on a study level using the `nlme` package with `id_article` as the grouping variable.


Key Action Points
	1. Check and Address Missing Data:
		○ Assess patterns of missingness and trustworthiness of studies.
		○ Implement robust imputation for error values only.
	2. Aggregate and Simplify:
		○ Aggregate soil texture levels and location data within exp_id.
	3. Review Model Structure:
		○ Simplify random effects.
		○ Include standardized year as a fixed effect.
	4. Evaluate Bias:
		○ Perform publication and variance error bias assessments.
	5. Use Multilevel Models:
		○ Implement multilevel modeling to manage hierarchical structures.
	6. Conduct Diagnostics:
		○ Apply influence diagnostics at the study level (nlme).
	7. Include Interaction Terms:
		○ Incorporate interactions only after main effects in the model.
	8. Global Response Comparisons:
Use a global mean for overarching response evaluations.

##########################################################################################################################################
LISTING RESPONSE VARIABLES AND SETTING UP COSTUM COLORS
##########################################################################################################################################

```{r}
# Custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Response variables to analyze
response_variables <- names(custom_colors)
```




#############
# STEP 2
##########################################################################################################################################
PERFORMING MULTIVARIATE/MULTILEVEL LINEAR (MIXED-EFFECTS) MODELLING 
##########################################################################################################################################

Assessment of Missing Data for Moderators
Imputation of Missing Values for Moderators Using mice()
Post-Imputation Assessment of Moderators
Selection of Moderators for Analysis
Fitting the Multivariate Random-Effects Model with Selected Moderators

```{r}
# Define the function for missing data assessment
assess_missing_data <- function(dataset, moderators, dataset_name = "Dataset") {
  
  cat("\nStarting missing data assessment for", dataset_name, "...\n")
  
  # Step 1: Calculate the proportion of missing values for each moderator
  missing_summary <- dataset %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_proportion")

  # Print the missing summary table
  cat("\nProportion of Missing Values for Each Moderator:\n")
  print(missing_summary)

  # Step 2: Create a basic bar chart of missing proportions
  missing_plot <- ggplot(missing_summary, aes(x = reorder(variable, -missing_proportion), y = missing_proportion)) +
    geom_bar(stat = "identity", fill = "#0072B2") +
    labs(
      title = paste("Proportion of Missing Data for Moderator Variables -", dataset_name),
      x = "Moderator Variable",
      y = "Missing Proportion"
    ) +
    scale_y_continuous(labels = scales::percent) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Step 3: Calculate missingness for each moderator by response_variable
  missing_by_response <- dataset %>%
    group_by(response_variable) %>%
    summarise(across(all_of(moderators), ~ mean(is.na(.), na.rm = TRUE))) %>%
    pivot_longer(cols = -response_variable, names_to = "moderator", values_to = "missing_proportion")

  # Print the summary table for missingness by response_variable
  cat("\nMissing Proportion by Response Variable for Each Moderator:\n")
  print(missing_by_response)

  # Step 4: Create a heatmap for missingness by response_variable
  missing_heatmap <- ggplot(missing_by_response, aes(x = moderator, y = response_variable, fill = missing_proportion)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "#56B1F7", high = "#132B43", na.value = "gray90", labels = percent) +
    labs(
      title = paste("Heatmap of Missing Data by Moderator and Response Variable -", dataset_name),
      x = "Moderator Variable",
      y = "Response Variable",
      fill = "Missing Proportion"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Display the plots
  print(missing_plot)
  print(missing_heatmap)
  
  cat("\nMissing data assessment completed for", dataset_name, ".\n")
}
```

```{r}
# Assessing Moderator missingness

moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Assess missing data for non-imputed dataset
assess_missing_data(non_imp_dataset, moderators, "Non-Imputed Dataset")

# Assess missing data for imputed dataset
assess_missing_data(imp_dataset, moderators, "Imputed Dataset")
```






##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################


```{r}
# Variance-Covariance Matrix Calculation Function
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  v_list <- list()
  for (study in unique(data$id_article)) {
    study_data <- data[data$id_article == study, ]
    
    if (nrow(study_data) > 1) {
      v <- diag(study_data$vi)
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      v_list[[as.character(study)]] <- v
    } else {
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }

  v_matrix <- bldiag(v_list)
  cat("\nGenerated Variance-Covariance Matrix:\n")
  print(v_matrix)
  
  return(v_matrix)
}
```

```{r}
# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset




# Directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Generate and save v_matrices for each response variable
v_matrices <- list()

for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Calculate the variance-covariance matrix
  v_matrix <- calculate_v_matrix(data_subset, correlation = 0.5)
  
  # Store the matrix in the list
  v_matrices[[response]] <- v_matrix
  
  # Save the matrix to an individual RDS file
  file_name <- paste0("v_matrix_", tolower(gsub(" ", "_", response)), ".rds")
  saveRDS(v_matrix, file = file.path(output_dir, file_name))
  
  cat("Saved v_matrix for response variable:", response, "to", file.path(output_dir, file_name), "\n")
}

# Also, save the entire list of v_matrices as a single file
saveRDS(v_matrices, file = file.path(output_dir, "v_matrices_by_response_variable.rds"))
cat("\nAll v_matrices saved to:", output_dir, "\n")
```




#############
# STEP 3
##########################################################################################################################################
MODEL FITTING ON EACH SUBSET DATA USING ASSOCIATED VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

```{r}
# Load the saved v_matrices
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(output_dir, "v_matrices_by_response_variable.rds"))
```

(1)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
  # the experiment_year (the year the experiment was conducted)
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  
(2)
  # Group data by relevant columns for Experiment ID
  # The exp_id variable is created as a unique identifier for experiments, 
  # based on grouping by: id_article (the article ID), location (the geographical location of the experiment), 
  # the experiment_year (the year the experiment was conducted), and study duration (the number of years the study has been completed)
  group_by(id_article, location, experiment_year, study duration) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 

A)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article,                           # Random intercept for each article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
B)      
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
C)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),

D)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 + Year | id_article,                    # Random intercept for each year and article/study
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
```{r}
# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }

# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      # Data: the dataset used for the analysis
      data = data_subset,
      # Method: optimization method for model fitting
      method = "ML",                                # Maximum likelihood estimation
      # Control settings: additional options for the optimization algorithm
      control = list(
        optimizer = "optim",                        # Specify the optimization function to use
        optim.method = "BFGS",                      # Use the Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                            # Maximum number of iterations allowed
        rel.tol = 1e-8                              # Convergence tolerance (stopping criterion for optimization)
      )
    )
}, error = function(e) {                            # Catch any errors during model fitting
    cat("Error in model fitting:", e$message, "\n") # Print the error message
    return(NULL)                                    # Return NULL if an error occurs
})


  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }

  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()

  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = list(
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data_subset,
      method = "ML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 11.96204 secs

# Last go (01/01-2025)
# Time difference of 8.034523 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 8.034523 secs
```

#############
# STEP 4
##########################################################################################################################################
MODEL DIAGNOSTICS ON EACH SUBSET MODEL FITTING 
##########################################################################################################################################

```{r}
# Load the saved models
model_results <- readRDS(file.path(output_dir, "fitted_models_by_response_variable.rds"))
```

```{r}
summary(model_results$Biodiversity)
```


```{r}
# Function to extract key diagnostics from a fitted model
extract_model_diagnostics <- function(model, response_variable) {
  if (is.null(model)) {
    return(data.frame(
      ResponseVariable = response_variable,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      Tau2 = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract diagnostics
  aic <- AIC(model)
  bic <- BIC(model)
  log_likelihood <- as.numeric(logLik(model))
  tau2 <- sum(model$sigma2)
  i2 <- round((tau2 / (tau2 + mean(model$vi))) * 100, 1)
  qm <- model$QM
  qmp <- model$QMp
  
  data.frame(
    ResponseVariable = response_variable,
    AIC = aic,
    BIC = bic,
    LogLikelihood = log_likelihood,
    Tau2 = tau2,
    I2 = i2,
    QM = qm,
    QMp = qmp
  )
}
```

```{r}
# Extract diagnostics for all models
model_diagnostics <- bind_rows(
  lapply(names(model_results), function(response) {
    extract_model_diagnostics(model_results[[response]], response)
  })
)
```

```{r}
# Save diagnostics table
write.csv(model_diagnostics, file.path(output_dir, "model_diagnostics_summary.csv"), row.names = FALSE)
```

```{r}
# Visualize AIC, BIC, and Log-Likelihood
diagnostics_plot <- model_diagnostics %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood), names_to = "Metric", values_to = "Value") %>%
  ggplot(aes(x = ResponseVariable, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Model Fit Comparison",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(diagnostics_plot)
```



Visualization 2: Fixed Effects Estimates Comparison

```{r}
# Plot Fixed Effects Estimates with Confidence Intervals
coef_plot <- ggplot(fixed_effects_data, aes(x = Term, y = Estimate, color = Dataset)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Comparison of Fixed Effects Estimates Across Models",
       x = "Fixed Effect Term",
       y = "Estimate",
       color = "Dataset") +
  theme_minimal()

print(coef_plot)
```


Visualization 3: Heterogeneity (I²) Comparison

```{r}
# Heterogeneity Comparison Plot
I2_plot <- ggplot(model_summaries, aes(x = Dataset, y = I2, fill = Dataset)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(I2, 2)), vjust = -0.5) +
  labs(title = "Comparison of I² (Heterogeneity) Across Models",
       x = "Dataset",
       y = "I² (%)") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))

print(I2_plot)

```

Comparison table of key model statistics

```{r}
colnames(model_summaries)
```

```{r}
# Visualize Variance Components (Tau2) and Heterogeneity (I²)
variance_plot <- model_diagnostics %>%
  ggplot(aes(x = ResponseVariable)) +
  geom_bar(aes(y = Tau2, fill = "Tau2 (Variance Components)"), stat = "identity", position = "dodge") +
  geom_point(aes(y = I2 / 100, color = "I² (Heterogeneity)"), size = 4) +
  scale_y_continuous(
    name = "Variance Components (Tau2)",
    sec.axis = sec_axis(~.*100, name = "Heterogeneity (I² %)"),
    limits = c(0, 0.02, na.rm = TRUE)
  ) +
  labs(
    title = "Variance Components and Heterogeneity",
    x = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

variance_plot
```

```{r}
# Create a formatted summary table using `gt`
diagnostics_table <- model_diagnostics %>%
  gt() %>%
  tab_header(
    title = "Model Diagnostics Summary",
    subtitle = "Comparison of Key Metrics Across Models"
  ) %>%
  fmt_number(
    columns = c(AIC, BIC, LogLikelihood, Tau2, I2, QM, QMp),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = "small",
    column_labels.font.size = "medium"
  )

# the table
model_diagnostics

# the gt table
diagnostics_table
```
```{r}
# Save table to an HTML file
gtsave(diagnostics_table, file.path(output_dir, "model_diagnostics_summary.html"))
```



#############
# STEP 5
##########################################################################################################################################
KEY INFLUENCE DIAGNOSTICS ON EACH SUBSET - SIMPLIFIED MODEL FITTING 
##########################################################################################################################################

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Fit models for each response variable
model_results_infdia <- list()

for (response in unique(meta_data$response_variable)) {
  cat("\nProcessing response variable:", response, "...\n")
  
  # Subset data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the model with random effects for `id_article`
  res <- tryCatch({
    rma(yi = yi, 
        vi = vi, 
        # Add study-level random effect
        random = ~ 1 | id_article,  
        data = data_subset, 
        # Restricted ML
        method = "REML")
  }, error = function(e) {
    cat("Model fitting failed for", response, ":", e$message, "\n")
    return(NULL)
  })
  
  # Save the fitted model
  model_results_infdia[[response]] <- res
}

# Recompute influence diagnostics
influence_diagnostics <- list()

for (response in names(model_results_infdia)) {
  cat("\nComputing influence diagnostics for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    inf <- tryCatch({
      influence(model)
    }, error = function(e) {
      cat("Influence diagnostics failed for:", response, ":", e$message, "\n")
      return(NULL)
    })
    influence_diagnostics[[response]] <- inf
  } else {
    cat("Skipping influence diagnostics for:", response, "due to missing model.\n")
  }
}


# Perform leave-one-out analysis
leave1out_results <- list()

for (response in names(model_results_infdia)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################

# Last go (02/12-2024)
# Time difference of 1.117442 mins
# Time difference of 3.097009 mins
# Time difference of 2.744736 mins

# Last go (01/01-2025)
# Time difference of 2.148423 mins
# Processing response variable: Biodiversity ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Crop yield ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Water quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Pest and Disease ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Soil quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Greenhouse gas emission ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Product quality ...
# Advarsel: Extra argument ('random') disregarded.
# Computing influence diagnostics for: Biodiversity ...
# Computing influence diagnostics for: Crop yield ...
# Computing influence diagnostics for: Water quality ...
# Computing influence diagnostics for: Pest and Disease ...
# Computing influence diagnostics for: Soil quality ...
# Computing influence diagnostics for: Greenhouse gas emission ...
# Computing influence diagnostics for: Product quality ...
# Running Leave-One-Out for: Biodiversity ...
# Running Leave-One-Out for: Crop yield ...
# Running Leave-One-Out for: Water quality ...
# Running Leave-One-Out for: Pest and Disease ...
# Running Leave-One-Out for: Soil quality ...
# Running Leave-One-Out for: Greenhouse gas emission ...
# Running Leave-One-Out for: Product quality ...
# Time difference of 2.148423 mins
```


=======
# Last go: (17/11-24)
# Time difference of 25.3915 secs
# 
# Calculating Bootstrap Confidence Intervals for Non-Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Dataset .
# 
# Forest Plot for Non-Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Dataset .
# 
# Forest Plot for Imputed Dataset :
# Advarsel: longer object length is not a multiple of shorter object length
# Calculating Bootstrap Confidence Intervals for Non-Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Imputed Dataset .
# 
# Forest Plot for Non-Imputed Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Imputed Dataset .
# 
# Forest Plot for Imputed Imputed Dataset :

Saving plots


Fit the simplified multivariate random-effects model for diagnostics

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Helper function to run the multivariate random-effects model
run_meta_analysis <- function(data, dataset_name) {
  cat("Running analysis for", dataset_name, "\n")
  
  # Step 1: Calculate the variance-covariance matrix if needed
  V_matrix <- as.matrix(data$vi) # Example assumes `vi` is already correct
  
  # Step 2: Fit the simplified multivariate random-effects model
  model_meta_diagnostics <- tryCatch({
    rma(
      yi = data$yi,
      vi = data$vi,
      random = list(
        ~ 1 | id_article,
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data,
      method = "ML"
    )
  }, error = function(e) {
    stop("Error in model fitting: ", e$message)
  })
  
  # Step 3: Compute influence diagnostics
  inf <- influence(model_meta_diagnostics)
  
  # Return the model and diagnostics
  return(list(model = model_meta_diagnostics, influence = inf))
}

# Step 1: Apply meta-analysis function to each dataset
results <- lapply(names(datasets), function(dataset_name) {
  tryCatch({
    run_meta_analysis(datasets[[dataset_name]], dataset_name)
  }, error = function(e) {
    cat("Error in dataset", dataset_name, ":", e$message, "\n")
    return(NULL)
  })
})

# Step 2: Name the results list
names(results) <- names(datasets)


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (27/11-24)
# Time difference of 2.942727 hours
```


```{r}
saveRDS(results, file = "meta_analysis_diagnosis_on_datasets_results.rds")
```


```{r}
# Step 3: Plot influence diagnostics
par(mfrow = c(8, 1), oma = c(2, 2, 2, 2)) # Set up plotting area
for (dataset_name in names(results)) {
  if (!is.null(results[[dataset_name]])) {
    plot(results[[dataset_name]]$influence) # Default plot
    mtext(paste("Influence Diagnostics for", dataset_name), side = 3, line = 0.5, outer = FALSE)
  }
}


# Optional: Inspect the results
# str(results)
```

```{r}
datasets_effect_sizes$non_imp_dataset %>% glimpse()
```


















##########################################################################################################################################
SAVING DATASETS AND MODEL OBJECTS
##########################################################################################################################################

```{r}
# Save the results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(model_results_infdia, file = file.path(output_dir, "simplified_fitted_models.rds"))
saveRDS(influence_diagnostics, file = file.path(output_dir, "simplified_influence_diagnostics.rds"))
cat("\nModels and influence diagnostics saved to:", output_dir, "\n")
```
```{r}
influence_diagnostics |> str()
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Generate plots for influence diagnostics
plot_dir <- file.path(output_dir, "Influence_Diagnostics_Plots")
if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE)

for (response in names(influence_diagnostics)) {
  cat("\nGenerating influence diagnostic plots for:", response, "...\n")
  
  inf <- influence_diagnostics[[response]]
  
  if (!is.null(inf)) {
    tryCatch({
      par(mfrow = c(8, 1))
      plot(inf)
      dev.copy(jpeg, file = file.path(plot_dir, paste0("influence_plot_", gsub(" ", "_", response), ".jpg")))
      dev.off()
    }, error = function(e) {
      cat("Plotting failed for", response, ":", e$message, "\n")
    })
  } else {
    cat("No influence diagnostics available for:", response, "\n")
  }
}
cat("\nInfluence diagnostic plots saved to:", plot_dir, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 49.21399 secs
```

```{r}
# Perform leave-one-out analysis at the study level
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    # Perform LOO only for studies (id_article)
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

```

```{r}
# Function to extract influence diagnostic data
extract_diagnostics <- function(inf_obj, response_variable) {
  if (is.null(inf_obj)) {
    cat("No influence diagnostics available for", response_variable, "\n")
    return(NULL)
  }
  
  # Extract diagnostics safely and pad missing elements with NA
  tryCatch({
    n_studies <- length(inf_obj$inf$rstudent)  # Total number of studies
    
    diagnostics <- data.frame(
      Study = if (!is.null(rownames(inf_obj$inf$rstudent))) rownames(inf_obj$inf$rstudent) else seq_len(n_studies),
      rstudent = inf_obj$inf$rstudent,
      dffits = inf_obj$inf$dffits,
      cook.d = inf_obj$inf$cook.d,
      cov.r = inf_obj$inf$cov.r,
      tau2.del = inf_obj$inf$tau2.del,
      QE.del = inf_obj$inf$QE.del,
      hat = inf_obj$inf$hat,
      weight = inf_obj$inf$weight,
      ResponseVariable = response_variable
    )
    
    return(diagnostics)
  }, error = function(e) {
    cat("Error extracting diagnostics for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
}
```

```{r}
# Combine diagnostics into a single data frame
# Combine diagnostics into a single data frame
diagnostics_list <- lapply(names(influence_diagnostics), function(response) {
  inf <- influence_diagnostics[[response]]
  extract_diagnostics(inf, response)
})

# Filter out NULL entries
diagnostics_data <- do.call(rbind, diagnostics_list[!sapply(diagnostics_list, is.null)])

# Check the resulting data frame
diagnostics_data |> glimpse()
```

```{r}
# Save diagnostics dataset
# diagnostics_list |> str()

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save the diagnostics list as an RDS file
saveRDS(diagnostics_list, file = file.path(output_dir, "diagnostics_list.rds"))
cat("Diagnostics list saved as RDS file to:", file.path(output_dir, "diagnostics_list.rds"), "\n")
```


```{r}
# Check the distribution of diagnostics by response variable
table(diagnostics_data$ResponseVariable)

# Summarize key diagnostics for each response variable
summary_stats <- diagnostics_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    Mean_rstudent = mean(rstudent, na.rm = TRUE),
    Max_rstudent = max(rstudent, na.rm = TRUE),
    Mean_dffits = mean(dffits, na.rm = TRUE),
    Max_dffits = max(dffits, na.rm = TRUE),
    Mean_cook.d = mean(cook.d, na.rm = TRUE),
    Max_cook.d = max(cook.d, na.rm = TRUE),
    .groups = "drop"
  )

summary_stats
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = rstudent, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Standardized Residuals by Study",
    x = "Study ID",
    y = "Standardized Residuals"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = cook.d, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Cook's Distance by Study",
    x = "Study ID",
    y = "Cook's Distance"
  ) +
  theme_minimal() +
        theme(
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "none",
          panel.spacing = unit(1, "lines")
        )
```

```{r}
# Define thresholds
thresholds <- diagnostics_data %>%
  mutate(
    IsInfluential = (abs(rstudent) > 2) | (cook.d > 0.5)
  )

# Check how many studies are flagged as influential
table(thresholds$ResponseVariable, thresholds$IsInfluential)

# Save flagged data for review
output_dir <- here::here("DATA", "OUTPUT_FROM_R")
write.csv(thresholds, file.path(output_dir, "flagged_influential_studies.csv"), row.names = FALSE)
```



```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################





##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 2.056713 mins
```


```{r}
# Define the output directory an file path for the plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
output_file <- file.path(output_dir, "leave_one_out_effect_sizes.png")

# Save the plot to the output directory
ggsave(output_file, plot = loo_plot, width = 12, height = 8, dpi = 300)
cat("Plot saved to:", output_file, "\n")
```


```{r}
# Inspect the diagnostics data structure for a single response variable
str(diagnostics_list[[1]])
```


```{r}
# Define a threshold for Cook's Distance
cooks_threshold <- 0.8  # Adjust as necessary

# Identify influential studies
influential_studies <- diagnostics_data %>%
  filter(cook.d > cooks_threshold) %>%
  distinct(Study, ResponseVariable)

# Map back to `meta_data`
mapped_influential <- meta_data %>%
  semi_join(influential_studies, by = c("id_article" = "Study"))

# View the result
print(mapped_influential)
```


#############
# STEP 6
##########################################################################################################################################
PUBLICATION-READY PLOTS OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################

Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.

```{r}
model_results |> str()
model_results |> glimpse()
```


```{r}
# Combine results from all response variables into a single data frame
# Prepare data for visualization
forest_plot_data <- bind_rows(
  lapply(names(model_results), function(response) {
    model <- model_results[[response]]
    
    if (!is.null(model) && length(model$yi) > 0) {
      data.frame(
        Study = model$data$id_article,  # Use the unique id_obs for studies
        EffectSize = model$yi,      # Effect sizes
        CI_Lower = model$yi - 1.96 * sqrt(model$vi),  # Lower CI
        CI_Upper = model$yi + 1.96 * sqrt(model$vi),  # Upper CI
        ResponseVariable = response # Response variable
      )
    } else {
      NULL  # Skip if no valid data
    }
  })
)

# Check the prepared data
forest_plot_data |> glimpse()
```
```{r}
# Prepare Aggregated Data
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

aggregated_data
```


```{r}
# Create the forest plot with custom colors
forest_plot <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank))) +
  # Add points for effect sizes
  geom_point(aes(size = size_category, color = ResponseVariable)) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = ResponseVariable), height = 0.2) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot of Response Variables with Custom Colors",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Number of Studies"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Display the plot
forest_plot
```























