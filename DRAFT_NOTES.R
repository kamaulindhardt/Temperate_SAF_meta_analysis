
*Points to address after the Tshering meeting with her supervisor (26/11-2024)*

* Subgroup Analysis:
Keep subgroup analyses for each response_variable (e.g., biodiversity, crop yield) to respect conceptual differences. This ensures tailored insights into unique trends and moderators like tree_type or crop_type. Account for reduced statistical power in smaller subgroups by adjusting for multiple comparisons (e.g., Bonferroni). Compare heterogeneity metrics (Q, I², τ²) across subgroups to identify variability differences.

* Meta-Regression:
Use meta-regression as a complementary step to capture overarching patterns. Include response_variable as a moderator and test interactions (e.g., tree_type * response_variable) to detect shared vs. outcome-specific effects. Start with a simpler model and add key moderators incrementally to maintain interpretability.

* Heterogeneity Comparison:
Compare heterogeneity metrics (I², τ²) between original and imputed datasets to assess the impact of imputation. Highlight changes in variability and their implications.

* Visualizations:
Simplify forest plots, focusing on clarity (e.g., one plot per response variable). Use ggplot for clean annotations, log-scaled axes (if relevant), and consistent aesthetics. Add funnel plots for individual response variable models and meta-regression to assess bias and precision.

*Points to address after the meeting with Maarit (03/12-2024*

#### **1. Imputation**
   - **Evaluate Missingness:** 
     - Check patterns of missingness in the original data and verify study reliability.
     - Assess missingness in moderator-response variable combinations.
   - **Non-Random Missingness:** 
     - Test for non-random missingness in standard deviations for moderator-response combinations.
   - **Imputation Methodology:**
     - Use robust methods (e.g., upper quartile) for imputation if data quality is uncertain.
     - Avoid imputing response variables, moderators, or fixed-effect variables (especially continuous ones).
   - **Model Comparison:**
     - Compare model results using imputed vs. non-imputed datasets to assess imputation impact.

---> missingness pattern assessment before imputation on the _sd (we should have received something from Maarit). The pmm method might not be appropriate. We should impute based on the missingness. We can also test the linear reg. method. Remind Maarit about the imputation paper + script. Add the test the linear reg. method

---> Missingness pattern (across response variable and moderators). 
Are there specific patterns. And then what to do with that? 


---

#### **2. Moderators**
   - Ensure a minimum of 10 studies per sublevel of moderators for sufficient statistical power.



---

#### **3. Random Effects**
   - **Revise `exp_id`:**
     - Aggregate location data within `exp_id` to simplify its structure.
   - **Simplify Random Effects:**
     - Remove unnecessary random effects (`id_article/response_variable`) if `exp_id` sufficiently captures random intercepts/slopes.

---

#### **4. Year as a Fixed Effect**
   - Add standardized year as a fixed-effect, continuous variable to capture potential slope effects.
   - Validate the structure and missingness of the `exp_id` variable before inclusion.
   - "This morning, I worked on our database to determine which level we can use for the location. Since we are already focusing on the temperate climatic zone, I thought it would be good to use the country as the location."
---

#### **5. Multilevel Modelling vs. Subgroup Analysis**
   - Use **multilevel modelling** to account for incomplete combinations of moderators and response variables.
   - Multilevel modelling is valid for disentangling hierarchical structures but, for transparency and communication purposes, subgroup meta-analysis will remain the primary approach.
   - Consider specific interaction terms between moderators after including additive effects in the model.

---

#### **6. Bias Assessment**
   - Test for publication bias and variance error bias for each response variable and moderator.

---

#### **7. Global Mean Comparisons**
   - Include a global mean to compare overall responses, particularly in cases of sign changes across responses.

---

#### **8. Model Diagnostics**
   - **Delta AIC:**
     - Use delta AIC to compare model performance between alternative specifications.
   - **Influence Diagnostics:**
     - Perform diagnostics on a study level using the `nlme` package with `id_article` as the grouping variable.




Key Action Points
	1. Check and Address Missing Data:
		○ Assess patterns of missingness and trustworthiness of studies.
		○ Implement robust imputation for error values only.
	2. Aggregate and Simplify:
		○ Aggregate soil texture levels and location data within exp_id.
	3. Review Model Structure:
		○ Simplify random effects.
		○ Include standardized year as a fixed effect.
	4. Evaluate Bias:
		○ Perform publication and variance error bias assessments.
	5. Use Multilevel Models:
		○ Implement multilevel modeling to manage hierarchical structures.
	6. Conduct Diagnostics:
		○ Apply influence diagnostics at the study level (nlme).
	7. Include Interaction Terms:
		○ Incorporate interactions only after main effects in the model.
	8. Global Response Comparisons:
Use a global mean for overarching response evaluations.


*Points to address after our 2025 meeting (02/01-2025)*

---> missingness pattern assessment before imputation on the _sd (we should have received something from Maarit). 
	The pmm method might not be appropriate. We should impute based on the missingness. We can also test the linear reg. method. 
	Remind Maarit about the imputation paper + script. Add the test the linear reg. method

---> Missingness pattern (across response variable and moderators). 
Are there specific patterns. And then what to do with that? 

----> Water qual and GGE is left out of the analysis because of too little data

----> Leave-one-out analysis for sensitivity analysis on moderators and articles/studies on the effect size. This should be done on the full dataset and not on subsets of response variables

----> Publication-ready plots: Forest plots, caterpillar plots, funnel plots, and geographical maps of study locations.

----> Update the methods section with the new approach for the multivariate analysis















(1)
# Group data by relevant columns for Experiment ID
# The exp_id variable is created as a unique identifier for experiments, 
# based on grouping by: id_article (the article ID), location (the geographical location of the experiment), and 
# the experiment_year (the year the experiment was conducted)
group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  
  (2)
# Group data by relevant columns for Experiment ID
# The exp_id variable is created as a unique identifier for experiments, 
# based on grouping by: id_article (the article ID), location (the geographical location of the experiment), 
# the experiment_year (the year the experiment was conducted), and study duration (the number of years the study has been completed)
group_by(id_article, location, experiment_year, study duration) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  A)
# Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
model <- tryCatch({
  # Fit a random-effects multivariate meta-analysis model
  rma.mv(
    # Dependent variable: effect size (yi)
    yi = yi,
    # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
    V = v_matrix,
    # Moderators: a formula specifying the covariates to include in the model
    mods = moderator_formula,
    # Random-effects structure: defines how the random effects are modeled hierarchically
    random = list(
      ~ 1 | id_article,                           # Random intercept for each article/study
      ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
      ~ 1 | exp_id                                # Random intercept for individual experiments
    ),
    
    
    B)      
  # Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
  model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      
      
      C)
    # Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
    model <- tryCatch({
      # Fit a random-effects multivariate meta-analysis model
      rma.mv(
        # Dependent variable: effect size (yi)
        yi = yi,
        # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
        V = v_matrix,
        # Moderators: a formula specifying the covariates to include in the model
        mods = moderator_formula,
        # Random-effects structure: defines how the random effects are modeled hierarchically
        random = list(
          ~ 1 | exp_id                                # Random intercept for individual experiments
        ),
        
        D)
      # Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
      model <- tryCatch({
        # Fit a random-effects multivariate meta-analysis model
        rma.mv(
          # Dependent variable: effect size (yi)
          yi = yi,
          # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
          V = v_matrix,
          # Moderators: a formula specifying the covariates to include in the model
          mods = moderator_formula,
          # Random-effects structure: defines how the random effects are modeled hierarchically
          random = list(
            ~ 1 + Year | id_article,                    # Random intercept for each year and article/study
            ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
            ~ 1 | exp_id                                # Random intercept for individual experiments
          ),
          
          





























```{r}
# Convert moderators to factors
preprocess_data <- function(data, moderators) {
  data <- data %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  return(data)
}

# Validate and adjust V_matrix
validate_v_matrix <- function(data, V_matrix) {
  # Check if rownames match between data and V_matrix
  if (!identical(rownames(V_matrix), rownames(data))) {
    stop("Row names of V_matrix and data do not match!")
  }
  
  # Ensure positive definiteness
  is_positive_definite <- function(mat) {
    if (!isSymmetric(mat)) {
      return(FALSE)
    }
    eigenvalues <- eigen(mat, symmetric = TRUE, only.values = TRUE)$values
    return(all(eigenvalues > 0))
  }
  
  if (!is_positive_definite(V_matrix)) {
    cat("Matrix is not positive definite. Adding small diagonal correction...\n")
    diag(V_matrix) <- diag(V_matrix) + 1e-6
  }
  
  return(V_matrix)
}
```


```{r}
# Create subsets for all datasets
test_non_imp <- create_test_subset(non_imp_dataset, V_matrix_non_imp, sample_size = 150)
test_imp <- create_test_subset(imp_dataset, V_matrix_imp, sample_size = 150)
test_non_imp_imputed <- create_test_subset(non_imp_dataset_imputed, V_matrix_non_imp_imputed, sample_size = 150)
test_imp_imputed <- create_test_subset(imp_dataset_imputed, V_matrix_imp_imputed, sample_size = 150)
```
```{r}
identical(rownames(non_imp_dataset), rownames(V_matrix_non_imp))
```

```{r}
identical(rownames(test_non_imp$data), rownames(test_non_imp$V_matrix))
```
```{r}
dim(V_matrix_non_imp) # Should show a square matrix
identical(rownames(V_matrix_non_imp), colnames(V_matrix_non_imp)) # Should be TRUE
```


```{r}
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Function to calculate I² statistic for meta-regression models
# Define calculate_I2 function
calculate_I2 <- function(model, data) {
  # Extract variance components
  tau2 <- sum(model$sigma2)  # Total heterogeneity
  sigma2 <- mean(data$vi, na.rm = TRUE)  # Average within-study variance
  
  # Calculate I² (heterogeneity percentage)
  if (!is.na(tau2) && !is.na(sigma2) && (tau2 + sigma2) > 0) {
    I2 <- (tau2 / (tau2 + sigma2)) * 100
  } else {
    I2 <- NA
  }
  
  return(I2)
}


run_loo_sensitivity <- function(data, V_matrix, moderator_formula) {
  cat("\nRunning LOO sensitivity analysis...\n")
  
  n <- nrow(data)
  loo_results <- vector("list", n)
  
  for (i in seq_len(n)) {
    cat("Removing study:", i, "/", n, "\n")
    
    # Subset the data excluding the i-th study
    loo_data <- data[-i, ]
    V_matrix_loo <- V_matrix[-i, -i]
    
    # Fit the full model on the LOO subset
    model <- tryCatch({
      rma.mv(
        yi = yi,
        V = V_matrix_loo,
        mods = moderator_formula,
        random = list(
          ~ 1 | id_article,
          ~ 1 | id_article/response_variable,
          ~ 1 | exp_id
        ),
        data = loo_data,
        method = "ML",
        control = list(
          optimizer = "optim",
          optim.method = "BFGS",
          iter.max = 1000,
          rel.tol = 1e-8
        )
      )
    }, error = function(e) {
      cat("Error fitting model:", e$message, "\n")
      return(NULL)
    })
    
    # Store results if model fitting was successful
    if (!is.null(model)) {
      loo_results[[i]] <- list(
        study_removed = loo_data$id_article[i],
        estimate = coef(model)[1],
        se = model$se[1],
        I2 = ifelse(length(model$sigma2) > 0, 100 * sum(model$sigma2) / (sum(model$sigma2) + mean(data$vi)), NA),
        AIC = AIC(model),
        BIC = BIC(model),
        LogLik = logLik(model)
      )
    }
  }
  
  return(loo_results)
}
```




**OBS! This is a subset because it takes long with the full datasets**
  ```{r, eval=FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

##########################################################################
# Run LOO sensitivity analysis on test subsets

results <- list(
  non_imp = process_and_fit_with_loo(test_non_imp, "Non-Imputed Test Subset", moderators),
  imp = process_and_fit_with_loo(test_imp, "Imputed Test Subset", moderators),
  non_imp_imputed = process_and_fit_with_loo(test_non_imp_imputed, "Non-Imputed Imputed Test Subset", moderators),
  imp_imputed = process_and_fit_with_loo(test_imp_imputed, "Imputed Test Subset", moderators)
)

# Save results
saveRDS(
  results,
  file = file.path(output_dir, "meta_model_results_with_loo_test_subsets.rds")
)

cat("Results saved to:", file.path(output_dir, "meta_model_results_with_loo_test_subsets.rds"), "\n")
##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (21/11-24)
# Time difference of 17.70906 mins
# Time difference of 12.06283 mins
```

Diagnostics of the LOO model fitting

```{r}
# Visualize fixed effects
ggplot(all_fixed_effects, aes(x = estimate, y = term, color = dataset)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub), height = 0.3, position = position_dodge(width = 0.5)) +
  facet_wrap(~ response_variable, scales = "free_y") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Comparison of Fixed Effects Across Models by Response Variable",
    subtitle = "Estimates with 95% Confidence Intervals",
    x = "Estimate",
    y = "Fixed Effect Term",
    color = "Dataset"
  ) +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_line(color = "gray", linetype = "dashed"),
    panel.grid.minor = element_blank()
  )
```

```{r}
# Check if a matrix is symmetric positive definite
is_positive_definite <- function(mat) {
  if (!isSymmetric(mat)) {
    return(FALSE)
  }
  eigenvalues <- eigen(mat, symmetric = TRUE, only.values = TRUE)$values
  return(all(eigenvalues > 0))
}

# Adjust the V_matrix to make it symmetric positive definite
make_positive_definite <- function(V_matrix) {
  if (!is_positive_definite(V_matrix)) {
    diag(V_matrix) <- diag(V_matrix) + 1e-6
  }
  return(V_matrix)
}

# Validate and align the dataset and its V_matrix
validate_data_and_v_matrix <- function(data, V_matrix) {
  # Symmetrize V_matrix if needed
  if (!isSymmetric(V_matrix)) {
    V_matrix <- (V_matrix + t(V_matrix)) / 2
    cat("V_matrix has been symmetrized.\n")
  }
  
  # Ensure positive definiteness
  if (!is_positive_definite(V_matrix)) {
    diag(V_matrix) <- diag(V_matrix) + 1e-6
    cat("V_matrix adjusted for positive definiteness.\n")
  }
  
  # Validate row alignment between data and V_matrix
  if (!identical(rownames(V_matrix), rownames(data))) {
    stop("Row names of V_matrix and data do not match!")
  }
  
  return(V_matrix)
}

# Function to create a test subset of data and its associated V_matrix
create_test_subset <- function(data, V_matrix, sample_size = 150) {
  if (nrow(data) < sample_size) {
    stop("Sample size is larger than the dataset.")
  }
  
  # Ensure rownames exist
  if (is.null(rownames(data))) {
    rownames(data) <- seq_len(nrow(data))
  }
  if (is.null(rownames(V_matrix))) {
    rownames(V_matrix) <- colnames(V_matrix) <- seq_len(nrow(V_matrix))
  }
  
  # Sample row indices
  sampled_indices <- sample(rownames(data), size = sample_size, replace = FALSE)
  
  # Subset data and V_matrix
  data_subset <- data[sampled_indices, , drop = FALSE]
  V_matrix_subset <- V_matrix[sampled_indices, sampled_indices, drop = FALSE]
  
  # Validate alignment
  if (!identical(rownames(data_subset), rownames(V_matrix_subset))) {
    stop("Row names of V_matrix and data do not match!")
  }
  
  # Return validated subsets
  list(data = data_subset, V_matrix = V_matrix_subset)
}
```





























```{r}
# Apply the function to all datasets
non_imp_results <- process_loo_diagnostics(diagnostics_non_imp, "Non-Imputed Data")
imp_results <- process_loo_diagnostics(diagnostics_imp, "Imputed Data")
non_imp_imputed_results <- process_loo_diagnostics(diagnostics_non_imp_imputed, "Non-Imputed Imputed Data")
imp_imputed_results <- process_loo_diagnostics(diagnostics_imp_imputed, "Imputed Imputed Data")

non_imp_results
imp_results
non_imp_imputed_results
imp_imputed_results
```

```{r}
# Debugging a single LOO iteration
loo_data <- test_non_imp$data[-1, ]
V_matrix_loo <- test_non_imp$V_matrix[-1, -1]

model <- rma.mv(
  yi = yi,
  V = V_matrix_loo,
  mods = as.formula("~ tree_type + crop_type + age_system + season + soil_texture"),
  random = list(
    ~ 1 | id_article,
    ~ 1 | id_article/response_variable,
    ~ 1 | exp_id
  ),
  data = loo_data,
  method = "ML",
  control = list(
    optimizer = "optim",
    optim.method = "BFGS",
    iter.max = 1000,
    rel.tol = 1e-8
  )
)

# Check diagnostics
AIC(model)
BIC(model)
logLik(model)

```

```{r}
# Load the LOO results
loo_results <- readRDS(file.path(output_dir, "meta_model_results_with_loo_test_subsets.rds"))

# Inspect the structure
# str(loo_results)

loo_results
```

Assessing the model-datasets fexed effects

```{r}
str(loo_results$non_imp$loo_results[[1]])

# loo_results$non_imp$model
# loo_results$imp$model
# loo_results$non_imp_imputed$model
# str(loo_results$imp_imputed$model)
```

```{r}
extract_fixed_effects <- function(model) {
  # Check if the model and required components exist
  if (!is.null(model)) {
    # Extract the term names from `b` (or fallback to sequential labels)
    term_names <- rownames(model$b)
    if (is.null(term_names)) {
      term_names <- paste0("term_", seq_along(model$beta))
    }
    
    # Ensure all components are properly aligned and handle missing ones
    num_terms <- length(model$beta)
    estimate <- model$beta
    se <- model$se
    zval <- model$zval
    pval <- model$pval
    ci.lb <- model$ci.lb
    ci.ub <- model$ci.ub
    
    # Handle cases where any component is missing
    if (is.null(se) || length(se) != num_terms) se <- rep(NA, num_terms)
    if (is.null(zval) || length(zval) != num_terms) zval <- rep(NA, num_terms)
    if (is.null(pval) || length(pval) != num_terms) pval <- rep(NA, num_terms)
    if (is.null(ci.lb) || length(ci.lb) != num_terms) ci.lb <- rep(NA, num_terms)
    if (is.null(ci.ub) || length(ci.ub) != num_terms) ci.ub <- rep(NA, num_terms)
    
    # Create a data frame
    fixed_effects <- data.frame(
      term = term_names,
      estimate = estimate,
      se = se,
      zval = zval,
      pval = pval,
      ci.lb = ci.lb,
      ci.ub = ci.ub,
      stringsAsFactors = FALSE
    )
    return(fixed_effects)
  }
  
  # Return an empty data frame if the model is invalid
  return(data.frame(
    term = character(0),
    estimate = numeric(0),
    se = numeric(0),
    zval = numeric(0),
    pval = numeric(0),
    ci.lb = numeric(0),
    ci.ub = numeric(0),
    stringsAsFactors = FALSE
  ))
}
```

```{r}
fixed_effects_non_imp <- extract_fixed_effects(loo_results$non_imp$model)
fixed_effects_imp <- extract_fixed_effects(loo_results$imp$model)
fixed_effects_non_imp_imputed <- extract_fixed_effects(loo_results$non_imp_imputed$model)
fixed_effects_imp_imputed <- extract_fixed_effects(loo_results$imp_imputed$model)

# Add dataset identifiers
fixed_effects_non_imp$dataset <- "Non-Imputed Dataset"
fixed_effects_imp$dataset <- "Imputed Dataset"
fixed_effects_non_imp_imputed$dataset <- "Non-Imputed Imputed Dataset"
fixed_effects_imp_imputed$dataset <- "Imputed Imputed Dataset"

# Combine all datasets
all_fixed_effects <- bind_rows(
  fixed_effects_non_imp,
  fixed_effects_imp,
  fixed_effects_non_imp_imputed,
  fixed_effects_imp_imputed
)

# View the combined fixed effects
head(all_fixed_effects)
```
Visually assessing

```{r}
# Prepare the data
all_fixed_effects <- all_fixed_effects %>%
  mutate(
    # Categorize significance levels
    signif = case_when(
      pval < 0.001 ~ "***",
      pval < 0.01 ~ "**",
      pval < 0.05 ~ "*",
      pval < 0.1 ~ ".",
      TRUE ~ ""
    )
  )

# Create a forest plot for fixed effects
all_fixed_effects |> 
  ggplot(aes(x = estimate, y = term, color = dataset)) +
  geom_point(size = 3, position = position_dodge(width = 0.8)) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub), height = 0.2, position = position_dodge(width = 0.8)) +
  geom_text(aes(label = signif), vjust = -1, hjust = -0.3, position = position_dodge(width = 0.8), size = 3.5) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Comparison of Fixed Effects Across Models and Datasets",
    subtitle = "Estimates with 95% Confidence Intervals",
    x = "Estimate",
    y = "Fixed Effect Term",
    color = "Dataset"
  ) +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_line(color = "gray", linetype = "dashed"),
    panel.grid.minor = element_blank()
  ) +
  facet_wrap(~dataset, scales = "free", ncol = 2)
```
```{r}
# Assuming the data used in the model is stored in `loo_results$<model>$model$data`
reference_levels <- sapply(
  loo_results$imp_imputed$model$data %>% select(where(is.factor)),
  levels
)
# View the reference group levels
print(sapply(reference_levels, function(x) x[1]))

```

```{r}
all_fixed_effects |> 
  ggplot(aes(x = estimate, y = term, color = dataset)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub), height = 0.3, position = position_dodge(width = 0.5)) +
  facet_grid(term ~ response_variable, scales = "free", space = "free_y") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Comparison of Fixed Effects Across Models and Response Variables",
    subtitle = "Estimates with 95% Confidence Intervals",
    x = "Estimate",
    y = "Fixed Effect Term",
    color = "Dataset"
  ) +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_line(color = "gray", linetype = "dashed"),
    panel.grid.minor = element_blank()
  )

```


Assessing I2, AIC, BIC, LogLik, k.all

```{r}
# Function to extract LOO diagnostics including I2, AIC, BIC, LogLik, and k.all
extract_loo_diagnostics <- function(loo_results) {
  # Loop through LOO results and extract diagnostics
  diagnostics_list <- lapply(seq_along(loo_results), function(i) {
    res <- loo_results[[i]]
    if (!is.null(res)) {
      data.frame(
        study_removed = res$study_removed,
        estimate = res$estimate, 
        se = res$se,
        I2 = ifelse(!is.null(res$I2), res$I2[1], NA),  # Extract I2 if available
        AIC = ifelse(!is.null(res$AIC), res$AIC, NA),  # Extract AIC
        BIC = ifelse(!is.null(res$BIC), res$BIC, NA),  # Extract BIC
        LogLik = ifelse(!is.null(res$LogLik), res$LogLik, NA),  # Extract LogLik
        k_all = ifelse(!is.null(res$k_all), res$k_all, NA)  # Extract k.all if available
      )
    } else {
      # Return NA for all fields if the result is NULL
      data.frame(
        study_removed = i,
        estimate = NA,
        se = NA,
        I2 = NA,
        AIC = NA,
        BIC = NA,
        LogLik = NA,
        k_all = NA
      )
    }
  })
  
  # Combine into a single data frame
  do.call(rbind, diagnostics_list)
}
```

```{r}
# Extract diagnostics for each test result
diagnostics_non_imp <- extract_loo_diagnostics(loo_results$non_imp$loo_results)
diagnostics_imp <- extract_loo_diagnostics(loo_results$imp$loo_results)
diagnostics_non_imp_imputed <- extract_loo_diagnostics(loo_results$non_imp_imputed$loo_results)
diagnostics_imp_imputed <- extract_loo_diagnostics(loo_results$imp_imputed$loo_results)
```
```{r}
# Add a dataset column to each data frame
diagnostics_non_imp$dataset <- "non_imp"
diagnostics_imp$dataset <- "imp"
diagnostics_non_imp_imputed$dataset <- "non_imp_imputed"
diagnostics_imp_imputed$dataset <- "imp_imputed"

# Combine all diagnostics into a single data frame
all_diagnostics_df <- bind_rows(
  diagnostics_non_imp,
  diagnostics_imp,
  diagnostics_non_imp_imputed,
  diagnostics_imp_imputed
)

# Preview the combined diagnostics
head(all_diagnostics_df)

```


```{r}
process_loo_diagnostics <- function(diagnostics, dataset_name) {
  if (nrow(diagnostics) == 0) {
    cat("The diagnostics dataset is empty for:", dataset_name, "\n")
    return(NULL)
  }
  
  # Filter rows with complete cases for estimate and se only
  diagnostics_filtered <- diagnostics %>%
    select(study_removed, estimate, se) %>%
    filter(!is.na(estimate) & !is.na(se))
  
  if (nrow(diagnostics_filtered) == 0) {
    cat("No valid Estimate and SE values for diagnostics in:", dataset_name, "\n")
    return(NULL)
  }
  
  # Reshape the data for plotting
  diagnostics_long <- diagnostics_filtered %>%
    pivot_longer(cols = c(estimate, se), names_to = "Metric", values_to = "Value")
  
  # Plot Estimate and SE
  estimate_se_plot <- ggplot(diagnostics_long, aes(x = factor(study_removed), y = Value, color = Metric)) +
    geom_point() +
    geom_line(aes(group = Metric)) +
    labs(
      title = paste("LOO Sensitivity Analysis: Estimate and SE (", dataset_name, ")", sep = ""),
      x = "Study Removed", y = "Value"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  return(list(estimate_se_plot = estimate_se_plot, filtered_diagnostics = diagnostics_filtered))
}
```





####################################################################################################################################################













































Moderator Analysis (Subgroup and Meta-Regression)

```{r}
# Function for subgroup analysis and meta-regression
run_moderator_analysis <- function(data, V_matrix, moderator) {
  cat("\nAnalyzing moderator:", moderator, "\n")
  
  # Check if the moderator has enough levels for analysis
  if (length(unique(data[[moderator]])) < 2) {
    cat("Insufficient levels for moderator:", moderator, "\n")
    return(NULL)
  }
  
  # Meta-regression model with the moderator
  tryCatch({
    model <- rma.mv(
      yi = yi,
      V = V_matrix,
      mods = as.formula(paste("~", moderator)),
      random = list(~ 1 | id_article, 
                    ~ 1 | id_article/response_variable, 
                    ~ 1 | exp_id),
      data = data,
      method = "REML"
    )
    return(summary(model))
  }, error = function(e) {
    cat("Error in meta-regression for moderator:", moderator, "-", e$message, "\n")
    return(NULL)
  })
}
```





```{r}
# Function to check if a matrix is positive definite
is_positive_definite <- function(V_matrix) {
  eigen_values <- eigen(V_matrix, symmetric = TRUE)$values
  all(eigen_values > 0)
}

# Function to regularize the V_matrix if it's not positive definite
regularize_v_matrix <- function(V_matrix) {
  cat("Regularizing V_matrix to ensure positive definiteness...\n")
  epsilon <- 1e-6  # Small value to add to diagonal
  diag(V_matrix) <- diag(V_matrix) + epsilon
  return(V_matrix)
}
```

```{r}
# Updated helper function to subset the V_matrix and check positive definiteness
subset_v_matrix <- function(V_matrix, data_subset) {
  # Update the rownames of the data subset to character type
  filtered_ids <- as.character(data_subset$id_article)
  
  # Intersect the filtered ids with the rownames of V_matrix
  matching_ids <- intersect(filtered_ids, rownames(V_matrix))
  
  # Check if there are any matching rownames
  if (length(matching_ids) == 0) {
    cat("Warning: No matching rownames found between filtered data and V_matrix. Returning NULL.\n")
    return(NULL)
  }
  
  # Subset the V_matrix using the matching rownames
  V_matrix_subset <- V_matrix[matching_ids, matching_ids, drop = FALSE]
  
  # Check if the V_matrix_subset is empty
  if (nrow(V_matrix_subset) == 0 || ncol(V_matrix_subset) == 0) {
    cat("Warning: Subset V_matrix is empty. Skipping analysis.\n")
    return(NULL)
  }
  
  # Check positive definiteness
  if (!is_positive_definite(V_matrix_subset)) {
    cat("Warning: V_matrix is not positive definite. Regularizing...\n")
    V_matrix_subset <- regularize_v_matrix(V_matrix_subset)
  }
  
  return(V_matrix_subset)
}
```

```{r}
# Moderator analysis for each response subset
moderator_results <- lapply(response_variables, function(response_var) {
  cat("\nAnalyzing response variable:", response_var, "\n")
  
  # Step 1: Get the data subset
  data_subset <- response_splits$non_imp[[response_var]]
  V_matrix <- v_matrices$v_matrix_non_imp[[response_var]]
  
  # Step 2: Filter the data for valid moderators and handle single-level factors
  filtered_output <- filter_data_for_moderator_analysis(data_subset, moderators)
  filtered_data <- filtered_output$data
  valid_moderators <- filtered_output$valid_moderators
  
  # Step 3: Subset the V_matrix based on filtered data
  V_matrix_subset <- subset_v_matrix(V_matrix, filtered_data)
  if (is.null(V_matrix_subset)) {
    cat("Skipping analysis due to issues with V_matrix.\n")
    return(NULL)
  }
  
  # Step 4: Run moderator analysis only if there are valid moderators and enough data
  if (nrow(filtered_data) > 1 && length(valid_moderators) > 0) {
    lapply(valid_moderators, run_moderator_analysis, data = filtered_data, V_matrix = V_matrix_subset)
  } else {
    cat("Insufficient data or no valid moderators after filtering. Skipping analysis for this subset.\n")
    NULL
  }
})

# Name the list of results by response variables
names(moderator_results) <- response_variables
```

```{r}
# Summary of moderator analysis results
summary_results <- lapply(moderator_results, function(response_result) {
  if (is.null(response_result)) {
    return("Skipped")
  } else {
    sapply(response_result, function(mod_result) {
      if (is.null(mod_result)) {
        return("Failed")
      } else {
        return("Success")
      }
    })
  }
})

print(summary_results)
```

```{r}
# Extract successful results
successful_results <- lapply(moderator_results, function(response_result) {
  if (!is.null(response_result)) {
    Filter(Negate(is.null), response_result)
  } else {
    return(NULL)
  }
})

# Display summaries of successful models
for (response_var in names(successful_results)) {
  cat("\nResults for response variable:", response_var, "\n")
  if (length(successful_results[[response_var]]) > 0) {
    lapply(successful_results[[response_var]], summary)
  } else {
    cat("No successful models found for this response variable.\n")
  }
}

```




```{r}
# Save plots if available
# if (!is.null(non_imp_results)) {
#   ggsave(non_imp_results$estimate_se_plot, filename = file.path(output_dir, "estimate_se_non_imp.png"), width = 8, height = 6)
# }
# 
# if (!is.null(imp_results)) {
#   ggsave(imp_results$estimate_se_plot, filename = file.path(output_dir, "estimate_se_imp.png"), width = 8, height = 6)
# }
# 
# if (!is.null(non_imp_imputed_results)) {
#   ggsave(non_imp_imputed_results$estimate_se_plot, filename = file.path(output_dir, "estimate_se_non_imp_imputed.png"), width = 8, height = 6)
# }
# 
# if (!is.null(imp_imputed_results)) {
#   ggsave(imp_imputed_results$estimate_se_plot, filename = file.path(output_dir, "estimate_se_imp_imputed.png"), width = 8, height = 6)
# }
```


```{r}
# Helper function to extract LOO metrics into a tidy format
extract_loo_metrics <- function(loo_results) {
  # Loop through results and extract key metrics
  metrics_list <- lapply(seq_along(loo_results), function(i) {
    res <- loo_results[[i]]  # Access the LOO result for the i-th study removed
    if (!is.null(res)) {
      # Extract relevant fields
      data.frame(
        study_removed = res$study_removed,
        estimate = ifelse(!is.null(res$estimate), res$estimate, NA),
        se = ifelse(!is.null(res$se), res$se, NA),
        zval = ifelse(!is.null(res$zval), res$zval, NA),
        pval = ifelse(!is.null(res$pval), res$pval, NA),
        ci.lb = ifelse(!is.null(res$ci.lb), res$ci.lb, NA),
        ci.ub = ifelse(!is.null(res$ci.ub), res$ci.ub, NA),
        signif = ifelse(!is.null(res$pval) && res$pval < 0.05, "**", "")
      )
    } else {
      # Return NA if the result is NULL
      data.frame(
        study_removed = i,
        estimate = NA,
        se = NA,
        zval = NA,
        pval = NA,
        ci.lb = NA,
        ci.ub = NA,
        signif = NA
      )
    }
  })
  
  # Combine into a single data frame
  metrics_df <- do.call(rbind, metrics_list)
  
  # Unnest estimates if they're named vectors (e.g., intrcpt/moderators)
  if (!is.null(metrics_df$estimate[[1]]) && length(metrics_df$estimate[[1]]) > 1) {
    metrics_df <- tidyr::unnest(metrics_df, c(estimate, se, zval, pval, ci.lb, ci.ub, signif))
  }
  
  return(metrics_df)
}
```

```{r}
# Extract metrics for each dataset
diagnostics_non_imp <- extract_loo_metrics(loo_results$non_imp$loo_results)
diagnostics_imp <- extract_loo_metrics(loo_results$imp$loo_results)
diagnostics_non_imp_imputed <- extract_loo_metrics(loo_results$non_imp_imputed$loo_results)
diagnostics_imp_imputed <- extract_loo_metrics(loo_results$imp_imputed$loo_results)

# Combine all diagnostics into a single list
all_diagnostics <- list(
  non_imp = diagnostics_non_imp,
  imp = diagnostics_imp,
  non_imp_imputed = diagnostics_non_imp_imputed,
  imp_imputed = diagnostics_imp_imputed
)

# Add dataset names and combine into one dataframe
all_diagnostics_df <- purrr::imap_dfr(all_diagnostics, ~ mutate(.x, dataset = .y))

# Preview the combined diagnostics
head(all_diagnostics_df)
```


```{r}
# Plot estimates with CIs for each dataset
all_diagnostics_df |> 
  ggplot(aes(x = study_removed, y = estimate, color = dataset)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci.lb, ymax = ci.ub), width = 0.2) +
  facet_wrap(~ dataset) +
  theme_minimal() +
  labs(
    title = "Leave-One-Out Sensitivity Diagnostics",
    x = "Study Removed",
    y = "Estimate (with 95% CI)"
  )
```




Create test sub-set of data (because it takes too long to run the code)

```{r}
# Helper functions for processing - (validate, and subset creation of data and v_matrix)
is_positive_definite <- function(mat) {
  if (!isSymmetric(mat)) {
    return(FALSE)
  }
  eigenvalues <- eigen(mat, symmetric = TRUE, only.values = TRUE)$values
  return(all(eigenvalues > 0))
}

make_positive_definite <- function(V_matrix) {
  if (!is_positive_definite(V_matrix)) {
    diag(V_matrix) <- diag(V_matrix) + 1e-6
  }
  return(V_matrix)
}

validate_data_and_v_matrix <- function(data, V_matrix) {
  if (!isSymmetric(V_matrix)) {
    V_matrix <- (V_matrix + t(V_matrix)) / 2
    cat("V_matrix has been symmetrized.\n")
  }
  
  if (!is_positive_definite(V_matrix)) {
    diag(V_matrix) <- diag(V_matrix) + 1e-6
    cat("V_matrix adjusted for positive definiteness.\n")
  }
  
  if (!identical(rownames(V_matrix), rownames(data))) {
    stop("Row names of V_matrix and data do not match!")
  }
  
  return(V_matrix)
}

# Subset creation helper
# Updated function for creating a test subset
create_test_subset <- function(data, V_matrix, sample_size = 150) {
  if (nrow(data) < sample_size) {
    stop("Sample size is larger than the dataset.")
  }
  
  # Ensure both data and V_matrix have row names
  if (is.null(rownames(data))) {
    rownames(data) <- seq_len(nrow(data))
  }
  if (is.null(rownames(V_matrix))) {
    rownames(V_matrix) <- colnames(V_matrix) <- seq_len(nrow(V_matrix))
  }
  
  # Sample row indices (ensure the sampled indices are consistent)
  sampled_indices <- sample(rownames(data), size = sample_size, replace = FALSE)
  
  # Subset data and V_matrix using the sampled indices
  data_subset <- data[sampled_indices, , drop = FALSE]
  V_matrix_subset <- V_matrix[sampled_indices, sampled_indices, drop = FALSE]
  
  # Validate alignment
  if (!identical(rownames(data_subset), rownames(V_matrix_subset))) {
    stop("Row names of V_matrix and data do not match!")
  }
  
  # Return subsets
  list(data = data_subset, V_matrix = V_matrix_subset)
}
```


```{r,  eval=FALSE}
extract_diagnostics <- function(model) {
  if (is.null(model)) {
    return(NULL)
  }
  list(
    AIC = model$aic,
    BIC = model$bic,
    LogLik = model$loglik,
    I2 = 100 * model$tau2 / (model$tau2 + model$sigma2),
    QM = model$QM,
    QM_pval = model$pval
  )
}

# Use the function
full_model_diagnostics <- extract_diagnostics(full_model)
print(full_model_diagnostics)

```



**OBS! This code takes extremely long to run ~ 2 days!**
  
  ```{r, eval=FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

####################################################################################################
# Run LOO sensitivity analysis on all datasets
loo_non_imp <- run_loo_sensitivity(non_imp_dataset, V_matrix_non_imp, moderator_formula)
loo_imp <- run_loo_sensitivity(imp_dataset, V_matrix_imp, moderator_formula)
loo_non_imp_imputed <- run_loo_sensitivity(non_imp_dataset_imputed, V_matrix_non_imp_imputed, moderator_formula)
loo_imp_imputed <- run_loo_sensitivity(imp_dataset_imputed, V_matrix_imp_imputed, moderator_formula)

####################################################################################################
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Ensure the directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Save the LOO results
saveRDS(
  list(
    loo_non_imp = loo_non_imp,
    loo_imp = loo_imp,
    loo_non_imp_imputed = loo_non_imp_imputed,
    loo_imp_imputed = loo_imp_imputed
  ),
  file = file.path(output_dir, "loo_sensitivity_results_full_dataset.rds")
)

cat("LOO sensitivity results saved to:", file.path(output_dir, "loo_sensitivity_results_full_dataset.rds"), "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (18/11-24)
# 
```




```{r}
split_by_response_variable <- function(test_data, selected_response_variables = NULL) {
  data <- test_data$data
  V_matrix <- test_data$V_matrix
  
  # Optionally filter for selected response variables
  if (!is.null(selected_response_variables)) {
    data <- data[data$response_variable %in% selected_response_variables, ]
    row_indices <- rownames(data)
    V_matrix <- V_matrix[row_indices, row_indices, drop = FALSE]
  }
  
  # Split data by response variable
  response_splits <- split(data, data$response_variable)
  
  # Align each subset's V_matrix
  splits <- lapply(response_splits, function(sub_data) {
    indices <- rownames(sub_data)
    V_matrix_subset <- V_matrix[indices, indices, drop = FALSE]
    
    # Validate alignment
    validate_data_and_v_matrix(sub_data, V_matrix_subset)
    list(data = sub_data, V_matrix = V_matrix_subset)
  })
  
  return(splits)
}
```

```{r}
# Define the selected response variables
selected_responses <- c("Crop yield", "Soil quality", "Biodiversity")


####################################################################################################

# Create splits for the non-imputed dataset
split_non_imp <- split_by_response_variable(test_non_imp, 
                                            selected_response_variables = selected_responses)

# Create splits for the imputed dataset
split_imp <- split_by_response_variable(test_imp, 
                                        selected_response_variables = selected_responses)

# Create splits for the non-imputed, imputed dataset
split_non_imp_imputed <- split_by_response_variable(test_non_imp_imputed, 
                                                    selected_response_variables = selected_responses)

# Create splits for the imputed, imputed dataset
split_imp_imputed <- split_by_response_variable(test_imp_imputed, 
                                                selected_response_variables = selected_responses)

# View sneak-peak
split_non_imp |> glimpse()
```

```{r}
split_non_imp$Biodiversity$data |> glimpse()

split_non_imp$Biodiversity$V_matrix |> glimpse()
```
```{r}
validate_data_and_v_matrix <- function(data, V_matrix) {
  if (!identical(rownames(data), rownames(V_matrix))) {
    stop("Row names of data and V_matrix do not match!")
  }
}

# Validate the split
split_non_imp$Biodiversity$data |> glimpse()
split_non_imp$Biodiversity$V_matrix |> glimpse()
```
```{r}
# Define model formulas
# as.formula("yi ~ 0 + tree_type + crop_type + age_system + season + soil_texture")
# Define the moderator formula
moderator_formula <- as.formula("~ tree_type + soil_texture")

# Define the random effects formula
random_effects_formula <- as.formula("~ 1 | id_article/exp_id")


# random_effects_formula <- list(~ 1 | id_article,
#                                ~ 1 | id_article/response_variable,
#                                ~ 1 | exp_id)
```


**OBS! This is a subset because it takes long with the full datasets**
  ```{r, eval=FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Define a directory to save results
output_model_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_model_dir)) {
  dir.create(output_model_dir, recursive = TRUE)
}

##########################################################################
# Run LOO sensitivity analysis on test subsets and each response variable split with integrated saving and collect results

# Run LOO sensitivity analysis
results <- list(
  non_imp = run_loo_sensitivity_for_splits(
    splits = split_non_imp,
    moderator_formula = moderator_formula,
    random_effects = random_effects_formula,
    output_dir = file.path(output_model_dir, "Non_Imputed")
  ),
  imp = run_loo_sensitivity_for_splits(
    splits = split_imp,
    moderator_formula = moderator_formula,
    random_effects = random_effects_formula,
    output_dir = file.path(output_model_dir, "Imputed")
  ),
  non_imp_imputed = run_loo_sensitivity_for_splits(
    splits = split_non_imp_imputed,
    moderator_formula = moderator_formula,
    random_effects = random_effects_formula,
    output_dir = file.path(output_model_dir, "Non_Imputed_Imputed")
  ),
  imp_imputed = run_loo_sensitivity_for_splits(
    splits = split_imp_imputed,
    moderator_formula = moderator_formula,
    random_effects = random_effects_formula,
    output_dir = file.path(output_model_dir, "Imputed_Imputed")
  )
)

# Save all results
results_path <- file.path(output_model_dir, "meta_model_results_with_loo_test_subsets.rds")
saveRDS(results, file = results_path)
cat("Results saved to:", results_path, "\n")


# str(loo_non_imp)



##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (21/11-24)
# Time difference of 17.70906 mins
# Time difference of 12.06283 mins
# Time difference of 1.435304 mins

```

Diagnostics of LOO sensitivity analysis

```{r}
# Load results from saved RDS file
results_path <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "meta_model_results_with_loo_test_subsets.rds")
if (file.exists(results_path)) {
  results <- readRDS(results_path)
  cat("Results loaded from:", results_path, "\n")
} else {
  stop("Results file not found! Please run the model fitting and saving step first.")
}
```

```{r}
#str(results)  

# Example for Non-Imputed Dataset
str(results$non_imp)

results$non_imp[[1]] |> 
  glimpse()

# Confirm the presence of key components
names(results$non_imp[[1]])
# Check 'model' and 'data' structures
glimpse(results$non_imp[[1]]$model)
glimpse(results$non_imp[[1]]$data)

```

```{r}
valid_model_objects <- purrr::keep(results$non_imp, ~ is.list(.x) && !is.null(.x$model))
valid_model_objects
```


```{r}
extract_forestplot_data <- function(model_object, moderator) {
  # Extract relevant components
  beta <- model_object$model$beta[, 1]
  se <- model_object$model$se
  ci.lb <- model_object$model$ci.lb
  ci.ub <- model_object$model$ci.ub
  pval <- model_object$model$pval
  term_names <- rownames(model_object$model$beta)
  
  # Match terms for the specified moderator
  matching_terms <- grep(moderator, term_names, ignore.case = TRUE, value = TRUE)
  if (length(matching_terms) == 0) {
    return(tibble())  # Return empty tibble if moderator not found
  }
  
  # Construct tibble for matching terms
  forestplot_data <- tibble(
    term = term_names,
    estimate = beta,
    std_error = se,
    ci_lower = ci.lb,
    ci_upper = ci.ub,
    p_value = pval
  ) %>%
    filter(term %in% matching_terms) %>%
    mutate(
      moderator_level = gsub(".*_", "", term)  # Extract only the level
    )
  
  return(forestplot_data)
}
```

```{r}
forest_plot_data <- purrr::map_dfr(valid_model_objects, ~ {
  purrr::map_dfr(selected_responses, function(response_variable) {
    tryCatch(
      {
        data <- extract_forestplot_data(
          .x,
          moderator = selected_moderator
        )
        data <- data %>% mutate(response_variable = response_variable)
        data
      },
      error = function(e) {
        message("Error in data extraction: ", e$message)
        tibble()
      }
    )
  })
})

# Inspect final data
forest_plot_data |> glimpse() 
```

```{r}
forest_plot_data_agg <- forest_plot_data %>%
  group_by(response_variable, moderator_level) %>%
  summarize(
    yi = mean(estimate, na.rm = TRUE),       # Mean effect size
    lower_ci = mean(ci_lower, na.rm = TRUE), # Mean lower CI
    upper_ci = mean(ci_upper, na.rm = TRUE), # Mean upper CI
    .groups = "drop"                         # Drop grouping
  )

forest_plot_data_agg |> glimpse()
```

```{r}
forest_plot <- forest_plot_data_agg %>%
  ggplot(aes(x = yi, y = moderator_level, color = response_variable)) +
  geom_point(size = 3) +  # Effect size
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) +  # Confidence intervals
  facet_wrap(~response_variable, scales = "free_x", ncol = 1) +  # Facet by response variable
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line at zero
  labs(
    title = glue("Forest Plot: {selected_moderator}"),
    x = "Effect Size (Estimate)",
    y = "Moderator Levels"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "bottom"
  )

forest_plot
```

```{r}
str(results$Biodiversity$model)
```
```{r}
if (is.null(results[[response_var]]$model)) {
  stop(paste("Model is missing for response variable:", response_var))
}
```
```{r}
print(paste("Processing response variable:", response_var))
print("Model structure:")
print(str(results[[response_var]]$model))

```
```{r}
overall_summary <- tryCatch({
  data.frame(
    id_obs = NA,  # Placeholder for meta-analysis
    estimate = model$b[1, 1],
    ci_lb = model$ci.lb[1],
    ci_ub = model$ci.ub[1],
    study = "Meta-analysis",
    response_var = response_var
  )
}, error = function(e) {
  message(paste("Error in overall_summary for", response_var, ":", e$message))
  data.frame()  # Return an empty data frame instead of NULL
})

```

```{r}
if (nrow(overall_summary) == 0) {
  message(paste("Skipping response variable due to empty overall_summary:", response_var))
  return(NULL)
}

```

```{r}
print("Overall summary created:")
print(overall_summary)

```
```{r}
prepare_forest_data <- function(results, response_var, meta_data) {
  # Extract meta-analysis model and LOO results
  model <- results[[response_var]]$model
  loo_results <- results[[response_var]]$loo_results
  
  # Check if the model or LOO results are NULL
  if (is.null(model) || is.null(loo_results)) {
    message(paste("Skipping", response_var, "due to missing model or LOO results"))
    return(NULL)
  }
  
  # Prepare overall meta-analysis summary
  overall_summary <- tryCatch({
    data.frame(
      id_obs = NA,  # Placeholder for meta-analysis
      estimate = model$b[1, 1],
      ci_lb = model$ci.lb[1],
      ci_ub = model$ci.ub[1],
      study = "Meta-analysis",
      response_var = response_var
    )
  }, error = function(e) {
    message(paste("Error creating overall_summary for", response_var, ":", e$message))
    data.frame()
  })
  
  # Check if overall_summary was successfully created
  if (nrow(overall_summary) == 0) {
    message(paste("Skipping response variable due to empty overall_summary:", response_var))
    return(NULL)
  }
  
  # Safely extract LOO results
  loo_data <- tryCatch({
    data.frame(
      id_obs = seq_along(loo_results),
      estimate = sapply(loo_results, function(res) if (!is.null(res["intrcpt"])) res["intrcpt"] else NA),
      ci_lb = sapply(loo_results, function(res) if (!is.null(res["intrcpt"])) res["intrcpt"] - 1.96 * model$se[1] else NA),
      ci_ub = sapply(loo_results, function(res) if (!is.null(res["intrcpt"])) res["intrcpt"] + 1.96 * model$se[1] else NA),
      study = "LOO Observations",
      response_var = response_var
    )
  }, error = function(e) {
    message(paste("Error creating LOO data for", response_var, ":", e$message))
    return(NULL)
  })
  
  # Handle cases where LOO data creation fails
  if (is.null(loo_data) || nrow(loo_data) == 0) {
    message(paste("No valid LOO data for", response_var))
    return(overall_summary)
  }
  
  # Merge LOO data with metadata
  merged_data <- tryCatch({
    merge(loo_data, meta_data, by.x = "id_obs", by.y = "id_obs", all.x = TRUE)
  }, error = function(e) {
    message(paste("Error merging LOO data with metadata for", response_var, ":", e$message))
    return(NULL)
  })
  
  # Handle cases where merging fails
  if (is.null(merged_data)) {
    return(overall_summary)
  }
  
  # Use base::union to resolve conflicts
  required_columns <- base::union(names(overall_summary), names(merged_data))
  
  # Add missing columns to overall_summary and merged_data
  overall_summary[setdiff(required_columns, names(overall_summary))] <- NA
  merged_data[setdiff(required_columns, names(merged_data))] <- NA
  
  # Reorder columns to match
  overall_summary <- overall_summary[, required_columns]
  merged_data <- merged_data[, required_columns]
  
  # Combine the data
  combined_data <- rbind(overall_summary, merged_data)
  
  return(combined_data)
}
```

```{r}
forest_data <- do.call(
  rbind, 
  lapply(
    names(results),
    function(response_var) prepare_forest_data(
      results = results,
      response_var = response_var,
      meta_data = splits[[response_var]]$data
    )
  )
)

forest_data |> glimpse()
```
```{r}
# Load necessary library
# Filter out rows with missing `estimate`, `ci_lb`, or `ci_ub`
forest_data_filtered <- forest_data |>
  dplyr::filter(!is.na(estimate), !is.na(ci_lb), !is.na(ci_ub))

# Create the forest plot with response variables in separate panes and moderators on the y-axis
forest_plot <- ggplot(forest_data_filtered, aes(x = estimate, y = study, color = response_var)) +
  geom_point(size = 3) +  # Points for effect sizes
  geom_errorbarh(aes(xmin = ci_lb, xmax = ci_ub), height = 0.2) +  # Error bars for confidence intervals
  facet_grid(rows = vars(study), cols = vars(response_var), scales = "free_y") +  # Separate panes for response variables
  labs(
    title = "Forest Plot of Meta-Analysis Results",
    x = "Effect Size (Estimate)",
    y = "Moderators",
    color = "Response Variable"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 10),
    strip.text = element_text(size = 12),
    legend.position = "bottom",
    panel.spacing = unit(1, "lines")
  )

# Print the forest plot
print(forest_plot)

```





```{r}
extract_model_diagnostics <- function(model) {
  list(
    tau2 = if (!is.null(model$tau2)) model$tau2 else NA,  # Between-study variance
    I2 = if (!is.null(model$tau2) && !is.null(model$sigma2)) {
      100 * model$tau2 / (model$tau2 + sum(model$sigma2))
    } else {
      NA
    },  # Proportion of heterogeneity
    AIC = tryCatch(AIC(model), error = function(e) NA),  # Akaike Information Criterion
    BIC = tryCatch(BIC(model), error = function(e) NA),  # Bayesian Information Criterion
    logLik = tryCatch(logLik(model), error = function(e) NA),  # Log-Likelihood
    QM = if (!is.null(model$QM)) model$QM else NA,  # Omnibus test statistic for moderators
    pval_QM = if (!is.null(model$pval.QM)) model$pval.QM else NA  # p-value for the omnibus test
  )
}

# Extract diagnostics for each response variable
diagnostics <- lapply(results, function(res) {
  if (!is.null(res$model)) {
    extract_model_diagnostics(res$model)
  } else {
    NULL
  }
})
```

```{r}
summarize_loo <- function(loo_results) {
  if (is.list(loo_results) && length(loo_results) > 0) {
    # Extract each observation's estimates
    data.frame(
      observation = seq_along(loo_results),
      estimate = sapply(loo_results, function(res) if (!is.null(res[1])) res[1] else NA),
      ci_lb = sapply(loo_results, function(res) if (length(res) >= 2 && !is.null(res[2])) res[2] else NA),
      ci_ub = sapply(loo_results, function(res) if (length(res) >= 3 && !is.null(res[3])) res[3] else NA)
    )
  } else {
    stop("Unexpected structure in loo_results. Check its structure using str().")
  }
}

# Summarize LOO results for each response variable
loo_summaries <- lapply(results, function(res) {
  if (!is.null(res$loo_results)) {
    tryCatch(
      summarize_loo(res$loo_results),
      error = function(e) {
        cat("Error in summarizing LOO results for a response variable:", conditionMessage(e), "\n")
        NULL
      }
    )
  } else {
    NULL
  }
})
```

```{r}
# Print diagnostics for inspection
str(diagnostics)

# Print one LOO summary to verify correctness
if (length(loo_summaries) > 0) {
  str(loo_summaries[[1]])
}
```

Visualize Leave-One-Out (LOO) influence:
  
  ```{r}
loo_data <- loo_data %>%
  mutate(
    se = abs((ci_ub - ci_lb) / (2 * 1.96)),
    ci_lb_new = estimate - 1.96 * se,
    ci_ub_new = estimate + 1.96 * se
  )

loo_data <- loo_data %>%
  filter(ci_lb_new <= estimate, ci_ub_new >= estimate) |> 
  arrange(-estimate)
```
```{r}
plot_loo_influence <- function(loo_data, response_var, top_n = 10) {
  # Select top influential observations
  top_influential <- head(loo_data, n = top_n)
  
  # Add observation labels
  top_influential <- top_influential %>%
    mutate(label = paste("Obs", observation))
  
  # Create the plot
  ggplot(top_influential, aes(x = estimate, y = reorder(label, estimate))) +
    geom_point(size = 3, color = "blue") +
    geom_errorbarh(aes(xmin = ci_lb_new, xmax = ci_ub_new), height = 0.2, color = "blue") +
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    labs(
      title = paste("Top", top_n, "Influential Observations: LOO Influence"),
      subtitle = paste("Response Variable:", response_var),
      x = "Effect Size Estimate",
      y = "Observation (Sorted by Estimate)"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      panel.grid.major = element_line(size = 0.5, linetype = "dotted"),
      panel.grid.minor = element_blank(),
      axis.text.y = element_text(size = 10, hjust = 1),
      axis.text.x = element_text(size = 10)
    )
}

# Example Usage
plot_loo_influence(loo_data, response_var = "Biodiversity")
```























































































Summarize and Visualize LOO Results:
  
  Create plots of the change in effect size estimates, I², and model fit metrics (AIC, BIC).
Highlight any influential studies that cause large changes when removed.
Integrate Findings:
  
  Compare the LOO results from the full model with those from the individual meta-regression models.
Discuss any discrepancies or notable findings with your collaborators.
























































Key Problems Identified:
  Optimizer Did Not Achieve Convergence: The nlminb optimizer is failing to converge. This is often caused by poor model fit, extreme variance ratios, or insufficient data.
Warnings About Variance Ratios: The warning "Ratio of largest to smallest sampling variance extremely large" suggests that your data may have high variability, which could make the models unstable.
Single-Level Factors in Random Effects: The error "Single-level factor(s) found in 'random' argument" indicates that some random effects have only one level, which causes issues in the meta-regression model.
Insufficient Levels for Moderators: For some moderators, there are not enough unique levels, leading to errors during analysis.
Null Model Outputs: Many of your moderator models are returning NULL, indicating that the model fitting process is failing.


##########################################################################################################################################
ASSESSING THE SPLITS
##########################################################################################################################################



















```{r}
if (nrow(test_subset_splits$Biodiversity$data) != nrow(test_subset_splits$Biodiversity$V_matrix)) {
  stop(sprintf(
    "Mismatch in data and V_matrix dimensions: Data rows = %d, V_matrix rows = %d for id_article '%s'",
    nrow(test_subset_splits$Biodiversity$data), nrow(test_subset_splits$Biodiversity$V_matrix), id_article
  ))
}
```
```{r}
valid_mods <- create_moderator_formula(test_subset_splits$Biodiversity$data, all_moderators)
if (identical(valid_mods, ~1)) {
  cat("Only one level left for moderators after excluding id_article:", id_article, "\n")
}
```
```{r}
if (anyNA(test_subset_splits$Biodiversity$data) || anyNA(test_subset_splits$Biodiversity$V_matrix)) {
  cat("NA values found after excluding id_article:", id_article, "\n")
  return(list(id_article = id_article, estimate = NA, ci.lb = NA, ci.ub = NA))
}
```
```{r}
# Function to check input data for issues in LOO analysis
check_subset_loo_input_data <- function(data, V_matrix, id_article_col, moderators) {
  unique_articles <- unique(data[[id_article_col]])  # Get unique id_article values
  issues <- list()  # To store problematic articles and their issues
  
  # Iterate through each `id_article`
  for (article_id in unique_articles) {
    # Exclude current `id_article`
    reduced_data <- data[data[[id_article_col]] != article_id, ]
    indices <- which(data[[id_article_col]] != article_id)
    reduced_V_matrix <- V_matrix[indices, indices, drop = FALSE]
    
    issue_list <- list()  # To store issues for the current article
    
    # Check 1: Dimension mismatch
    if (nrow(reduced_data) != nrow(reduced_V_matrix)) {
      issue_list$dimension_mismatch <- TRUE
    }
    
    # Check 2: Single-level factors
    single_level_factors <- names(Filter(function(col) length(unique(col)) < 2, reduced_data[moderators]))
    if (length(single_level_factors) > 0) {
      issue_list$single_level_factors <- single_level_factors
    }
    
    # Check 3: Insufficient data
    if (nrow(reduced_data) < 2) {
      issue_list$insufficient_data <- TRUE
    }
    
    # Check 4: Variance-covariance validity
    if (!is.matrix(reduced_V_matrix) || !isSymmetric(reduced_V_matrix) || nrow(reduced_V_matrix) != ncol(reduced_V_matrix)) {
      issue_list$v_matrix_invalid <- TRUE
    }
    
    # Check 5: Redundant predictors
    valid_mods <- create_moderator_formula(reduced_data, moderators)
    if (identical(valid_mods, ~1)) {
      issue_list$redundant_predictors <- TRUE
    }
    
    # Store issues for this article if any
    if (length(issue_list) > 0) {
      issues[[as.character(article_id)]] <- issue_list
    }
  }
  
  return(issues)  # Return the list of issues for all `id_article`
}
```

```{r}
# Test the function with the subset causing issues
subset <- test_subset_splits[["Biodiversity"]]  # Replace with the response variable name
data_subset <- subset$data
V_matrix_subset <- subset$V_matrix

# Check for issues
issues <- check_subset_loo_input_data(
  data = data_subset,
  V_matrix = V_matrix_subset,
  id_article_col = "id_article",
  moderators = all_moderators
)

# Print issues
if (length(issues) > 0) {
  cat("Issues found for the following articles:\n")
  print(issues)
} else {
  cat("No issues found.\n")
}
```




















##########################################################################################################################################
REMOVED FROM 4_SENSITIVITY ANALYSIS
##########################################################################################################################################




```{r}
##########################################################################
# Define the moderator formula
moderator_formula <- as.formula("~ tree_type + crop_type + age_system + season + soil_texture")

##########################################################################
```


```{r}
# Split data and V_matrix by response variable with pre selection
split_by_response_variable <- function(test_data, selected_response_variables = NULL) {
  data <- test_data$data
  V_matrix <- test_data$V_matrix
  
  # If selected_response_variables is provided, filter the data
  if (!is.null(selected_response_variables)) {
    data <- data[data$response_variable %in% selected_response_variables, , drop = FALSE]
  }
  
  # Check if any data remains after filtering
  if (nrow(data) == 0) {
    stop("No data remains after filtering for selected response variables.")
  }
  
  # Split data by response variable
  response_splits <- split(data, data$response_variable)
  
  # Align each subset's V_matrix
  splits <- lapply(response_splits, function(sub_data) {
    indices <- rownames(sub_data)
    V_matrix_subset <- V_matrix[indices, indices, drop = FALSE]
    
    # Validate alignment and return
    validate_data_and_v_matrix(sub_data, V_matrix_subset)
    list(data = sub_data, V_matrix = V_matrix_subset)
  })
  
  return(splits)
}
```



```{r}
# Custom transformation function
apply_transf <- function(results, transf = NULL) {
  if (!is.null(transf) && is.function(transf)) {
    results$estimate <- transf(results$estimate)
    results$ci.lb <- transf(results$ci.lb)
    results$ci.ub <- transf(results$ci.ub)
  }
  return(results)
}


manual_leave1out <- function(model, data, V_matrix, transf = NULL) {
  results <- lapply(seq_len(nrow(data)), function(i) {
    tryCatch({
      # Subset data and V_matrix
      reduced_data <- data[-i, , drop = FALSE]
      reduced_V_matrix <- V_matrix[-i, -i, drop = FALSE]
      
      # Refit the model
      reduced_model <- rma.mv(
        yi = yi,
        V = reduced_V_matrix,
        mods = model$mods,
        random = model$random,
        data = reduced_data,
        method = "REML"
      )
      
      # Extract relevant diagnostics
      res <- list(
        estimate = coef(reduced_model),
        ci.lb = confint(reduced_model)$random["lower"],
        ci.ub = confint(reduced_model)$random["upper"]
      )
      
      # Apply transformation if specified
      apply_transf(res, transf = transf)
    }, error = function(e) {
      cat("Error in iteration", i, ":", conditionMessage(e), "\n")
      NULL
    })
  })
  
  # Return results as a data frame
  do.call(rbind, results)
}
```


```{r}
# Define a directory to save results
# output_model_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# if (!dir.exists(output_model_dir)) {
#   dir.create(output_model_dir, recursive = TRUE)
# }
```

```{r}
run_loo_sensitivity <- function(data, 
                                V_matrix, 
                                moderator_formula, 
                                random_effects = NULL, 
                                output_dir = NULL, 
                                transf = NULL) {
  
  # Ensure the V_matrix is positive definite
  V_matrix <- make_positive_definite(V_matrix)
  
  # Fit the model
  model <- tryCatch(
    rma.mv(
      yi = yi,
      V = V_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data,
      method = "ML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    ),
    error = function(e) {
      cat("Error in model fitting:", conditionMessage(e), "\n")
      NULL
    }
  )
  
  # Run leave-one-out diagnostics manually
  loo_results <- if (!is.null(model)) {
    tryCatch(
      manual_leave1out(model = model, data = data, V_matrix = V_matrix, transf = transf),
      error = function(e) {
        cat("Error in manual leave-one-out diagnostics:", conditionMessage(e), "\n")
        NULL
      }
    )
  } else {
    NULL
  }
  
  # Save the model and diagnostics
  if (!is.null(output_dir) && !is.null(model)) {
    if (!dir.exists(output_dir)) {
      dir.create(output_dir, recursive = TRUE)
    }
    save_path_model <- file.path(output_dir, paste0("model_", make.names(deparse(moderator_formula)), ".rds"))
    save_path_loo <- file.path(output_dir, paste0("loo_results_", make.names(deparse(moderator_formula)), ".rds"))
    tryCatch({
      saveRDS(model, file = save_path_model)
      if (!is.null(loo_results)) {
        saveRDS(loo_results, file = save_path_loo)
      }
    }, error = function(e) {
      cat("Error saving results:", conditionMessage(e), "\n")
    })
  }
  
  # Return the model and diagnostics
  list(model = model, loo_results = loo_results)
}
```

```{r}
run_loo_sensitivity_for_splits <- function(splits, 
                                           moderator_formula, 
                                           random_effects = NULL, 
                                           output_dir = NULL) {
  
  results <- lapply(names(splits), function(response_var) {
    split <- splits[[response_var]]
    
    # Fit model and run diagnostics
    loo_result <- run_loo_sensitivity(
      data = split$data,
      V_matrix = split$V_matrix,
      moderator_formula = moderator_formula,
      random_effects = random_effects
    )
    
    # Add response variable metadata
    loo_result$response_variable <- response_var
    
    # Save each result in a structured way
    response_dir <- file.path(output_dir, response_var)
    if (!dir.exists(response_dir)) {
      dir.create(response_dir, recursive = TRUE)
    }
    
    # Save model and diagnostics separately
    if (!is.null(loo_result$model)) {
      saveRDS(loo_result$model, file = file.path(response_dir, "model.rds"))
    }
    if (!is.null(loo_result$loo_results)) {
      saveRDS(loo_result$loo_results, file = file.path(response_dir, "loo_results.rds"))
    }
    
    # Return the results in memory
    loo_result
  })
  
  # Combine results into a single list
  return(results)
}
```

```{r}
# Set row names for data
rownames(non_imp_dataset) <- as.character(non_imp_dataset$id_obs)

# Ensure V_matrix is a matrix and assign dimension names
V_matrix_non_imp <- as.matrix(V_matrix_non_imp)
dimnames(V_matrix_non_imp) <- list(rownames(non_imp_dataset), rownames(non_imp_dataset))

# Prepare test_data
test_data <- list(data = non_imp_dataset, V_matrix = V_matrix_non_imp)

```

```{r}
# Define the selected response variables
selected_responses <- c("Crop yield", "Soil quality", "Biodiversity")
```

```{r}
# Split the data with selected response variables
splits <- split_by_response_variable(test_data, selected_response_variables = selected_responses)
```

```{r}
splits$Biodiversity$data |> glimpse()
splits$Biodiversity$V_matrix |> glimpse()
```

```{r}
# Function to create a moderator formula excluding variables with only one level
create_moderator_formula <- function(data, moderators) {
  # Identify moderators with more than one level
  valid_moderators <- moderators[sapply(data[, moderators, drop = FALSE], function(x) length(unique(x)) > 1)]
  if (length(valid_moderators) == 0) {
    # If no moderators vary, return an intercept-only model
    return(~ 1)
  }
  # Create the formula
  formula_str <- paste("~", paste(valid_moderators, collapse = " + "))
  as.formula(formula_str)
}
```

```{r}
# Define random effects structure
random_effects <- ~ 1 | id_article/id_obs

# Define all potential moderators
all_moderators <- c("tree_type", "crop_type", "season", "soil_texture")  # Update with actual moderator column names

```

```{r}
# Function to apply a transformation to model results (optional)
apply_transf <- function(results, transf = NULL) {
  # Check if a transformation function is provided and valid
  if (!is.null(transf) && is.function(transf)) {
    # Apply the transformation to the estimate and confidence intervals
    results$estimate <- transf(results$estimate)
    results$ci.lb <- transf(results$ci.lb)
    results$ci.ub <- transf(results$ci.ub)
  }
  return(results)  # Return the transformed (or original) results
}






# Function to perform manual Leave-One-Out (LOO) sensitivity analysis by `id_article`
manual_leave1out <- function(data, V_matrix, id_article_column = "id_article") {
  unique_articles <- unique(data[[id_article_column]])
  
  results <- lapply(unique_articles, function(article_id) {
    reduced_data <- data[data[[id_article_column]] != article_id, , drop = FALSE]
    indices <- which(data[[id_article_column]] != article_id)
    reduced_V_matrix <- V_matrix[indices, indices, drop = FALSE]
    
    if (nrow(reduced_data) == 0 || nrow(reduced_V_matrix) != nrow(reduced_data)) {
      cat("Skipping id_article", article_id, ": Dimension mismatch!\n")
      return(NULL)
    }
    
    tryCatch({
      model <- rma.mv(
        yi = yi,
        V = reduced_V_matrix,
        random = ~1 | id_article,
        data = reduced_data,
        method = "REML"
      )
      list(
        id_article = article_id,
        estimate = coef(model),
        ci.lb = confint(model)$random["lower"],
        ci.ub = confint(model)$random["upper"]
      )
    }, error = function(e) {
      cat("Error excluding id_article", article_id, ":", conditionMessage(e), "\n")
      NULL
    })
  })
  
  do.call(rbind, lapply(results, as.data.frame))
}
```

```{r}
# Length of dataset/v_matrix
n_no <- 100

# Generic function to create test subsets
create_test_subset <- function(dataset, n = n_no) {
  # Extract the data and V_matrix
  data <- dataset$data
  V_matrix <- dataset$V_matrix
  
  # Ensure the data and V_matrix are properly aligned
  if (nrow(data) != nrow(V_matrix)) {
    stop("Data and V_matrix row counts do not match.")
  }
  
  # Randomly sample n rows for the test subset
  sampled_indices <- sample(seq_len(nrow(data)), size = n_no)
  
  # Subset the data and V_matrix
  test_data <- data[sampled_indices, , drop = FALSE]
  test_V_matrix <- V_matrix[sampled_indices, sampled_indices, drop = FALSE]
  
  # Return the test subset
  list(data = test_data, V_matrix = test_V_matrix)
}

####################################################################################################
# Apply the function to all subsets in 'splits'
subset_splits <- splits  # Ensure we are working on the original splits structure
test_subset_splits <- lapply(subset_splits, create_test_subset, n = n_no)

####################################################################################################
# Save the test subsets for future reference
test_output_dir <- here::here("DATA", "OUTPUT_FROM_R", "TEST_SUBSETS")
if (!dir.exists(test_output_dir)) {
  dir.create(test_output_dir, recursive = TRUE)
}

# Save the test subsets
saveRDS(test_subset_splits, file = file.path(test_output_dir, "test_subset_splits.rds"))
cat("Test subsets saved to:", file.path(test_output_dir, "test_subset_splits.rds"), "\n")

test_subset_splits |> str()
test_subset_splits$Biodiversity$data |> glimpse()
test_subset_splits$Biodiversity$data |> str()
```


```{r}
if (nrow(test_subset_splits$Biodiversity$data) != nrow(test_subset_splits$Biodiversity$V_matrix)) {
  cat("Skipping id_article", article_id, ": Dimension mismatch!\n")
  cat("Data rows:", nrow(reduced_data), "V_matrix rows:", nrow(reduced_V_matrix), "\n")
  return(NULL)
}
```








```{r}
# Load the saved results
results_path <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "meta_model_results_with_loo_test_subsets.rds")
results <- readRDS(results_path)
results |> glimpse() |> head()
```





























































**OBS! This takes long, as its running on the full datasets**
  ```{r, eval=FALSE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################
# Run Leave-One-Out (LOO) Sensitivity Analysis on dataset subsets for each response variable [FULL DATASETS]

# Initialize a list to store the results
results <- list()

# Enable progress reporting
with_progress({
  # Create a progressor object
  p <- progressr::progressor(along = names(splits))
  
  # Using the full splits
  for (response_var in names(splits)) {
    # Update the progress bar
    p(message = sprintf("Processing %s", response_var))
    
    # Extract the test subset and its corresponding variance-covariance matrix
    subset <- splits[[response_var]]
    data_subset <- subset$data
    V_matrix_subset <- subset$V_matrix
    
    cat("Processing response variable:", response_var, "\n")
    
    # Dynamically create a moderator formula for the current test subset
    moderator_formula <- create_moderator_formula(data_subset, all_moderators)
    cat("Using moderator formula:", deparse(moderator_formula), "\n")
    
    # Fit the random-effects meta-analysis model_full
    model_full <- tryCatch(
      rma.mv(
        yi = yi,  # Effect sizes
        V = V_matrix_subset,  # Variance-covariance matrix
        mods = if (identical(moderator_formula, ~1)) NULL else moderator_formula,
        random = random_effects,  # Random-effects structure
        data = data_subset,  # Test subset data
        method = "REML"  # Restricted maximum likelihood estimation
      ),
      error = function(e) {
        # Log an error message if the model_full fitting fails
        cat("Error in model_full fitting for", response_var, ":", conditionMessage(e), "\n")
        NULL
      }
    )
    
    # If the model_full fitting was successful, perform LOO sensitivity analysis
    if (!is.null(model_full)) {
      loo_results <- manual_leave1out(
        model_full = model_full,
        data = data_subset,
        V_matrix = V_matrix_subset,
        mods = moderator_formula,
        random = random_effects,
        transf = NULL  # Replace with a transformation function if needed
      )
      
      # Store the model_full and LOO results
      results[[response_var]] <- list(
        model_full = model_full,
        loo_results = loo_results
      )
    }
  }
})


# Save the results to an RDS file
output_model_full_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_model_full_dir)) {
  dir.create(output_model_full_dir, recursive = TRUE)
}
results_path <- file.path(output_model_full_dir, "meta_model_full_results_with_loo.rds")
saveRDS(results, file = results_path)

# Confirm that the results have been saved
cat("Results saved to:", results_path, "\n")
##################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("Total time taken:", time.taken, "\n")
##############################################################
# Last go: (21/11-24)
```


Prepare and Process LOO Data for Plotting
```{r}
prepare_loo_data <- function(loo_data) {
  loo_data %>%
    mutate(
      se = abs((ci_ub - ci_lb) / (2 * 1.96)),  # Calculate standard error
      ci_lb_new = estimate - 1.96 * se,       # Recompute lower confidence bound
      ci_ub_new = estimate + 1.96 * se        # Recompute upper confidence bound
    ) %>%
    filter(ci_lb_new <= estimate, ci_ub_new >= estimate) %>%  # Filter valid intervals
    arrange(-estimate)  # Sort by descending estimate
}
```

Visualize LOO Influence
```{r}
plot_loo_influence <- function(loo_data, response_var, top_n = 10) {
  top_influential <- head(loo_data, n = top_n) %>%
    mutate(label = paste("Obs", observation))  # Add labels
  
  ggplot(top_influential, aes(x = estimate, y = reorder(label, estimate))) +
    geom_point(size = 3, color = "blue") +
    geom_errorbarh(aes(xmin = ci_lb_new, xmax = ci_ub_new), height = 0.2, color = "blue") +
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    labs(
      title = paste("Top", top_n, "Influential Observations: LOO Influence"),
      subtitle = paste("Response Variable:", response_var),
      x = "Effect Size Estimate",
      y = "Observation (Sorted by Estimate)"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      panel.grid.major = element_line(size = 0.5, linetype = "dotted"),
      panel.grid.minor = element_blank(),
      axis.text.y = element_text(size = 10, hjust = 1),
      axis.text.x = element_text(size = 10)
    )
}
```

Apply Viz Workflow to All Response Variables
```{r}
# Loop through all response variables
for (i in seq_along(loo_summaries)) {
  loo_data <- loo_summaries[[i]]
  
  if (!is.null(loo_data)) {
    loo_data <- prepare_loo_data(loo_data)  # Process LOO data
    response_var <- names(loo_summaries)[i]  # Get response variable name
    
    # Plot and display
    print(plot_loo_influence(loo_data, response_var = response_var))
  }
}

loo_data
```
```{r}
# Add confidence intervals to the data
all_loo_results_for_plotting <- combined_loo_res %>%
  mutate(
    lower_ci = estimate - 1.96 * se,
    upper_ci = estimate + 1.96 * se
  )

# Create the forest plot with study_id on the y-axis
all_loo_results_for_plotting %>%
  ggplot(aes(y = as.factor(id_article), x = estimate, color = response_variable)) +
  geom_point(size = 2) +  # Effect size points
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2, alpha = 0.7) +  # Horizontal CI bars
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line at 0
  facet_grid(dataset ~ response_variable, scales = "free_y") +  # Split by dataset and response variable
  labs(
    y = "Excluded Study (ID)",
    x = "Estimated Effect Size",
    title = "Leave-One-Out Sensitivity Analysis",
    subtitle = "Point estimates with 95% confidence intervals",
    color = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    legend.position = "bottom",
    strip.text = element_text(size = 12)  # Larger facet labels
  )
```



Forest Plots
```{r}

```

Heterogeneity Metrics Across Response Variables

```{r}
# str(results[["Biodiversity"]])
test_subset_splits
# Calculate Heterogeneity Metrics
heterogeneity_metrics <- lapply(names(results), function(response_var) {
  # Extract the relevant data
  if (!is.null(results[[response_var]]$data)) {
    res <- results[[response_var]]$data
    
    # Check for `yi` and `vi`
    if (!all(c("yi", "vi") %in% names(res))) {
      stop(paste("Missing `yi` or `vi` in response variable:", response_var))
    }
    
    # Fit the meta-analysis model
    model <- rma.uni(yi = res$yi, vi = res$vi, method = "REML")
    
    # Return heterogeneity metrics
    tibble(
      response_var = response_var,
      I2 = model$I2,
      Q = model$QE,
      tau2 = model$tau2,
      Q_pval = model$QEp
    )
  } else {
    warning(paste("No data available for response variable:", response_var))
    return(NULL)
  }
}) |> bind_rows()

# Display the heterogeneity metrics
heterogeneity_metrics

```
```{r}
# Check structure of `data` for each response variable
lapply(names(results), function(response_var) {
  list(
    response_var = response_var,
    structure = if (!is.null(results[[response_var]]$data)) {
      str(results[[response_var]]$data, max.level = 1)
    } else {
      "No data"
    }
  )
})
```

Funnel Plot
```{r}
# str(test_subset_splits)

# Example for one response variable
response_var <- "Biodiversity"  # Replace with your target response variable
data <- test_subset_splits[[response_var]]$data

# Ensure the data contains yi and vi
if (!is.null(data) && all(c("yi", "vi") %in% names(data))) {
  funnel_data <- data %>%
    mutate(se = sqrt(vi))
} else {
  stop("Data is missing `yi` or `vi` for the selected response variable.")
}
```

```{r}
funnel_plot <- ggplot(funnel_data, aes(x = yi, y = 1 / se)) +
  geom_point(alpha = 0.6, size = 3) +  # Points for studies
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +  # Reference line
  labs(
    title = paste("Funnel Plot for", response_var),
    x = "Effect Size (yi)",
    y = "Precision (1/SE)"
  ) +
  theme_minimal()

funnel_plot <- funnel_plot +
  geom_abline(slope = c(-1.96, 1.96), intercept = 0, linetype = "dotted", color = "red")


funnel_plot
```


```{r}
funnel_plots <- lapply(names(test_subset_splits), function(response_var) {
  data <- test_subset_splits[[response_var]]$data
  
  if (!is.null(data) && all(c("yi", "vi") %in% names(data))) {
    funnel_data <- data %>%
      mutate(se = sqrt(vi))
    
    ggplot(funnel_data, aes(x = yi, y = 1 / se)) +
      geom_point(alpha = 0.6, size = 3) +
      geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
      labs(
        title = paste("Funnel Plot for", response_var),
        x = "Effect Size (yi)",
        y = "Precision (1/SE)"
      ) +
      theme_minimal()
  } else {
    NULL  # Skip if data is missing
  }
})

# Print all funnel plots
lapply(funnel_plots, print)

```
```{r}
# Example for "Biodiversity" response variable
response_var <- "Biodiversity"
data <- test_subset_splits[[response_var]]$data

if (!is.null(data) && all(c("yi", "vi") %in% names(data))) {
  res <- rma(yi, vi, data = data, measure = "GEN", method = "EE")
} else {
  stop("Data is missing `yi` or `vi` for the selected response variable.")
}
```

```{r}
# Set up a 2x2 plotting area
par(mfrow = c(2, 2))

# Standard Error on the y-axis
funnel(res, main = "Standard Error")

# Sampling Variance on the y-axis
funnel(res, yaxis = "vi", main = "Sampling Variance")

# Inverse Standard Error on the y-axis
funnel(res, yaxis = "seinv", main = "Inverse Standard Error")

# Inverse Sampling Variance on the y-axis
funnel(res, yaxis = "vinv", main = "Inverse Sampling Variance")

```

```{r}
results <- lapply(names(test_subset_splits), function(response_var) {
  data <- test_subset_splits[[response_var]]$data
  
  if (!is.null(data) && all(c("yi", "vi") %in% names(data))) {
    res <- rma(yi, vi, data = data, measure = "GEN", method = "EE")
    
    # Set up 2x2 plotting area for each response variable
    par(mfrow = c(2, 2))
    funnel(res, main = paste("Standard Error -", response_var))
    funnel(res, yaxis = "vi", main = paste("Sampling Variance -", response_var))
    funnel(res, yaxis = "seinv", main = paste("Inverse Standard Error -", response_var))
    funnel(res, yaxis = "vinv", main = paste("Inverse Sampling Variance -", response_var))
  } else {
    message(paste("No valid data for response variable:", response_var))
  }
})
```




```{r}
final_loo_results %>%
  ggplot(aes(x = estimate, y = 1 / se)) +  # Precision (1/se) on y-axis
  geom_point(size = 3, aes(color = response_variable)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~response_variable, scales = "free", ncol = 1) +
  labs(
    x = "Effect Size Estimate",
    y = "Precision (1/SE)",
    title = "Funnel Plot for LOO Analysis",
    subtitle = "Grouped by Response Variable"
  ) +
  theme_minimal(base_size = 14)
```

```{r}
# Loop through response variables and generate funnel plots
results <- lapply(names(test_subset_splits), function(response_var) {
  data_split <- test_subset_splits[[response_var]]
  
  # Ensure data and V_matrix are valid
  if (!is.null(data_split$data) && all(c("yi", "vi") %in% names(data_split$data))) {
    # Fit a random-effects model
    res <- rma(
      yi = yi,
      vi = vi,
      data = data_split$data,
      method = "REML"  # Replace "EE" if random-effects model fits better
    )
    
    # Create a 2x2 plotting area for each response variable
    par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))  # Adjust margins for clarity
    funnel(res, main = paste("Standard Error -", response_var))
    funnel(res, yaxis = "vi", main = paste("Sampling Variance -", response_var))
    funnel(res, yaxis = "seinv", main = paste("Inverse Standard Error -", response_var))
    funnel(res, yaxis = "vinv", main = paste("Inverse Sampling Variance -", response_var))
    
    return(res)  # Save model results for further evaluation
  } else {
    message(paste("No valid data for response variable:", response_var))
    return(NULL)
  }
})

# Reset the plotting area to 1x1 after the loop
par(mfrow = c(1, 1))
```







```{r}
# Subset data for Biodiversity
response_var <- "Biodiversity"
data <- test_subset_splits[[response_var]]$data

# Fit a random-effects model
# Fit the multivariate random-effects model
model_res <- rma.mv(
  yi = yi,
  vi = vi,
  #V = V_matrix,
  mods = moderator_formula,
  random = list(
    # ~ 1 | id_article,
    #     ~ 1 | id_article/response_variable,
    ~ 1 | exp_id),
  data = data,
  method = "ML",
  control = list(
    optimizer = "optim",
    optim.method = "BFGS",
    iter.max = 1000,
    rel.tol = 1e-8
  )
)

```


Influence Diagnostics
```{r}
# Print heterogeneity metrics
cat("Heterogeneity Metrics:\n")
cat("Tau²:", res$tau2, "\n")
cat("I²:", res$I2, "\n")
cat("Q-test (QE):", res$QE, "\n")
cat("Q-test p-value (QEp):", res$QEp, "\n")

# Influence diagnostics
inf <- influence(res)

# Print diagnostics
print(inf)

# Plot diagnostics
par(mfrow = c(8, 1))
plot(inf)
```



Between-Study Heterogeneity
```{r}
# Ensure clean data
data <- data[!is.na(data$yi) & !is.na(data$vi) & data$vi > 0, ]

# Fit a random-effects model
res <- rma(yi, vi, data = data, method = "REML")

# Extract heterogeneity metrics
heterogeneity <- list(
  Q = res$QE,
  Q_df = res$k - res$p,
  Q_pval = res$QEp,
  I2 = res$I2,
  tau2 = res$tau2
)

print(heterogeneity)

```
```{r}
# Convert heterogeneity metrics to a data frame
het_df <- data.frame(
  Metric = c("Q", "I2 (%)", "Tau2"),
  Value = c(heterogeneity$Q, heterogeneity$I2, heterogeneity$tau2)
)

# Plot
ggplot(het_df, aes(x = Metric, y = Value)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    title = "Between-Study Heterogeneity Metrics",
    x = "Metric",
    y = "Value"
  ) +
  theme_minimal()

```
```{r}
# Plot Q-test result
barplot(
  height = heterogeneity$Q,
  names.arg = "Q-Test",
  col = "blue",
  ylim = c(0, max(heterogeneity$Q, heterogeneity$Q_df) * 1.2),
  main = "Q-Test for Heterogeneity",
  ylab = "Q Value"
)
abline(h = heterogeneity$Q_df, col = "red", lty = 2)  # Add critical value
legend("topright", legend = c("Q Value", "Degrees of Freedom"),
       fill = c("blue", "red"), bty = "n")

```

```{r}
# Example of preparing a GOSH object
res.gosh <- gosh(res)
```

```{r}
# Perform GOSH diagnostics
res.gosh.diag <- gosh.diagnostics(
  res.gosh,
  km.params = list(centers = 2), # k-means clustering with 2 centers
  db.params = list(
    eps = 0.08, # Epsilon value for DBSCAN
    MinPts = 50 # Minimum points for a cluster
  )
)

# View diagnostics
print(res.gosh.diag)

```

```{r}
# Explore clusters identified by k-means and DBSCAN
str(res.gosh.diag)

# Summary of the diagnostics
summary(res.gosh.diag)

```

```{r}
# Visualize clusters (if available)
plot(res.gosh.diag, type = "clusters")

# Plot goodness-of-fit metrics
plot(res.gosh.diag, type = "diagnostics")

```



```{r, eval=FALSE}
# Function to safely convert to numeric, replacing non-numeric values with NA
safe_as_numeric <- function(x) {
  suppressWarnings(as.numeric(x))
}

# Data Preprocessing
database_clean_dummy <- database_dummy |>
  # Step 1: Clean column names
  janitor::clean_names() |>
  
  # Step 2: Convert id_article and id_obs to integer
  mutate(
    id_article = as.integer(id_article),
    id_obs = as.integer(id_obs)
  ) |>
  
  # Step 3: Convert standard errors and other numeric columns
  mutate(
    silvo_mean = safe_as_numeric(silvo_mean),
    silvo_se = safe_as_numeric(silvo_se),
    silvo_sd = safe_as_numeric(silvo_sd),
    silvo_n = safe_as_numeric(silvo_n),
    control_mean = safe_as_numeric(control_mean),
    control_se = safe_as_numeric(control_se),
    control_sd = safe_as_numeric(control_sd),
    control_n = safe_as_numeric(control_n),
    tree_age = safe_as_numeric(tree_age),
    no_tree_per_m = as.character(no_tree_per_m)) |> 
  
  # Step 4: Create Identifiers (Experiment, Treatment, Common Control)
  # Group data by relevant columns for Treatment ID
  group_by(id_article, tree_type, crop_type, location, experiment_year) |>
  mutate(treat_id = cur_group_id()) |>
  ungroup() |>
  
  # Group data by relevant columns for Experiment ID
  group_by(id_article, location, experiment_year) |>
  mutate(exp_id = cur_group_id()) |>
  ungroup() |> 
  
  # Step 5: Ensure no infinite or NaN values are present in any columns
  mutate(across(everything(), ~ifelse(is.infinite(.) | is.nan(.), NA, .))
  ) |> 
  
  # Step 6: Convert "NA" strings to real NA values, excluding 'id_article' and 'id_obs'
  mutate(
    across(
      .cols = where(is.character) & !c("id_article", "id_obs"),
      .fns = ~ na_if(., "NA")
    )
  ) |>
  
  # Step 7: Convert year columns to date format
  # Convert to proper Date format using "YYYY-01-01"
  mutate(
    experiment_year = as.Date(paste0(experiment_year, "-01-01")),
    year_est_exp = as.Date(paste0(year_est_exp, "-01-01")),
    study_year_start = as.Date(paste0(study_year_start, "-01-01")),
    study_year_end = as.Date(paste0(study_year_end, "-01-01"))
  ) |> 
  # Step 8: Rename Latitude and Longitude to lat and lon
  rename(
    lat = latitude,
    lon = longitude
  ) |>
  
  # Step 9: Convert lat and lon to numeric coordinates
  mutate(
    lat = str_replace_all(lat, "[°NS]", "") |> safe_as_numeric(),
    lon = str_replace_all(lon, "[°EW]", "") |> safe_as_numeric(),
    lat = if_else(str_detect(lat, "S$"), -lat, lat),
    lon = if_else(str_detect(lon, "W$"), -lon, lon)
  ) |>
  
  # Step 10: Create a Coherent 'site_x' Column
  mutate(
    # If `lat` and `lon` are present, use them; otherwise, use the `location` name
    site_x = case_when(
      !is.na(lat) & !is.na(lon) ~ paste(lat, lon, sep = ", "),
      !is.na(location) ~ location,
      TRUE ~ NA_character_
    )
  ) 
```

mutate(
  # Clean site_x values to remove extra spaces
  site_x = str_trim(site_x),
  # Extract latitude and longitude from 'site_x' with a more robust pattern
  extracted_lat = str_extract(site_x, "[-]?\\d+\\.\\d+(?=, ?[-]?\\d+\\.\\d+)") |> as.numeric(),
  extracted_lon = str_extract(site_x, "(?<=, ?)[-]?\\d+\\.\\d+") |> as.numeric()
)


The geocoder is not needed becasue all observations has geographical coorordinates (lat. lon.)

# Step 2: Identify rows that need geocoding (i.e., where coordinates are missing)
locations_to_geocode <- database_clean |>
  filter(is.na(extracted_lat) | is.na(extracted_lon)) |>
  distinct(location) |>
  filter(!is.na(location))

# Step 3: Geocode Location Names
geocoded_locations <- locations_to_geocode |>
  geocode(address = location, method = "osm", lat = "geo_lat", long = "geo_lon")

# Step 4: Merge Geocoded Coordinates Back to the Dataset
database_clean <- database_clean |>
  left_join(geocoded_locations, by = "location") |>
  mutate(
    # Use extracted coordinates if available, otherwise use geocoded coordinates
    final_lat = coalesce(extracted_lat, geo_lat),
    final_lon = coalesce(extracted_lon, geo_lon),
    # Create the `exp_site_loc` column with final coordinates
    exp_site_loc = if_else(!is.na(final_lat) & !is.na(final_lon),
                           paste(final_lat, final_lon, sep = ", "),
                           NA_character_)
  ) 



```{r}
# Step 1: Extract Coordinates from `site_x` if available
database_clean <- database_clean |>
  mutate(
    # Extract latitude: Matches integers or decimals before a comma
    extracted_lat = str_extract(site_x, "[-]?\\d+(\\.\\d+)?(?=, )"),
    # Extract longitude: Matches integers or decimals after a comma and space
    extracted_lon = str_extract(site_x, "(?<=, )[-]?\\d+(\\.\\d+)?")
  ) |>
  mutate(
    # Convert extracted values to numeric
    extracted_lat = as.numeric(extracted_lat),
    extracted_lon = as.numeric(extracted_lon)
  )

# Step 2: Identify rows that need geocoding (i.e., where coordinates are missing)
locations_to_geocode <- database_clean |>
  filter(is.na(extracted_lat) | is.na(extracted_lon)) |>
  distinct(location) |>
  filter(!is.na(location))

# Step 3: Geocode Location Names
geocoded_locations <- locations_to_geocode |>
  geocode(address = location, method = "osm", lat = "geo_lat", long = "geo_lon")

# Step 4: Merge Geocoded Coordinates Back to the Dataset
database_clean <- database_clean |>
  left_join(geocoded_locations, by = "location") |>
  mutate(
    # Use extracted coordinates if available, otherwise use geocoded coordinates
    final_lat = coalesce(extracted_lat, geo_lat),
    final_lon = coalesce(extracted_lon, geo_lon),
    # Create the `exp_site_loc` column with final coordinates
    exp_site_loc = if_else(!is.na(final_lat) & !is.na(final_lon),
                           paste(final_lat, final_lon, sep = ", "),
                           NA_character_)
  ) |>
  select(-extracted_lat, -extracted_lon, -geo_lat, -geo_lon)|> 
  
  
  
  # Step 5: Relocate columns to the desired order
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

# Passing 23 addresses to the Nominatim single address geocoder
# Query completed in: 23.4 seconds

# Last run (03/12-2024)
# Passing 4 addresses to the Nominatim single address geocoder
# Query completed in: 4.3 seconds

# Last run (01/01-2025)
# Passing 3 addresses to the Nominatim single address geocoder
# Query completed in: 3 seconds
```

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type
10	366	Leeds	England	53.883333° N	1.5491° W	NA	1992	7	Monoculture	Biomass	Cereal

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
29	873	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	874	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	875	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	876	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	877	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	878	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	879	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	880	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	881	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	882	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	883	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	884	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	885	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	886	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	887	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	888	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	889	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	890	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	891	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	892	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	893	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	894	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	895	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	896	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	897	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	898	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	899	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	900	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	901	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	902	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	903	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	904	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	905	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	906	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	907	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987
29	908	Nothern England	England	53.866667°N	1.320278°W	NA	1990	>2	Monoculture	Timber	Legume	1987

Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
36	1074	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011
36	1075	Xinjiang	China	73.370° N	34.200° E	Single-factor design	2011	2	Monoculture	Fruit,nut & other	Cereal	2011



Id_article	Id_obs	Site	Location	Latitude	Longitude	Experimental_design	Experiment_Year	Study_duration	Comparator	Tree_type	Crop_type	Year_est_exp
10	380	Silsoe	UK	52.0° N	0.433333° W	NA	1995	7	Monoculture	Biomass	Cereal	1992
10	381	Silsoe	UK	52.0° N	0.433333° W	NA	1995	7	Monoculture	Biomass	Cereal	1992
10	382	Silsoe	UK	52.0° N	0.433333° W	NA	1996	7	Monoculture	Biomass	Cereal	1992
10	383	Silsoe	UK	52.0° N	0.433333° W	NA	1996	7	Monoculture	Biomass	Cereal	1992
10	384	Silsoe	UK	52.0° N	0.433333° W	NA	1997	7	Monoculture	Biomass	Cereal	1992
10	385	Silsoe	UK	52.0° N	0.433333° W	NA	1997	7	Monoculture	Biomass	Cereal	1992
10	386	Silsoe	UK	52.0° N	0.433333° W	NA	1998	7	Monoculture	Biomass	Legume	1992
21	622	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	623	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	624	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	625	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	626	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	627	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	628	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	629	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	630	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	631	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
21	632	Bedfordshire	UK	52.000000° N	-0.430000° W	NA	2011	>2	Monoculture	Biomass	Cereal	1992
















```{r}
# Example: Adding a placeholder RiskOfBias column
meta_data$RiskOfBias <- sample(c("Low", "Moderate", "High"), nrow(meta_data), replace = TRUE)
```

```{r}
# Updated function to compute key metrics for a given dataset and V_matrix
compute_model_metrics <- function(data, V_matrix, dataset_name) {
  tryCatch({
    # Define the moderators (if applicable)
    moderators <- c("tree_type", "crop_type", "age_system", "season", 
                    "soil_texture", "no_tree_per_m", "tree_height", "alley_width")
    moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
    
    # Prepare the data
    data <- data %>%
      mutate(across(all_of(moderators), as.factor)) %>%
      as.data.frame()
    
    # Fit the random-effects model
    model <- rma.mv(
      yi = yi,
      V = V_matrix,
      mods = moderator_formula,
      random = list(
        ~ 1 | id_article,
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
    
    # Extract key metrics
    k.all <- model$k.all
    tau2 <- sum(model$sigma2)  # Between-study variance
    sigma2 <- mean(data$vi)    # Within-study variance
    I2 <- (tau2 / (tau2 + sigma2)) * 100  # Heterogeneity proportion
    QM <- model$QM             # Test statistic for moderators
    QMp <- model$QMp           # p-value for moderators
    aic <- AIC(model)          # Akaike Information Criterion
    bic <- BIC(model)          # Bayesian Information Criterion
    logLik_val <- as.numeric(logLik(model))  # Log-Likelihood
    
    tibble(
      dataset = dataset_name,
      k.all = k.all,
      tau2 = tau2,
      I2 = I2,
      QM = QM,
      QMp = QMp,
      AIC = aic,
      BIC = bic,
      logLik = logLik_val
    )
  }, error = function(e) {
    # Handle errors by returning NA values
    tibble(
      dataset = dataset_name,
      k.all = NA, tau2 = NA, I2 = NA, QM = NA, QMp = NA,
      AIC = NA, BIC = NA, logLik = NA
    )
  })
}
```

```{r}
datasets <- list(
  non_imp_dataset = non_imp_dataset,
  imp_dataset = imp_dataset,
  non_imp_dataset_imputed = non_imp_dataset_imputed,
  imp_dataset_imputed = imp_dataset_imputed
)

V_matrices <- lapply(names(datasets), function(dataset_name) {
  calculate_v_matrix(datasets[[dataset_name]], correlation = 0.5)
})
names(V_matrices) <- names(datasets)
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Loop through datasets and compute metrics
model_results <- lapply(names(datasets), function(dataset_name) {
  cat("Processing dataset:", dataset_name, "\n")
  compute_model_metrics(
    data = datasets[[dataset_name]], 
    V_matrix = V_matrices[[dataset_name]], 
    dataset_name = dataset_name
  )
})

# Combine results into a single data frame
model_results_df <- bind_rows(model_results)

# View the results
print(model_results_df)


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################

# Time difference of 1.29801 mins
```
```{r}
# Compute whole-model metrics
model_metrics <- tibble(
  tau2 = full_model$tau2,  # Between-study variance
  I2 = if (!is.null(full_model$tau2) && !is.null(full_model$sigma2)) {
    100 * full_model$tau2 / (full_model$tau2 + sum(full_model$sigma2))
  } else {
    NA
  },  # Proportion of heterogeneity
  QM = full_model$QM,  # Omnibus test for moderators
  pval_QM = full_model$pval.QM,  # p-value for QM
  QE = full_model$QE,  # Cochran's Q
  pval_QE = full_model$QEp  # p-value for heterogeneity
)

# Print the metrics
model_metrics
```


```{r}
# Prepare data for the forest plot
forest_plot_data <- combined_loo_res %>%
  mutate(
    slab = paste0("Study ", id_article),  # Study labels
    ci.lb = estimate - 1.96 * se,        # Lower bound of 95% CI
    ci.ub = estimate + 1.96 * se         # Upper bound of 95% CI
  )

```
```{r}
res <- rma(yi = estimate, vi = se^2, data = forest_plot_data, method = "DL")
```

```{r}
# Set up the margins for the plot
par(mar = c(4, 4, 2, 2))  # Adjust margins as needed

# Generate the forest plot
sav <- forest(
  x = forest_plot_data$estimate,  # Effect sizes
  sei = forest_plot_data$se,     # Standard errors
  slab = forest_plot_data$slab,  # Study labels
  xlim = c(-1, 2),               # X-axis limits (adjust based on your data)
  alim = c(-0.5, 1.5),           # Area for effect sizes
  at = seq(-1, 2, by = 0.5),     # X-axis tick marks
  refline = 0,                   # Reference line at 0
  transf = exp,                  # Transform effect sizes (e.g., for ORs)
  digits = 2,                    # Number of decimal places
  cex = 0.8                      # Text size
)

# Add annotations for summary estimate (optional)
addpoly(x = res$b, ci.lb = res$ci.lb, ci.ub = res$ci.ub, row = -1, cex = 0.8, mlab = "Overall")

# Add custom annotations (heterogeneity stats, etc.)
text(-1, -2, pos = 4, bquote(paste("Heterogeneity: ",
                                   "Tau"^2, " = ", .(round(res$tau2, 3)), "; ",
                                   "I"^2, " = ", .(round(res$I2, 1)), "%")))

```
```{r}
# Loop through response variables
unique(forest_plot_data$response_variable) %>%
  lapply(function(resp) {
    data_subset <- subset(forest_plot_data, response_variable == resp)
    res <- rma(yi = estimate, vi = se^2, data = data_subset, method = "DL")
    
    forest(
      x = data_subset$estimate,
      sei = data_subset$se,
      slab = data_subset$slab,
      xlim = c(-1, 2),
      alim = c(-0.5, 1.5),
      at = seq(-1, 2, by = 0.5),
      refline = 0,
      transf = exp,
      digits = 2,
      cex = 0.8,
      main = paste("Forest Plot:", resp)  # Add title for each response variable
    )
    addpoly(res$b, ci.lb = res$ci.lb, ci.ub = res$ci.ub, row = -1, cex = 0.8, mlab = "Overall")
  })

```


```{r}
# Generic function to create forest plots
create_forest_plots <- function(data, moderators, response_var_col, yi_col, vi_col) {
  
  # Validate inputs
  if (!all(c(response_var_col, yi_col, vi_col) %in% names(data))) {
    stop("The specified column names do not exist in the data.")
  }
  if (!all(moderators %in% names(data))) {
    stop("Some specified moderators are not in the data.")
  }
  
  # Add confidence intervals to the data
  forest_plot_data <- data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(
      all_of(response_var_col),
      all_of(moderators),
      !!sym(yi_col),
      lower_ci,
      upper_ci
    ) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci)) %>%
    pivot_longer(cols = all_of(moderators), names_to = "moderator", values_to = "moderator_value") %>%
    filter(!is.na(moderator_value))
  
  # Create plots for each combination of response variable and moderator
  forest_plots <- forest_plot_data %>%
    group_by(across(all_of(response_var_col)), moderator) %>%
    group_split() %>%
    map(~ {
      response_variable <- unique(.x[[response_var_col]])
      moderator <- unique(.x$moderator)
      
      ggplot(.x, aes(
        x = !!sym(yi_col),
        y = moderator_value,
        color = !!sym(response_var_col)
      )) +
        geom_point(size = 3) +  # Effect size
        geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) +  # Confidence intervals
        facet_grid(moderator ~ ., scales = "free_x", switch = "y") +  # Stack facets vertically
        geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line
        labs(
          title = paste("Forest Plot: ", moderator, "vs", response_variable),
          x = "Effect Size (yi)",
          y = moderator
        ) +
        theme_minimal() +
        theme(
          strip.text.y = element_text(size = 12, face = "bold"),
          strip.placement = "outside",
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          legend.position = "bottom",
          panel.spacing = unit(1, "lines")
        )
    })
  
  # Return the list of ggplot objects
  return(forest_plots)
}
```

```{r}
# Specify the required arguments
forest_plots <- create_forest_plots(
  data = imp_dataset_imputed,
  moderators = c("crop_type", "tree_type", "age_system", "season", "soil_texture"),
  response_var_col = "response_variable",
  yi_col = "yi",
  vi_col = "vi"
)

# Display plots (example for the first plot)
forest_plots[[20]]

# Save all plots (optional)
# map2(forest_plots, seq_along(forest_plots), ~ ggsave(
#   filename = paste0("forest_plot_", .y, ".png"),
#   plot = .x,
#   width = 10,
#   height = 8
# ))

```



```{r}
# Prepare and clean the data for effect sizes
forest_plot_data_clean <- forest_plot_data %>%
  rename(
    estimate = yi,
    lower_ci = lower_ci,
    upper_ci = upper_ci
  )

# Calculate densities for each response variable
density_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarise(
    density_x = list(density(estimate, na.rm = TRUE)$x),  # Extract density x-coordinates
    density_y = list(density(estimate, na.rm = TRUE)$y),  # Extract density y-coordinates
    .groups = "drop"
  ) %>%
  unnest(cols = c(density_x, density_y))  # Flatten the lists into columns

# Add ranking and summary statistics
aggregated_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarize(
    overall_effect = mean(estimate, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(id_article),
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_label = paste0(response_variable, " (", num_studies, " studies)"),
    mean_ci_label = paste0(
      sprintf("%.2f", overall_effect), " [",
      sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
    ),
    response_rank = rank(-overall_effect)  # Negative for descending order
  )

# Merge the rank into the density data
density_data <- density_data %>%
  left_join(aggregated_data %>% select(response_variable, response_label, response_rank), by = "response_variable")

# Calculate overall effect size for all response variables
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = n_distinct(forest_plot_data_clean$id_obs),
    num_studies = n_distinct(forest_plot_data_clean$id_article),
    size_category = NA,
    mean_ci_label = paste0(
      sprintf("%.2f", overall_effect), " [",
      sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
    )
  )

# Combine aggregated data with overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  mutate(
    response_label = ifelse(
      response_variable == "Overall",
      paste0("Overall Effect Size (", num_studies, " studies)"),
      paste0(response_variable, " (", num_studies, " studies)")
    ),
    response_rank = ifelse(response_variable == "Overall", Inf, response_rank)  # Place "Overall" at the bottom
  )

# Adjust density data for plotting (exclude "Overall")
density_data <- density_data %>%
  filter(response_variable != "Overall")

# Adjust ranks to explicitly place "Overall" at the bottom
# Explicitly define the levels for response_label with "Overall" at the bottom
plot_data <- plot_data %>%
  arrange(response_variable != "Overall", desc(response_rank)) %>%
  mutate(
    response_label = factor(response_label, levels = unique(response_label))
  )


ggplot() +
  # Density ridges for individual response variables
  geom_ridgeline(
    data = density_data,
    aes(x = density_x, y = response_label, height = density_y, fill = response_variable),
    alpha = 0.3,
    scale = 0.05,
    color = NA
  ) +
  # Points for individual response variables
  geom_point(
    data = plot_data %>% filter(response_variable != "Overall"),
    aes(x = overall_effect, y = response_label, size = size_category),
    color = "black"
  ) +
  # Diamond for overall effect size
  geom_point(
    data = plot_data %>% filter(response_variable == "Overall"),
    aes(x = overall_effect, y = response_label),
    shape = 18,  # Diamond shape
    size = 5,
    color = "black"
  ) +
  # Error bars for all data
  geom_errorbarh(
    data = plot_data,
    aes(xmin = lower_ci, xmax = upper_ci, y = response_label),
    height = 0.2,
    color = "darkgray"
  ) +
  # Text annotations for mean and confidence intervals
  geom_text(
    data = plot_data,
    aes(x = 0.55, y = response_label, label = mean_ci_label),
    size = 3.5,
    hjust = 0,
    fontface = "italic",
    color = "black"
  ) +
  # Reference line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize scales
  scale_fill_manual(values = custom_colors, guide = "none") +
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Adjust x-axis
  scale_x_continuous(limits = c(-0.25, 0.6), breaks = seq(-0.25, 0.5, by = 0.25)) +
  # Labels and titles
  labs(
    title = "Forest Plot with Adjusted Density and Study Details",
    x = "Effect Size (yi)",
    y = NULL
  ) +
  # Themes
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```


```{r}
# Cleaning Data
clean_forest_data <- function(data, yi_col, vi_col, other_cols) {
  data %>%
    mutate(
      lower_ci = !!sym(yi_col) - 1.96 * sqrt(!!sym(vi_col)),
      upper_ci = !!sym(yi_col) + 1.96 * sqrt(!!sym(vi_col))
    ) %>%
    select(all_of(c(other_cols, yi_col, "lower_ci", "upper_ci"))) %>%
    filter(!is.na(!!sym(yi_col)), !is.na(lower_ci), !is.na(upper_ci))
}

forest_data_clean <- clean_forest_data(
  data = imp_dataset_imputed,
  yi_col = "yi",
  vi_col = "vi",
  other_cols = c("id_article", "id_obs", "response_variable", "crop_type", "tree_type")
)

```

```{r}
# Aggregating Data
aggregate_forest_data <- function(data, yi_col, ci_cols, group_col) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarize(
      overall_effect = mean(!!sym(yi_col), na.rm = TRUE),
      lower_ci = mean(!!sym(ci_cols[1]), na.rm = TRUE),
      upper_ci = mean(!!sym(ci_cols[2]), na.rm = TRUE),
      num_observations = n(),
      num_studies = n_distinct(id_article),
      size_category = case_when(
        num_studies <= 2 ~ "1-2",
        num_studies <= 4 ~ "3-4",
        num_studies > 4 ~ "5+"
      ),
      .groups = "drop"
    ) %>%
    mutate(
      size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
      response_label = paste0(!!sym(group_col), " (", num_studies, " studies)"),
      mean_ci_label = paste0(
        sprintf("%.2f", overall_effect), " [",
        sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
      ),
      response_rank = rank(-overall_effect)  # Add rank for ordering
    )
}

aggregated_data <- aggregate_forest_data(
  data = forest_data_clean,
  yi_col = "yi",
  ci_cols = c("lower_ci", "upper_ci"),
  group_col = "response_variable"
)
```
```{r}
# Computing Densities
compute_densities <- function(data, yi_col, group_col, aggregated_data) {
  data %>%
    group_by(!!sym(group_col)) %>%
    summarise(
      density_x = list(density(!!sym(yi_col), na.rm = TRUE)$x),
      density_y = list(density(!!sym(yi_col), na.rm = TRUE)$y),
      .groups = "drop"
    ) %>%
    unnest(cols = c(density_x, density_y)) %>%
    left_join(
      aggregated_data %>% select(response_variable, response_label, response_rank),
      by = "response_variable"
    )
}

density_data <- compute_densities(
  data = forest_data_clean,
  yi_col = "yi",
  group_col = "response_variable",
  aggregated_data = aggregated_data
)
```

```{r}
# Adding Overall Effect Size
add_overall_effect <- function(agg_data, original_data) {
  agg_data %>%
    summarise(
      response_variable = "Overall",
      overall_effect = mean(overall_effect, na.rm = TRUE),
      lower_ci = mean(lower_ci, na.rm = TRUE),
      upper_ci = mean(upper_ci, na.rm = TRUE),
      num_observations = sum(num_observations, na.rm = TRUE),
      num_studies = sum(num_studies, na.rm = TRUE),
      size_category = NA,
      mean_ci_label = paste0(
        sprintf("%.2f", mean(overall_effect, na.rm = TRUE)), " [",
        sprintf("%.2f", mean(lower_ci, na.rm = TRUE)), ", ",
        sprintf("%.2f", mean(upper_ci, na.rm = TRUE)), "]"
      )
    ) %>%
    mutate(
      response_label = "Overall Effect Size (36 studies)",
      response_rank = 999  # Explicitly assign 999 to place at the bottom
    )
}

overall_effect <- add_overall_effect(
  agg_data = aggregated_data,
  original_data = forest_data_clean
)

overall_effect |> glimpse()
```

```{r}
# Combine aggregated data with overall effect size
# Ensure response_label exists in both aggregated_data and overall_effect
aggregated_data <- aggregated_data %>%
  mutate(
    response_label = paste0(response_variable, " (", num_studies, " studies)")
  )

overall_effect <- overall_effect %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)"
  )

# Combine aggregated data with overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  # Define factor levels explicitly to place "Overall" at the bottom
  mutate(
    response_label = factor(
      response_label,
      levels = c(
        setdiff(unique(aggregated_data$response_label), "Overall Effect Size (36 studies)"),
        "Overall Effect Size (36 studies)"
      )
    )
  ) %>%
  arrange(desc(response_rank))  # Arrange by descending rank

```
```{r}
# Ensure response_label is added to density_data during the join
density_data <- density_data %>%
  left_join(
    aggregated_data %>% select(response_variable, response_label, response_rank),
    by = "response_variable"
  ) %>%
  # Exclude the "Overall" row
  filter(response_variable != "Overall")

# Combine aggregated data with overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  # Add response_label for the overall effect
  mutate(
    response_label = ifelse(
      response_variable == "Overall",
      "Overall Effect Size (36 studies)",
      response_label
    )
  ) %>%
  # Ensure factor levels place "Overall" at the bottom
  mutate(
    response_label = factor(
      response_label,
      levels = c(
        setdiff(unique(aggregated_data$response_label), "Overall Effect Size (36 studies)"),
        "Overall Effect Size (36 studies)"
      )
    )
  ) %>%
  arrange(desc(response_rank))  # Arrange by descending rank

```

```{r}
# Arrange `plot_data` to place "Overall" at the bottom
plot_data <- plot_data %>%
  arrange(ifelse(response_variable == "Overall", Inf, response_rank))

# Add `response_rank` to `density_data` by joining with `plot_data`
density_data <- density_data %>%
  left_join(
    plot_data %>% select(response_variable, response_rank),
    by = "response_variable"
  ) %>%
  # Exclude "Overall" and arrange by rank
  filter(response_variable != "Overall") %>%
  arrange(desc(response_rank))


# plot_data |> glimpse()
# density_data |> glimpse()
```


```{r}
# Plotting
# Prepare and clean the data for effect sizes
forest_plot_data_clean <- forest_plot_data %>%
  rename(
    estimate = yi,
    lower_ci = lower_ci,
    upper_ci = upper_ci
  )

# Calculate densities for each response variable
density_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarise(
    density_x = list(density(estimate, na.rm = TRUE)$x),  # Extract density x-coordinates
    density_y = list(density(estimate, na.rm = TRUE)$y),  # Extract density y-coordinates
    .groups = "drop"
  ) %>%
  unnest(cols = c(density_x, density_y))  # Flatten the lists into columns

# Add ranking and summary statistics
aggregated_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarize(
    overall_effect = mean(estimate, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(id_article),
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_label = paste0(response_variable, " (", num_studies, " studies)"),
    mean_ci_label = paste0(
      sprintf("%.2f", overall_effect), " [",
      sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
    ),
    response_rank = rank(-overall_effect)  # Negative for descending order
  )

# Merge the rank into the density data
density_data <- density_data %>%
  left_join(aggregated_data %>% select(response_variable, response_label, response_rank), by = "response_variable")

# Calculate overall effect size for all response variables
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = n_distinct(forest_plot_data_clean$id_obs),
    num_studies = n_distinct(forest_plot_data_clean$id_article),
    size_category = NA,
    mean_ci_label = paste0(
      sprintf("%.2f", overall_effect), " [",
      sprintf("%.2f", lower_ci), ", ", sprintf("%.2f", upper_ci), "]"
    )
  ) %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)",
    response_rank = Inf  # Ensure "Overall" is at the bottom
  )

# Combine aggregated data with overall effect size
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  arrange(response_rank) %>%
  mutate(
    response_label = factor(response_label, levels = unique(response_label))
  )

# Adjust density data for plotting (exclude "Overall")
density_data <- density_data %>%
  filter(response_variable != "Overall")

# Plot Forest Plot
# Add a new column to differentiate "Overall" from other response variables
plot_data <- plot_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

# Add a new column for facet labels
plot_data <- plot_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables")
  )

# Ensure the ordering within facets is correct
plot_data <- plot_data %>%
  mutate(
    response_label = factor(
      response_label,
      levels = c(
        unique(aggregated_data$response_label),  # Other response variables
        "Overall Effect Size (36 studies)"      # Overall effect size at the end
      )
    )
  )

# Create a factor for facet_label with explicit levels
facet_order <- c("Response Variables", "Overall Effect Size")

plot_data <- plot_data %>%
  mutate(facet_label = factor(facet_label, levels = facet_order))

density_data <- density_data %>%
  mutate(facet_label = factor(facet_label, levels = facet_order))

# Add a relative space for facets (using strip scales to control heights)
forest_plot_density <- ggplot() +
  # Add density ridges for response variables
  geom_ridgeline(
    data = density_data %>% filter(response_variable != "Overall"),
    aes(
      x = density_x,
      y = response_label,
      height = density_y,
      fill = response_variable
    ),
    alpha = 0.3,
    scale = 0.05,
    color = NA
  ) +
  # Add points for response variables
  geom_point(
    data = plot_data %>% filter(response_variable != "Overall"),
    aes(
      x = overall_effect,
      y = response_label,
      size = size_category
    ),
    color = "black"
  ) +
  # Add diamond shape for the overall effect size
  geom_point(
    data = plot_data %>% filter(response_variable == "Overall"),
    aes(
      x = overall_effect,
      y = response_label
    ),
    shape = 18,
    size = 5,
    color = "black"
  ) +
  # Add horizontal error bars for all data
  geom_errorbarh(
    data = plot_data,
    aes(
      xmin = lower_ci,
      xmax = upper_ci,
      y = response_label
    ),
    height = 0.2,
    color = "darkgray"
  ) +
  # Add text annotations for mean and confidence intervals
  geom_text(
    data = plot_data,
    aes(
      x = 0.55,
      y = response_label,
      label = mean_ci_label
    ),
    size = 3.5,
    hjust = 0,
    fontface = "italic",
    color = "black"
  ) +
  # Add a vertical reference line at zero
  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "red"
  ) +
  # Customize fill colors
  scale_fill_manual(
    values = custom_colors,
    guide = "none"
  ) +
  # Customize point sizes for number of studies
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize x-axis scale
  scale_x_continuous(
    limits = c(-0.25, 0.6),
    breaks = seq(-0.25, 0.5, by = 0.25)
  ) +
  # Facet by the new facet_label column with adjusted ratios
  facet_grid(
    facet_label ~ .,
    scales = "free_y",
    space = "free",
    switch = "y"
  ) +
  theme_minimal() +
  # Adjust strip and layout to control the space ratio
  theme(
    strip.text.y = element_text(size = 12, face = "bold"),
    strip.background = element_blank(),
    panel.spacing = unit(0.5, "lines"),
    panel.grid = element_blank(),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  ) +
  labs(
    title = "Forest Plot with Adjusted Density and Study Details",
    x = "Effect Size (yi)",
    y = NULL
  )

# Display the plot
forest_plot_density

```
```{r}
# Prepare aggregated data with proper error bar adjustments
aggregated_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarize(
    overall_effect = mean(estimate, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),  # Include lower confidence interval
    upper_ci = mean(upper_ci, na.rm = TRUE),  # Include upper confidence interval
    num_observations = n(),
    num_studies = n_distinct(id_article),
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)  # Add rank based on overall effect size
  )

# Adjust error bars to ensure they stay within x-axis limits
aggregated_data <- aggregated_data %>%
  mutate(
    lower_ci = ifelse(lower_ci < -0.5, -0.5, lower_ci),
    upper_ci = ifelse(upper_ci > 0.5, 0.5, upper_ci)
  )

# Combine `aggregated_data` with "Overall" effect size
overall_effect <- aggregated_data %>%
  summarise(
    response_variable = "Overall",
    overall_effect = mean(overall_effect, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_observations = n_distinct(forest_plot_data_clean$id_obs),
    num_studies = n_distinct(forest_plot_data_clean$id_article),
    size_category = NA
  ) %>%
  mutate(
    response_label = "Overall Effect Size (36 studies)",
    response_rank = Inf  # Place "Overall" at the bottom
  )

# Combine data for plotting
plot_data <- bind_rows(aggregated_data, overall_effect) %>%
  mutate(
    response_label = ifelse(
      response_variable == "Overall",
      paste0("Overall Effect Size (", num_studies, " studies)"),
      paste0(response_variable, " (", num_studies, " studies)")
    ),
    response_rank = ifelse(response_variable == "Overall", Inf, response_rank)
  ) %>%
  arrange(response_rank) %>%
  mutate(response_label = factor(response_label, levels = unique(response_label)))

# Adjust density data
density_data <- density_data %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables"),
    facet_label = factor(facet_label, levels = c("Response Variables", "Overall Effect Size"))
  )

# Merge the rank and labels into density_data
density_data <- density_data %>%
  left_join(
    aggregated_data %>% select(response_variable, response_label),
    by = "response_variable"
  ) %>%
  mutate(
    facet_label = ifelse(response_variable == "Overall", "Overall Effect Size", "Response Variables"),
    facet_label = factor(facet_label, levels = c("Response Variables", "Overall Effect Size"))
  )

# Ensure `response_label` exists in `density_data`
if (!"response_label" %in% colnames(density_data)) {
  stop("Error: response_label column is missing in density_data.")
}

# Create the forest plot
forest_plot_density <- ggplot() +
  # Density ridges for response variables
  geom_ridgeline(
    data = density_data %>% filter(response_variable != "Overall"),
    aes(
      x = density_x,
      y = response_label,
      height = density_y,
      fill = response_variable
    ),
    alpha = 0.3,
    scale = 0.05,
    color = NA
  ) +
  # Points for response variables
  geom_point(
    data = plot_data %>% filter(response_variable != "Overall"),
    aes(
      x = overall_effect,
      y = response_label,
      size = size_category
    ),
    color = "black"
  ) +
  # Diamond for overall effect size
  geom_point(
    data = plot_data %>% filter(response_variable == "Overall"),
    aes(
      x = overall_effect,
      y = response_label
    ),
    shape = 18,
    size = 5,
    color = "black"
  ) +
  # Error bars for all data
  geom_errorbarh(
    data = plot_data,
    aes(
      xmin = lower_ci,
      xmax = upper_ci,
      y = response_label
    ),
    height = 0.2,
    color = "darkgray"
  ) +
  # Adjust x-axis scale
  scale_x_continuous(
    limits = c(-0.5, 0.5),
    breaks = seq(-0.5, 0.5, by = 0.25)
  ) +
  # Facet by overall and response variables
  facet_grid(
    facet_label ~ .,
    scales = "free_y",
    space = "free"
  ) +
  # Add labels and themes
  labs(
    title = "Forest Plot with Adjusted Density and Study Details",
    x = "Effect Size (yi)",
    y = NULL,
    size = "Number of Studies"
  ) +
  theme_minimal() +
  theme(
    strip.text.y = element_text(size = 12, face = "bold"),
    strip.background = element_blank(),
    panel.spacing = unit(1, "lines"),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

# Display the plot
forest_plot_density

```

```{r}
# Filter and prepare the data for Forest plot
forest_plot_data <- imp_dataset_imputed %>%
  mutate(
    lower_ci = yi - 1.96 * sqrt(vi),
    upper_ci = yi + 1.96 * sqrt(vi)
  ) |> 
  select(
    id_article,
    id_obs,
    # Key response variables (Ecosystem Services)
    response_variable, 
    # Key Moderators
    crop_type, 
    tree_type,
    age_system,
    season,
    soil_texture,
    # Effect Size Measure
    yi, 
    lower_ci, 
    upper_ci
  ) |> 
  filter(!is.na(yi), !is.na(lower_ci), !is.na(upper_ci))

forest_plot_data |> glimpse()
```

```{r}
# Ensure correct column names for lower and upper confidence intervals
forest_plot_data_clean <- forest_plot_data %>%
  rename(
    estimate = yi,        # Rename yi to estimate
    lower_ci = lower_ci,  # Use correct column name for lower confidence bound
    upper_ci = upper_ci   # Use correct column name for upper confidence bound
  )

# Create meaningful size categories based on the number of studies
aggregated_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarize(
    overall_effect = mean(estimate, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_studies = n_distinct(crop_type)  # Number of unique crop types as a proxy for studies
  ) %>%
  mutate(
    size_category = case_when(
      num_studies <= 2 ~ "1-2",   # Category 1
      num_studies <= 4 ~ "3-4",   # Category 2
      num_studies > 4 ~ "5+"      # Category 3
    )
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+"))  # Set the desired order
  )
```

```{r}
# Define a custom color palette for response_variable
custom_colors <- c(
  "Biodiversity" = "#FF9999",          # Light red
  "Greenhouse gas emission" = "#66C266",  # Green
  "Product quality" = "#FFC000",       # Yellow
  "Crop yield" = "#FF9933",           # Orange
  "Pest and Disease" = "#33CCCC",     # Teal
  "Soil quality" = "#9966CC",         # Purple
  "Water quality" = "#9999FF"         # Light blue
)


# Generate the forest plot with black dots and colored error bars
overall_effect_size <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(response_variable, overall_effect))) +
  geom_point(aes(size = size_category), color = "black", alpha = 0.8) +  # Mean effect size points are black
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = response_variable), height = 0.2, size = 1) +  # Error bars colored by response_variable
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line at 0
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),  # Map categories to sizes
    name = "Number of Studies"
  ) +
  scale_color_manual(
    values = custom_colors,  # Apply custom colors to error bars
    name = "Response Variable"
  ) +
  labs(
    title = "Overall Effect Size by Response Variable",
    x = "Effect Size (Mean Estimate)",
    y = "Response Variable"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 12),  # Larger y-axis text
    axis.text.x = element_text(size = 12),  # Larger x-axis text
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),  # Centered and bold title
    legend.position = "none",
    panel.spacing = unit(1, "lines")  # Increase spacing between panels
  )

overall_effect_size
```
```{r}
# Prepare and clean the data for effect sizes
forest_plot_data_clean <- forest_plot_data %>%
  rename(
    estimate = yi,
    lower_ci = lower_ci,
    upper_ci = upper_ci
  )

# Calculate densities for each response variable
density_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarise(
    density_x = list(density(estimate, na.rm = TRUE)$x),  # Extract density x-coordinates
    density_y = list(density(estimate, na.rm = TRUE)$y),  # Extract density y-coordinates
    .groups = "drop"
  ) %>%
  unnest(cols = c(density_x, density_y))  # Flatten the lists into columns

# Add ranking for response variables based on overall effect size
aggregated_data <- forest_plot_data_clean %>%
  group_by(response_variable) %>%
  summarize(
    overall_effect = mean(estimate, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),  # Ensure lower_ci is included
    upper_ci = mean(upper_ci, na.rm = TRUE),  # Ensure upper_ci is included
    num_observations = n(),
    num_studies = n_distinct(id_article),
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)  # Add rank based on descending effect size
  ) # |> 
# Adjust error bars to fit within x-axis limits
# mutate(
#   lower_ci = ifelse(lower_ci < -0.25, -0.25, lower_ci),
#   upper_ci = ifelse(upper_ci > 0.5, 0.5, upper_ci)
# )

# Merge the rank into the density data
density_data <- density_data %>%
  left_join(aggregated_data %>% select(response_variable, response_rank), by = "response_variable")  # Add rank

# Plot with added error bars
density_forrest_plot <- ggplot() +
  # Add density ridges for actual effect sizes (`yi`) per response variable
  geom_ridgeline(
    data = density_data,
    aes(x = density_x, y = reorder(response_variable, response_rank), height = density_y, fill = response_variable),
    alpha = 0.3,
    scale = 0.08,  # Adjust scale for ridge height
    color = NA
  ) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(
    data = aggregated_data,
    aes(xmin = lower_ci, xmax = upper_ci, y = reorder(response_variable, response_rank)),
    height = 0.1,
    color = "darkgray",
    size = 0.8
  ) +
  # Add clipped horizontal error bars
  # geom_errorbarh(
  #   data = aggregated_data,
  #   aes(xmin = lower_ci, xmax = upper_ci, y = reorder(response_variable, response_rank)),
  #   height = 0.1,
  #   color = "darkgray",
  #   size = 0.8
  # ) +
  # Add points for overall mean effect sizes
  geom_point(
    data = aggregated_data,
    aes(x = overall_effect, y = reorder(response_variable, response_rank), size = size_category),
    color = "black",
    alpha = 0.8
  ) +
  # Reference line at zero for effect sizes
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize scales
  scale_fill_manual(values = custom_colors, guide = "none") +
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Adjust x-axis scale
  scale_x_continuous(
    breaks = seq(-0.5, 1.0, by = 0.25)  # Customize tick marks without setting limits
    # limits = c(-0.25, 0.5),  # Set x-axis range
    # breaks = seq(-0.5, 1.0, by = 0.25)  # Optional: Customize tick marks
  ) +
  # Labels and titles
  labs(
    title = "Forest Plot with Adjusted Density of Effect Sizes",
    x = "Effect Size (yi)",
    y = NULL
  ) +
  # Themes and aesthetics
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

density_forrest_plot
```


```{r}
# Function for the generic forest plot for each moderator - response variable combination
create_forest_plot_moderator <- function(data, custom_colors = NULL) {
  # Calculate summary statistics (mean effect size and CI) for each response variable and tree_type
  summary_stats <- data %>%
    group_by(response_variable, tree_type) %>%
    summarize(
      mean_effect = mean(yi, na.rm = TRUE),
      lower_ci_mean = mean(lower_ci, na.rm = TRUE),
      upper_ci_mean = mean(upper_ci, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Create the plot
  forest_plot <- ggplot(data) +
    # Add ridgeline for number of observations
    geom_density_ridges(
      aes(
        x = yi,
        y = tree_type,
        fill = response_variable
      ),
      alpha = 0.3,
      scale = 0.8
    ) +
    # Add points for individual observations
    geom_point(
      aes(
        x = yi,
        y = tree_type,
        color = response_variable
      ),
      size = 2,
      position = position_jitter(height = 0.1)  # Add slight jitter for visibility
    ) +
    # Add horizontal error bars for individual observations
    geom_errorbarh(
      aes(
        xmin = lower_ci,
        xmax = upper_ci,
        y = tree_type
      ),
      height = 0.2,
      color = "darkgray"
    ) +
    # Add text for mean effect size and CI on the right
    geom_text(
      data = summary_stats,
      aes(
        x = 2.5,  # Adjust this to position the text correctly
        y = tree_type,
        label = sprintf("%.2f [%.2f, %.2f]", mean_effect, lower_ci_mean, upper_ci_mean)
      ),
      size = 3.5,
      hjust = 0,
      color = "black"
    ) +
    # Facet by response variable
    facet_wrap(~response_variable, ncol = 1, scales = "free_x") +
    # Add vertical reference line
    geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
    # Customize axes and scales
    scale_x_continuous(
      breaks = seq(-1, 3, by = 0.5),
      limits = c(-1, 3)  # Adjust limits as needed
    ) +
    scale_fill_manual(values = custom_colors) +
    scale_color_manual(values = custom_colors) +
    # Add labels and themes
    labs(
      title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
      x = "Effect Size",
      y = "Tree Type"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      strip.placement = "outside",
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 12),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      legend.position = "none",
      legend.title = element_text(size = 12),
      legend.text = element_text(size = 10)
    )
  
  return(forest_plot)
}
```
```{r}
# Generate the forest plot for tree_type moderator
forest_plot_tree_type <- create_forest_plot_moderator(
  forest_moderator, 
  custom_colors = custom_colors
)

forest_plot_tree_type
```
```{r}
# Summarize the data for error bars and unique studies
forest_moderator_summary <- forest_moderator %>%
  group_by(response_variable, tree_type) %>%
  summarise(
    mean_effect = mean(yi, na.rm = TRUE),
    lower_ci = mean(lower_ci, na.rm = TRUE),
    upper_ci = mean(upper_ci, na.rm = TRUE),
    num_studies = n_distinct(id_article),  # Unique studies
    ci_label = sprintf("%.2f [%.2f, %.2f]", mean(yi, na.rm = TRUE), mean(lower_ci, na.rm = TRUE), mean(upper_ci, na.rm = TRUE)),
    .groups = "drop"
  )

# Plot with density ridges, error bars, and unique study counts
forest_plot_tree_type <- ggplot() +
  # Density ridges for number of observations
  geom_density_ridges(
    data = forest_moderator,
    aes(x = yi, y = tree_type, fill = response_variable),
    alpha = 0.3,
    scale = 1.2,  # Adjust ridge height scaling
    rel_min_height = 0.01,  # Exclude very small densities
    color = "black"
  ) +
  # Error bars for mean effect sizes
  geom_errorbarh(
    data = forest_moderator_summary,
    aes(
      xmin = lower_ci,
      xmax = upper_ci,
      y = tree_type,
      color = response_variable
    ),
    height = 0.2
  ) +
  # Mean effect size markers
  geom_point(
    data = forest_moderator_summary,
    aes(
      x = mean_effect,
      y = tree_type,
      color = response_variable
    ),
    size = 3
  ) +
  # CI labels on the right side of the plot
  geom_text(
    data = forest_moderator_summary,
    aes(
      x = max(forest_moderator$yi, na.rm = TRUE) + 0.5,  # Position slightly outside the range
      y = tree_type,
      label = ci_label
    ),
    hjust = 0,  # Left-align the text
    size = 3.5,
    color = "black"
  ) +
  # Add study counts (number of unique studies) below the tree types
  geom_text(
    data = forest_moderator_summary,
    aes(
      x = min(forest_moderator$yi, na.rm = TRUE) - 0.5,  # Position slightly outside the range
      y = tree_type,
      label = paste0("n=", num_studies)
    ),
    hjust = 1,  # Right-align the text
    size = 3.5,
    color = "black"
  ) +
  # Facet for response variables
  facet_wrap(~response_variable, scales = "free", ncol = 1) +
  # Add vertical reference line
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize fill and color scales
  scale_fill_manual(values = custom_colors) +
  scale_color_manual(values = custom_colors) +
  # Adjust plot labels and theme
  labs(
    title = "Generic Forest Plot with Moderators and Mean Effect Sizes",
    x = "Effect Size",
    y = "Tree Type"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    legend.position = "none",
    panel.spacing = unit(1, "lines")
  )

# Display the plot
forest_plot_tree_type

```


```{r}
# Adjusted forest plot with free x-axis scale
forest_plot_response_var_tree_type <- forest_plot_data |> 
  ggplot(aes(x = yi, y = tree_type, color = response_variable)) +
  geom_point(size = 3) +  # Effect size
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) +  # Confidence intervals
  facet_wrap(~response_variable, scales = "free_x") +  # Free x-axis scale for each facet
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line
  labs(
    title = "Forest Plot by Ecosystem Service and Response Variable (AES)",
    x = "Effect Size",
    y = "Tree Type"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "none"
  )

forest_plot_response_var_tree_type
```
```{r}
forest_plot_data |> 
  ggplot(aes(x = yi, y = tree_type, color = response_variable)) +
  geom_point(size = 3) +  # Effect size
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) +  # Confidence intervals
  facet_wrap(~response_variable, scales = "free_x") +  # Free x-axis scale for each facet
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Reference line
  labs(
    title = "Forest Plot by Ecosystem Service and Response Variable (AES)",
    x = "Effect Size",
    y = "Tree Type"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "none"
  )
```




```{r}
impute_and_merge <- function(dataset, moderators, dataset_name = "Dataset") {
  
  cat("Starting imputation for", dataset_name, "...\n")
  
  # Step 1: Prepare data for imputation
  cols_for_impute <- dataset %>%
    select(
      yi, vi,
      id_article, id_obs, exp_id,
      response_variable, all_of(moderators)
    )
  
  # Step 2: Convert categorical variables to factors
  cols_for_impute <- cols_for_impute %>%
    mutate(across(all_of(moderators), as.factor))
  
  # Step 3: Perform multiple imputation using mice
  set.seed(1234)
  imputed_data <- mice(
    cols_for_impute,
    m = 20,         # Number of imputations
    maxit = 100,    # Maximum iterations
    method = 'pmm', # Predictive Mean Matching
    printFlag = FALSE
  )
  
  # Step 4: Extract the first imputed dataset for merging
  completed_data <- complete(imputed_data, 1)
  
  # Step 5: Join the imputed values back to the original dataset
  merged_dataset <- dataset %>%
    left_join(
      completed_data %>%
        select(id_article, id_obs, exp_id, all_of(moderators)),
      by = c("id_article", "id_obs", "exp_id"),
      suffix = c("_original", "_imputed")
    )
  
  # Step 6: Replace missing values in the original columns with imputed values
  for (mod in moderators) {
    original_col <- paste0(mod, "_original")
    imputed_col <- paste0(mod, "_imputed")
    
    if (original_col %in% colnames(merged_dataset) && imputed_col %in% colnames(merged_dataset)) {
      merged_dataset[[mod]] <- ifelse(
        is.na(merged_dataset[[original_col]]),
        merged_dataset[[imputed_col]],
        merged_dataset[[original_col]]
      )
    }
  }
  
  # Step 7: Drop the temporary columns
  merged_dataset <- merged_dataset %>%
    select(-ends_with("_original"), -ends_with("_imputed"))
  
  cat("Imputation completed for", dataset_name, ".\n")
  
  return(merged_dataset)
}
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Performing moderator imputations
moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Impute and merge for non-imputed dataset
non_imp_dataset_imputed <- impute_and_merge(non_imp_dataset, moderators, "Non-Imputed Dataset")

# Impute and merge for imputed dataset
imp_dataset_imputed <- impute_and_merge(imp_dataset, moderators, "Imputed Dataset")


############################################################################################################################
# Helper function to convert numeric imputed values to categorical factors
convert_to_factors <- function(data) {
  # Convert 'no_tree_per_m' to character factors (Low, High)
  data <- data %>%
    mutate(
      no_tree_per_m = case_when(
        no_tree_per_m %in% c(1, "1") ~ "Low",
        no_tree_per_m %in% c(2, "2") ~ "High",
        TRUE ~ as.character(no_tree_per_m)
      ) %>% as.factor()
    )
  
  # Convert 'tree_height' to character factors (Short, Tall)
  data <- data %>%
    mutate(
      tree_height = case_when(
        tree_height %in% c(1, "1") ~ "Short",
        tree_height %in% c(2, "2") ~ "Tall",
        TRUE ~ as.character(tree_height)
      ) %>% as.factor()
    )
  
  # Convert 'alley_width' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      alley_width = case_when(
        alley_width %in% c(1, "1") ~ "Narrow",
        alley_width %in% c(2, "2") ~ "Wide",
        TRUE ~ as.character(alley_width)
      ) %>% as.factor()
    )
  
  # Convert 'age_system' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      age_system = case_when(
        age_system %in% c(1, "1") ~ "Young",
        age_system %in% c(2, "2") ~ "Medium",
        age_system %in% c(3, "3") ~ "Mature",
        TRUE ~ as.character(age_system)
      ) %>% as.factor()
    )
  
  # Convert 'season' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      season = case_when(
        season %in% c(1, "1") ~ "Summer",
        season %in% c(2, "2") ~ "Winter",
        season %in% c(3, "3") ~ "WinterSummer",
        TRUE ~ as.character(season)
      ) %>% as.factor()
    )
  
  return(data)
}

# Apply the conversion function to both datasets
non_imp_dataset_imputed <- convert_to_factors(non_imp_dataset_imputed) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


imp_dataset_imputed <- convert_to_factors(imp_dataset_imputed) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )



############################################################################################################################


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (16/11-24)
# Starting imputation for Non-Imputed Dataset ...
# Advarsel: Number of logged events: 1Imputation completed for Non-Imputed Dataset .
# Starting imputation for Imputed Dataset ...
# Advarsel: Number of logged events: 1Imputation completed for Imputed Dataset .
# Time difference of 1.3963 mins

# Check the structure of the datasets
# str(non_imp_dataset_imputed)
# str(imp_dataset_imputed)
```

Assessing imputation of moderators again

```{r}
# Assessing Moderator missingness

moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Assess missing data for non-imputed dataset
assess_missing_data(non_imp_dataset_imputed, moderators, "Non-Imputed Dataset")

# Assess missing data for imputed dataset
assess_missing_data(imp_dataset_imputed, moderators, "Imputed Dataset")
```

Additional assessment of the moderator imputation

```{r}
# Function to calculate missing data proportions
calculate_missing_proportions <- function(data, moderators) {
  data %>%
    pivot_longer(cols = all_of(moderators), names_to = "moderator", values_to = "value") %>%
    group_by(response_variable, moderator) %>%
    summarise(
      missing_proportion = mean(is.na(value), na.rm = TRUE)
    )
}

# Function to plot missing data proportions per response variable
plot_missing_proportions <- function(original_data, imputed_data, moderators, dataset_name) {
  cat("\nStarting plot creation for", dataset_name, "...\n")
  
  # Calculate missing proportions for original and imputed datasets
  missing_original <- calculate_missing_proportions(original_data, moderators) %>%
    mutate(data_source = "Original")
  
  missing_imputed <- calculate_missing_proportions(imputed_data, moderators) %>%
    mutate(data_source = "Imputed")
  
  # Combine the results
  combined_missing <- bind_rows(missing_original, missing_imputed)
  
  # Create the plot
  plot <- ggplot(combined_missing, aes(x = response_variable, y = missing_proportion, fill = data_source)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ moderator, scales = "free_y") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(
      title = paste("Proportion of Missing Data per Response Variable -", dataset_name),
      x = "Response Variable",
      y = "Missing Proportion"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "top")
  
  cat("\nPlot creation completed for", dataset_name, ".\n")
  
  return(plot)
}

# List of moderators
moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Create plots for Non-Imputed and Imputed datasets
plot_non_imp <- plot_missing_proportions(non_imp_dataset, non_imp_dataset_imputed, moderators, "Non-Imputed Dataset")
plot_imp <- plot_missing_proportions(imp_dataset, imp_dataset_imputed, moderators, "Imputed Dataset")

# Display the plots side by side
plot_non_imp + plot_imp
```


Perform imputation using "mice" (Multivariate Imputation by Chained Equations)

```{r, eval = TRUE}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

############################################################################################################################
# Set seed for reproducibility
set.seed(1234)

# Perform imputation using mice
# - read about mice() here: https://www.metafor-project.org/doku.php/tips:multiple_imputation_with_mice_and_metafor
# - col_for_impute: the data frame containing the columns to be imputed
# - m = 5: number of multiple imputations to perform
# - maxit = 100: maximum number of iterations to perform for each imputation
# - method = 'pmm': method to use for imputation, 'pmm' stands for predictive mean matching
# - seed = 500: random seed for reproducibility of the imputations
# - printFlag: If TRUE, mice will print history on console. Use print=FALSE for silent computation.



# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, silvo_n, control_n,
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, sub_region, experiment_year, alley_width,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  # Convert relevant columns to the correct data types
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    sub_region = as.factor(sub_region),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

# Step 2: Define the predictor matrix
pred_matrix <- mice::make.predictorMatrix(col_for_impute)

# Allow only specific columns to be imputed
# Set all columns except 'silvo_se', 'control_se', 'silvo_n', and 'control_n' to be non-imputed
pred_matrix[, c("tree_age", "crop_type", "tree_type", "sub_region", "experiment_year", "alley_width", "id_article", "id_obs", "treat_id", "exp_id")] <- 0

# Step 3: Update the method vector to specify imputation only for target columns
# Use 'pmm' (predictive mean matching) for numeric columns to be imputed and "" for others
method <- c(
  "silvo_se" = "pmm",
  "control_se" = "pmm",
  "silvo_n" = "pmm",
  "control_n" = "pmm",
  "tree_age" = "",           # Not imputed
  "crop_type" = "",          # Not imputed
  "tree_type" = "",          # Not imputed
  "sub_region" = "",         # Not imputed
  "experiment_year" = "",    # Not imputed
  "alley_width" = "",        # Not imputed
  "id_article" = "",         # Not imputed
  "id_obs" = "",             # Not imputed
  "treat_id" = "",           # Not imputed
  "exp_id" = ""              # Not imputed
)

# Step 4: Perform imputation using mice
set.seed(1234)
imputed_data <- mice(
  col_for_impute,
  m = 20,
  maxit = 100,
  method = method,
  predictorMatrix = pred_matrix,
  seed = 1234,
  printFlag = FALSE
)

# Step 5: Extract a completed dataset for inspection
completed_data <- mice::complete(imputed_data, 1)
print(head(completed_data))
# Step 5: Extract the completed data
completed_data <- mice::complete(imputed_data)

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (16/11-24)
# Time difference of 14.25402 secs
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Start time tracking
start.time <- Sys.time()

##########################################################################
# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, silvo_n, control_n,
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, sub_region, experiment_year, alley_width,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    sub_region = as.factor(sub_region),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

##########################################################################
# Step 2: Define the function for each imputation method
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    # Predictive Mean Matching
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "sub_region", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0
    
    # Define imputation method for PMM
    method <- c(
      "silvo_se" = "pmm",
      "control_se" = "pmm",
      "silvo_n" = "pmm",
      "control_n" = "pmm",
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "sub_region" = "",         # Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",         # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    # Perform imputation using mice
    imputed_data <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(mice::complete(imputed_data))
    
  } else if (method_name == "upper_quartile") {
    # Upper Quartile Imputation for Variance
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
    
    # Impute missing variance with the upper quartile
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    # Example: Mean Imputation
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)
  } else {
    stop("Invalid method name.")
  }
}

##########################################################################
# Step 3: Apply each imputation method
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation")
imputed_datasets <- list()

for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
}

##########################################################################
# Step 4: Compare results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################

```

```{r}
# Step 1: Extract observed values for 'silvo_se'
# - `col_for_impute` is the data frame containing the columns to be imputed.
# - We filter out the non-missing values from the original 'silvo_se' column.
observed_silvo_se <- col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)]

# Step 2: Extract imputed values for 'silvo_se'
# - We use `lapply()` to loop over all 20 imputed datasets generated by `mice`.
# - `mice::complete(imputed_data, i)` extracts the completed dataset for the i-th imputation.
# - `data$silvo_se[is.na(col_for_impute$silvo_se)]` selects the imputed values where the original data had missing values.
# - `unlist()` is used to flatten the list of imputed values into a vector.
imputed_silvo_se <- unlist(lapply(1:20, function(i) {
  data <- mice::complete(imputed_data, i) # Extract the i-th imputed dataset
  data$silvo_se[is.na(col_for_impute$silvo_se)] # Select only the imputed values
}))

# Step 3: Create a combined data frame for plotting
# - `value`: A combined vector of both observed and imputed values.
# - `type`: A vector indicating whether the value is "Original" (observed) or "Imputed".
# - `rep()`: Repeats the labels for the respective lengths of observed and imputed values.
plot_data <- data.frame(
  value = c(observed_silvo_se, imputed_silvo_se),
  type = c(rep("Original", length(observed_silvo_se)),
           rep("Imputed", length(imputed_silvo_se)))
)

# Step 4: Plot the density of observed vs. imputed values using ggplot2
ggplot(plot_data, aes(x = value, fill = type)) +
  # `geom_density()`: Plots the density curve for each type ("Original" and "Imputed").
  # `alpha = 0.5`: Sets the transparency of the density curves (0 = fully transparent, 1 = fully opaque).
  geom_density(alpha = 0.5) +
  # `labs()`: Adds titles and labels to the plot.
  labs(
    title = "Density Plot of Original vs. Imputed Values for silvo_se",
    x = "silvo_se Values",
    y = "Density"
  ) +
  # `scale_fill_manual()`: Manually sets the colors for the fill based on the "type" variable.
  # - "blue" for the "Original" values and "red" for the "Imputed" values.
  scale_fill_manual(values = c("blue", "red")) +
  # `scale_x_log10()`: Applies a log10 transformation to the x-axis (silvo_se values).
  # - This transformation helps visualize the data if there is a large range or skewness.
  scale_x_log10() +
  # vertical lines indicating the mean or median of each group
  geom_vline(aes(xintercept = mean(observed_silvo_se)), color = "blue", linetype = "dashed") +
  geom_vline(aes(xintercept = mean(imputed_silvo_se)), color = "red", linetype = "dashed") +
  # `theme_minimal()`: Uses a minimal theme for a clean look.
  theme_minimal() +
  # `theme()`: Customizes the appearance of the plot.
  theme(
    legend.title = element_text(size = 10), # Sets the font size for the legend title
    legend.position = "top" # Places the legend at the top of the plot
  )
```

```{r}
# Step 1: Initialize a list to store summaries of all imputations
imputed_summaries <- list()

# Loop through all 20 imputed datasets in the mids object
for (i in 1:20) {
  data <- mice::complete(imputed_mids_pmm, i) # Extract the i-th imputed dataset
  
  # Calculate summary statistics for each imputation
  summary <- data %>%
    summarise(
      mean_silvo_se = mean(silvo_se, na.rm = TRUE),
      sd_silvo_se = sd(silvo_se, na.rm = TRUE),
      mean_control_se = mean(control_se, na.rm = TRUE),
      sd_control_se = sd(control_se, na.rm = TRUE)
    )
  
  # Store the summary in the list
  imputed_summaries[[i]] <- summary
}

# Step 2: Combine summaries into a single data frame
imputed_summaries_df <- bind_rows(imputed_summaries, .id = "imputation")

# Step 3: Calculate the median values for both silvo_se and control_se
median_silvo_se <- median(imputed_summaries_df$mean_silvo_se)
median_control_se <- median(imputed_summaries_df$mean_control_se)

# Add a column calculating the Euclidean distance from the median for both silvo_se and control_se
imputed_summaries_df <- imputed_summaries_df %>%
  mutate(
    distance_from_median = sqrt(
      (mean_silvo_se - median_silvo_se)^2 + (mean_control_se - median_control_se)^2
    )
  )

# Step 4: Choose the imputation with the smallest distance
chosen_imputation <- imputed_summaries_df %>%
  slice(which.min(distance_from_median))

# Print the chosen imputation
cat("Chosen imputation based on combined proximity to medians of silvo_se and control_se:\n")
print(chosen_imputation)

# Step 5: Extract the complete dataset corresponding to the chosen imputation
chosen_imputation_number <- as.integer(chosen_imputation$imputation)
imputed_col_data <- mice::complete(imputed_mids_pmm, chosen_imputation_number)

# Check the structure of the chosen imputation dataset
cat("\nStructure of the chosen imputed dataset:\n")
glimpse(imputed_col_data)

# Step 6: Add the chosen imputation to the imputed_datasets list
imputed_datasets$pmm_best <- imputed_col_data
```




```{r}
# Update the original data with imputed values
# Step 1: Join the imputed values back to the original dataset using identifiers
imp_dataset <- database_clean_sd %>%
  left_join(
    completed_data %>%
      select(id_article, id_obs, silvo_se, control_se),
    by = c("id_article", "id_obs"),
    suffix = c("_original", "_imputed")
  )|> 
  as.data.frame() |> 
  select(-geometry)

# Step 2: Replace missing values in the original columns with imputed values
imp_dataset <- imp_dataset %>%
  mutate(
    silvo_se = ifelse(is.na(silvo_se_original), silvo_se_imputed, silvo_se_original),
    control_se = ifelse(is.na(control_se_original), control_se_imputed, control_se_original)
  ) %>%
  select(-silvo_se_original, -silvo_se_imputed, -control_se_original, -control_se_imputed)

imp_dataset
```


Visualising the distribution of imputed values for silvo_se and control_se together with the original data for the chosen imputation

```{r}
# Prepare the original data
original_data_x <- database_clean_sd %>%
  select(id_article, id_obs, response_variable, silvo_se, control_se) |> 
  mutate(data_source = "Original") |> 
  as.data.frame() 

imputed_data_y <- imp_dataset |> 
  select(id_article, id_obs, response_variable, silvo_se, control_se) |> 
  mutate(data_source = "Imputed") 

# Combine the original and imputed data
combined_data <- bind_rows(original_data_x, imputed_data_y)

combined_data
```


```{r}
# Join the original and imputed data to directly compare
comparison_data <- original_data_x %>%
  full_join(imputed_data_y, by = c("id_article", "response_variable"), suffix = c("_original", "_imputed")) %>%
  distinct()
# Advarsel: Detected an unexpected many-to-many relationship between `x` and `y`

# Identify rows where imputation occurred by checking if originally missing values are filled
imputation_evaluation <- comparison_data %>%
  filter(
    (is.na(silvo_se_original) & !is.na(silvo_se_imputed)) |
      (is.na(control_se_original) & !is.na(control_se_imputed))
  ) %>%
  select(id_article, response_variable) %>%
  distinct()

# Count the number of unique articles where imputation occurred
n_imputed_studies <- imputation_evaluation %>%
  distinct(id_article) %>%
  nrow()

# Output the results
imputation_evaluation
n_imputed_studies
```
```{r}
imputation_summary <- comparison_data %>%
  summarise(
    total_missing = sum(is.na(silvo_se_original) & !is.na(silvo_se_imputed)),
    total_imputed = sum(!is.na(silvo_se_imputed)),
    proportion_imputed = total_missing / total_imputed
  )

imputation_summary
```
```{r}
# Q-Q plot for silvo_se
qqplot_silvo_se <- ggplot(combined_data, aes(sample = silvo_se)) +
  stat_qq(aes(color = data_source)) +
  stat_qq_line(aes(color = data_source)) +
  ggtitle("Q-Q Plot of Original vs. Imputed silvo_se") +
  theme_minimal()

qqplot_silvo_se
```


```{r}
# Create density plots for silvo_se with log transformation
silvo_se_impute_original_plot <- combined_data |> 
  ggplot(aes(x = silvo_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of silvo_se (Log-Transformed)") +
  theme_minimal()

# Create density plots for control_se with log transformation
control_se_impute_original_plot <- combined_data |> 
  ggplot(aes(x = control_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of control_se (Log-Transformed)") +
  theme_minimal()

library(patchwork)

silvo_se_impute_original_plot + control_se_impute_original_plot
```



















```{r}
# Q-Q plot for silvo_se
qqplot_silvo_se <- ggplot(combined_data, aes(sample = silvo_se)) +
  stat_qq(aes(color = data_source)) +
  stat_qq_line(aes(color = data_source)) +
  ggtitle("Q-Q Plot of Original vs. Imputed silvo_se") +
  theme_minimal()

qqplot_silvo_se
```


```{r}
# Create density plots for silvo_se with log transformation
silvo_se_impute_original_plot <- combined_data |> 
  ggplot(aes(x = silvo_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of silvo_se (Log-Transformed)") +
  theme_minimal()

# Create density plots for control_se with log transformation
control_se_impute_original_plot <- combined_data |> 
  ggplot(aes(x = control_se, color = data_source)) +
  geom_density(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Density Distribution of control_se (Log-Transformed)") +
  theme_minimal()

library(patchwork)

silvo_se_impute_original_plot + control_se_impute_original_plot
```











Comparing and evaluating the two meta-data sets (imputed vs. non-imputed)

```{r}
# Create a combined dataset for comparison
comparison_data <- non_imp_data_rom %>%
  select(id_article, id_obs, response_variable, yi_non_imp = yi, vi_non_imp = vi) %>%
  left_join(imp_data_rom %>%
              select(id_article, id_obs, yi_imp = yi, vi_imp = vi),
            by = c("id_article", "id_obs"))

# Scatter plot of effect sizes
scatter_plot <- ggplot(comparison_data, aes(x = yi_non_imp, y = yi_imp)) +
  geom_point(alpha = 0.6, color = "#0072B2") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Comparison of Effect Sizes (ROM): Imputed vs. Non-Imputed",
       x = "Effect Size (Non-Imputed Data)", y = "Effect Size (Imputed Data)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

scatter_plot
```

Paired t-test for Effect Sizes (yi)
A paired t-test can help assess if there is a significant difference between the effect sizes of the imputed and non-imputed datasets.

```{r}
clean_data <- comparison_data %>%
  filter(!is.na(yi_non_imp), !is.na(yi_imp), 
         !is.infinite(yi_non_imp), !is.infinite(yi_imp)) %>%
  mutate(diff = yi_imp - yi_non_imp)

summary(clean_data$diff)
```

Bland-Altman Analysis
A Bland-Altman plot can provide a graphical method to assess agreement between the two datasets by plotting the differences against the means.

```{r}
# Calculate the mean and difference of effect sizes
comparison_data <- comparison_data %>%
  mutate(
    mean_yi = (yi_non_imp + yi_imp) / 2,
    diff_yi = yi_imp - yi_non_imp
  )

# Create a Bland-Altman plot
bland_altman_plot <- ggplot(comparison_data, aes(x = mean_yi, y = diff_yi)) +
  geom_point(alpha = 0.6, color = "#0072B2") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Bland-Altman Plot: Imputed vs. Non-Imputed Effect Sizes",
       x = "Mean Effect Size", y = "Difference in Effect Size (Imputed - Non-Imputed)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

print(bland_altman_plot)

```

Correlation Analysis
Calculate the correlation between the effect sizes from the imputed and non-imputed datasets.
```{r}
# Calculate Pearson and Spearman correlations
# Calculate Pearson and Spearman correlations
pearson_corr <- cor(comparison_data$yi_non_imp, comparison_data$yi_imp, method = "pearson")
spearman_corr <- cor(comparison_data$yi_non_imp, comparison_data$yi_imp, method = "spearman")

# Print the correlation results
cat("Pearson Correlation:", pearson_corr, "\n")
cat("Spearman Correlation:", spearman_corr, "\n")

```

Pearson correlation measures the linear relationship between the two sets of effect sizes. A value close to 1 indicates strong linear agreement.
Spearman correlation measures the rank correlation, providing a non-parametric measure of the relationship. This is useful if the data has outliers or is not normally distributed.


Mean Absolute Difference (MAD) and Root Mean Square Error (RMSE)
These metrics provide an indication of the overall difference between the two sets of effect sizes.

```{r}
# Calculate Mean Absolute Difference (MAD)
mad <- mean(abs(comparison_data$yi_imp - comparison_data$yi_non_imp), na.rm = TRUE)

# Calculate Root Mean Square Error (RMSE)
rmse <- sqrt(mean((comparison_data$yi_imp - comparison_data$yi_non_imp)^2, na.rm = TRUE))

# Print the results
cat("Mean Absolute Difference (MAD):", mad, "\n")
cat("Root Mean Square Error (RMSE):", rmse, "\n")

```























##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

Creating a variance-covariance matrix is crucial in multivariate meta-analysis because it captures the dependencies among the effect sizes from different outcomes measured within the same study. Without accounting for these dependencies, the analysis could be biased and less efficient.

Why a Variance-Covariance Matrix is Needed
- Account for Within-Study Correlations: When multiple outcomes are reported within the same study, they are often correlated. Ignoring these correlations can lead to inaccurate estimates of the overall effect size and its variance.
- Borrowing Strength: The variance-covariance matrix allows the analysis to borrow strength across different outcomes, leading to more precise estimates.
- Improve Model Accuracy: Including the correct variance-covariance structure improves the accuracy of the random-effects model, leading to better inference.

```{r}
# Function to calculate the variance-covariance matrix for a given dataset
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  # Initialize an empty list to store the variance-covariance matrices for each study
  v_list <- list()
  
  # Loop through each unique study ID
  for (study in unique(data$id_article)) {
    # Subset the data for the current study
    study_data <- data[data$id_article == study, ]
    
    # Check if the study has more than one outcome
    if (nrow(study_data) > 1) {
      # Create a diagonal matrix of variances
      v <- diag(study_data$vi)
      
      # Set the off-diagonal elements assuming a constant correlation
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      
      # Store the matrix in the list
      v_list[[as.character(study)]] <- v
    } else {
      # For single outcome studies, use the variance directly
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }
  
  # Combine all matrices into a block-diagonal matrix
  V_matrix <- bldiag(v_list)
  cat("Variance-Covariance Matrix Calculation Complete.\n")
  
  return(V_matrix)
}
```



##########################################################################################################################################
MODEL FITTING
##########################################################################################################################################

Fitting the multivariate random-effects model on both moderator-imputed and non-moderator-imputed datasets

Applying model fitting process on both datasets (non_imp_dataset and imp_dataset)

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Updated function to fit the multivariate random-effects model using V_matrix
fit_meta_model <- function(data, dataset_name, V_matrix) {
  cat("\nStarting model fitting for", dataset_name, "...\n")
  
  # Define the formula for moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture", "no_tree_per_m", "tree_height", "alley_width")
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  # Prepare the data
  data <- data %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Fit the multivariate random-effects model
  model <- tryCatch({
    rma.mv(
      yi = yi,                             # Fixed effect
      V = V_matrix,
      mods = moderator_formula,
      random = list(                       # Random effects $ ssh -T git@github.com
        ~ 1 | id_article,
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data,
      method = "ML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", dataset_name, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Model fitting completed for", dataset_name, ".\n")
    
    # Extract model statistics
    aic <- AIC(model)
    bic <- BIC(model)
    logLik_val <- logLik(model)
    
    # Calculate I² (heterogeneity)
    tau2 <- sum(model$sigma2)
    sigma2 <- mean(data$vi)
    I2 <- (tau2 / (tau2 + sigma2)) * 100
    
    # Create a summary list
    model_summary <- list(
      model = model,
      aic = aic,
      bic = bic,
      logLik = logLik_val,
      I2 = I2
    )
    
    return(model_summary)
  } else {
    return(NULL)
  }
}


# List of datasets and names
datasets <- list(
  non_imp_dataset = non_imp_dataset,
  imp_dataset = imp_dataset
)

# Calculate the variance-covariance matrix for each dataset
V_matrices <- lapply(names(datasets), function(dataset_name) {
  calculate_v_matrix(datasets[[dataset_name]], correlation = 0.5)
})
names(V_matrices) <- names(datasets)

# Fit the model on all datasets using the calculated V_matrices
model_results <- lapply(names(datasets), function(dataset_name) {
  fit_meta_model(datasets[[dataset_name]], dataset_name, V_matrices[[dataset_name]])
})
names(model_results) <- names(datasets)

# Function to extract and compile model statistics
extract_model_summary <- function(model_summary, dataset_name) {
  if (is.null(model_summary)) {
    return(data.frame(
      Dataset = dataset_name,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      I2 = NA
    ))
  }
  
  data.frame(
    Dataset = dataset_name,
    AIC = model_summary$aic,
    BIC = model_summary$bic,
    LogLikelihood = model_summary$logLik,
    I2 = model_summary$I2
  )
}

# Compile the model summaries into a single data frame
model_summaries <- bind_rows(
  extract_model_summary(model_results$non_imp_dataset, "Non-Imputed Dataset"),
  extract_model_summary(model_results$imp_dataset, "Imputed Dataset")
)

# Print the combined summary table
model_summaries

# Save the summary table
# write.csv(model_summaries, file = "model_summaries.csv")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (17/11-24)
# Time difference of 3.211481 mins

# str(model_results)
```










# Function to fit meta-analytic model for a specific subgroup
fit_subgroup_model <- function(data, subgroup_name, V_matrix, moderators = NULL) {
  cat("\nFitting model for subgroup:", subgroup_name, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model if no moderators
  }
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = V_matrix,
      mods = moderator_formula,
      random = list(~ 1 | id_article, ~ 1 | id_article/response_variable, ~ 1 | exp_id),
      data = data,
      method = "ML",
      control = list(optimizer = "optim", optim.method = "BFGS", iter.max = 1000, rel.tol = 1e-8)
    )
  }, error = function(e) {
    cat("Error fitting model for subgroup:", subgroup_name, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Model fitting completed for subgroup:", subgroup_name, ".\n")
    # Extract statistics
    return(list(
      subgroup = subgroup_name,
      aic = AIC(model),
      bic = BIC(model),
      logLik = logLik(model),
      tau2 = sum(model$sigma2),  # Variance components
      model = model
    ))
  } else {
    return(NULL)
  }
}

# Function to run subgroup analyses
run_subgroup_analysis <- function(data, V_matrix, split_var, moderators = NULL) {
  # Split data into subgroups
  subgroups <- split(data, data[[split_var]])
  
  # Fit a model for each subgroup
  results <- map(names(subgroups), ~ {
    fit_subgroup_model(subgroups[[.x]], .x, V_matrix, moderators)
  })
  
  # Compile results
  results <- results[!sapply(results, is.null)]  # Remove failed models
  summary_df <- bind_rows(lapply(results, function(res) {
    data.frame(
      Subgroup = res$subgroup,
      AIC = res$aic,
      BIC = res$bic,
      LogLikelihood = as.numeric(res$logLik),
      Tau2 = res$tau2
    )
  }))
  
  list(results = results, summary = summary_df)
}

# Define dataset and variables
meta_dataset <- imp_data_rom  # Example dataset
response_variable <- "response_variable"  # Column to split by
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")  # Example moderators

# Generate the variance-covariance matrix (replace with your function)
V_matrix <- calculate_v_matrix(meta_dataset, correlation = 0.5)

# Run subgroup analysis
subgroup_analysis_results <- run_subgroup_analysis(meta_dataset, V_matrix, response_variable, moderators)

# Display subgroup summary
print(subgroup_analysis_results$summary)

# Save results for each subgroup
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

lapply(subgroup_analysis_results$results, function(res) {
  if (!is.null(res)) {
    saveRDS(res$model, file = file.path(output_dir, paste0("subgroup_", res$subgroup, "_model.rds")))
  }
})


```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- imp_data_rom

# Subgroup levels (e.g., unique response variables)
subgroups <- unique(data$response_variable)

# Fit Overall Random-Effects Model
overall_model <- rma(yi, vi, data = data)

# Helper function for Q-test, I², and τ² information
mlabfun <- function(text, x) {
  list(bquote(paste(
    .(text), " (Q = ", .(formatC(x$QE, digits = 2, format = "f")),
    ", df = ", .(x$k - x$p), ", ", .(format.pval(x$QEp, digits = 2)), "; ",
    I^2, " = ", .(formatC(x$I2, digits = 1, format = "f")), "%, ",
    tau^2, " = ", .(formatC(x$tau2, digits = 2, format = "f")), ")"
  )))
}

# Prepare the supplementary information
ilab_data <- cbind(
  "Response Variable" = data$response_variable,
  "Metric" = data$measured_metrics,
  "Silvo N" = data$silvo_n,
  "Control N" = data$control_n
)

# Forest plot with `ilab`
forest(
  overall_model,
  xlim = c(-4, 2),
  at = log(c(0.1, 0.5, 1, 2)), 
  atransf = exp,
  ilab = ilab_data,  # Add the supplementary data
  ilab.xpos = c(-6, -4, -2, -1),  # Adjust column positions
  cex = 0.8,
  ylim = c(-2, 5 + length(data$yi)), 
  top = 2,
  mlab = mlabfun("Random-Effects Model for All Data", overall_model),
  header = c("Subgroup and Study", "Effect Size [95% CI]")
)

# Add labels for ilab columns
text(c(-6, -4, -2, -1), max(data$yi) + 2, 
     c("Response Variable", "Metric", "Silvo N", "Control N"), pos = 4, font = 2)



# Subgroup Row Positions
row_positions <- cumsum(sapply(subgroups, function(sg) sum(data$response_variable == sg)))
row_positions <- c(1, row_positions + 1)  # Adjust for spacing

# Add Subgroup Models and Summary Polygons
for (i in seq_along(subgroups)) {
  subgroup <- subgroups[i]
  
  # Fit random-effects model for the subgroup
  subgroup_model <- rma(
    yi, vi, data = data, subset = (response_variable == subgroup)
  )
  
  # Add summary polygon for the subgroup
  addpoly(
    subgroup_model, row = row_positions[i], 
    mlab = mlabfun(paste("Random-Effects Model for", subgroup), subgroup_model)
  )
  
  # Add text for subgroup
  text(-4, row_positions[i] + 1, pos = 4, subgroup, font = 4)
}

# Test for Subgroup Differences (Meta-Regression Model)
subgroup_test <- rma(yi, vi, mods = ~ response_variable, data = data)

# Add Test for Subgroup Differences
text(-4, -1.5, pos = 4, cex = 0.75, bquote(paste(
  "Test for Subgroup Differences: ",
  Q[M], " = ", .(formatC(subgroup_test$QM, digits = 2, format = "f")),
  ", df = ", .(subgroup_test$p - 1), ", ", .(format.pval(subgroup_test$QMp, digits = 2))
)))

```

```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- imp_data_rom

# Select only specific subgroups
selected_subgroups <- c("Biodiversity", "Crop yield", "Soil quality")
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)

# Subgroup levels (filtered)
subgroups <- unique(filtered_data$response_variable)

# Fit Overall Random-Effects Model for the filtered dataset
overall_model <- rma(yi, vi, data = filtered_data)

# Helper function for Q-test, I², and τ² information
mlabfun <- function(text, x) {
  list(bquote(paste(
    .(text), " (Q = ", .(formatC(x$QE, digits = 2, format = "f")),
    ", df = ", .(x$k - x$p), ", ", .(format.pval(x$QEp, digits = 2)), "; ",
    I^2, " = ", .(formatC(x$I2, digits = 1, format = "f")), "%, ",
    tau^2, " = ", .(formatC(x$tau2, digits = 2, format = "f")), ")"
  )))
}

# Prepare supplementary data for ilab
ilab_data <- cbind(
  "Silvo N" = filtered_data$silvo_n,
  "Control N" = filtered_data$control_n
)

# Forest Plot for the Filtered Data
forest(
  overall_model,
  xlim = c(-4, 2),
  at = log(c(0.1, 0.5, 1, 2)), 
  atransf = exp,
  ilab = ilab_data,  # Add the supplementary data
  ilab.xpos = c(-2, -1),  # Adjust column positions for supplementary info
  cex = 0.8,
  ylim = c(-2, 3 + length(subgroups) * 2),  # Adjust ylim for fewer elements
  top = 2,
  mlab = mlabfun("Random-Effects Model for Selected Subgroups", overall_model),
  header = c("Subgroup and Study", "Effect Size [95% CI]")
)

# Add labels for ilab columns
text(c(-2, -1), max(filtered_data$yi) + 2, 
     c("Silvo N", "Control N"), pos = 4, font = 2)

# Row positions for subgroup summary polygons
row_positions <- seq(1, by = 2, length.out = length(subgroups))

# Add Subgroup Summary Polygons
for (i in seq_along(subgroups)) {
  subgroup <- subgroups[i]
  
  # Fit random-effects model for each subgroup
  subgroup_model <- rma(
    yi, vi, data = filtered_data, subset = (response_variable == subgroup)
  )
  
  # Add summary polygon for the subgroup
  addpoly(
    subgroup_model, row = row_positions[i], 
    mlab = mlabfun(paste("Random-Effects Model for", subgroup), subgroup_model)
  )
  
  # Add subgroup label
  text(-4, row_positions[i] + 1, pos = 4, subgroup, font = 4)
}

# Test for Subgroup Differences (Meta-Regression Model)
subgroup_test <- rma(yi, vi, mods = ~ response_variable, data = filtered_data)

# Add Test for Subgroup Differences
text(-4, -1.5, pos = 4, cex = 0.75, bquote(paste(
  "Test for Subgroup Differences: ",
  Q[M], " = ", .(formatC(subgroup_test$QM, digits = 2, format = "f")),
  ", df = ", .(subgroup_test$p - 1), ", ", .(format.pval(subgroup_test$QMp, digits = 2))
)))

```
```{r}
# Filter the dataset to include only selected subgroups
selected_subgroups <- c("Biodiversity", "Crop yield", "Soil quality", "Greenhouse gas emission")
filtered_data <- imp_data_rom %>% filter(response_variable %in% selected_subgroups)

# Unique subgroup levels
subgroups <- unique(filtered_data$response_variable)

# Fit the overall random-effects model
overall_model <- rma(yi, vi, data = filtered_data)

# Helper function for Q-test, I², and τ²
mlabfun <- function(text, x) {
  list(bquote(paste(
    .(text), " (Q = ", .(formatC(x$QE, digits = 2, format = "f")),
    ", df = ", .(x$k - x$p), ", ", .(format.pval(x$QEp, digits = 2)), "; ",
    I^2, " = ", .(formatC(x$I2, digits = 1, format = "f")), "%, ",
    tau^2, " = ", .(formatC(x$tau2, digits = 2, format = "f")), ")"
  )))
}

# Prepare supplementary data for the ilab columns
ilab_data <- cbind(
  "Silvo N" = filtered_data$silvo_n,
  "Control N" = filtered_data$control_n
)

# Set plot limits based on data
plot_rows <- length(filtered_data$yi) + length(subgroups) * 2  # Add space for subgroups

# Create the forest plot
forest(
  overall_model,
  xlim = c(-4, 2),                           # Horizontal axis limits
  at = log(c(0.1, 0.5, 1, 2)),               # Tick marks for log scale
  atransf = exp,                             # Back-transform log values
  ilab = ilab_data,                          # Add supplementary info
  ilab.xpos = c(-2, -1),                     # Position of ilab columns
  ylim = c(-2, plot_rows),                   # Vertical axis limits
  cex = 0.8,                                 # Font size for main plot
  top = 2,                                   # Extra space at the top
  mlab = mlabfun("Random-Effects Model for Selected Subgroups", overall_model),
  header = c("Subgroup and Study", "Effect Size [95% CI]")
)

# Add labels for ilab columns
text(c(-2, -1), plot_rows - 1, c("Silvo N", "Control N"), pos = 4, font = 2)

# Adjust row positions for subgroup summaries
row_positions <- seq(1, by = 3, length.out = length(subgroups))

# Add subgroup summaries and labels
for (i in seq_along(subgroups)) {
  subgroup <- subgroups[i]
  
  # Fit random-effects model for the subgroup
  subgroup_model <- rma(yi, vi, data = filtered_data, subset = (response_variable == subgroup))
  
  # Add summary polygon for the subgroup
  addpoly(
    subgroup_model,
    row = row_positions[i], 
    mlab = mlabfun(paste("Random-Effects Model for", subgroup), subgroup_model),
    cex = 0.8
  )
  
  # Add subgroup label
  text(-4, row_positions[i] + 1, pos = 4, subgroup, font = 4, cex = 0.9)
}

# Add test for subgroup differences
subgroup_test <- rma(yi, vi, mods = ~ response_variable, data = filtered_data)
text(-4, -1.5, pos = 4, cex = 0.8, bquote(paste(
  "Test for Subgroup Differences: ",
  Q[M], " = ", .(formatC(subgroup_test$QM, digits = 2, format = "f")),
  ", df = ", .(subgroup_test$p - 1), ", ", .(format.pval(subgroup_test$QMp, digits = 2))
)))

```

```{r}
# Filter data to include only subgroups with sufficient studies
min_studies <- 10
subgroup_counts <- table(imp_data_rom$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- imp_data_rom %>%
  filter(response_variable %in% selected_subgroups)

# Fit random-effects models for each subgroup and calculate summary statistics
subgroup_results <- filtered_data %>%
  group_by(response_variable) %>%
  summarise(
    model = list(rma(yi, vi, data = pick(everything()))),  # Use `pick()` instead of `cur_data()`
    .groups = "drop"
  ) %>%
  mutate(
    estimate = map_dbl(model, ~ coef(.x)["intrcpt"]),
    lower_ci = map_dbl(model, ~ confint(.x)$random[1, 1]),
    upper_ci = map_dbl(model, ~ confint(.x)$random[1, 2]),
    I2 = map_dbl(model, ~ .x$I2)
  )

# Add summary data back to the main dataset for plotting
filtered_data <- filtered_data %>%
  left_join(subgroup_results, by = "response_variable")

# Generate the forest plot using ggplot2
forest_plot <- ggplot() +
  # Add forest plot points and confidence intervals
  geom_pointrange(
    data = subgroup_results,
    aes(
      x = estimate, ymin = lower_ci, ymax = upper_ci,
      y = response_variable, color = response_variable
    ),
    size = 0.8
  ) +
  # Add ridge density plot for effect sizes
  geom_density_ridges(
    data = filtered_data,
    aes(x = yi, y = response_variable, fill = response_variable),
    alpha = 0.5, scale = 1.5, rel_min_height = 0.01
  ) +
  # Add vertical reference line at x = 1
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  # Custom colors for subgroups
  scale_color_viridis_d(option = "D", name = "Subgroups") +
  scale_fill_viridis_d(option = "D", guide = "none") +
  # Customize labels and theme
  labs(
    title = "Forest Plot with Subgroups and Ridge Density",
    x = "Effect Size (Log Scale)", y = "Subgroups",
    caption = "Random-effects models fitted for each subgroup"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10)
  )

# Show the Forest Plot
forest_plot

```


```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)

# Initialize a list to store subgroup summaries
subgroup_summaries <- list()

# Fit random-effects model for each subgroup
for (subgroup in selected_subgroups) {
  model <- tryCatch(
    rma(yi, vi, data = filtered_data, subset = (response_variable == subgroup)), 
    error = function(e) NULL
  )
  
  if (!is.null(model)) {
    summary <- list(
      response_variable = subgroup,
      mean_effect = coef(model)["intrcpt"],
      conf_low = confint(model)$random["ci.lb"],
      conf_high = confint(model)$random["ci.ub"],
      I2 = model$I2,
      tau2 = model$tau2,
      n_studies = model$k
    )
  } else {
    summary <- list(
      response_variable = subgroup,
      mean_effect = NA,
      conf_low = NA,
      conf_high = NA,
      I2 = NA,
      tau2 = NA,
      n_studies = NA
    )
  }
  
  subgroup_summaries[[subgroup]] <- summary
}

# Combine summaries into a dataframe
summary_df <- do.call(rbind, lapply(subgroup_summaries, as.data.frame))

# Inspect the summary dataframe
print(summary_df)

# Plot the overall effect sizes for each subgroup
forest_plot <- ggplot(summary_df, aes(x = mean_effect, y = response_variable)) +
  geom_point(size = 4, color = "blue") +
  geom_errorbarh(aes(xmin = conf_low, xmax = conf_high), height = 0.2, color = "blue") +
  labs(
    title = "Subgroup Overall Effects",
    x = "Mean Effect Size (Log Scale)",
    y = "Subgroups"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 12)
  )

# Display the plot
print(forest_plot)

```






















########################################################################################################################################################################################################################################################
########################################################################################################################################################################################################################################################
####################################################################################################################################################################################################################################################################################################################################################################################
MODEL FITTING FROM SCRATCH


Points to address after the Tshering meeting with her supervisor (26/11-2024)


########################################################################################################################################################################################################################################################
########################################################################################################################################################################################################################################################
########################################################################################################################################################################################################################################################


### Workflow Modifications and Notes Post-Meeting (26/11/2024)

#### Subgroup Analysis vs. Meta-Regression
**Key Decisions and Rationale:**
  - **Subgroup Analysis**: 
  - Keep separate subgroup analyses for each `response_variable` (e.g., biodiversity, crop yield) to respect conceptual differences.
- Allows us to focus on unique trends and moderators relevant to each outcome.
- Plan to include subgroup-specific moderators like `tree_type` and `crop_type` to explore targeted relationships.
- Acknowledge the limitations: potential for reduced power in smaller subgroups and the need to adjust for multiple comparisons (e.g., Bonferroni).

- **Meta-Regression**:
  - Use meta-regression as a complementary step, not a replacement, to capture cross-cutting trends.
- Include `response_variable` as a moderator to leverage the entire dataset, preserving power.
- Test interaction terms (e.g., `tree_type * response_variable`) to detect shared vs. outcome-specific patterns.
- Be mindful of complexity—flag variables or interactions that might make interpretation unwieldy.

#### Immediate Adjustments:
1. **Subgroup Models**:
  - Ensure models are well-documented to highlight their scope (e.g., biodiversity-specific vs. crop-yield-specific).
- Add notes to ensure consistent handling of moderators within subgroups (e.g., same scaling, inclusion criteria).
- Compare subgroup heterogeneity indices (`Q`, `I²`, `τ²`) to identify differences in variability across outcomes.

2. **Meta-Regression**:
  - Begin with a simpler meta-regression using `response_variable` as the sole moderator.
- Gradually add key moderators (e.g., `tree_type`, `crop_type`) and their interactions with `response_variable`.
- Consider whether subgroup findings are confirmed or contradicted by meta-regression results.
- Prepare visualizations that overlay subgroup-specific results with meta-regression trends.

3. **Heterogeneity Focus**:
  - Compare heterogeneity indices (I², τ²) between original and imputed datasets.
- Report changes in heterogeneity metrics after imputation to understand its influence on variability.

4. **Visualizations**:
  - Update forest plots to simplify presentation, particularly for subgroup models (e.g., one plot per response variable).
- Add funnel plots for individual response variable models and meta-regression.
- Use ggplot for standardized aesthetics, log scales (if needed), and cleaner annotations.

#### To-Do List for Next Steps:
- [ ] Refactor subgroup analysis workflow to make models more modular and reusable across response variables.
- [ ] Draft comparison table of heterogeneity indices across subgroups and the meta-regression.
- [ ] Pilot a meta-regression with key moderators and test its alignment with subgroup findings.
- [ ] Investigate trends in imputed vs. original data heterogeneity metrics.
- [ ] Review final visualizations with the team for clarity and presentation impact.

**Key Takeaway**: This combined workflow maximizes insights by leveraging the strengths of both subgroup analyses and meta-regression while mitigating their respective limitations.



```{r}

meta_data <- non_imp_dataset

base_model <- rma.mv(
  yi = yi, V = vi,
  random = ~ 1 | id_article/response_variable/exp_id, 
  data = meta_data,
  method = "REML",
  tdist = TRUE
)
summary(base_model)
```

```{r}
subgroups <- split(meta_data, meta_data$response_variable)


subgroup_models <- lapply(names(subgroups), function(subgroup) {
  subgroup_data <- subgroups[[subgroup]]
  tryCatch(
    rma.mv(
      yi = yi, V = vi,
      random = ~ 1 | id_article/exp_id,
      data = subgroup_data,
      method = "REML"
    ),
    error = function(e) {
      cat("Error for subgroup:", subgroup, "\n", e$message, "\n")
      return(NULL)
    }
  )
})

subgroups
```

```{r}
sapply(subgroup_models, function(model) {
  if (!is.null(model)) {
    list(
      Q = model$QE, 
      I2 = model$I2, 
      tau2 = model$sigma2
    )
  }
})
```

```{r}
lapply(subgroup_models, function(model) {
  if (!is.null(model)) forest(model, header = TRUE)
})
```
```{r}
funnel(base_model)
```

```{r}
leave_one_out <- function(data, model) {
  lapply(unique(data$id_article), function(article) {
    data_subset <- data[data$id_article != article, ]
    rma.mv(
      yi = yi, V = V_lnR_imputed,
      random = ~ 1 | id_article/exp_id,
      data = data_subset,
      method = "REML"
    )
  })
}
sensitivity_results <- leave_one_out(meta_data, base_model)

```

```{r}
# Summarize the data by response variable
summary_data <- meta_data %>%
  group_by(response_variable) %>%
  summarise(
    mean_yi = mean(yi, na.rm = TRUE),              # Mean effect size
    lower_ci = mean_yi - 1.96 * sqrt(mean(vi, na.rm = TRUE)), # Lower CI
    upper_ci = mean_yi + 1.96 * sqrt(mean(vi, na.rm = TRUE)), # Upper CI
    n = n()                                        # Number of studies
  ) %>%
  ungroup()


# Define custom color palette for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Create the forest plot
# Updated forest plot with log scale and vertical line
forest_plot <- ggplot(summary_data, aes(x = mean_yi, y = response_variable)) +
  geom_point(aes(color = response_variable), size = 3) +  # Effect size points
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) +  # Confidence intervals
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", size = 1) +  # Red dotted vertical line
  scale_x_continuous(
    name = "Effect Size (Log Response Ratio ± 95% CI)",
    breaks = c(-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2)# Customize x-axis breaks
  ) +
  scale_color_manual(values = custom_colors) +  # Use custom color palette
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",  # Remove legend
    axis.text.y = element_text(size = 12, hjust = 1),  # Adjust y-axis text
    axis.title.x = element_text(size = 14),  # Larger font for x-axis title
    axis.title.y = element_text(size = 14)   # Larger font for y-axis title
  ) +
  labs(
    y = "Response Variable",
    title = "Summarized Forest Plot by Response Variable (Log Scale)"
  )



# Display the plot
forest_plot
```





#############
# STEP 1
##########################################################################################################################################
CREATING A VARIANCE-COVARIANCE MATRIX
##########################################################################################################################################

Creating a variance-covariance matrix is crucial in multivariate meta-analysis because it captures the dependencies among the effect sizes from different outcomes measured within the same study. Without accounting for these dependencies, the analysis could be biased and less efficient.

Why a Variance-Covariance Matrix is Needed
- Account for Within-Study Correlations: When multiple outcomes are reported within the same study, they are often correlated. Ignoring these correlations can lead to inaccurate estimates of the overall effect size and its variance.
- Borrowing Strength: The variance-covariance matrix allows the analysis to borrow strength across different outcomes, leading to more precise estimates.
- Improve Model Accuracy: Including the correct variance-covariance structure improves the accuracy of the random-effects model, leading to better inference.

```{r}
# Function to calculate the variance-covariance matrix for a given dataset
calculate_v_matrix <- function(data, correlation = 0.5) {
  cat("\nCalculating Variance-Covariance Matrix...\n")
  
  # Initialize an empty list to store the variance-covariance matrices for each study
  v_list <- list()
  
  # Loop through each unique study ID
  for (study in unique(data$id_article)) {
    # Subset the data for the current study
    study_data <- data[data$id_article == study, ]
    
    # Check if the study has more than one outcome
    if (nrow(study_data) > 1) {
      # Create a diagonal matrix of variances
      v <- diag(study_data$vi)
      
      # Set the off-diagonal elements assuming a constant correlation
      for (i in 1:nrow(v)) {
        for (j in 1:nrow(v)) {
          if (i != j) {
            v[i, j] <- correlation * sqrt(v[i, i] * v[j, j])
          }
        }
      }
      
      # Store the matrix in the list
      v_list[[as.character(study)]] <- v
    } else {
      # For single outcome studies, use the variance directly
      v_list[[as.character(study)]] <- matrix(study_data$vi, nrow = 1, ncol = 1)
    }
  }
  
  # Combine all matrices into a block-diagonal matrix
  v_matrix <- bldiag(v_list)
  cat("Variance-Covariance Matrix Calculation Complete.\n")
  
  return(v_matrix)
}
```







#############
# STEP 2
##########################################################################################################################################
SUBGROUP META-ANALYSIS, MULTIVARIATE/MULTILEVEL LINEAR (MIXED-EFFECTS) MODELLING 
##########################################################################################################################################

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Function to fit a meta-analytic model for a given subgroup
# This function takes a subset of the data, a subgroup name, the variance-covariance matrix (v_matrix),
# and optional moderators to fit a random-effects meta-analytic model.
fit_subgroup_model <- function(data, subgroup_name, v_matrix, moderators = NULL) {
  cat("\nFitting model for subgroup:", subgroup_name, "...\n")
  
  # Create a formula for the model. If moderators are provided, they are included in the model.
  # Otherwise, it defaults to an intercept-only model.
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }
  
  # Try fitting the meta-analytic model, handling errors gracefully.
  model <- tryCatch({
    rma.mv(
      yi = yi,                               # Dependent variable (effect sizes)
      V = v_matrix,                          # Variance-covariance matrix
      mods = moderator_formula,              # Moderators (if any)
      random = list(                         # Random effects structure
        ~ 1 | id_article,                    # Random intercept for articles
        ~ 1 | id_article/response_variable,  # Nested random effect for response variables
        ~ 1 | exp_id                         # Random intercept for experiments
      ),
      data = data,                           # Data subset for this subgroup
      method = "ML",                         # Maximum Likelihood estimation
      control = list(                        # Optimization settings
        optimizer = "optim",
        optim.method = "BFGS",               # Optimization algorithm
        iter.max = 1000,                     # Maximum number of iterations
        rel.tol = 1e-8                       # Convergence tolerance
      )
    )
  }, error = function(e) {              # Error handling
    cat("Error fitting model for subgroup:", subgroup_name, ":", e$message, "\n")
    return(NULL)                        # Return NULL if the model fitting fails
  })
  
  # If the model is successfully fitted, extract key statistics and return them.
  if (!is.null(model)) {
    cat("Model fitting completed for subgroup:", subgroup_name, ".\n")
    return(list(
      subgroup = subgroup_name,         # Name of the subgroup
      aic = AIC(model),                 # Akaike Information Criterion (model fit)
      bic = BIC(model),                 # Bayesian Information Criterion (model fit)
      logLik = logLik(model),           # Log-likelihood value
      tau2 = sum(model$sigma2),         # Total variance components
      model = model                     # The fitted model object
    ))
  } else {
    return(NULL)                        # Return NULL if model fitting fails
  }
}

# Function to perform subgroup analysis across different levels of a specified variable
# This function splits the data into subgroups, fits meta-analytic models for each subgroup,
# and compiles the results into a summary table.
run_subgroup_analysis <- function(data, v_matrix, split_var, moderators = NULL) {
  # Split the data into subsets based on the levels of the specified variable (split_var).
  subgroups <- split(data, data[[split_var]])
  
  # Fit a meta-analytic model for each subgroup using map.
  results <- map(names(subgroups), ~ {
    # Extract the data for the current subgroup
    subgroup_data <- subgroups[[.x]]
    
    # Subset the variance-covariance matrix to match the rows of the current subgroup
    subgroup_indices <- which(data[[split_var]] == .x)
    v_matrix_subgroup <- v_matrix[subgroup_indices, subgroup_indices, drop = FALSE]
    
    # Fit the model for the current subgroup
    fit_subgroup_model(subgroup_data, .x, v_matrix_subgroup, moderators)
  })
  
  # Remove any NULL results (models that failed to fit).
  results <- results[!sapply(results, is.null)]
  
  # Compile the results into a summary data frame with key statistics for each subgroup.
  summary_df <- bind_rows(lapply(results, function(res) {
    data.frame(
      Subgroup = res$subgroup,          # Subgroup name
      AIC = res$aic,                    # Akaike Information Criterion
      BIC = res$bic,                    # Bayesian Information Criterion
      LogLikelihood = as.numeric(res$logLik),  # Log-likelihood
      Tau2 = res$tau2                   # Total variance components
    )
  }))
  
  # Return both the detailed results and the summary table.
  list(results = results, summary = summary_df)
}


# Define the dataset to use for meta-analysis
meta_dataset <- imp_data_rom  # Imputed dataset (replace with your actual dataset)

# Variable to split the data into subgroups
response_variable <- "response_variable"  # Column to split the data on (e.g., response type)

# Moderators to include in the model (optional)
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Calculate the variance-covariance matrix (replace with your actual function)
v_matrix <- calculate_v_matrix(meta_dataset, correlation = 0.5)

# Run the subgroup analysis
subgroup_analysis_results <- run_subgroup_analysis(meta_dataset, v_matrix, response_variable, moderators)

# Print the summary of subgroup results
print(subgroup_analysis_results$summary)

# Define output directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Save fitted models for each subgroup
lapply(subgroup_analysis_results$results, function(res) {
  if (!is.null(res)) {
    saveRDS(res$model, file = file.path(output_dir, paste0("subgroup_", res$subgroup, "_model.rds")))
  }
})

# Save the summary table
write.csv(subgroup_analysis_results$summary, file = file.path(output_dir, "subgroup_analysis_summary.csv"))


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (17/11-24)
# Time difference of 3.211481 mins

# str(model_results)
```

```{r}
# Run the subgroup analysis with the corrected V_matrix handling
subgroup_analysis_results <- run_subgroup_analysis(meta_dataset, v_matrix, response_variable, moderators)

# Print the summary of subgroup results
print(subgroup_analysis_results$summary)

# Save the summary table
write.csv(subgroup_analysis_results$summary, file = file.path(output_dir, "subgroup_analysis_summary.csv"))

```




















































```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)

# Subgroup levels (filtered)
subgroups <- unique(filtered_data$response_variable)

# Fit Overall Random-Effects Model
overall_model <- rma(yi, vi, data = filtered_data)

# Helper function for Q-test, I², and τ² information
mlabfun <- function(text, x) {
  list(bquote(paste(
    .(text), " (Q = ", .(formatC(x$QE, digits = 2, format = "f")),
    ", df = ", .(x$k - x$p), ", ", .(format.pval(x$QEp, digits = 2)), "; ",
    I^2, " = ", .(formatC(x$I2, digits = 1, format = "f")), "%, ",
    tau^2, " = ", .(formatC(x$tau2, digits = 2, format = "f")), ")"
  )))
}

# Prepare supplementary data for ilab
ilab_data <- cbind(
  "Silvo N" = filtered_data$silvo_n,
  "Control N" = filtered_data$control_n
)

# Forest Plot for the Overall Model
forest(
  overall_model,
  xlim = c(-4, 2),
  at = log(c(0.1, 0.5, 1, 2)), 
  atransf = exp,
  ilab = ilab_data,  # Add the supplementary data
  ilab.xpos = c(-2, -1),  # Adjust column positions for supplementary info
  cex = 0.8,
  ylim = c(-2, 3 + length(subgroups) * 2),  # Adjust ylim for fewer elements
  top = 2,
  mlab = mlabfun("Random-Effects Model for All Data", overall_model),
  header = c("Subgroup and Study", "Effect Size [95% CI]")
)

# Add labels for ilab columns
text(c(-2, -1), max(filtered_data$yi) + 2, 
     c("Silvo N", "Control N"), pos = 4, font = 2)

# Row positions for subgroup summary polygons
row_positions <- seq(1, by = 2, length.out = length(subgroups))

# Add Subgroup Summary Polygons
for (i in seq_along(subgroups)) {
  subgroup <- subgroups[i]
  
  # Fit random-effects model for each subgroup
  subgroup_model <- rma(
    yi, vi, data = filtered_data, subset = (response_variable == subgroup)
  )
  
  # Add summary polygon for the subgroup
  addpoly(
    subgroup_model, row = row_positions[i], 
    mlab = mlabfun(paste("Random-Effects Model for", subgroup), subgroup_model)
  )
  
  # Add subgroup label
  text(-4, row_positions[i] + 1, pos = 4, subgroup, font = 4)
}

# Test for Subgroup Differences (Meta-Regression Model)
subgroup_test <- rma(yi, vi, mods = ~ response_variable, data = filtered_data)

# Add Test for Subgroup Differences
text(-4, -1.5, pos = 4, cex = 0.75, bquote(paste(
  "Test for Subgroup Differences: ",
  Q[M], " = ", .(formatC(subgroup_test$QM, digits = 2, format = "f")),
  ", df = ", .(subgroup_test$p - 1), ", ", .(format.pval(subgroup_test$QMp, digits = 2))
)))

```

```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)


# Fit random-effects models for each subgroup and extract correct metrics
subgroup_results <- filtered_data %>%
  group_by(response_variable) %>%
  summarise(
    model = list(rma(yi, vi, data = cur_data())),
    .groups = "drop"
  ) %>%
  mutate(
    conf_low = map_dbl(model, ~ exp(confint(.x)$random[1, 1])),
    conf_high = map_dbl(model, ~ exp(confint(.x)$random[1, 2])),
    mean_effect = map_dbl(model, ~ exp(coef(.x)["intrcpt"]))
  )

# Add subgroup diagnostics to the dataset
filtered_data <- filtered_data %>%
  left_join(
    subgroup_results %>% select(response_variable, conf_low, conf_high, mean_effect),
    by = "response_variable"
  )

# Create ridge density plot with subgroup diagnostics
ridge_plot_with_metrics <- ggplot(filtered_data, aes(x = yi, y = response_variable, fill = response_variable)) +
  ggridges::geom_density_ridges(alpha = 0.8, scale = 1.2, rel_min_height = 0.01) +
  scale_fill_manual(values = c(
    "Biodiversity" = "#7B3294", "Crop yield" = "#C2A5CF",
    "Greenhouse gas emission" = "#008837", "Soil quality" = "#A6D96A",
    "Pest and Disease" = "#1B7837", "Product quality" = "#E6F5D0",
    "Water quality" = "#FFD700"
  )) +
  labs(
    title = "Forest Plot with Subgroups and Diagnostics",
    subtitle = "Random-effects models fitted for each subgroup",
    x = "Effect Size (Log Scale)", y = "Subgroups",
    fill = "Subgroups"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.title.y = element_text(angle = 90, vjust = 0.5)
  ) +
  geom_text(
    data = subgroup_results,
    aes(
      x = 3.2, y = response_variable,
      label = paste0(
        "n = ", map_int(model, ~ .x$k), "\n",
        "Mean: ", round(mean_effect, 2), "\n",
        "CI: [", round(conf_low, 2), ", ", round(conf_high, 2), "]"
      )
    ),
    hjust = 0, vjust = 0, size = 4, color = "black"
  ) +
  scale_x_continuous(limits = c(-2, 3.5))  # Adjust x-axis range for annotations

ridge_plot_with_metrics

```


```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_dataset

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)


# Fit random-effects models for each subgroup and extract correct metrics
subgroup_results <- filtered_data %>%
  group_by(response_variable) %>%
  summarise(
    model = list(rma(yi, vi, data = cur_data())),
    .groups = "drop"
  ) %>%
  mutate(
    conf_low = map_dbl(model, ~ exp(confint(.x)$random[1, 1])),
    conf_high = map_dbl(model, ~ exp(confint(.x)$random[1, 2])),
    mean_effect = map_dbl(model, ~ exp(coef(.x)["intrcpt"]))
  )

# Add subgroup diagnostics to the dataset
filtered_data <- filtered_data %>%
  left_join(
    subgroup_results %>% select(response_variable, conf_low, conf_high, mean_effect),
    by = "response_variable"
  )

# Create ridge density plot with subgroup diagnostics
ridge_plot_with_metrics <- ggplot(filtered_data, aes(x = yi, y = response_variable, fill = response_variable)) +
  ggridges::geom_density_ridges(alpha = 0.8, scale = 1.2, rel_min_height = 0.01) +
  scale_fill_manual(values = c(
    "Biodiversity" = "#7B3294", "Crop yield" = "#C2A5CF",
    "Greenhouse gas emission" = "#008837", "Soil quality" = "#A6D96A",
    "Pest and Disease" = "#1B7837", "Product quality" = "#E6F5D0",
    "Water quality" = "#FFD700"
  )) +
  labs(
    title = "Forest Plot with Subgroups and Diagnostics",
    subtitle = "Random-effects models fitted for each subgroup",
    x = "Effect Size (Log Scale)", y = "Subgroups",
    fill = "Subgroups"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.title.y = element_text(angle = 90, vjust = 0.5)
  ) +
  geom_text(
    data = subgroup_results,
    aes(
      x = 3.2, y = response_variable,
      label = paste0(
        "n = ", map_int(model, ~ .x$k), "\n",
        "Mean: ", round(mean_effect, 2), "\n",
        "CI: [", round(conf_low, 2), ", ", round(conf_high, 2), "]"
      )
    ),
    hjust = 0, vjust = 0, size = 4, color = "black"
  ) +
  scale_x_continuous(limits = c(-2, 3.5))  # Adjust x-axis range for annotations

ridge_plot_with_metrics

```

```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)

# Subgroup levels (filtered)
subgroups <- unique(filtered_data$response_variable)

# Initialize empty list to store subgroup metrics
subgroup_metrics <- list()

# Iterate over subgroups and extract metrics
for (subgroup in subgroups) {
  # Fit random-effects model for each subgroup
  subgroup_model <- tryCatch(
    rma(yi, vi, data = filtered_data, subset = (response_variable == subgroup)), 
    error = function(e) NULL
  )
  
  if (!is.null(subgroup_model)) {
    # Extract metrics
    metrics <- list(
      response_variable = subgroup,
      mean_effect = coef(subgroup_model)["intrcpt"], # Mean effect size
      conf_low = confint(subgroup_model)$random["ci.lb"], # CI lower bound
      conf_high = confint(subgroup_model)$random["ci.ub"], # CI upper bound
      I2 = subgroup_model$I2,  # Heterogeneity (I^2)
      tau2 = subgroup_model$tau2,  # Between-study variance (τ²)
      n_studies = subgroup_model$k  # Number of studies
    )
  } else {
    # If model fitting fails, populate NA values
    metrics <- list(
      response_variable = subgroup,
      mean_effect = NA,
      conf_low = NA,
      conf_high = NA,
      I2 = NA,
      tau2 = NA,
      n_studies = NA
    )
  }
  
  # Append metrics to the list
  subgroup_metrics[[subgroup]] <- metrics
}

# Convert metrics list to dataframe
subgroup_metrics_df <- do.call(rbind, lapply(subgroup_metrics, as.data.frame))

# Inspect the resulting dataframe
print(subgroup_metrics_df)

```

```{r}

# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points and valid variances
filtered_data <- data %>%
  filter(response_variable %in% selected_subgroups & !is.na(yi) & !is.na(vi) & vi > 0)

# Fit random-effects models for each subgroup and safely extract metrics
# Fit random-effects models for each subgroup and calculate CI
subgroup_results <- filtered_data %>%
  group_by(response_variable) %>%
  summarise(
    # Fit model with tryCatch to handle potential errors
    model = list(
      tryCatch(
        rma(yi, vi, data = cur_data()), 
        error = function(e) NULL
      )
    ),
    .groups = "drop"
  ) %>%
  mutate(
    # Extract confidence intervals and metrics safely
    conf_low = map_dbl(model, ~ if (!is.null(.x)) {
      tryCatch(confint(.x)$random["ci.lb"], error = function(e) NA_real_)
    } else NA_real_),
    conf_high = map_dbl(model, ~ if (!is.null(.x)) {
      tryCatch(confint(.x)$random["ci.ub"], error = function(e) NA_real_)
    } else NA_real_),
    mean_effect = map_dbl(model, ~ if (!is.null(.x)) {
      tryCatch(coef(.x)["intrcpt"], error = function(e) NA_real_)
    } else NA_real_),
    n_studies = map_int(model, ~ if (!is.null(.x)) .x$k else NA_integer_),
    I2 = map_dbl(model, ~ if (!is.null(.x)) .x$I2 else NA_real_)
  )

# Add subgroup results to the dataset
filtered_data <- filtered_data %>%
  left_join(
    subgroup_results %>% select(response_variable, conf_low, conf_high, mean_effect, n_studies, I2),
    by = "response_variable"
  )

# Ridge density plot with subgroup diagnostics
ridge_plot_with_metrics <- ggplot(filtered_data, aes(x = yi, y = response_variable, fill = response_variable)) +
  ggridges::geom_density_ridges(alpha = 0.8, scale = 1.2, rel_min_height = 0.01) +
  scale_fill_manual(values = c(
    "Biodiversity" = "#7B3294", "Crop yield" = "#C2A5CF",
    "Greenhouse gas emission" = "#008837", "Soil quality" = "#A6D96A",
    "Pest and Disease" = "#1B7837", "Product quality" = "#E6F5D0",
    "Water quality" = "#FFD700"
  )) +
  labs(
    title = "Forest Plot with Subgroups and Diagnostics",
    subtitle = "Random-effects models fitted for each subgroup",
    x = "Effect Size (Log Scale)", y = "Subgroups",
    fill = "Subgroups"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.title.y = element_text(angle = 90, vjust = 0.5)
  ) +
  geom_text(
    data = subgroup_results,
    aes(
      x = 3.2, y = response_variable,
      label = paste0(
        "n = ", ifelse(is.na(n_studies), "NA", n_studies), "\n",
        "Mean: ", ifelse(is.na(mean_effect), "NA", round(mean_effect, 2)), "\n",
        "CI: [", ifelse(is.na(conf_low), "NA", round(conf_low, 2)), ", ",
        ifelse(is.na(conf_high), "NA", round(conf_high, 2)), "]\n",
        "I² = ", ifelse(is.na(I2), "NA", round(I2, 1)), "%"
      )
    ),
    hjust = 0, vjust = 0, size = 4, color = "black"
  ) +
  scale_x_continuous(limits = c(-2, 3.5))  # Adjust x-axis range for annotations

filtered_data |> glimpse()
ridge_plot_with_metrics
```

```{r}
# Ensure 'yi', 'vi', and 'response_variable' columns exist in your dataset
data <- non_imp_data_dummy

# Filter subgroups with sufficient data points (e.g., at least 10 studies)
min_studies <- 10
subgroup_counts <- table(data$response_variable)
selected_subgroups <- names(subgroup_counts[subgroup_counts >= min_studies])
filtered_data <- data %>% filter(response_variable %in% selected_subgroups)



# Fit random-effects models for each subgroup and handle potential errors
subgroup_results <- filtered_data %>%
  group_by(response_variable) %>%
  summarise(
    model = list(
      tryCatch(
        rma(yi, vi, data = cur_data()), # Fit random-effects model
        error = function(e) NULL        # Return NULL if model fails
      )
    ),
    .groups = "drop"
  ) %>%
  mutate(
    # Safely extract confidence intervals
    conf_low = map_dbl(model, ~ if (!is.null(.x) && !is.null(confint(.x)$fixed)) {
      tryCatch(
        exp(confint(.x)$fixed["intrcpt", "ci.lb"]), # Exponentiate lower bound
        error = function(e) NA_real_
      )
    } else if (!is.null(.x) && !is.null(confint(.x)$random)) { # Fall back to random effects
      tryCatch(
        exp(confint(.x)$random["tau2", "ci.lb"]), # Example for random-effects CI
        error = function(e) NA_real_
      )
    } else {
      NA_real_
    }),
    conf_high = map_dbl(model, ~ if (!is.null(.x) && !is.null(confint(.x)$fixed)) {
      tryCatch(
        exp(confint(.x)$fixed["intrcpt", "ci.ub"]), # Exponentiate upper bound
        error = function(e) NA_real_
      )
    } else if (!is.null(.x) && !is.null(confint(.x)$random)) { # Fall back to random effects
      tryCatch(
        exp(confint(.x)$random["tau2", "ci.ub"]), # Example for random-effects CI
        error = function(e) NA_real_
      )
    } else {
      NA_real_
    }),
    mean_effect = map_dbl(model, ~ if (!is.null(.x)) {
      tryCatch(exp(coef(.x)["intrcpt"]), error = function(e) NA_real_)
    } else {
      NA_real_
    }),
    n_studies = map_int(model, ~ if (!is.null(.x)) .x$k else NA_integer_),
    I2 = map_dbl(model, ~ if (!is.null(.x)) .x$I2 else NA_real_)
  )


# Log problematic subgroups for debugging
problematic <- subgroup_results %>% filter(is.na(conf_low) | is.na(conf_high))
if (nrow(problematic) > 0) {
  cat("Warning: Issues with the following subgroups:\n")
  print(problematic$response_variable)
}


# Add subgroup diagnostics to the dataset for plotting
filtered_data <- filtered_data %>%
  left_join(
    subgroup_results %>% select(response_variable, conf_low, conf_high, mean_effect, n_studies, I2),
    by = "response_variable"
  )

# Ridge density plot with subgroup diagnostics
ridge_plot_with_metrics <- ggplot(filtered_data, aes(x = yi, y = response_variable, fill = response_variable)) +
  ggridges::geom_density_ridges(alpha = 0.8, scale = 1.2, rel_min_height = 0.01) +
  scale_fill_manual(values = c(
    "Biodiversity" = "#7B3294", "Crop yield" = "#C2A5CF",
    "Greenhouse gas emission" = "#008837", "Soil quality" = "#A6D96A",
    "Pest and Disease" = "#1B7837", "Product quality" = "#E6F5D0",
    "Water quality" = "#FFD700"
  )) +
  labs(
    title = "Forest Plot with Subgroups and Diagnostics",
    subtitle = "Random-effects models fitted for each subgroup",
    x = "Effect Size (Log Scale)", y = "Subgroups",
    fill = "Subgroups"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.title.y = element_text(angle = 90, vjust = 0.5)
  ) +
  geom_text(
    data = subgroup_results,
    aes(
      x = 3.2, y = response_variable,
      label = paste0(
        "n = ", ifelse(is.na(n_studies), "NA", n_studies), "\n",
        "Mean: ", ifelse(is.na(mean_effect), "NA", round(mean_effect, 2)), "\n",
        "CI: [", ifelse(is.na(conf_low), "NA", round(conf_low, 2)), ", ",
        ifelse(is.na(conf_high), "NA", round(conf_high, 2)), "]\n",
        "I² = ", ifelse(is.na(I2), "NA", round(I2, 1)), "%"
      )
    ),
    hjust = 0, vjust = 0, size = 4, color = "black"
  ) +
  scale_x_continuous(limits = c(-2, 3.5))  # Adjust x-axis range for annotations

ridge_plot_with_metrics

```
























INTERPRETATION




```{r}
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R")

# Save the V_matrix for each dataset
saveRDS(V_matrices$non_imp_dataset, file = file.path(output_dir, "v_matrix_non_imp_dataset.rds"))
saveRDS(V_matrices$imp_dataset, file = file.path(output_dir, "v_matrix_imp_dataset.rds"))

cat("Variance-covariance matrices have been saved to:", output_dir, "\n")
```

#############
# STEP 2
##########################################################################################################################################
EVALUATION OF MODEL FITTING 
##########################################################################################################################################


##########################################################################################################################################
Evaluation of model fitting - comparing the two models
##########################################################################################################################################

```{r}
# Extract AIC, BIC, Log-Likelihood, and I²
model_stats <- model_summaries %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood, I2),
               names_to = "Statistic",
               values_to = "Value")

# Extract fixed effects estimates for each model
extract_fixed_effects <- function(model_summary, dataset_name) {
  if (is.null(model_summary)) {
    return(data.frame(
      Dataset = dataset_name,
      Term = NA,
      Estimate = NA,
      CI_Lower = NA,
      CI_Upper = NA
    ))
  }
  
  coef_df <- data.frame(
    Term = rownames(model_summary$model$b),
    Estimate = model_summary$model$b[, 1],
    CI_Lower = model_summary$model$ci.lb,
    CI_Upper = model_summary$model$ci.ub
  )
  
  coef_df$Dataset <- dataset_name
  return(coef_df)
}

# Combine fixed effects data across all models
fixed_effects_data <- bind_rows(
  extract_fixed_effects(model_results$non_imp_dataset, "Non-Imputed Dataset"),
  extract_fixed_effects(model_results$imp_dataset, "Imputed Dataset")
)

# Filter out rows with NA values
fixed_effects_data <- fixed_effects_data %>% drop_na()
fixed_effects_data
```

##########################################################################################################################################
Visualization 1: Model Fit Comparison (AIC, BIC, Log-Likelihood, and I²)
##########################################################################################################################################

```{r}
# Plot Model Fit Statistics
fit_plot <- ggplot(model_stats, aes(x = fct_reorder(Dataset, Value), y = Value, fill = Statistic)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Statistic, scales = "free") +
  labs(title = "Comparison of Model Fit Statistics",
       x = "Dataset",
       y = "Value",
       fill = "Statistic") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(fit_plot)

```



Visualization 2: Fixed Effects Estimates Comparison

```{r}
# Plot Fixed Effects Estimates with Confidence Intervals
coef_plot <- ggplot(fixed_effects_data, aes(x = Term, y = Estimate, color = Dataset)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Comparison of Fixed Effects Estimates Across Models",
       x = "Fixed Effect Term",
       y = "Estimate",
       color = "Dataset") +
  theme_minimal()

print(coef_plot)

```


Visualization 3: Heterogeneity (I²) Comparison

```{r}
# Heterogeneity Comparison Plot
I2_plot <- ggplot(model_summaries, aes(x = Dataset, y = I2, fill = Dataset)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(I2, 2)), vjust = -0.5) +
  labs(title = "Comparison of I² (Heterogeneity) Across Models",
       x = "Dataset",
       y = "I² (%)") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))

print(I2_plot)

```

Comparison table of key model statistics

```{r}
colnames(model_summaries)

```

```{r}
# Define the updated function to extract key statistics
extract_model_summary <- function(model_list, dataset_name) {
  # Check if the model list is not NULL and contains a model object
  if (is.null(model_list) || !inherits(model_list, "list")) {
    return(data.frame(
      Dataset = dataset_name,
      k.all = NA,
      LogLikelihood = NA,
      AIC = NA,
      BIC = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract the actual model object from the list
  model <- model_list$model
  
  # If the model object is NULL or does not have class "rma.mv", return NA
  if (is.null(model) || !inherits(model, "rma.mv")) {
    return(data.frame(
      Dataset = dataset_name,
      k.all = NA,
      LogLikelihood = NA,
      AIC = NA,
      BIC = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract key statistics
  k.all <- model$k.all
  logLik <- as.numeric(logLik(model))
  AIC <- AIC(model)
  BIC <- BIC(model)
  I2 <- round((sum(model$sigma2) / (sum(model$sigma2) + mean(model$vi))) * 100, 1)
  QM <- model$QM
  QMp <- model$QMp
  
  # Create a summary data frame
  data.frame(
    Dataset = dataset_name,
    k.all = k.all,
    LogLikelihood = logLik,
    AIC = AIC,
    BIC = BIC,
    I2 = I2,
    QM = QM,
    QMp = QMp
  )
}

# Apply the updated function to all models in `model_results`
model_summaries <- bind_rows(
  extract_model_summary(model_results$non_imp_dataset, "Non-Imputed Dataset"),
  extract_model_summary(model_results$imp_dataset, "Imputed Dataset"),
  extract_model_summary(model_results$non_imp_dataset_imputed, "Non-Imputed Imputed Dataset"),
  extract_model_summary(model_results$imp_dataset_imputed, "Imputed Imputed Dataset")
)

# View the combined summary table
print(model_summaries)

```


```{r}
# Create a summary table with existing columns
comparison_table <- model_summaries %>%
  mutate(
    LogLikelihood = round(LogLikelihood, 2),
    AIC = round(AIC, 2),
    BIC = round(BIC, 2),
    I2 = paste0(round(I2, 1), "%")
  )

# Create a formatted table using `gt`
comparison_gt <- comparison_table %>%
  gt() %>%
  tab_header(
    title = "Model Comparison Summary",
    subtitle = "Key Statistics for Evaluating Model Fit"
  ) %>%
  cols_label(
    Dataset = "Dataset",
    LogLikelihood = "Log-Likelihood",
    AIC = "AIC",
    BIC = "BIC",
    I2 = "I² (%)"
  ) %>%
  fmt_number(
    columns = c(LogLikelihood, AIC, BIC),
    decimals = 2
  ) %>%
  fmt_missing(
    columns = everything(),
    missing_text = "-"
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "#f5f5f5"),
      cell_borders(sides = "all", color = "gray", weight = px(1))
    ),
    locations = cells_body()
  ) %>%
  tab_options(
    table.font.size = "small",
    table.border.top.color = "gray",
    table.border.bottom.color = "gray"
  )

# Optionally export the table
# Define the output folder path
output_folder <- here("DATA", "OUTPUT_FROM_R")

# Export the table to HTML and PDF in the specified folder
gtsave(comparison_gt, file.path(output_folder, "model_comparison_summary.html"))
gtsave(comparison_gt, file.path(output_folder, "model_comparison_summary.pdf"))

# Display the table
comparison_gt
```






INTERPRETATION OF MODEL 



Evaluation plots

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Define the model names and colors
model_names <- c("Non-Imputed Dataset", "Imputed Dataset", "Non-Imputed Imputed Dataset", "Imputed Imputed Dataset")
colors <- c("#0072B2", "#E69F00", "#009E73", "#D55E00")

# Initialize lists to store the plots
residuals_plots <- list()
conf_intervals_plots <- list()
std_residuals_plots <- list()

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Loop through each model and generate the plots
for (i in seq_along(model_results)) {
  model <- model_results[[i]]$model
  model_name <- model_names[i]
  color <- colors[i]
  V_matrix <- V_matrices[[i]]
  
  # Generate Residuals vs. Fitted Values Plot
  residuals_plot <- tryCatch({
    plot_residuals_vs_fitted(model, model_name, color)
  }, error = function(e) {
    cat("Error generating Residuals vs. Fitted plot for", model_name, ":", e$message, "\n")
    NULL
  })
  residuals_plots[[i]] <- residuals_plot
  
  # Updated function to calculate bootstrap confidence intervals and return a ggplot
  bootstrap_conf_intervals <- function(model, model_name, V_matrix, n_boot = 1000, alpha = 0.05) {
    cat("\nCalculating Bootstrap Confidence Intervals for", model_name, "...\n")
    
    # Initialize a matrix to store the bootstrap estimates
    boot_estimates <- matrix(NA, nrow = n_boot, ncol = length(coef(model)))
    colnames(boot_estimates) <- names(coef(model))
    
    # Bootstrap loop
    for (b in 1:n_boot) {
      resample_indices <- sample(nrow(model$data), replace = TRUE)
      resampled_data <- model$data[resample_indices, ]
      
      boot_model <- tryCatch({
        rma.mv(
          yi = resampled_data$yi,
          V = V_matrix[resample_indices, resample_indices],
          mods = model$mods,
          random = model$random,
          data = resampled_data,
          method = "ML"
        )
      }, error = function(e) NULL)
      
      if (!is.null(boot_model)) {
        boot_estimates[b, ] <- coef(boot_model)
      }
    }
    
    boot_estimates <- boot_estimates[complete.cases(boot_estimates), ]
    lower_bound <- apply(boot_estimates, 2, quantile, probs = alpha / 2)
    upper_bound <- apply(boot_estimates, 2, quantile, probs = 1 - alpha / 2)
    
    conf_int_df <- data.frame(
      Term = names(coef(model)),
      Estimate = coef(model),
      CI.Lower = lower_bound,
      CI.Upper = upper_bound
    )
    
    cat("Bootstrap Confidence Intervals Calculation Complete for", model_name, ".\n")
    
    # Create a ggplot object
    ggplot(conf_int_df, aes(x = Term, y = Estimate)) +
      geom_point(color = "#0072B2") +
      geom_errorbar(aes(ymin = CI.Lower, ymax = CI.Upper), width = 0.2) +
      geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
      labs(
        title = paste("Bootstrap Confidence Intervals -", model_name),
        x = "Terms",
        y = "Estimates"
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  }
  
  
  # Generate Standardized Residuals Plot
  std_residuals_plot <- tryCatch({
    plot_standardized_residuals(model, model_name, color)
  }, error = function(e) {
    cat("Error generating Standardized Residuals plot for", model_name, ":", e$message, "\n")
    NULL
  })
  std_residuals_plots[[i]] <- std_residuals_plot
  
  # Generate and Save Forest Plot Individually
  tryCatch({
    create_forest_plot(model, model_name, V_matrix)
    ggsave(
      filename = file.path(output_dir, paste0("forest_plot_", model_name, ".jpg")),
      width = 10, height = 8, dpi = 300
    )
    cat("Forest plot saved for", model_name, ".\n")
  }, error = function(e) {
    cat("Error generating or saving Forest plot for", model_name, ":", e$message, "\n")
  })
}


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (17/11-24)
# Time difference of 25.3915 secs
# 
# Calculating Bootstrap Confidence Intervals for Non-Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Dataset .
# 
# Forest Plot for Non-Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Dataset .
# 
# Forest Plot for Imputed Dataset :
# Advarsel: longer object length is not a multiple of shorter object length
# Calculating Bootstrap Confidence Intervals for Non-Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Imputed Dataset .
# 
# Forest Plot for Non-Imputed Imputed Dataset :
# 
# Calculating Bootstrap Confidence Intervals for Imputed Imputed Dataset ...
# Bootstrap Confidence Intervals Calculation Complete for Imputed Imputed Dataset .
# 
# Forest Plot for Imputed Imputed Dataset :
```

Saving plots

```{r}

```



##########################################################################################################################################
SAVING DATASETS AND MODEL OBJECTS
##########################################################################################################################################

```{r}
# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# List of datasets and their names
datasets <- list(
  non_imp_dataset = non_imp_dataset,
  imp_dataset = imp_dataset,
  non_imp_dataset_imputed = non_imp_dataset_imputed,
  imp_dataset_imputed = imp_dataset_imputed
)

# Save each dataset
for (dataset_name in names(datasets)) {
  dataset <- datasets[[dataset_name]]
  saveRDS(dataset, file = file.path(output_dir, paste0(dataset_name, ".rds")))
  cat("Dataset saved:", dataset_name, "\n")
}


# Save each variance-covariance matrix
for (matrix_name in names(V_matrices)) {
  V_matrix <- V_matrices[[matrix_name]]
  saveRDS(V_matrix, file = file.path(output_dir, paste0("V_matrix_", matrix_name, ".rds")))
  cat("Variance-Covariance Matrix saved:", matrix_name, "\n")
}


# Save each model object
for (model_name in names(model_results)) {
  model <- model_results[[model_name]]$model
  if (!is.null(model)) {
    saveRDS(model, file = file.path(output_dir, paste0("model_", model_name, ".rds")))
    cat("Model object saved:", model_name, "\n")
  } else {
    cat("Model object for", model_name, "is NULL. Skipping save.\n")
  }
}


# Save the model summary table
saveRDS(model_summaries, file = file.path(output_dir, "model_summaries.rds"))
cat("Model summary table saved.\n")


# List all saved files
saved_files <- list.files(output_dir, full.names = TRUE)
cat("All saved files:\n")
print(saved_files)

```


```{r}
# Define a threshold for Cook's distance
cook_threshold <- 4 / nrow(meta_data) # Adjust as needed

# Initialize a data frame to store results
influential_studies <- data.frame()

# Loop through each diagnostics entry
for (i in seq_along(diagnostics_list)) {
  diagnostics <- diagnostics_list[[i]]
  
  if (is.null(diagnostics)) {
    cat("\nDiagnostics for index", i, "is NULL. Skipping...\n")
    next
  }
  
  # Get the response variable name from the data
  response_variable <- unique(diagnostics$ResponseVariable)[1]
  cat("\nProcessing diagnostics for response variable:", response_variable, "\n")
  
  # Identify highly influential observations
  influential_obs <- diagnostics %>%
    filter(cook.d > cook_threshold) %>%
    select(Study, cook.d) # Keep Study ID and Cook's distance
  
  # Skip if no influential observations are found
  if (nrow(influential_obs) == 0) {
    cat("No influential observations for:", response_variable, "\n")
    next
  }
  
  # Map to `id_article` using `meta_data`
  influential_obs <- influential_obs %>%
    left_join(meta_data, by = c("Study" = "id_obs")) %>%
    select(id_article, cook.d, response_variable) %>%
    rename(StudyID = id_article, CookDistance = cook.d, ResponseVariable = response_variable)
  
  # Append to results
  influential_studies <- bind_rows(influential_studies, influential_obs)
}

# Check the results
influential_studies

# Save as RDS and CSV
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(influential_studies, file.path(output_dir, "influential_studies.rds"))
write.csv(influential_studies, file.path(output_dir, "influential_studies.csv"), row.names = FALSE)

cat("Influential studies saved to:", output_dir, "\n")
```



# Fit the model with random effects for id_article
res <- tryCatch({
  rma(
    yi = yi,
    vi = vi,
    # Random-effects structure: defines how the random effects are modeled hierarchically
    random = list(
      ~ 1 | id_article,                           # Random intercept for each article/study
      ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
      ~ 1 | exp_id                                # Random intercept for individual experiments
    ),
    data = data_subset,
    method = "REML"
  )
}, error = function(e) {
  cat("Model fitting failed for", response, ":", e$message, "\n")
  return(NULL)
})

# Save the fitted model
model_results[[response]] <- res



# Initialize leave-one-out results
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  # Retrieve the fitted model for the current response variable
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    loo <- leave1out(model)  # Perform LOO at observation level
    
    # Map LOO results to `id_article`
    loo_data <- meta_data[meta_data$response_variable == response, ]
    study_level_results <- loo_data %>%
      group_by(id_article) %>%
      summarise(
        Estimate = mean(loo$estimate[match(id_obs, rownames(loo$estimate))], na.rm = TRUE),
        SE = mean(loo$se[match(id_obs, rownames(loo$estimate))], na.rm = TRUE),
        CI_Lower = mean(loo$estimate[match(id_obs, rownames(loo$estimate))] - 
                          1.96 * loo$se[match(id_obs, rownames(loo$estimate))], na.rm = TRUE),
        CI_Upper = mean(loo$estimate[match(id_obs, rownames(loo$estimate))] + 
                          1.96 * loo$se[match(id_obs, rownames(loo$estimate))], na.rm = TRUE)
      )
    
    study_level_results$ResponseVariable <- response
    leave1out_results[[response]] <- study_level_results
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

# Combine results into a single data frame
loo_combined <- bind_rows(leave1out_results)


```{r}
# Initialize list for study-level results
study_level_loo_results <- list()

for (response in names(model_results)) {
  cat("\nProcessing Response Variable:", response, "...\n")
  
  # Retrieve the model
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    # Perform Leave-One-Out Diagnostics
    loo <- leave1out(model)
    
    # Map results back to meta_data
    loo_data <- meta_data[meta_data$response_variable == response, ] %>%
      mutate(
        Estimate = loo$estimate[match(id_obs, rownames(loo$estimate))],
        SE = loo$se[match(id_obs, rownames(loo$estimate))],
        CI_Lower = Estimate - 1.96 * SE,
        CI_Upper = Estimate + 1.96 * SE
      )
    
    # Aggregate results by id_article
    aggregated_results <- loo_data %>%
      group_by(id_article) %>%
      summarise(
        Estimate = mean(Estimate, na.rm = TRUE),
        SE = mean(SE, na.rm = TRUE),
        CI_Lower = mean(CI_Lower, na.rm = TRUE),
        CI_Upper = mean(CI_Upper, na.rm = TRUE),
        ResponseVariable = first(response)
      )
    
    study_level_loo_results[[response]] <- aggregated_results
  }
}
```
```{r}
loo_data |> glimpse()
```

```{r}
# Combine all study-level results into a single data frame
study_level_loo_combined <- bind_rows(study_level_loo_results)

# Visualize the Study-Level Leave-One-Out Results
loo_plot_corrected <- study_level_loo_combined %>%
  ggplot(aes(x = factor(id_article), y = Estimate, color = ResponseVariable)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  facet_wrap(~ ResponseVariable, scales = "free", ncol = 2) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    legend.position = "right"
  ) +
  labs(
    title = "Leave-One-Out Effect Sizes by Study",
    x = "Study (id_article)",
    y = "Effect Size (Estimate ± 95% CI)",
    color = "Response Variable"
  )

# Save the plot
ggsave(
  filename = file.path(output_dir, "LOO_Study_Level_Effect_Sizes_Corrected.png"),
  plot = loo_plot_corrected,
  width = 12,
  height = 8,
  dpi = 300
)

# Print the plot
loo_plot_corrected
```

```{r}
loo_data |> glimpse()
```


```{r}
# Inspect the structure of leave1out_results
str(leave1out_results)

# Check the structure
str(loo_combined)
```

# Extract Leave-One-Out diagnostics into a data frame
loo_data <- bind_rows(
  lapply(names(leave1out_results), function(response) {
    loo <- leave1out_results[[response]]
    if (!is.null(loo)) {
      data.frame(
        StudyRemoved = if (!is.null(names(loo$estimate))) names(loo$estimate) else seq_along(loo$estimate),
        Estimate = loo$estimate,
        SE = loo$se,
        CI_Lower = loo$ci.lb,
        CI_Upper = loo$ci.ub,
        ResponseVariable = response,
        stringsAsFactors = FALSE
      )
    } else {
      NULL
    }
  })
)

# Inspect the resulting data frame
str(loo_data)

```{r}
# Visualize Leave-One-Out Results at Study Level
loo_plot_corrected <- loo_combined %>%
  ggplot(aes(x = factor(id_article), y = Estimate, color = ResponseVariable)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +
  facet_wrap(~ ResponseVariable, scales = "free", ncol = 2) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    legend.position = "right"
  ) +
  labs(
    title = "Leave-One-Out Effect Sizes by Study",
    x = "Study (id_article)",
    y = "Effect Size (Estimate ± 95% CI)",
    color = "Response Variable"
  )

```


leave1out_results <- list()

for (response in unique(diagnostics_data$ResponseVariable)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  # Retrieve the fitted model for the current response variable
  model <- model_results[[response]]
  
  # Perform Leave-One-Out analysis if the model is valid
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

# Save Leave-One-Out results
saveRDS(leave1out_results, file.path(output_dir, "leave1out_results.rds"))
cat("\nLeave-One-Out results saved to:", file.path(output_dir, "leave1out_results.rds"), "\n")



```{r}
# Visualize Leave-One-Out Effect Sizes with Confidence Intervals
loo_plot <-
  loo_data |> 
  ggplot(aes(x = factor(StudyRemoved), y = Estimate, color = ResponseVariable)) +
  geom_point(size = 2) +  # Plot points for effect sizes
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) +  # Add confidence intervals
  facet_wrap(~ ResponseVariable, scales = "free", ncol = 2) +  # Create panels for each response variable
  theme_minimal(base_size = 14) +  # Use a minimal theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  # Rotate x-axis text for readability
    legend.position = "right"  # Place the legend at the bottom
  ) +
  labs(
    title = "Leave-One-Out Effect Sizes by Response Variable",
    x = "Study Removed",
    y = "Effect Size (Estimate ± 95% CI)",
    color = "Response Variable"
  )


loo_plot
```


# Initialize leave-one-out results
# Initialize leave-one-out results
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  # Retrieve the fitted model for the current response variable
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    # Perform Leave-One-Out diagnostics
    loo <- leave1out(model)
    
    # Map LOO results to meta_data
    loo_data <- meta_data %>%
      filter(response_variable == response) %>%
      mutate(
        Estimate = loo$estimate[match(as.character(id_article), rownames(loo$estimate))],
        SE = loo$se[match(as.character(id_article), rownames(loo$estimate))],
        CI_Lower = Estimate - 1.96 * SE,
        CI_Upper = Estimate + 1.96 * SE
      )
    
    # Check if mapping worked
    if (all(is.na(loo_data$Estimate))) {
      stop("Matching id_obs to leave-one-out results failed!")
    }
    
    # Aggregate results by id_article
    study_level_results <- loo_data %>%
      group_by(id_article) %>%
      summarise(
        Estimate = mean(Estimate, na.rm = TRUE),
        SE = mean(SE, na.rm = TRUE),
        CI_Lower = mean(CI_Lower, na.rm = TRUE),
        CI_Upper = mean(CI_Upper, na.rm = TRUE),
        ResponseVariable = first(response)
      )
    
    leave1out_results[[response]] <- study_level_results
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

# Combine all results into a single data frame
loo_combined <- bind_rows(leave1out_results)

# Glimpse the results
glimpse(loo_combined)




# Initialize leave-one-out results
# Initialize leave-one-out results
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  # Retrieve the fitted model for the current response variable
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    # Perform Leave-One-Out diagnostics
    loo <- leave1out(model)
    
    # Map LOO results to meta_data
    loo_data <- meta_data %>%
      filter(response_variable == response) %>%
      mutate(
        Estimate = loo$estimate[match(as.character(id_article), rownames(loo$estimate))],
        SE = loo$se[match(as.character(id_article), rownames(loo$estimate))],
        CI_Lower = Estimate - 1.96 * SE,
        CI_Upper = Estimate + 1.96 * SE
      )
    
    # Check if mapping worked
    if (all(is.na(loo_data$Estimate))) {
      stop("Matching id_obs to leave-one-out results failed!")
    }
    
    # Aggregate results by id_article
    study_level_results <- loo_data %>%
      group_by(id_article) %>%
      summarise(
        Estimate = mean(Estimate, na.rm = TRUE),
        SE = mean(SE, na.rm = TRUE),
        CI_Lower = mean(CI_Lower, na.rm = TRUE),
        CI_Upper = mean(CI_Upper, na.rm = TRUE),
        ResponseVariable = first(response)
      )
    
    leave1out_results[[response]] <- study_level_results
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}

# Combine all results into a single data frame
loo_combined <- bind_rows(leave1out_results)

# Glimpse the results
glimpse(loo_combined)








```{r}
# Function to fit an rma model for a given subset
fit_response_variable_rma <- function(data, response_variable, moderators = NULL) {
  cat("\nFitting rma model for response variable:", response_variable, "...\n")
  
  # Define moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }
  
  # Fit the model
  model <- tryCatch({
    rma(
      yi = yi,
      vi = vi,
      mods = moderator_formula,
      random = ~ 1 | id_article,
      data = data,
      method = "REML"
    )
  }, error = function(e) {
    cat("Error for response variable:", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}
```


```{r}
# Fit models for each response variable
model_results <- list()

for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the model
  model <- fit_response_variable_rma(data_subset, response, moderators = c("tree_type", "crop_type", "age_system", "season", "soil_texture"))
  
  # Save the model
  model_results[[response]] <- model
}

# Save fitted models
saveRDS(model_results, file = file.path(output_dir, "fitted_rma_models_by_response_variable.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")
```
```{r}
# Define response variable
response_var <- "Biodiversity"

# Subset data
data <- meta_data[meta_data$response_variable == response_var, ]

# Define moderator formula (customize as needed)
moderator_formula <- as.formula("yi ~ tree_type + crop_type + age_system + season + soil_texture")

# Fit the model
cat("Fitting model for response variable:", response_var, "...\n")
model_res <- tryCatch({
  rma.mv(
    yi = yi,
    vi = vi,
    mods = moderator_formula,
    random = ~ 1 | exp_id,  # Simplified random-effects structure
    data = data,
    method = "ML",
    control = list(
      optimizer = "optim",
      optim.method = "BFGS",
      iter.max = 1000,
      rel.tol = 1e-8
    )
  )
}, error = function(e) {
  cat("Error fitting model for", response_var, ":", e$message, "\n")
  return(NULL)
})

if (!is.null(model_res)) {
  cat("Model fitting completed for response variable:", response_var, ".\n")
} else {
  stop("Model fitting failed for response variable:", response_var)
}

```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Compute diagnostics for all models
influence_diagnostics <- bind_rows(
  lapply(names(model_results), function(response) {
    compute_influence_diagnostics_rma(model_results[[response]], response)
  })
)

# Filter out rows with NA studies if needed
influence_diagnostics <- influence_diagnostics %>% filter(!is.na(Study))

# Save influence diagnostics
write.csv(influence_diagnostics, file.path(output_dir, "influence_diagnostics_rma_summary.csv"), row.names = FALSE)
cat("\nInfluence diagnostics saved to:", output_dir, "\n")



##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
```

```{r}
# Function to create and save influence diagnostic plots
plot_influence_diagnostics_rma <- function(diagnostics, response_variable) {
  if (is.null(diagnostics)) {
    cat("No diagnostics available for response variable:", response_variable, "\n")
    return(NULL)
  }
  
  # Filter diagnostics for the response variable
  data <- diagnostics %>% filter(ResponseVariable == response_variable)
  
  # Create plots for residuals and Cook's Distance
  plot <- ggplot(data, aes(x = Study)) +
    geom_point(aes(y = StandardizedResiduals, color = "Standardized Residuals")) +
    geom_point(aes(y = CookDistance, color = "Cook's Distance")) +
    geom_point(aes(y = HatValues, color = "Hat Values")) +
    labs(
      title = paste("Influence Diagnostics -", response_variable),
      x = "Study",
      y = "Value",
      color = "Diagnostics"
    ) +
    theme_minimal(base_size = 14) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Save the plot
  plot_file <- file.path(influence_dir, paste0("influence_plot_", tolower(gsub(" ", "_", response_variable)), ".png"))
  ggsave(plot_file, plot, width = 10, height = 6, dpi = 300)
  cat("Influence plot saved for response variable:", response_variable, "at", plot_file, "\n")
  
  return(plot)
}

# Generate influence plots for each response variable
plots <- lapply(unique(influence_diagnostics$ResponseVariable), function(response) {
  plot_influence_diagnostics_rma(influence_diagnostics, response)
})

```

```{r}
# Example dataset for visualizing results
forest_data <- model_diagnostics %>% 
  mutate(
    ci.lb = LogLikelihood - 1.96 * sqrt(Tau2),
    ci.ub = LogLikelihood + 1.96 * sqrt(Tau2)
  )

# Create the forest plot
forest_plot <- ggplot(forest_data, aes(x = LogLikelihood, y = ResponseVariable, color = ResponseVariable)) +
  geom_point(size = 3) +  # Effect size points
  geom_errorbarh(aes(xmin = ci.lb, xmax = ci.ub), height = 0.2) +  # Confidence intervals
  geom_vline(xintercept = 0, linetype = "dotted", color = "red") +  # Null line
  scale_color_manual(values = custom_colors) +  # Custom colors
  labs(
    title = "Forest Plot: Effects of Silvoarable Agroforestry on Ecosystem Services",
    x = "Effect Size (Log Scale ± 95% CI)",
    y = "Response Variables"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 12, hjust = 1),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5)
  )

# Display the forest plot
print(forest_plot)

```


Correcting unclassified in sub_region

```{r}
# Manually update the sub-region for the specific locations
database_clean_sf <- database_clean_sf %>%
  mutate(
    # Update both sub_region and climate_zone based on location and site
    sub_region = case_when(
      location == "Vézénobres" ~ "Mediterranean Europe",
      location == "Restinclières" ~ "Mediterranean Europe",
      location == "Xinjiang" ~ "Continental Asia",
      site == "Vézénobres" ~ "Mediterranean Europe",
      site == "Restinclières" ~ "Mediterranean Europe",
      site == "Leeds" ~ "Continental Europe",
      site == "Nothern England" ~ "Continental Europe",
      TRUE ~ sub_region
    ))

,
climate_zone = case_when(
  sub_region == "England" ~ 3,  # Example climate zone codes
  sub_region == "China" ~ 7,
  sub_region == "Continental Europe" ~ 5,
  TRUE ~ climate_zone
)
)

# Verify if all missing values for climate_zone are handled
database_clean_sf %>%
  filter(is.na(climate_zone))
```

```{r}
database_clean |> glimpse() 

# database_clean %>%
#   mutate(
#     running_year = as.numeric(experiment_year - min(experiment_year, na.rm = TRUE))
#   ) |> 
#   relocate(running_year, experiment_year)
# 

database_clean %>%
  as.data.frame() |> 
  select(-geometry) |> 
  mutate(
    # Extract numeric years from Date columns
    study_year_start_numeric = as.numeric(format(study_year_start, "%Y")),
    study_year_end_numeric = as.numeric(format(study_year_end, "%Y")),
    
    # Handle cases where study_year_end is NA
    study_year_end_numeric = ifelse(is.na(study_year_end_numeric), 
                                    study_year_start_numeric, 
                                    study_year_end_numeric),
    
    # Calculate the midpoint year as the average of start and end years
    midpoint_year = (study_year_start_numeric + study_year_end_numeric) / 2,
    
    # Running year based on the earliest midpoint year
    running_year_midpoint = midpoint_year - min(midpoint_year, na.rm = TRUE),
    
    # Z-score normalization for running year
    running_year_normalized = scale(running_year_midpoint)
  ) %>%
  select(exp_id, id_article,
         running_year_normalized, midpoint_year, running_year_midpoint, study_year_start_numeric, study_year_end_numeric,
         study_year_start, study_year_end, experiment_year) |> 
  relocate(running_year_normalized, midpoint_year, running_year_midpoint, study_year_start_numeric, study_year_end_numeric,
           study_year_start, study_year_end, experiment_year) |> 
  arrange(study_year_start_numeric) |> 
  glimpse()

```

```{r}
# Fix invalid year data
d <- database_clean %>%
  mutate(
    study_year_start = if_else(study_year_start < as.Date("1900-01-01"), NA, study_year_start),
    study_year_end = if_else(study_year_end < as.Date("1900-01-01"), NA, study_year_end),
    study_year_end = if_else(is.na(study_year_end), study_year_start, study_year_end)
  ) %>%
  
  # Recalculate temporal variables
  mutate(
    study_year_start_numeric = as.numeric(format(study_year_start, "%Y")),
    study_year_end_numeric = as.numeric(format(study_year_end, "%Y")),
    midpoint_year = (study_year_start_numeric + study_year_end_numeric) / 2,
    running_year_midpoint = midpoint_year - min(midpoint_year, na.rm = TRUE),
    running_year_normalized = scale(running_year_midpoint)
  )

# Visualization: Distribution of Midpoint Years
ggplot(d, aes(x = midpoint_year)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.7) +
  labs(
    title = "Distribution of Experiment Midpoint Years",
    x = "Midpoint Year",
    y = "Count"
  ) +
  theme_minimal()

# Visualization: Running Year vs Response Variables
ggplot(d, aes(x = running_year_normalized, y = silvo_mean)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  labs(
    title = "Normalized Running Year vs. Silvo Mean",
    x = "Normalized Running Year",
    y = "Silvo Mean"
  ) +
  theme_minimal()
```


database_clean %>%
  as.data.frame() |> 
  select(-geometry) |>
  group_by(id_article, location, experiment_year) %>%
  summarise(exp_id_count = n_distinct(exp_id), .groups = "drop") %>%
  pivot_longer(
    cols = c(id_article, location, experiment_year),
    names_to = "component",
    values_to = "count"
  ) %>%
  ggplot(aes(x = component, y = count, fill = component)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(
    title = "Distribution of exp_id Across Components",
    x = "Component",
    y = "Count of exp_id"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


```{r}
# Check the names of moderator-related columns
moderator_columns <- c("tree_type", "crop_type", "age_system", "soil_texture", "alley_width")  # Replace with actual moderator names

# Prepare data for visualization
moderators_data <- database_clean %>%
  select(exp_id, all_of(moderator_columns)) %>%
  pivot_longer(
    cols = all_of(moderator_columns),
    names_to = "moderator",
    values_to = "value"
  ) %>%
  mutate(missing_count = is.na(value)) %>%
  group_by(exp_id, moderator) %>%
  summarise(missing_count = sum(missing_count), .groups = "drop")

# Visualize missingness for individual moderators
ggplot(moderators_data, aes(x = exp_id, y = missing_count, fill = moderator)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Missingness Overview for Individual Moderators",
    x = "Experiment ID (exp_id)",
    y = "Missing Values Count",
    fill = "Moderator"
  ) +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )
```




AFTER MEETING WITH MAARIT



# Dummy non-imputed dataset 
non_imp_dataset_dummy <- database_clean_sd_dummy |> 
  as.data.frame() |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    #location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )

```{r}
impute_and_merge <- function(dataset, moderators, dataset_name = "Dataset") {
  
  cat("Starting imputation for", dataset_name, "...\n")
  
  # Step 1: Prepare data for imputation
  cols_for_impute <- dataset %>%
    select(
      yi, vi,
      id_article, id_obs, exp_id,
      response_variable, all_of(moderators)
    )
  
  # Step 2: Convert categorical variables to factors
  cols_for_impute <- cols_for_impute %>%
    mutate(across(all_of(moderators), as.factor))
  
  # Step 3: Perform multiple imputation using mice
  set.seed(1234)
  imputed_data <- mice(
    cols_for_impute,
    m = 20,         # Number of imputations
    maxit = 100,    # Maximum iterations
    method = 'pmm', # Predictive Mean Matching
    printFlag = FALSE
  )
  
  # Step 4: Extract the first imputed dataset for merging
  completed_data <- complete(imputed_data, 1)
  
  # Step 5: Join the imputed values back to the original dataset
  merged_dataset <- dataset %>%
    left_join(
      completed_data %>%
        select(id_article, id_obs, exp_id, all_of(moderators)),
      by = c("id_article", "id_obs", "exp_id"),
      suffix = c("_original", "_imputed")
    )
  
  # Step 6: Replace missing values in the original columns with imputed values
  for (mod in moderators) {
    original_col <- paste0(mod, "_original")
    imputed_col <- paste0(mod, "_imputed")
    
    if (original_col %in% colnames(merged_dataset) && imputed_col %in% colnames(merged_dataset)) {
      merged_dataset[[mod]] <- ifelse(
        is.na(merged_dataset[[original_col]]),
        merged_dataset[[imputed_col]],
        merged_dataset[[original_col]]
      )
    }
  }
  
  # Step 7: Drop the temporary columns
  merged_dataset <- merged_dataset %>%
    select(-ends_with("_original"), -ends_with("_imputed"))
  
  cat("Imputation completed for", dataset_name, ".\n")
  
  return(merged_dataset)
}
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Performing moderator imputations
moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Impute and merge for non-imputed dataset
non_imp_dataset_imputed <- impute_and_merge(non_imp_dataset, moderators, "Non-Imputed Dataset")

# Impute and merge for imputed dataset
imp_dataset_imputed <- impute_and_merge(imp_dataset, moderators, "Imputed Dataset")


############################################################################################################################
# Helper function to convert numeric imputed values to categorical factors
convert_to_factors <- function(data) {
  # Convert 'no_tree_per_m' to character factors (Low, High)
  data <- data %>%
    mutate(
      no_tree_per_m = case_when(
        no_tree_per_m %in% c(1, "1") ~ "Low",
        no_tree_per_m %in% c(2, "2") ~ "High",
        TRUE ~ as.character(no_tree_per_m)
      ) %>% as.factor()
    )
  
  # Convert 'tree_height' to character factors (Short, Tall)
  data <- data %>%
    mutate(
      tree_height = case_when(
        tree_height %in% c(1, "1") ~ "Short",
        tree_height %in% c(2, "2") ~ "Tall",
        TRUE ~ as.character(tree_height)
      ) %>% as.factor()
    )
  
  # Convert 'alley_width' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      alley_width = case_when(
        alley_width %in% c(1, "1") ~ "Narrow",
        alley_width %in% c(2, "2") ~ "Wide",
        TRUE ~ as.character(alley_width)
      ) %>% as.factor()
    )
  
  # Convert 'age_system' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      age_system = case_when(
        age_system %in% c(1, "1") ~ "Young",
        age_system %in% c(2, "2") ~ "Medium",
        age_system %in% c(3, "3") ~ "Mature",
        TRUE ~ as.character(age_system)
      ) %>% as.factor()
    )
  
  # Convert 'season' to character factors (Narrow, Wide)
  data <- data %>%
    mutate(
      season = case_when(
        season %in% c(1, "1") ~ "Summer",
        season %in% c(2, "2") ~ "Winter",
        season %in% c(3, "3") ~ "WinterSummer",
        TRUE ~ as.character(season)
      ) %>% as.factor()
    )
  
  return(data)
}

# Apply the conversion function to both datasets
non_imp_dataset_imputed <- convert_to_factors(non_imp_dataset_imputed) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


imp_dataset_imputed <- convert_to_factors(imp_dataset_imputed) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Effect size measure
    yi, vi,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )



############################################################################################################################


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (16/11-24)
# Starting imputation for Non-Imputed Dataset ...
# Advarsel: Number of logged events: 1Imputation completed for Non-Imputed Dataset .
# Starting imputation for Imputed Dataset ...
# Advarsel: Number of logged events: 1Imputation completed for Imputed Dataset .
# Time difference of 1.3963 mins

# Check the structure of the datasets
# str(non_imp_dataset_imputed)
# str(imp_dataset_imputed)
```

Assessing imputation of moderators again

```{r}
# Assessing Moderator missingness

moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Assess missing data for non-imputed dataset
assess_missing_data(non_imp_dataset_imputed, moderators, "Non-Imputed Dataset")

# Assess missing data for imputed dataset
assess_missing_data(imp_dataset_imputed, moderators, "Imputed Dataset")
```

Additional assessment of the moderator imputation

```{r}
# Function to calculate missing data proportions
calculate_missing_proportions <- function(data, moderators) {
  data %>%
    pivot_longer(cols = all_of(moderators), names_to = "moderator", values_to = "value") %>%
    group_by(response_variable, moderator) %>%
    summarise(
      missing_proportion = mean(is.na(value), na.rm = TRUE)
    )
}

# Function to plot missing data proportions per response variable
plot_missing_proportions <- function(original_data, imputed_data, moderators, dataset_name) {
  cat("\nStarting plot creation for", dataset_name, "...\n")
  
  # Calculate missing proportions for original and imputed datasets
  missing_original <- calculate_missing_proportions(original_data, moderators) %>%
    mutate(data_source = "Original")
  
  missing_imputed <- calculate_missing_proportions(imputed_data, moderators) %>%
    mutate(data_source = "Imputed")
  
  # Combine the results
  combined_missing <- bind_rows(missing_original, missing_imputed)
  
  # Create the plot
  plot <- ggplot(combined_missing, aes(x = response_variable, y = missing_proportion, fill = data_source)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ moderator, scales = "free_y") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(
      title = paste("Proportion of Missing Data per Response Variable -", dataset_name),
      x = "Response Variable",
      y = "Missing Proportion"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "top")
  
  cat("\nPlot creation completed for", dataset_name, ".\n")
  
  return(plot)
}

# List of moderators
moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Create plots for Non-Imputed and Imputed datasets
plot_non_imp <- plot_missing_proportions(non_imp_dataset, non_imp_dataset_imputed, moderators, "Non-Imputed Dataset")
plot_imp <- plot_missing_proportions(imp_dataset, imp_dataset_imputed, moderators, "Imputed Dataset")

# Display the plots side by side
plot_non_imp + plot_imp
```



```{r}
# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }
  
  # Fit a multivariate meta-analytic model using `rma.mv` within a try-catch block
  model <- tryCatch({
    # Fit a random-effects multivariate meta-analysis model
    rma.mv(
      # Dependent variable: effect size (yi)
      yi = yi,
      # Variance-covariance matrix (V): accounts for within-study sampling variance and potential correlations
      V = v_matrix,
      # Moderators: a formula specifying the covariates to include in the model
      mods = moderator_formula,
      # Random-effects structure: defines how the random effects are modeled hierarchically
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random intercept for individual experiments
      ),
      # Data: the dataset used for the analysis
      data = data_subset,
      # Method: optimization method for model fitting
      method = "ML",                                # Maximum likelihood estimation
      # Control settings: additional options for the optimization algorithm
      control = list(
        optimizer = "optim",                        # Specify the optimization function to use
        optim.method = "BFGS",                      # Use the Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                            # Maximum number of iterations allowed
        rel.tol = 1e-8                              # Convergence tolerance (stopping criterion for optimization)
      )
    )
  }, error = function(e) {                            # Catch any errors during model fitting
    cat("Error in model fitting:", e$message, "\n") # Print the error message
    return(NULL)                                    # Return NULL if an error occurs
  })
  
  
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}
```




```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Start time tracking
start.time <- Sys.time()

##########################################################################
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Start time tracking
start.time <- Sys.time()

##########################################################################
# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

##########################################################################
# Step 2: Define the function for each imputation method
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    # Predictive Mean Matching
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0
    
    # Define imputation method for PMM
    method <- c(
      "silvo_se" = "pmm",
      "control_se" = "pmm",
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",         # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    # Perform imputation using mice
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    # Return the raw mids object for PMM
    return(imputed_mids)
    
  } else if (method_name == "upper_quartile") {
    # Upper Quartile Imputation for Variance
    upper_quartile_variance <- data %>%
      # The 75th percentile represents a value higher than the median, ensuring that the imputed variances 
      # are not unrealistically small while 
      # still grounded in observed data. This helps maintain a conservative weighting in the meta-analysis.
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
    
    # Impute missing variance with the upper quartile
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    # Example: Mean Imputation
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)
  } else {
    stop("Invalid method name.")
  }
}

##########################################################################
# Step 3: Apply each imputation method
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation")
imputed_datasets <- list()

# Separate storage for the raw mids object from PMM
imputed_mids_pmm <- NULL

for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  if (method_name == "pmm") {
    # Save the mids object for PMM
    imputed_mids_pmm <- impute_data(col_for_impute, method_name)
    # Convert to a completed dataset for combined list
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_pmm)
  } else {
    # Directly store the completed dataset for other methods
    imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
  }
}

##########################################################################
# Step 4: Compare results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm is the raw mids object for PMM
# imputed_datasets contains completed datasets for all methods

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last run (28/11-24)
# Total time taken: 11.16786 sec
# Total time taken: 13.66807 

# Last run (01/01-25)
# Total time taken: 10.50317 
```










```{r}
# Visualize Model Comparisons
# Create visualizations for the AIC and likelihood ratio test (LRT) results from `model_comparisons`.

# Extract AIC values and LRT results
comparison_results <- lapply(names(model_comparisons), function(response) {
  comparison <- model_comparisons[[response]]
  if (!is.null(comparison)) {
    tibble(
      Response = response,
      AIC_Original = comparison$AIC[["Original"]],
      AIC_Alternative = comparison$AIC[["Alternative"]],
      AIC_Minimal = comparison$AIC[["Minimal"]],
      LRT_Original_vs_Alternative = comparison$LR_Original_vs_Alternative$pval,
      LRT_Original_vs_Minimal = comparison$LR_Original_vs_Minimal$pval
    )
  } else {
    tibble(
      Response = response,
      AIC_Original = NA,
      AIC_Alternative = NA,
      AIC_Minimal = NA,
      LRT_Original_vs_Alternative = NA,
      LRT_Original_vs_Minimal = NA
    )
  }
}) %>% bind_rows()

comparison_results |> 
  
  # Melt the data for AIC visualization
  aic_data <- comparison_results %>%
  select(Response, 
         AIC_Original, 
         AIC_Alternative, 
         AIC_Minimal) %>%
  pivot_longer(cols = starts_with("AIC"), names_to = "Model", values_to = "AIC") %>%
  mutate(Model = factor(Model, levels = c("AIC_Original", "AIC_Alternative", "AIC_Minimal"),
                        labels = c("Original", "Alternative", "Minimal")))

# Plot AIC values
plot_aic <- ggplot(aic_data, aes(x = Response, y = AIC, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AIC Comparisons Across Models",
       x = "Response Variable",
       y = "AIC",
       fill = "Model") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Prepare LRT data
lrt_data <- comparison_results %>%
  select(Response, LRT_Original_vs_Alternative, LRT_Original_vs_Minimal) %>%
  pivot_longer(cols = starts_with("LRT"), names_to = "Comparison", values_to = "p_value") %>%
  mutate(Comparison = factor(Comparison, 
                             levels = c("LRT_Original_vs_Alternative", "LRT_Original_vs_Minimal"),
                             labels = c("Original vs. Alternative", "Original vs. Minimal")))

# Plot LRT p-values
plot_lrt <- ggplot(lrt_data, aes(x = Response, y = p_value, color = Comparison)) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  labs(title = "LRT P-Values for Model Comparisons",
       x = "Response Variable",
       y = "P-Value",
       color = "Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the plots
print(plot_aic)
print(plot_lrt)

```






























Original Model
This model incorporates two levels of random effects: one nested within articles and response variables, and another at the experiment level.

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }
  
  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      # Dependent variable: effect size
      yi = yi,
      # Variance-covariance matrix for within-study variability
      V = v_matrix,
      # Moderator formula: relationship between the effect size and moderators
      mods = moderator_formula,
      # Random effects structure
      random = list(
        ~ 1 | id_article/response_variable,         # Nested random intercept for each response variable within articles
        ~ 1 | exp_id                                # Random effect: accounts for variability at the experiment level
      ),
      # Data used for model fitting
      data = data_subset,
      # Method for model fitting
      method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
      # Optimization settings
      control = list(
        optimizer = "optim",                  # Optimizer function to use for fitting
        optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                      # Maximum number of iterations allowed
        rel.tol = 1e-8                        # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 11.96204 secs

# Last go (01/01-2025)
# Time difference of 8.034523 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 8.034523 secs

# Last go (02/12-2024)
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 9.78518 secs
```

Alternative Model 1: Simplified Random Effects
This model simplifies the random effects structure, focusing only on variability at the experiment level.
```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################


# Function to fit models for each response variable using precomputed v_matrices
fit_response_variable_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Define the moderator formula
  moderator_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")  # Intercept-only model
  }
  
  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      # Dependent variable: effect size
      yi = yi,
      # Variance-covariance matrix for within-study variability
      V = v_matrix,
      # Moderator formula: relationship between the effect size and moderators
      mods = moderator_formula,
      # Random effects structure
      random = ~ 1 | exp_id,                 # Random effect: accounts for variability at the experiment level
      # Data used for model fitting
      data = data_subset,
      # Method for model fitting
      method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
      # Optimization settings
      control = list(
        optimizer = "optim",                  # Optimizer function to use for fitting
        optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                      # Maximum number of iterations allowed
        rel.tol = 1e-8                        # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit models for each response variable using the precomputed v_matrices
model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the model for the current response variable
  model <- fit_response_variable_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  model_results[[response]] <- model
}

# Save the fitted models to a file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_by_response_variable_simplified.rds"))
cat("\nAll models fitted and saved to:", output_dir, "\n")


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 4.593977 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 4.593977 secs

# Last go (02/12-2024)
# Time difference of 4.643164 secs
# Processing response variable: Biodiversity 
# Fitting model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Time difference of 4.643164 secs
```

Minimally Reduced Model:
  This model further simplifies the random effects structure by removing all nested random effects,
focusing only on the experiment level and an intercept-only model.

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Start time tracking
start.time <- Sys.time()

##########################################################################
# Define the function to fit minimally reduced models for each response variable
fit_minimal_model <- function(data_subset, response_variable, v_matrix, moderators = NULL) {
  cat("\nFitting minimally reduced model for response variable:", response_variable, "...\n")
  
  # Define the intercept-only formula
  minimal_formula <- if (!is.null(moderators)) {
    as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  } else {
    as.formula("yi ~ 1")
  }
  
  # Ensure all moderators are treated as factors
  if (!is.null(moderators)) {
    data_subset <- data_subset %>%
      mutate(across(all_of(moderators), as.factor)) %>%
      as.data.frame()
  }
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      # Dependent variable: effect size
      yi = yi,
      # Variance-covariance matrix for within-study variability
      V = v_matrix,
      # Moderator formula: Intercept-only model
      mods = minimal_formula,
      # Random effects structure
      random = ~ 1 | exp_id,                 # Random effect: accounts for variability at the experiment level
      # Data used for model fitting
      data = data_subset,
      # Method for model fitting
      method = "ML",                          # Maximum Likelihood (ML) for parameter estimation
      # Optimization settings
      control = list(
        optimizer = "optim",                  # Optimizer function to use for fitting
        optim.method = "BFGS",                # Broyden–Fletcher–Goldfarb–Shanno algorithm for optimization
        iter.max = 1000,                      # Maximum number of iterations allowed
        rel.tol = 1e-8                        # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Fit minimally reduced models for each response variable using the precomputed v_matrices
minimal_model_results <- list()

for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators (if applicable)
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the minimally reduced model for the current response variable
  model <- fit_minimal_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  minimal_model_results[[response]] <- model
}

##########################################################################
# Save the fitted minimally reduced models to a file
saveRDS(minimal_model_results, file = file.path(output_dir, "fitted_models_by_response_variable_minimal.rds"))
cat("\nAll minimally reduced models fitted and saved to:", output_dir, "\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (02/12-2024)
# Total time taken: 4.232735 
# Processing response variable: Biodiversity 
# Fitting minimally reduced model for response variable: Biodiversity ...
# Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting minimally reduced model for response variable: Greenhouse gas emission ...
# Model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting minimally reduced model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting minimally reduced model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting minimally reduced model for response variable: Pest and Disease ...
# Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting minimally reduced model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting minimally reduced model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Water quality .
# All minimally reduced models fitted and saved to: C:/Users/au759124/OneDrive - Aarhus universitet/Documents/Temp_SAF_meta_analysis/DATA/OUTPUT_FROM_R/SAVED_OBJECTS_FROM_R 
# Total time taken: 4.232735 
```







```{r}
# Compare models for each response variable
model_comparisons <- list()

for (response in names(original_model_results)) {
  cat("\nComparing models for response variable:", response, "\n")
  
  original_model <- original_model_results[[response]]
  alternative_model <- simplified_model_results[[response]]
  minimal_model <- minimal_model_results[[response]]
  
  if (!is.null(original_model) & !is.null(alternative_model) & !is.null(minimal_model)) {
    # Extract AIC for each model, handle NULL cases
    aic_values <- c(
      "Original" = if (!is.null(original_model)) original_model$aic else NA,
      "Alternative" = if (!is.null(alternative_model)) alternative_model$aic else NA,
      "Minimal" = if (!is.null(minimal_model)) minimal_model$aic else NA
    )
    
    # Perform likelihood ratio tests, handle potential errors
    lr_original_vs_alternative <- tryCatch(
      anova(original_model, alternative_model),
      error = function(e) { cat("LR test failed for Original vs. Alternative:", e$message, "\n"); NULL }
    )
    lr_original_vs_minimal <- tryCatch(
      anova(original_model, minimal_model),
      error = function(e) { cat("LR test failed for Original vs. Minimal:", e$message, "\n"); NULL }
    )
    
    # Store results
    model_comparisons[[response]] <- list(
      AIC = aic_values,
      LR_Original_vs_Alternative = lr_original_vs_alternative,
      LR_Original_vs_Minimal = lr_original_vs_minimal
    )
    
    # Print AIC values
    cat("AIC Values:\n", aic_values, "\n")
  } else {
    cat("Model fitting failed for one or more models for", response, "\n")
  }
}

# Display results for all response variables
model_comparisons |> glimpse() |> head()
```

```{r}
model_comparisons |> str()
```

```{r}
# Inspect log-likelihood values and model structures
inspect_model_fit <- function(model, model_name) {
  if (!is.null(model)) {
    cat("\nModel:", model_name, "\n")
    cat("Log-Likelihood:", logLik(model), "\n")
    cat("Fixed Effects:\n")
    print(model$beta)
    cat("Random Effects:\n")
    print(model$sigma2)
  } else {
    cat("\nModel:", model_name, "is NULL\n")
  }
}

##################################################################################
# Replace with the specific response variable to inspect
response_variable <- "Crop yield"  

inspect_model_fit(original_model_results[[response_variable]], "Original")
inspect_model_fit(simplified_model_results[[response_variable]], "Alternative")
inspect_model_fit(minimal_model_results[[response_variable]], "Minimal")
```


```{r}
##########################################################################
# Compare models systematically for each response variable
##########################################################################

# Initialize an empty list to store model comparison results
meta_analysis_model_comparisons <- list()

# Loop through each response variable
for (response in names(full_model_results)) {
  cat("\nComparing models for response variable:", response, "\n")
  
  # Retrieve models for the current response variable
  full_model <- full_model_results[[response]]
  simplified_model <- simplified_model_results[[response]]
  minimal_model <- minimal_model_results[[response]]
  fixed_effects_model <- fixed_effects_model_results[[response]]
  
  if (!is.null(full_model) & !is.null(simplified_model) & !is.null(minimal_model) & !is.null(fixed_effects_model)) {
    
    # Extract AIC for each model, handle NULL cases
    aic_values <- c(
      "Full" = if (!is.null(full_model)) full_model$aic else NA,
      "Simplified" = if (!is.null(simplified_model)) simplified_model$aic else NA,
      "Minimal" = if (!is.null(minimal_model)) minimal_model$aic else NA,
      "Fixed Effects Only" = if (!is.null(fixed_effects_model)) fixed_effects_model$aic else NA
    )
    
    # Perform likelihood ratio tests, handle potential errors
    lr_full_vs_simplified <- tryCatch(
      anova(full_model, simplified_model),
      error = function(e) { cat("LR test failed for Full vs. Simplified:", e$message, "\n"); NULL }
    )
    lr_full_vs_minimal <- tryCatch(
      anova(full_model, minimal_model),
      error = function(e) { cat("LR test failed for Full vs. Minimal:", e$message, "\n"); NULL }
    )
    lr_full_vs_fixed <- tryCatch(
      anova(full_model, fixed_effects_model),
      error = function(e) { cat("LR test failed for Full vs. Fixed Effects Only:", e$message, "\n"); NULL }
    )
    
    # Store results
    meta_analysis_model_comparisons[[response]] <- list(
      AIC = aic_values,
      LR_Full_vs_Simplified = lr_full_vs_simplified,
      LR_Full_vs_Minimal = lr_full_vs_minimal,
      LR_Full_vs_Fixed = lr_full_vs_fixed
    )
    
    # Print AIC values
    cat("AIC Values:\n", aic_values, "\n")
  } else {
    cat("Model fitting failed for one or more models for", response, "\n")
  }
}

##########################################################################
# Display comparison results summary
##########################################################################
# Print a glimpse of the comparison results
meta_analysis_model_comparisons |> glimpse()

##########################################################################
# Save comparison results to file for further analysis
##########################################################################
saveRDS(meta_analysis_model_comparisons, file = "meta_analysis_model_comparison_results.rds")
cat("Model comparison results saved to model_comparison_results.rds\n")
```


```{r}
##########################################################################
# Visualizations for Model Comparisons
##########################################################################

# Load necessary libraries
library(ggplot2)

##########################################################################
# 1. AIC Comparison Plot
##########################################################################
# Combine AIC values into a data frame for visualization
aic_df <- do.call(rbind, lapply(meta_analysis_model_comparisons, function(x) {
  if (!is.null(x$AIC)) {
    data.frame(
      Response = names(meta_analysis_model_comparisons),
      Model = names(x$AIC),
      AIC = x$AIC
    )
  }
}))

# Plot AIC values
ggplot(aic_df, aes(x = Response, y = AIC, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "AIC Comparison Across Models", x = "Response Variable", y = "AIC") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

##########################################################################
# 2. Likelihood Ratio Test (LRT) Results
##########################################################################
# Extract p-values from LRT results
lrt_pvalues <- do.call(rbind, lapply(meta_analysis_model_comparisons, function(x) {
  if (!is.null(x$LR_Full_vs_Simplified) && !is.null(x$LR_Full_vs_Minimal)) {
    data.frame(
      Response = names(meta_analysis_model_comparisons),
      Comparison = c("Full vs Simplified", "Full vs Minimal"),
      PValue = c(x$LR_Full_vs_Simplified$pval, x$LR_Full_vs_Minimal$pval)
    )
  }
}))

# Plot p-values
ggplot(lrt_pvalues, aes(x = Response, y = -log10(PValue), color = Comparison)) +
  geom_point(size = 3) +
  labs(title = "Likelihood Ratio Test P-Values", x = "Response Variable", y = "-log10(P-Value)") +
  geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "red") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

##########################################################################
# 3. Residual Variance (QE) Comparison
##########################################################################
# Extract QE values
qe_df <- do.call(rbind, lapply(meta_analysis_model_comparisons, function(x) {
  if (!is.null(x$LR_Full_vs_Simplified)) {
    data.frame(
      Response = names(meta_analysis_model_comparisons),
      Model = c("Full", "Simplified", "Minimal", "Fixed Effects"),
      QE = c(x$LR_Full_vs_Simplified$QE.f, x$LR_Full_vs_Simplified$QE.r,
             x$LR_Full_vs_Minimal$QE.r, x$LR_Full_vs_Fixed$QE.r)
    )
  }
}))

# Plot QE values
ggplot(qe_df, aes(x = Response, y = QE, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Residual Variance (QE) Comparison", x = "Response Variable", y = "Residual Variance (QE)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

##########################################################################
# 4. R-Squared Values
##########################################################################
# Extract R-squared values
r2_df <- do.call(rbind, lapply(meta_analysis_model_comparisons, function(x) {
  if (!is.null(x$LR_Full_vs_Simplified)) {
    data.frame(
      Response = names(meta_analysis_model_comparisons),
      Model = c("Full", "Simplified", "Minimal", "Fixed Effects"),
      R2 = c(x$LR_Full_vs_Simplified$R2, NA, NA, NA) # Add R2 if available
    )
  }
}))

# Plot R-squared values
ggplot(r2_df, aes(x = Response, y = R2, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "R-Squared Comparison Across Models", x = "Response Variable", y = "R-Squared") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
### Visual Evaluation Protocol: AIC and BIC Comparison

# Function to create a performance comparison table for individual response variables
compare_models <- function(original_model_list, simplified_model_list, response_variable) {
  original_model <- original_model_list[[response_variable]]
  simplified_model <- simplified_model_list[[response_variable]]
  
  if (is.null(original_model) || is.null(simplified_model)) {
    cat("Model missing for response variable:", response_variable, "\n")
    return(NULL)
  }
  
  performance_metrics <- tibble(
    ResponseVariable = response_variable,
    Model = c("Original", "Simplified"),
    AIC = c(AIC(original_model), AIC(simplified_model)),
    BIC = c(BIC(original_model), BIC(simplified_model))
  )
  return(performance_metrics)
}

# Visualize AIC and BIC side-by-side for all response variables
plot_model_comparison_by_response <- function(performance_comparison) {
  performance_long <- performance_comparison %>%
    pivot_longer(cols = c(AIC, BIC), names_to = "Metric", values_to = "Value") %>%
    mutate(ResponseVariable = as.factor(ResponseVariable))
  
  ggplot(performance_long, aes(x = Model, y = Value, fill = Metric)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    facet_wrap(~ResponseVariable, scales = "free_y") +
    labs(
      title = "Model Performance Comparison by Response Variable",
      x = "Model",
      y = "Value",
      fill = "Metric"
    ) +
    theme_minimal() +
    scale_fill_brewer(palette = "Set2")
}

# Example Usage
# Assuming `original_model_results` and `simplified_model_results` are lists of model objects by response variable
response_variables <- names(original_model_results)

# Generate performance metrics for all response variables
all_performance_metrics <- lapply(response_variables, function(rv) {
  compare_models(original_model_results, simplified_model_results, rv)
})

# Combine all metrics into a single data frame
all_performance_metrics <- bind_rows(all_performance_metrics)

# Print the performance metrics
glimpse(all_performance_metrics)
```
```{r}

# Generate the visualization
plot_model_comparison_by_response(all_performance_metrics)
```

```{r}
summary(original_model)$random
summary(simplified_model)$random
```










```{r}
##########################################################################
# Systematic Comparison of Models for Each Response Variable
##########################################################################
# Perform likelihood ratio tests for each response variable with additional validation
lrt_results <- lapply(names(full_model_results), function(response) {
  tryCatch({
    full_model <- full_model_results[[response]]
    simplified_model <- simplified_model_results[[response]]
    minimal_model <- minimal_model_results[[response]]
    fixed_model <- fixed_effects_model_results[[response]]
    
    # Debug: Check if models are non-NULL
    if (is.null(full_model) || is.null(simplified_model) || is.null(minimal_model) || is.null(fixed_model)) {
      stop("One or more models are NULL for response: ", response)
    }
    
    # Debug: Check data consistency
    if (!identical(length(full_model$yi), length(simplified_model$yi)) || 
        !identical(length(full_model$vi), length(simplified_model$vi))) {
      stop("Mismatch in data lengths for full vs. simplified model for response: ", response)
    }
    if (!identical(length(full_model$yi), length(minimal_model$yi)) || 
        !identical(length(full_model$vi), length(minimal_model$vi))) {
      stop("Mismatch in data lengths for full vs. minimal model for response: ", response)
    }
    if (!identical(length(full_model$yi), length(fixed_model$yi)) || 
        !identical(length(full_model$vi), length(fixed_model$vi))) {
      stop("Mismatch in data lengths for full vs. fixed model for response: ", response)
    }
    
    # Run ANOVA
    simplified_lrt <- anova(full_model, simplified_model) $`Pr(>Chi)`[2]
    minimal_lrt <- anova(full_model, minimal_model)$`Pr(>Chi)`[2]
    fixed_lrt <- anova(full_model, fixed_model)$`Pr(>Chi)`[2]
    
    data.frame(
      Response = response,
      LRT_Full_vs_Simplified = simplified_lrt,
      LRT_Full_vs_Minimal = minimal_lrt,
      LRT_Full_vs_Fixed = fixed_lrt
    )
  }, error = function(e) {
    message("Error processing response variable: ", response, " - ", e$message)
    NULL
  })
})

# Filter out NULL elements and validate results
lrt_results <- lrt_results[!sapply(lrt_results, is.null)]
if (length(lrt_results) > 0) {
  lrt_results <- do.call(rbind, lrt_results)
  lrt_results <- lrt_results %>% mutate(across(starts_with("LRT"), ~ -log10(.)))
} else {
  message("No valid results to process.")
  lrt_results <- data.frame()
}
```

```{r}
full_ids <- full_model_results[["Biodiversity"]]$data$id_obs
minimal_ids <- minimal_model_results[["Biodiversity"]]$data$id_obs

extra_ids_in_minimal <- setdiff(minimal_ids, full_ids)
missing_ids_in_minimal <- setdiff(full_ids, minimal_ids)

message("Extra IDs in Minimal Model: ", length(extra_ids_in_minimal))
message("Missing IDs in Minimal Model: ", length(missing_ids_in_minimal))

# Inspect the discrepancies
extra_ids_in_minimal
missing_ids_in_minimal
```
```{r}
minimal_model_results[["Biodiversity"]] <- update(
  minimal_model_results[["Biodiversity"]],
  data = full_model_results[["Biodiversity"]]$data
)
```


#############
# STEP 5
##########################################################################################################################################
KEY INFLUENCE DIAGNOSTICS ON EACH SUBSET - SIMPLIFIED MODEL FITTING 
##########################################################################################################################################


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Fit models for each response variable
model_results_infdia <- list()

for (response in unique(meta_data$response_variable)) {
  cat("\nProcessing response variable:", response, "...\n")
  
  # Subset data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the model with random effects for `id_article`
  res <- tryCatch({
    rma(yi = yi, 
        vi = vi, 
        # Add study-level random effect
        random = ~ 1 | id_article,  
        data = data_subset, 
        # Restricted ML
        method = "REML")
  }, error = function(e) {
    cat("Model fitting failed for", response, ":", e$message, "\n")
    return(NULL)
  })
  
  # Save the fitted model
  model_results_infdia[[response]] <- res
}

# Recompute influence diagnostics
influence_diagnostics <- list()

for (response in names(model_results_infdia)) {
  cat("\nComputing influence diagnostics for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    inf <- tryCatch({
      influence(model)
    }, error = function(e) {
      cat("Influence diagnostics failed for:", response, ":", e$message, "\n")
      return(NULL)
    })
    influence_diagnostics[[response]] <- inf
  } else {
    cat("Skipping influence diagnostics for:", response, "due to missing model.\n")
  }
}


# Perform leave-one-out analysis
leave1out_results <- list()

for (response in names(model_results_infdia)) {
  cat("\nRunning Leave-One-Out for:", response, "...\n")
  
  model <- model_results_infdia[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out(model)
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################

# Last go (02/12-2024)
# Time difference of 1.117442 mins
# Time difference of 3.097009 mins
# Time difference of 2.744736 mins

# Last go (01/01-2025)
# Time difference of 2.148423 mins
# Processing response variable: Biodiversity ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Crop yield ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Water quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Pest and Disease ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Soil quality ...
# Advarsel: Extra argument ('random') disregarded.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.
# Processing response variable: Greenhouse gas emission ...
# Advarsel: Extra argument ('random') disregarded.
# Processing response variable: Product quality ...
# Advarsel: Extra argument ('random') disregarded.
# Computing influence diagnostics for: Biodiversity ...
# Computing influence diagnostics for: Crop yield ...
# Computing influence diagnostics for: Water quality ...
# Computing influence diagnostics for: Pest and Disease ...
# Computing influence diagnostics for: Soil quality ...
# Computing influence diagnostics for: Greenhouse gas emission ...
# Computing influence diagnostics for: Product quality ...
# Running Leave-One-Out for: Biodiversity ...
# Running Leave-One-Out for: Crop yield ...
# Running Leave-One-Out for: Water quality ...
# Running Leave-One-Out for: Pest and Disease ...
# Running Leave-One-Out for: Soil quality ...
# Running Leave-One-Out for: Greenhouse gas emission ...
# Running Leave-One-Out for: Product quality ...
# Time difference of 2.148423 mins
```


=======
  # Last go: (17/11-24)
  # Time difference of 25.3915 secs
  # 
  # Calculating Bootstrap Confidence Intervals for Non-Imputed Dataset ...
  # Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Dataset .
  # 
  # Forest Plot for Non-Imputed Dataset :
  # 
  # Calculating Bootstrap Confidence Intervals for Imputed Dataset ...
  # Bootstrap Confidence Intervals Calculation Complete for Imputed Dataset .
  # 
  # Forest Plot for Imputed Dataset :
  # Advarsel: longer object length is not a multiple of shorter object length
  # Calculating Bootstrap Confidence Intervals for Non-Imputed Imputed Dataset ...
  # Bootstrap Confidence Intervals Calculation Complete for Non-Imputed Imputed Dataset .
  # 
  # Forest Plot for Non-Imputed Imputed Dataset :
  # 
  # Calculating Bootstrap Confidence Intervals for Imputed Imputed Dataset ...
  # Bootstrap Confidence Intervals Calculation Complete for Imputed Imputed Dataset .
  # 
  # Forest Plot for Imputed Imputed Dataset :
  
  Saving plots


Fit the simplified multivariate random-effects model for diagnostics

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Helper function to run the multivariate random-effects model
run_meta_analysis <- function(data, dataset_name) {
  cat("Running analysis for", dataset_name, "\n")
  
  # Step 1: Calculate the variance-covariance matrix if needed
  V_matrix <- as.matrix(data$vi) # Example assumes `vi` is already correct
  
  # Step 2: Fit the simplified multivariate random-effects model
  model_meta_diagnostics <- tryCatch({
    rma(
      yi = data$yi,
      vi = data$vi,
      random = list(
        ~ 1 | id_article,
        ~ 1 | id_article/response_variable,
        ~ 1 | exp_id
      ),
      data = data,
      method = "ML"
    )
  }, error = function(e) {
    stop("Error in model fitting: ", e$message)
  })
  
  # Step 3: Compute influence diagnostics
  inf <- influence(model_meta_diagnostics)
  
  # Return the model and diagnostics
  return(list(model = model_meta_diagnostics, influence = inf))
}

# Step 1: Apply meta-analysis function to each dataset
results <- lapply(names(datasets), function(dataset_name) {
  tryCatch({
    run_meta_analysis(datasets[[dataset_name]], dataset_name)
  }, error = function(e) {
    cat("Error in dataset", dataset_name, ":", e$message, "\n")
    return(NULL)
  })
})

# Step 2: Name the results list
names(results) <- names(datasets)


##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go: (27/11-24)
# Time difference of 2.942727 hours
```


```{r}
saveRDS(results, file = "meta_analysis_diagnosis_on_datasets_results.rds")
```


```{r}
# Step 3: Plot influence diagnostics
par(mfrow = c(8, 1), oma = c(2, 2, 2, 2)) # Set up plotting area
for (dataset_name in names(results)) {
  if (!is.null(results[[dataset_name]])) {
    plot(results[[dataset_name]]$influence) # Default plot
    mtext(paste("Influence Diagnostics for", dataset_name), side = 3, line = 0.5, outer = FALSE)
  }
}


# Optional: Inspect the results
# str(results)
```

```{r}
datasets_effect_sizes$non_imp_dataset %>% glimpse()
```












##########################################################################################################################################
SAVING DATASETS AND MODEL OBJECTS
##########################################################################################################################################

```{r}
# Save the results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(model_results_infdia, file = file.path(output_dir, "simplified_fitted_models.rds"))
saveRDS(influence_diagnostics, file = file.path(output_dir, "simplified_influence_diagnostics.rds"))
cat("\nModels and influence diagnostics saved to:", output_dir, "\n")
```

```{r}
influence_diagnostics |> str()
```


```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################

# Generate plots for influence diagnostics
plot_dir <- file.path(output_dir, "Influence_Diagnostics_Plots")
if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE)

for (response in names(influence_diagnostics)) {
  cat("\nGenerating influence diagnostic plots for:", response, "...\n")
  
  inf <- influence_diagnostics[[response]]
  
  if (!is.null(inf)) {
    tryCatch({
      par(mfrow = c(8, 1))
      plot(inf)
      dev.copy(jpeg, file = file.path(plot_dir, paste0("influence_plot_", gsub(" ", "_", response), ".jpg")))
      dev.off()
    }, error = function(e) {
      cat("Plotting failed for", response, ":", e$message, "\n")
    })
  } else {
    cat("No influence diagnostics available for:", response, "\n")
  }
}
cat("\nInfluence diagnostic plots saved to:", plot_dir, "\n")

##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (02/12-2024)
# Time difference of 49.21399 secs
```
```{r}
model_results |> str()
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##################################################
# Start time tracking
start.time <- Sys.time()
##################################################
##################################################



# Custom Leave-One-Out for rma.mv models
leave1out_mv <- function(model, data, study_id_col) {
  unique_studies <- unique(data[[study_id_col]])
  results <- list()
  
  for (study in unique_studies) {
    cat("Leaving out study:", study, "...\n")
    
    # Subset data to exclude the current study
    data_subset <- data[data[[study_id_col]] != study, ]
    
    # Refit the model without the excluded study
    tryCatch({
      refit_model <- rma.mv(yi = model$yi,
                            V = model$V,
                            mods = model$X,
                            random = model$s.names,
                            data = data_subset,
                            method = "ML")
      
      # Store refit model's coefficients
      results[[as.character(study)]] <- list(
        coefficients = refit_model$b,
        fit_stats = refit_model$fit.stats
      )
    }, error = function(e) {
      cat("Error with study:", study, "->", e$message, "\n")
      results[[as.character(study)]] <- NULL
    })
  }
  
  return(results)
}

# Perform Leave-One-Out for all response variables
leave1out_results <- list()

for (response in names(model_results)) {
  cat("\nPerforming Leave-One-Out for:", response, "...\n")
  
  model <- model_results[[response]]
  
  if (!is.null(model)) {
    leave1out_results[[response]] <- leave1out_mv(
      model = model,
      data = model$data,
      study_id_col = "id_article"
    )
  } else {
    cat("Skipping Leave-One-Out for:", response, "due to missing model.\n")
  }
}




##################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
time.taken
##############################################################
# Last go (01/01-2025)
# Time difference of  mins
```

```{r}
# Function to extract influence diagnostic data
extract_diagnostics <- function(inf_obj, response_variable) {
  if (is.null(inf_obj)) {
    cat("No influence diagnostics available for", response_variable, "\n")
    return(NULL)
  }
  
  # Extract diagnostics safely and pad missing elements with NA
  tryCatch({
    n_studies <- length(inf_obj$inf$rstudent)  # Total number of studies
    
    diagnostics <- data.frame(
      Study = if (!is.null(rownames(inf_obj$inf$rstudent))) rownames(inf_obj$inf$rstudent) else seq_len(n_studies),
      rstudent = inf_obj$inf$rstudent,
      dffits = inf_obj$inf$dffits,
      cook.d = inf_obj$inf$cook.d,
      cov.r = inf_obj$inf$cov.r,
      tau2.del = inf_obj$inf$tau2.del,
      QE.del = inf_obj$inf$QE.del,
      hat = inf_obj$inf$hat,
      weight = inf_obj$inf$weight,
      ResponseVariable = response_variable
    )
    
    return(diagnostics)
  }, error = function(e) {
    cat("Error extracting diagnostics for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
}
```

```{r}
# Combine diagnostics into a single data frame
# Combine diagnostics into a single data frame
diagnostics_list <- lapply(names(influence_diagnostics), function(response) {
  inf <- influence_diagnostics[[response]]
  extract_diagnostics(inf, response)
})

# Filter out NULL entries
diagnostics_data <- do.call(rbind, diagnostics_list[!sapply(diagnostics_list, is.null)])

# Check the resulting data frame
diagnostics_data |> glimpse()
```

```{r}
# Save diagnostics dataset
# diagnostics_list |> str()

# Define the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save the diagnostics list as an RDS file
saveRDS(diagnostics_list, file = file.path(output_dir, "diagnostics_list.rds"))
cat("Diagnostics list saved as RDS file to:", file.path(output_dir, "diagnostics_list.rds"), "\n")
```


```{r}
# Check the distribution of diagnostics by response variable
table(diagnostics_data$ResponseVariable)

# Summarize key diagnostics for each response variable
summary_stats <- diagnostics_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    Mean_rstudent = mean(rstudent, na.rm = TRUE),
    Max_rstudent = max(rstudent, na.rm = TRUE),
    Mean_dffits = mean(dffits, na.rm = TRUE),
    Max_dffits = max(dffits, na.rm = TRUE),
    Mean_cook.d = mean(cook.d, na.rm = TRUE),
    Max_cook.d = max(cook.d, na.rm = TRUE),
    .groups = "drop"
  )

summary_stats
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = rstudent, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Standardized Residuals by Study",
    x = "Study ID",
    y = "Standardized Residuals"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "none",
    panel.spacing = unit(1, "lines")
  )
```
```{r}
ggplot(diagnostics_data, aes(x = Study, y = cook.d, color = ResponseVariable)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  facet_wrap(~ ResponseVariable, scales = "free") +
  theme_minimal(base_size = 14) +
  labs(
    title = "Cook's Distance by Study",
    x = "Study ID",
    y = "Cook's Distance"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "none",
    panel.spacing = unit(1, "lines")
  )
```

```{r}
# Define thresholds
thresholds <- diagnostics_data %>%
  mutate(
    IsInfluential = (abs(rstudent) > 2) | (cook.d > 0.5)
  )

# Check how many studies are flagged as influential
table(thresholds$ResponseVariable, thresholds$IsInfluential)

# Save flagged data for review
output_dir <- here::here("DATA", "OUTPUT_FROM_R")
write.csv(thresholds, file.path(output_dir, "flagged_influential_studies.csv"), row.names = FALSE)
```






```{r}
# Define the output directory an file path for the plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
output_file <- file.path(output_dir, "leave_one_out_effect_sizes.png")

# Save the plot to the output directory
ggsave(output_file, plot = loo_plot, width = 12, height = 8, dpi = 300)
cat("Plot saved to:", output_file, "\n")
```


```{r}
# Inspect the diagnostics data structure for a single response variable
str(diagnostics_list[[1]])
```


```{r}
# Define a threshold for Cook's Distance
cooks_threshold <- 0.8  # Adjust as necessary

# Identify influential studies
influential_studies <- diagnostics_data %>%
  filter(cook.d > cooks_threshold) %>%
  distinct(Study, ResponseVariable)

# Map back to `meta_data`
mapped_influential <- meta_data %>%
  semi_join(influential_studies, by = c("id_article" = "Study"))

# View the result
print(mapped_influential)
```



##########################################################################################################################################
FOR ALL RESPONSE VARIABLES
##########################################################################################################################################

```{r}
# Step 1: Validate Model Components
validate_model_components <- function(model) {
  data <- model$data
  v_matrix <- model$V
  
  if (is.null(data)) stop("Data is NULL for the model.")
  if (is.null(v_matrix)) stop("V matrix is NULL for the model.")
  
  if (nrow(v_matrix) != nrow(data) || ncol(v_matrix) != nrow(data)) {
    stop("Dimension mismatch: V matrix does not align with data for model.")
  }
}

# Step 2: Filter and Recreate Models
filter_and_recreate_model <- function(model, common_ids) {
  data <- model$data
  v_matrix <- model$V
  
  # Filter data and V matrix
  filtered_data <- data[data$id_obs %in% common_ids, ]
  filtered_v <- v_matrix[data$id_obs %in% common_ids, data$id_obs %in% common_ids, drop = FALSE]
  
  # Validate filtered components
  if (nrow(filtered_data) != nrow(filtered_v)) {
    stop("Mismatch between filtered data and V matrix dimensions.")
  }
  
  # Recreate model
  rma.mv(
    yi = filtered_data$yi,
    V = filtered_v,
    mods = formula(model$mods),
    random = model$random,
    method = "ML",
    data = filtered_data
  )
}

# Step 3: Process ANOVA Per Response
process_anova_per_response <- function(full_models, other_models) {
  aic_differences <- lapply(names(full_models), function(response) {
    tryCatch({
      # Extract and validate models
      full_model <- full_models[[response]]
      reduced_model <- other_models[[response]]
      
      validate_model_components(full_model)
      validate_model_components(reduced_model)
      
      # Find common IDs
      full_data <- full_model$data
      reduced_data <- reduced_model$data
      common_ids <- intersect(full_data$id_obs, reduced_data$id_obs)
      
      message("Response: ", response, " - Common IDs: ", length(common_ids))
      
      if (length(common_ids) == 0) stop("No matching observations for ", response)
      
      # Recreate models with filtered data
      updated_full_model <- filter_and_recreate_model(full_model, common_ids)
      updated_reduced_model <- filter_and_recreate_model(reduced_model, common_ids)
      
      # Perform ANOVA
      return(process_anova(updated_full_model, updated_reduced_model))
    }, error = function(e) {
      message("Error for response ", response, ": ", e$message)
      return(NA)
    })
  })
  
  names(aic_differences) <- names(full_models)
  return(aic_differences)
}

# Step 4: Validate Components for Full and Simplified Models
lapply(full_model_results, function(model) {
  tryCatch({
    validate_model_components(model)
  }, error = function(e) {
    message("Validation failed: ", e$message)
  })
})

lapply(simplified_model_results, function(model) {
  tryCatch({
    validate_model_components(model)
  }, error = function(e) {
    message("Validation failed: ", e$message)
  })
})

# Step 5: Process AIC Differences
simplified_aic_diffs <- process_anova_per_response(full_model_results, simplified_model_results)
minimal_aic_diffs <- process_anova_per_response(full_model_results, minimal_model_results)
fixed_aic_diffs <- process_anova_per_response(full_model_results, fixed_effects_model_results)

# Step 6: Combine and Display Results
aic_differences_df <- data.frame(
  Response = names(simplified_aic_diffs),
  Simplified = unlist(simplified_aic_diffs),
  Minimal = unlist(minimal_aic_diffs),
  Fixed = unlist(fixed_aic_diffs)
)

# Step 7: Log Summary of Results
if (any(is.na(aic_differences_df))) {
  message("Some models failed during processing. Review logs for details.")
}

print(aic_differences_df)

```


```{r}
# Function to extract key diagnostics from a fitted model
extract_model_diagnostics <- function(model, response_variable) {
  if (is.null(model)) {
    return(data.frame(
      ResponseVariable = response_variable,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      Tau2 = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }
  
  # Extract diagnostics
  aic <- AIC(model)
  bic <- BIC(model)
  log_likelihood <- as.numeric(logLik(model))
  tau2 <- sum(model$sigma2)
  i2 <- round((tau2 / (tau2 + mean(model$vi))) * 100, 1)
  qm <- model$QM
  qmp <- model$QMp
  
  data.frame(
    ResponseVariable = response_variable,
    AIC = aic,
    BIC = bic,
    LogLikelihood = log_likelihood,
    Tau2 = tau2,
    I2 = i2,
    QM = qm,
    QMp = qmp
  )
}
```

```{r}
# Extract diagnostics for all models
model_diagnostics_full_model <- bind_rows(
  lapply(names(full_model_results), function(response) {
    extract_model_diagnostics(full_model_results[[response]], response)
  })
)

model_diagnostics_simplified_model <- bind_rows(
  lapply(names(simplified_model_results), function(response) {
    extract_model_diagnostics(simplified_model_results[[response]], response)
  })
)
```

```{r}
# Save diagnostics table
write.csv(model_diagnostics, file.path(output_dir, "model_diagnostics_summary.csv"), row.names = FALSE)
```

```{r}
# Visualize AIC, BIC, and Log-Likelihood
diagnostics_plot <- model_diagnostics %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood), names_to = "Metric", values_to = "Value") %>%
  ggplot(aes(x = ResponseVariable, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Model Fit Comparison",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(diagnostics_plot)
```



Visualization 2: Fixed Effects Estimates Comparison

```{r}
# Plot Fixed Effects Estimates with Confidence Intervals
coef_plot <- ggplot(fixed_effects_data, aes(x = Term, y = Estimate, color = Dataset)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Comparison of Fixed Effects Estimates Across Models",
       x = "Fixed Effect Term",
       y = "Estimate",
       color = "Dataset") +
  theme_minimal()

print(coef_plot)
```




```{r}
# Pivot the data
pivoted_diagnostics <- all_meta_analysis_model_diagnostics %>%
  pivot_wider(
    names_from = ModelType,
    values_from = c(AIC, BIC, LogLikelihood),
    names_prefix = "Model_"
  )

# Debug: Check the resulting structure
pivoted_diagnostics |> str()
pivoted_diagnostics |> glimpse()

# Debugging: Check for NAs in pivoted_diagnostics
pivoted_diagnostics %>%
  summarise(across(starts_with("AIC_"), ~ sum(is.na(.)), .names = "NA_count_{col}")) %>%
  print()

# Compute relative differences
relative_diagnostics <- pivoted_diagnostics %>%
  mutate(
    # AIC relative differences
    AIC_Relative_Difference_To_Full_Model_Simplified = (AIC_Model_Simplified / AIC_Model_Full) * 100,
    AIC_Relative_Difference_To_Full_Model_Minimal = (AIC_Model_Minimal / AIC_Model_Full) * 100,
    AIC_Relative_Difference_To_Full_Model_Fixed = (AIC_Model_Fixed / AIC_Model_Full) * 100,
    
    # BIC relative differences
    BIC_Relative_Difference_To_Full_Model_Simplified = (BIC_Model_Simplified / BIC_Model_Full) * 100,
    BIC_Relative_Difference_To_Full_Model_Minimal = (BIC_Model_Minimal / BIC_Model_Full) * 100,
    BIC_Relative_Difference_To_Full_Model_Fixed = (BIC_Model_Fixed / BIC_Model_Full) * 100,
    
    # LogLikelihood relative differences
    LogLikelihood_Relative_Difference_To_Full_Model_Simplified = (LogLikelihood_Model_Simplified / LogLikelihood_Model_Full) * 100,
    LogLikelihood_Relative_Difference_To_Full_Model_Minimal = (LogLikelihood_Model_Minimal / LogLikelihood_Model_Full) * 100,
    LogLikelihood_Relative_Difference_To_Full_Model_Fixed = (LogLikelihood_Model_Fixed / LogLikelihood_Model_Full) * 100
  ) %>%
  # Select relevant columns for comparison
  select(
    ResponseVariable, starts_with("AIC_Relative"), starts_with("BIC_Relative"), starts_with("LogLikelihood_Relative")
  ) %>%
  # Pivot to long format for visualization or export
  pivot_longer(
    cols = -ResponseVariable,
    names_to = c("Metric", "Comparison"),
    names_sep = "_Relative_Difference_To_Full_Model_",
    values_to = "RelativeValue"
  )

relative_diagnostics |> str()

```



```{r}

# Preprocessing step: Align and fill missing values with `NA` to ensure comparability
# Step 1: Preprocess pivoted_diagnostics
preprocessed_diagnostics <- pivoted_diagnostics %>%
  rowwise() %>%
  mutate(
    AIC_Full = ifelse(is.na(AIC_Full), max(c(AIC_Simplified, AIC_Minimal, AIC_Fixed), na.rm = TRUE), AIC_Full),
    BIC_Full = ifelse(is.na(BIC_Full), max(c(BIC_Simplified, BIC_Minimal, BIC_Fixed), na.rm = TRUE), BIC_Full),
    LogLikelihood_Full = ifelse(is.na(LogLikelihood_Full), min(c(LogLikelihood_Simplified, LogLikelihood_Minimal, LogLikelihood_Fixed), na.rm = TRUE), LogLikelihood_Full)
  ) %>%
  ungroup()

preprocessed_diagnostics |> str()
preprocessed_diagnostics |>  glimpse()

# Perform relative calculations based on the Full model
relative_diagnostics <- preprocessed_diagnostics %>%
  mutate(
    AIC_Relative_Simplified = AIC_Simplified - AIC_Full,
    AIC_Relative_Minimal = AIC_Minimal - AIC_Full,
    AIC_Relative_Fixed = AIC_Fixed - AIC_Full,
    BIC_Relative_Simplified = BIC_Simplified - BIC_Full,
    BIC_Relative_Minimal = BIC_Minimal - BIC_Full,
    BIC_Relative_Fixed = BIC_Fixed - BIC_Full,
    LogLikelihood_Relative_Simplified = LogLikelihood_Simplified - LogLikelihood_Full,
    LogLikelihood_Relative_Minimal = LogLikelihood_Minimal - LogLikelihood_Full,
    LogLikelihood_Relative_Fixed = LogLikelihood_Fixed - LogLikelihood_Full
  ) %>%
  select(ResponseVariable, starts_with("AIC_Relative"), starts_with("BIC_Relative"), starts_with("LogLikelihood_Relative")) %>%
  pivot_longer(
    cols = -ResponseVariable,
    names_to = c("Metric", "ModelType"),
    names_sep = "_Relative_",
    values_to = "Value"
  )

# Debug: Check the resulting relative diagnostics
relative_diagnostics |> glimpse()

# Aggregate performance metrics across all response variables
aggregated_relative_diagnostics <- relative_diagnostics %>%
  group_by(ModelType, Metric) %>%
  summarise(MeanRelativeValue = mean(Value, na.rm = TRUE), .groups = "drop")

# Debug: Check the aggregated results
aggregated_relative_diagnostics |> str()

relative_diagnostics |> str()
aggregated_diagnostics |> str()
```


```{r}
# Heterogeneity Comparison Plot
I2_plot <- ggplot(model_summaries, aes(x = Dataset, y = I2, fill = Dataset)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(I2, 2)), vjust = -0.5) +
  labs(title = "Comparison of I² (Heterogeneity) Across Models",
       x = "Dataset",
       y = "I² (%)") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))

print(I2_plot)

```

Comparison table of key model statistics

```{r}
colnames(model_summaries)
```

```{r}
# Visualize Variance Components (Tau2) and Heterogeneity (I²)
variance_plot <- model_diagnostics %>%
  ggplot(aes(x = ResponseVariable)) +
  geom_bar(aes(y = Tau2, fill = "Tau2 (Variance Components)"), stat = "identity", position = "dodge") +
  geom_point(aes(y = I2 / 100, color = "I² (Heterogeneity)"), size = 4) +
  scale_y_continuous(
    name = "Variance Components (Tau2)",
    sec.axis = sec_axis(~.*100, name = "Heterogeneity (I² %)"),
    limits = c(0, 0.02, na.rm = TRUE)
  ) +
  labs(
    title = "Variance Components and Heterogeneity",
    x = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

variance_plot
```


















```{r}
# Combine results from all response variables into a single data frame
forest_plot_data <- bind_rows(
  lapply(names(model_results), function(response) {
    model <- model_results[[response]]
    
    if (!is.null(model) && !is.null(model$data)) {
      n_effects <- length(model$yi)
      n_studies <- nrow(model$data)
      
      # Ensure the lengths match or skip inconsistent data
      if (n_effects == n_studies) {
        data.frame(
          Study = model$data$id_article,                 # Study IDs
          EffectSize = model$yi,                        # Effect sizes
          CI_Lower = model$yi - 1.96 * sqrt(model$vi),  # Lower CI
          CI_Upper = model$yi + 1.96 * sqrt(model$vi),  # Upper CI
          ResponseVariable = response                   # Response variable
        )
      } else {
        warning(sprintf(
          "Skipping response variable '%s': Mismatched lengths between model$yi (%d) and model$data (%d).",
          response, n_effects, n_studies
        ))
        NULL
      }
    } else {
      warning(sprintf("Skipping response variable '%s': Missing model or data.", response))
      NULL
    }
  })
)

# Check the prepared data
forest_plot_data |> glimpse()
```
```{r}
# Prepare Aggregated Data
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

aggregated_data
```



```{r}
# Create the forest plot with custom colors
forest_plot <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank))) +
  # Add points for effect sizes
  geom_point(aes(size = size_category, color = ResponseVariable)) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = ResponseVariable), height = 0.2) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot of Response Variables with Custom Colors",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Number of Studies"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Display the plot
forest_plot
```






```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Four Suggested Models for Meta-Analysis

##########################################################################
# Model 1: Comprehensive (Full) Model
# Incorporates the most complex structure with nested random effects and all moderators.
##########################################################################

# Define the function to fit the full model for each response variable
fit_full_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting full model for response variable:", response_variable, "...\n")
  
  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Define the moderator formula
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,                              # Dependent variable: effect size
      V = v_matrix,                         # Variance-covariance matrix for within-study variability
      mods = moderator_formula,             # Moderator formula: relationship between the effect size and moderators
      random = list(
        ~ 1 | id_article/response_variable, # Nested random intercept for response variable nested within articles
        ~ 1 | exp_id                        # Random effect: accounts for variability at the experiment level
      ),
      data = data_subset,                   # Data used for model fitting
      method = "REML",                      # REML instead of ML to estimate variance components more robustly.
      control = list(
        optimizer = "optim",                # Optimizer function to use for fitting
        optim.method = "BFGS",              # Optimization algorithm
        iter.max = 1000,                    # Maximum number of iterations allowed
        rel.tol = 1e-8                      # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit full models for each response variable
full_model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the full model for the current response variable
  model <- fit_full_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  full_model_results[[response]] <- model
}

##########################################################################
# Model 2: Moderately Simplified Model
# Simplifies random effects and removes interaction terms.
##########################################################################

# Define the function to fit simplified models for each response variable
fit_simplified_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting simplified model for response variable:", response_variable, "...\n")
  
  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Define the moderator formula
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = v_matrix,                      # Variance-covariance matrix for within-study variability
      mods = moderator_formula,          # Moderator formula: relationship between the effect size and moderators
      random = ~ 1 | exp_id,             # Simplified random effect: variability at the experiment level only
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # REML instead of ML to estimate variance components more robustly.
      control = list(
        optimizer = "optim",             # Optimizer function to use for fitting
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum number of iterations allowed
        rel.tol = 1e-8                   # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit simplified models for each response variable
simplified_model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the simplified model for the current response variable
  model <- fit_simplified_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  simplified_model_results[[response]] <- model
}

##########################################################################
# Model 3: Minimal Random Effects Model
# Focuses on intercept-only model with minimal random effects.
##########################################################################

# Define the function to fit minimally reduced models for each response variable
fit_minimal_model <- function(data_subset, response_variable, v_matrix) {
  cat("\nFitting minimal model for response variable:", response_variable, "...\n")
  
  # Define the intercept-only formula
  minimal_formula <- as.formula("yi ~ 1")
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = v_matrix,                      # Variance-covariance matrix for within-study variability
      mods = minimal_formula,            # Intercept-only model
      random = ~ 1 | exp_id,             # Random effect: accounts for variability at the experiment level
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # REML instead of ML to estimate variance components more robustly.
      control = list(
        optimizer = "optim",             # Optimizer function to use for fitting
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum number of iterations allowed
        rel.tol = 1e-8                   # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit minimal models for each response variable
minimal_model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Fit the minimal model for the current response variable
  model <- fit_minimal_model(data_subset, response, v_matrix)
  
  # Save the model result
  minimal_model_results[[response]] <- model
}

##########################################################################
# Model 4: Fixed Effects Only Model
# Removes random effects entirely and focuses solely on fixed effects.
##########################################################################

# Define the function to fit fixed effects only models for each response variable
fit_fixed_effects_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting fixed effects only model for response variable:", response_variable, "...\n")
  
  # Ensure all moderators are treated as factors
  data_subset <- data_subset %>%
    mutate(across(all_of(moderators), as.factor)) %>%
    as.data.frame()
  
  # Define the moderator formula
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = v_matrix,                      # Variance-covariance matrix for within-study variability
      mods = moderator_formula,          # Moderator formula: relationship between the effect size and moderators
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # REML instead of ML to estimate variance components more robustly.
      control = list(
        optimizer = "optim",             # Optimizer function to use for fitting
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum number of iterations allowed
        rel.tol = 1e-8                   # Convergence tolerance for optimization
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return the model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

# Fit fixed effects only models for each response variable
fixed_effects_model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Retrieve the precomputed v_matrix
  v_matrix <- v_matrices[[response]]
  
  # Define moderators
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit the fixed effects only model for the current response variable
  model <- fit_fixed_effects_model(data_subset, response, v_matrix, moderators)
  
  # Save the model result
  fixed_effects_model_results[[response]] <- model
}

##########################################################################
# Save All Fitted Models
##########################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(full_model_results, file = file.path(output_dir, "fitted_models_full.rds"))
saveRDS(simplified_model_results, file = file.path(output_dir, "fitted_models_simplified.rds"))
saveRDS(minimal_model_results, file = file.path(output_dir, "fitted_models_minimal.rds"))
saveRDS(fixed_effects_model_results, file = file.path(output_dir, "fitted_models_fixed_effects.rds"))

cat("\nAll models have been saved successfully!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
# Calculate time taken
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n" )
##########################################################################
# Last go (02/12-2024)
# Total time taken: 18.05626 secs

# Processing response variable: Biodiversity 
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting full model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting full model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting full model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting full model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Biodiversity 
# 
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting simplified model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting simplified model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting simplified model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# 
# Processing response variable: Biodiversity 
# Fitting minimal model for response variable: Biodiversity ...
# Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting minimal model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting minimal model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting minimal model for response variable: Pest and Disease ...
# Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting minimal model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Water quality .
# 
# Processing response variable: Biodiversity 
# Fitting fixed effects only model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting fixed effects only model for response variable: Greenhouse gas emission ...
# Error in model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting fixed effects only model for response variable: Product quality ...
# Model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting fixed effects only model for response variable: Crop yield ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting fixed effects only model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting fixed effects only model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting fixed effects only model for response variable: Water quality ...
# Error in model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# 
# All models have been saved successfully!
# 
# Total time taken: 18.05626 
```


The results show varying success in fitting models across different response variables and levels of model complexity. For *biodiversity*, all models were fitted successfully, though there was a warning about some rows with missing data being omitted. This indicates that the data for this variable may be incomplete but still sufficient to run and compare models effectively. The fitted models for *biodiversity* can move forward to the evaluation phase without major concerns.

For *greenhouse gas emissions*, only the minimal model was fitted successfully, with other models failing due to an error related to the levels of categorical variables. This suggests that some moderators do not have sufficient variability within this response variables dataset. Addressing this issue by preprocessing the data, such as removing unused factor levels, could enable the fitting of more complex models.

The *product quality* response variable performed well across all model types, with no critical issues reported during fitting. This indicates that the data for this variable is robust and suitable for detailed comparisons across models.

For *crop yield*, all models were fitted, but there were warnings about large variance ratios, which suggest potential instability in the results. This likely reflects heterogeneity in the dataset, and further diagnostics could clarify whether certain studies or data points are driving this variability.

The *pest and disease* response variable also saw successful fitting across models, though there were instances of redundant predictors being dropped. This points to the value of simplified models in handling such issues efficiently. For *soil quality*, similar warnings about variance ratios and redundant predictors were observed, meaning these results also require further evaluation to ensure reliability.

*Water quality*, like *greenhouse gas emissions*, presented challenges in fitting models beyond the minimal level. This again indicates the need to refine the dataset by addressing issues related to categorical variables with insufficient levels. Overall, these results highlight both strengths and areas needing refinement in the data, allowing for targeted improvements before moving forward with model comparisons and interpretations.












```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    # Predictive Mean Matching
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0

    ##########################################################################
    # Define imputation method for PMM
    method <- c(
      "silvo_se" = "pmm",         # Imputed using predictive mean matching
      "control_se" = "pmm",       # Imputed using predictive mean matching
      #"silvo_n" = "",            # Not imputed
      #"control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = ""          # Not imputed
      #"id_obs" = "",             # Not imputed
      #"treat_id" = "",           # Not imputed
      #"exp_id" = ""              # Not imputed
    )
    
    # Perform imputation using mice
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "upper_quartile") {
    ##########################################################################
    # Upper Quartile Imputation for Variance
    upper_quartile_variance <- data %>%
      # The 75th percentile represents a value higher than the median, ensuring that the imputed variances 
      # are not unrealistically small while 
      # still grounded in observed data. This helps maintain a conservative weighting in the meta-analysis.
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")

    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)

  } else if (method_name == "linear_imputation") {
    ##########################################################################
    # Linear Regression Imputation (norm.predict)
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0

    method <- c(
      "silvo_se" = "norm.predict",   # Imputed using linear regression
      "control_se" = "norm.predict", # Imputed using linear regression
       #"silvo_n" = "",            # Not imputed
      #"control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = ""          # Not imputed
      #"id_obs" = "",             # Not imputed
      #"treat_id" = "",           # Not imputed
      #"exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation")
imputed_datasets <- list()

# Separate storage for the raw mids objects
imputed_mids_pmm <- NULL
imputed_mids_linear <- NULL

for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  if (method_name == "pmm") {
    imputed_mids_pmm <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_pmm)
  } else if (method_name == "linear_imputation") {
    imputed_mids_linear <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_linear)
  } else {
    imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
  }
}

##########################################################################
# Step 4: Compare results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed datasets for all methods

##########################################################################
# Last run (02/01-25)
# Total time taken: 21.27449 
```




```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  ) |> 
  # Removing column variables not used in the imputation
  select(-c(silvo_n, control_n,
            id_obs, treat_id, exp_id))

#######################################################################################
# Step 2: Define the function for each imputation method
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    # Predictive Mean Matching
    pred_matrix <- mice::make.predictorMatrix(data)
    # Exclude these columns from imputation: "id_obs", "treat_id", "exp_id"
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", "id_article")] <- 0 

    ##########################################################################
    # Define imputation method for PMM
    method <- c(
      "silvo_se" = "pmm",         # Imputed using predictive mean matching
      "control_se" = "pmm",       # Imputed using predictive mean matching
      #"silvo_n" = "",            # Not imputed
      #"control_n" = "",          # Not imputed
      "tree_age" = "",            # Not imputed
      "crop_type" = "",           # Not imputed
      "tree_type" = "",           # Not imputed
      "bioclim_sub_regions" = "", # Not imputed
      "experiment_year" = "",     # Not imputed
      "alley_width" = "",         # Not imputed
      "id_article" = ""           # Not imputed
      #"id_obs" = "",             # Not imputed
      #"treat_id" = "",           # Not imputed
      #"exp_id" = ""              # Not imputed
    )

    # Perform imputation using mice
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else if (method_name == "upper_quartile") {
    ##########################################################################
    # Upper Quartile Imputation for Variance
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")

    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)

  } else if (method_name == "mean_imputation") {
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)

  } else if (method_name == "linear_imputation") {
    ##########################################################################
    # Linear Regression Imputation (norm.predict)
    pred_matrix <- mice::make.predictorMatrix(data)
     # Exclude these columns from imputation: "id_obs", "treat_id", "exp_id"
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", "id_article")] <- 0 

    method <- c(
      "silvo_se" = "norm.predict",   # Imputed using linear regression
      "control_se" = "norm.predict", # Imputed using linear regression
       #"silvo_n" = "",            # Not imputed
      #"control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = ""          # Not imputed
      #"id_obs" = "",             # Not imputed
      #"treat_id" = "",           # Not imputed
      #"exp_id" = ""              # Not imputed
    )

    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)

  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation")
imputed_datasets <- list()

# Separate storage for the raw mids objects
imputed_mids_pmm <- NULL
imputed_mids_linear <- NULL

for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  if (method_name == "pmm") {
    imputed_mids_pmm <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_pmm)
  } else if (method_name == "linear_imputation") {
    imputed_mids_linear <- impute_data(col_for_impute, method_name)
    imputed_datasets[[method_name]] <- mice::complete(imputed_mids_linear)
  } else {
    imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
  }
}

##########################################################################
# Step 4: Cap Imputed Values
cap_values <- function(data, lower_quantile = 0.05, upper_quantile = 0.95) {
  quantiles <- data |> summarise(
    silvo_se_lower = quantile(silvo_se, lower_quantile, na.rm = TRUE),
    silvo_se_upper = quantile(silvo_se, upper_quantile, na.rm = TRUE),
    control_se_lower = quantile(control_se, lower_quantile, na.rm = TRUE),
    control_se_upper = quantile(control_se, upper_quantile, na.rm = TRUE)
  )

  data <- data |> mutate(
    silvo_se = pmin(pmax(silvo_se, quantiles$silvo_se_lower), quantiles$silvo_se_upper),
    control_se = pmin(pmax(control_se, quantiles$control_se_lower), quantiles$control_se_upper)
  )

  return(data)
}

# Apply capping to each imputed dataset
for (method_name in names(imputed_datasets)) {
  cat("Capping values for", method_name, "dataset...\n")
  imputed_datasets[[method_name]] <- cap_values(imputed_datasets[[method_name]])
}

##########################################################################
# Step 5: Compare results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed and Capped Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed and capped datasets


##########################################################################
# Last run (02/01-25)
```

```{r}
# Recreate the violin plot with better scaling
ggplot() +
  # Violin plot for bootstrapped data
  geom_violin(data = bootstrapped_data, aes(y = response_variable, x = exp(bootstrapped_rr), fill = response_variable), 
              alpha = 0.5, scale = "area") + # Use 'area' scaling for better proportional representation
  # Overlay mean and confidence intervals
  geom_point(data = summary_data, aes(y = response_variable, x = exp(WeightedMeanRR)), color = "black", size = 3) +
  geom_errorbarh(data = summary_data, aes(y = response_variable, xmin = exp(LowerCI), xmax = exp(UpperCI)), 
                height = 0.2, color = "black") +
  # Add annotations for proportions and study counts
  geom_text(data = summary_data, aes(
    y = response_variable, x = 1.1, 
    label = paste0("RR<1: ", round(RR_Less_1), "%\nRR>1: ", round(RR_Greater_1), "%\n[N=", Studies, ", NO=", Observations, "]")
  ), size = 3, hjust = 0.5) +
  # Apply custom colors
  scale_fill_manual(values = custom_colors) +
  # Customize plot
  scale_x_continuous(trans = "log", breaks = scales::log_breaks(base = 10)) +
  labs(
    title = "Weighted Mean Response Ratio",
    subtitle = "Agroforestry vs. Non-Agroforestry Effects by Response Variable",
    x = "Response Ratio (Log Scale)",
    y = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    legend.position = "none"
  )

```


##########################################################################
# Step 5: Cap Imputed Values
cap_values <- function(data, lower_quantile = 0.05, upper_quantile = 0.95) {
  quantiles <- data |> summarise(
    silvo_se_lower = quantile(silvo_se, lower_quantile, na.rm = TRUE),
    silvo_se_upper = quantile(silvo_se, upper_quantile, na.rm = TRUE),
    control_se_lower = quantile(control_se, lower_quantile, na.rm = TRUE),
    control_se_upper = quantile(control_se, upper_quantile, na.rm = TRUE)
  )
  
  data <- data |> mutate(
    silvo_se = pmin(pmax(silvo_se, quantiles$silvo_se_lower), quantiles$silvo_se_upper),
    control_se = pmin(pmax(control_se, quantiles$control_se_lower), quantiles$control_se_upper)
  )
  
  return(data)
}

# Apply capping to each imputed dataset
for (method_name in names(imputed_datasets)) {
  cat("Capping values for", method_name, "dataset...\n")
  imputed_datasets[[method_name]] <- cap_values(imputed_datasets[[method_name]])
}



```{r}
# Function to create pre- and post-capping datasets for visualization
compare_capping <- function(original_data, capped_data, variable_name) {
  bind_rows(
    original_data |> select(!!sym(variable_name)) |> mutate(stage = "Original"),
    capped_data |> select(!!sym(variable_name)) |> mutate(stage = "Capped")
  )
}

# Visualize pre- and post-capping for a single variable
visualize_capping <- function(data, variable_name) {
  ggplot(data, aes(x = stage, y = !!sym(variable_name), fill = stage)) +
    geom_boxplot(outlier.color = "red", alpha = 0.6) +
    labs(
      title = paste("Capping Effect on", variable_name),
      x = "Stage",
      y = variable_name
    ) +
    theme_minimal()
}

# Example for silvo_se and control_se
for (method_name in names(imputed_datasets)) {
  original_data <- col_for_impute
  capped_data <- imputed_datasets[[method_name]]
  
  # Create comparison datasets
  silvo_se_data <- compare_capping(original_data, capped_data, "silvo_se")
  control_se_data <- compare_capping(original_data, capped_data, "control_se")
  
  # Plot silvo_se
  cat("Visualizing capping for", method_name, "- silvo_se\n")
  print(visualize_capping(silvo_se_data, "silvo_se"))
  
  # Plot control_se
  cat("Visualizing capping for", method_name, "- control_se\n")
  print(visualize_capping(control_se_data, "control_se"))
}

```
```{r}
# Function to identify capped observations


```


```{r}
# Create a formatted table using `gt`
comparison_gt <- prepared_data_gt %>%
  # Reorder the columns in the data frame before passing to gt
  select(
    metric, category, original, 
    linear_imputation,
    linear_imputation_relative,
    mean_imputation, 
    mean_imputation_relative,
    upper_quartile, 
    upper_quartile_relative, 
    bayesian,
    bayesian_relative,
    pmm, 
    pmm_relative,  
    pmm_best,  
    pmm_best_relative,
    rf,
    rf_relative
  ) |> 
  # Set `metric` as row names
  gt(rowname_col = "metric") %>%  
  tab_header(
    title = "Comparison of Imputation Methods Across Metrics",
    subtitle = "Including Original Data and Relative Differences"
  ) %>%
  # Rename columns for clarity
  cols_label(
    original = "Original",
    linear_imputation = "Linear Imputation",
    linear_imputation_relative = "Linear Imputation Relative",
    mean_imputation = "Mean Imputation",
    mean_imputation_relative = "Mean Imputation Relative",
    upper_quartile = "Upper Quartile",
    upper_quartile_relative = "Upper Quartile Relative",
    bayesian = "Bayesian",
    bayesian_relative = "Bayesian Relative",
    pmm = "PMM",
    pmm_relative = "PMM Relative",
    pmm_best = "PMM Best",
    pmm_best_relative = "PMM Best Relative",
    rf = "Random Forest",
    rf_relative = "Random Forest Relative"
  ) %>%
  # Format numeric columns to two decimal places
  fmt_number(
    columns = c(pmm, upper_quartile, mean_imputation, pmm_best, original),
    decimals = 2
  ) %>%
  fmt_number(
    columns = ends_with("_relative"),  # Format relative difference columns
    decimals = 3
  ) %>%
  # Replace 0 values in the relative columns with "NA"
  fmt_missing(
    columns = ends_with("_relative"),
    missing_text = "NA"
  ) %>%
  # Add horizontal and vertical lines for clarity
  tab_style(
    style = list(
      cell_fill(color = "#f9f9f9"),
      cell_borders(sides = "all", color = "gray", weight = px(1))
    ),
    locations = cells_body()
  ) %>%
  # Adjust table options
  tab_options(
    table.font.size = "small",
    table.border.top.color = "gray",
    table.border.bottom.color = "gray",
    column_labels.font.size = "medium",
    row_group.font.size = "small"
  ) %>%
  # Add footnotes for explanation of relative differences
  tab_footnote(
    footnote = "Relative differences are calculated as |(imputed - original) / original|.",
    locations = cells_column_labels(ends_with("_relative"))
  )

# Optionally, export the table
output_folder <- here("DATA", "OUTPUT_FROM_R")
gtsave(comparison_gt, file.path(output_folder, "comparison_table.html"))
gtsave(comparison_gt, file.path(output_folder, "comparison_table.pdf"))

# Display the table
comparison_gt

```



```{r}
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Step 1: Extract individual datasets for each response variable
simplified_model_data_crop_yield <- simplified_model_results$`Crop yield`$data
simplified_model_data_biodiversity <- simplified_model_results$Biodiversity$data
simplified_model_data_product_quality <- simplified_model_results$`Product quality`$data
simplified_model_data_soil_quality <- simplified_model_results$`Soil quality`$data
simplified_model_data_water_quality <- simplified_model_results$`Water quality`$data

# Combine all response variable datasets into one for the simplified model
simplified_model_data <- do.call(
  rbind,
  list(
    simplified_model_data_crop_yield,
    simplified_model_data_biodiversity,
    simplified_model_data_product_quality,
    simplified_model_data_soil_quality,
    simplified_model_data_water_quality
  )
)

# Ensure the merged dataset is structured properly
simplified_model_data <- as.data.frame(simplified_model_data)

# Step 2: Repeat for the full model
full_model_data_crop_yield <- full_model_results$`Crop yield`$data
full_model_data_biodiversity <- full_model_results$Biodiversity$data
full_model_data_product_quality <- full_model_results$`Product quality`$data
full_model_data_soil_quality <- full_model_results$`Soil quality`$data
full_model_data_water_quality <- full_model_results$`Water quality`$data

# Combine all response variable datasets into one for the full model
full_model_data <- do.call(
  rbind,
  list(
    full_model_data_crop_yield,
    full_model_data_biodiversity,
    full_model_data_product_quality,
    full_model_data_soil_quality,
    full_model_data_water_quality
  )
)

# Ensure the merged dataset is structured properly
full_model_data <- as.data.frame(full_model_data)
```
```{r}

# Check the structure of the final datasets
str(simplified_model_data)
str(full_model_data)



# Refit the "simplified" model using ML
simplified_ml <- rma.mv(
  yi = yi,
  V = simplified_model_data$vi,  # Replace with the correct variance structure
  mods = ~ tree_type + crop_type + age_system + season + soil_texture,  # Simplified model moderators
  random = ~ 1 | exp_id,          # Random effects structure
  data = simplified_model_data,   # Dataset for simplified model
  method = "ML"                   # Maximum Likelihood method
)


# Refit the "full" model using ML
full_ml <- rma.mv(
  yi = yi,
  V = full_model_data$vi,         # Replace with the correct variance structure
  mods = ~ tree_type * crop_type * age_system * season * soil_texture,  # Full model with interactions
  random = ~ 1 | exp_id,          # Random effects structure
  data = full_model_data,         # Dataset for full model
  method = "ML"                   # Maximum Likelihood method
)

# Perform a likelihood ratio test to compare the models
lrt <- anova(simplified_ml, full_ml)
print(lrt)

# Interpret the results
if (lrt$pval < 0.05) {
  cat("The full model significantly improves the fit compared to the simplified model (p =", lrt$pval, ").\n")
} else {
  cat("The simplified model is sufficient; the full model does not significantly improve the fit (p =", lrt$pval, ").\n")
}

```










```{r}
##########################################################################
# Extract AIC Values for Each Response Variable (Handling NULLs)
##########################################################################

# Initialize a data frame to store AIC results
aic_results <- data.frame(
  Response = character(),
  Null = numeric(),
  Minimal = numeric(),
  Fixed = numeric(),
  Simplified = numeric(),
  Full = numeric(),
  BestModel = character(),
  stringsAsFactors = FALSE
)

# Loop through each response variable
for (response in names(model_results)) {
  cat("\nExtracting AIC for response variable:", response, "\n")
  
  # Retrieve models for the current response variable
  models <- model_results[[response]]
  null <- models$null
  minimal <- models$minimal
  fixed <- models$fixed
  simplified <- models$simplified
  full <- models$full
  
  # Extract AIC values, replacing NULL with NA
  aic_values <- c(
    Null = if (!is.null(null)) null$aic else NA,
    Minimal = if (!is.null(minimal)) minimal$aic else NA,
    Fixed = if (!is.null(fixed)) fixed$aic else NA,
    Simplified = if (!is.null(simplified)) simplified$aic else NA,
    Full = if (!is.null(full)) full$aic else NA
  )
  
  # Check if all AIC values are NA; skip this response variable if true
  if (all(is.na(aic_values))) {
    cat("All models are missing for response variable:", response, "\n")
    next
  }
  
  # Determine the best model based on the lowest AIC
  best_model <- if (!all(is.na(aic_values))) {
    names(aic_values)[which.min(aic_values)]
  } else {
    NA
  }
  
  # Append results to the data frame
  aic_results <- rbind(
    aic_results,
    data.frame(
      Response = response,
      Null = aic_values["Null"],
      Minimal = aic_values["Minimal"],
      Fixed = aic_values["Fixed"],
      Simplified = aic_values["Simplified"],
      Full = aic_values["Full"],
      BestModel = best_model,
      stringsAsFactors = FALSE
    )
  )
}

##########################################################################
# Save AIC Results to File
##########################################################################
write.csv(aic_results, "aic_results.csv", row.names = FALSE)
cat("AIC results saved to 'aic_results.csv'\n")

##########################################################################
# Print AIC Results
##########################################################################
print(aic_results)


```





```{r}
# meta_analysis_model_comparisons |> str()
# meta_analysis_model_comparisons$Biodiversity$LR_Full_vs_Fixed |> str()

aic_results
```










```{r}
# Check if LRT statistics are present in each response variable's model object
lrt_presence <- data.frame(
  Response = character(),
  LRT_Statistic_Present = logical(),
  LRT_pval_Present = logical(),
  stringsAsFactors = FALSE
)

for (response in names(ml_comparison_results)) {
  result <- ml_comparison_results[[response]]
  
  if (!is.null(result) && !is.null(result$lrt)) {
    lrt_stat_present <- !is.null(result$lrt$statistic)
    lrt_pval_present <- !is.null(result$lrt$pval)
  } else {
    lrt_stat_present <- FALSE
    lrt_pval_present <- FALSE
  }
  
  lrt_presence <- rbind(
    lrt_presence,
    data.frame(
      Response = response,
      LRT_Statistic_Present = lrt_stat_present,
      LRT_pval_Present = lrt_pval_present,
      stringsAsFactors = FALSE
    )
  )
}

# Print results
print(lrt_presence)























##########################################################################################################################################
FOR INDIVIDUAL MODELS PER RESPONSE VARIABLE
##########################################################################################################################################
null = fit_null_model(data_subset, response),
minimal = fit_minimal_model(data_subset, response, v_matrix),
fixed = fit_fixed_effects_model(data_subset, response, v_matrix, moderators),
simplified = fit_simplified_model(data_subset, response, v_matrix, moderators),
full = fit_full_model(data_subset, response, v_matrix, moderators)



```{r}
##########################################################################################################################################
# Compare Models for Individual Response Variables (Extract AIC Values)
##########################################################################################################################################

# Initialize a data frame to store AIC results
aic_results <- data.frame(
  Response = character(),
  Null = numeric(),
  Minimal = numeric(),
  Fixed = numeric(),
  Simplified = numeric(),
  Full = numeric(),
  BestModel = character(),
  stringsAsFactors = FALSE
)

# Loop through each response variable to extract and compare AIC values
for (response in names(model_results)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Retrieve models for the current response variable
  models <- model_results[[response]]
  null_model <- models$null
  minimal_model <- models$minimal
  fixed_model <- models$fixed
  simplified_model <- models$simplified
  full_model <- models$full
  
  # Extract AIC values, replacing NULL with NA for missing models
  aic_values <- c(
    Null = if (!is.null(null_model)) null_model$aic else NA,
    Minimal = if (!is.null(minimal_model)) minimal_model$aic else NA,
    Fixed = if (!is.null(fixed_model)) fixed_model$aic else NA,
    Simplified = if (!is.null(simplified_model)) simplified_model$aic else NA,
    Full = if (!is.null(full_model)) full_model$aic else NA
  )
  
  # Skip this response variable if all AIC values are missing
  if (all(is.na(aic_values))) {
    cat("All models are missing for response variable:", response, "\n")
    next
  }
  
  # Determine the best model based on the lowest AIC
  best_model <- names(aic_values)[which.min(aic_values, na.rm = TRUE)]
  
  # Append the extracted AIC values and the best model to the results data frame
  aic_results <- rbind(
    aic_results,
    data.frame(
      Response = response,
      Null = aic_values["Null"],
      Minimal = aic_values["Minimal"],
      Fixed = aic_values["Fixed"],
      Simplified = aic_values["Simplified"],
      Full = aic_values["Full"],
      BestModel = best_model,
      stringsAsFactors = FALSE
    )
  )
}
```


```{r}
##########################################################################
# Systematic Comparison of Models for Each Response Variable
##########################################################################

# Extract model comparison data
comparison_results <- lapply(names(meta_analysis_model_comparisons), function(response) {
  models <- meta_analysis_model_comparisons[[response]]
  
  if (!is.null(models)) {
    list(
      Response = response,
      AIC_Full = if (!is.null(models$AIC)) models$AIC["Full"] else NA,
      AIC_Simplified = if (!is.null(models$AIC)) models$AIC["Simplified"] else NA,
      AIC_Minimal = if (!is.null(models$AIC)) models$AIC["Minimal"] else NA,
      AIC_Fixed = if (!is.null(models$AIC)) models$AIC["Fixed Effects Only"] else NA,
      LRT_Full_vs_Simplified = if (!is.null(models$LR_Full_vs_Simplified)) models$LR_Full_vs_Simplified$pval else NA,
      LRT_Full_vs_Minimal = if (!is.null(models$LR_Full_vs_Minimal)) models$LR_Full_vs_Minimal$pval else NA,
      LRT_Full_vs_Fixed = if (!is.null(models$LR_Full_vs_Fixed)) models$LR_Full_vs_Fixed$pval else NA,
      QE_Full = if (!is.null(models$Full_vs_Fixed_Details)) models$Full_vs_Fixed_Details$ResidualQEFull else NA,
      QE_Fixed = if (!is.null(models$Full_vs_Fixed_Details)) models$Full_vs_Fixed_Details$ResidualQEReduced else NA
    )
  } else {
    NULL
  }
})

# Filter out NULL elements and convert to a data frame
comparison_df <- do.call(rbind, lapply(comparison_results, function(x) {
  if (!is.null(x)) {
    data.frame(x, stringsAsFactors = FALSE)
  }
}))

comparison_df |> str()
```

##########################################################################
# Visualization of Model Comparisons
##########################################################################


```{r}
# Plot AIC Comparison Across Models
aic_plot <- ggplot(comparison_df, aes(x = Response)) +
  geom_bar(aes(y = AIC_Full, fill = "Full Model"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = AIC_Simplified, fill = "Simplified Model"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = AIC_Minimal, fill = "Minimal Model"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = AIC_Fixed, fill = "Fixed Effects Only"), stat = "identity", position = "dodge") +
  labs(title = "AIC Comparison Across Models", x = "Response Variable", y = "AIC") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(aic_plot)
```
```{r}
# Plot Residual Variance (QE) Comparison
qe_plot <- ggplot(comparison_df, aes(x = Response)) +
  geom_point(aes(y = QE_Full, color = "Full Model"), size = 3) +
  geom_point(aes(y = QE_Fixed, color = "Fixed Effects Only"), size = 3) +
  labs(title = "Residual Variance (QE) Comparison", x = "Response Variable", y = "Residual Variance") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(qe_plot)
```
```{r}
# Plot Likelihood Ratio Test (LRT) P-Values
lrt_plot <- ggplot(comparison_df, aes(x = Response)) +
  geom_point(aes(y = -log10(LRT_Full_vs_Simplified), color = "Full vs Simplified"), size = 3) +
  geom_point(aes(y = -log10(LRT_Full_vs_Minimal), color = "Full vs Minimal"), size = 3) +
  geom_point(aes(y = -log10(LRT_Full_vs_Fixed), color = "Full vs Fixed"), size = 3) +
  geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "red") +
  labs(title = "Likelihood Ratio Test P-Values (Log10 Scale)", x = "Response Variable", y = "-log10(P-Value)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

lrt_plot
```

```{r}
##########################################################################
# Interpretation and Next Steps
##########################################################################
# - Use AIC and BIC values to identify the best model for each response variable.
# - Compare QE values to assess residual variance.
# - Analyze LRT p-values to determine significant differences between models.
# - Select the simplest model that performs well based on these metrics.

```






























```{r}
lapply(names(full_model_results), function(response) {
  message("Inspecting response variable: ", response)
  full_model <- full_model_results[[response]]
  simplified_model <- simplified_model_results[[response]]
  minimal_model <- minimal_model_results[[response]]
  fixed_model <- fixed_effects_model_results[[response]]
  
  message("Full model length: ", length(full_model$yi))
  message("Simplified model length: ", length(simplified_model$yi))
  message("Minimal model length: ", length(minimal_model$yi))
  message("Fixed model length: ", length(fixed_model$yi))
})
```

```{r}
full_model <- full_model_results[[response]]
simplified_model <- simplified_model_results[[response]]
minimal_model <- minimal_model_results[[response]]
fixed_model <- fixed_effects_model_results[[response]]

# Run ANOVA
simplified_anova <- anova(full_model, simplified_model)
minimal_anova <- anova(full_model, minimal_model)
fixed_anova <- anova(full_model, fixed_model)

simplified_anova |> str()
minimal_anova |> str()
fixed_anova |> str()

simplified_aic_full <- simplified_anova[[1]]
simplified_aic_reduced <- simplified_anova[[2]]

simplified_aic_full
simplified_aic_reduced

simplified_aic_comparison <- (simplified_aic_full - simplified_aic_reduced)
simplified_aic_comparison_aic <- simplified_aic_comparison["AIC"]

simplified_aic_comparison_aic

# Visualize!
```

```{r}
# Define a function to process ANOVA results and extract AIC differences
process_anova <- function(full_model, other_model) {
  anova_result <- anova(full_model, other_model)
  
  # Extract AIC values for full and reduced models
  aic_full <- anova_result$fit.stats.f["AIC"]
  aic_reduced <- anova_result$fit.stats.r["AIC"]
  
  # Compute the AIC difference
  aic_diff <- aic_full - aic_reduced
  
  # Return the difference
  return(aic_diff)
}

# Apply the function to all models
simplified_aic_diff <- process_anova(full_model, simplified_model)
minimal_aic_diff <- process_anova(full_model, minimal_model)
fixed_aic_diff <- process_anova(full_model, fixed_model)

# Print the results
cat("Simplified AIC Difference:", simplified_aic_diff, "\n")
cat("Minimal AIC Difference:", minimal_aic_diff, "\n")
cat("Fixed AIC Difference:", fixed_aic_diff, "\n")
```

```{r}
# Prepare data for visualization
aic_data <- data.frame(
  Model = c("Simplified", "Minimal", "Fixed"),
  AIC_Difference = c(simplified_aic_diff, minimal_aic_diff, fixed_aic_diff)
)

# Create the bar plot
ggplot(aic_data, aes(x = Model, y = AIC_Difference, fill = Model)) +
  geom_bar(stat = "identity", width = 0.7) +
  labs(
    title = "AIC Differences Between Models",
    x = "Model Comparison",
    y = "AIC Difference"
  ) +
  scale_y_log10() +
  theme_minimal() +
  theme(legend.position = "none")

```


The AIC differences reveal important insights about how well each model fits the data compared to the full model. 

1. The **simplified model** has a small AIC difference of 3.69 compared to the full model. This indicates that while it simplifies the structure, it retains much of the explanatory power. The minimal reduction in fit suggests the simplified model could serve as a practical alternative, balancing complexity and accuracy.

2. In contrast, the **minimal model** shows a significant AIC difference of -57.58. This indicates a noticeable loss in explanatory power relative to the full model. While the minimal model removes many complexities, it sacrifices too much accuracy, making it unsuitable for capturing the relationships in the data effectively.

3. The **fixed-effects model** has the worst performance, with an AIC difference of -2792.28. This massive gap suggests that removing random effects entirely leads to a model that fails to account for essential variability in the data. The fixed-effects approach is overly simplistic and does not provide an adequate fit for the observed outcomes.

In summary, the **simplified model** offers a strong balance between simplicity and performance, while the minimal and fixed-effects models fail to retain the necessary explanatory power. The simplified model could be recommended when prioritizing model parsimony, but the full model remains the best choice when accuracy is the primary goal. This analysis highlights the trade-offs between model complexity and goodness-of-fit, underscoring the importance of selecting the right level of simplification for the data.



```{r}
meta_data |> glimpse()
```




# Create a faceted bar plot
heterogeneity_facet_plot <- ggplot(long_heterogeneity_data_adjusted, aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", alpha = 0.7) +
  scale_y_break(c(10000, 200000), scales = 0.5) +  # Add y-axis breaks
  facet_wrap(~ HeterogeneityType, scales = "free_y", ncol = 1) +  # Facet by Heterogeneity Type
  labs(
    title = "Partitioned Heterogeneity Across Models (Faceted View with Adjustments)",
    x = "Response Variable",
    y = "Heterogeneity Value",
    fill = "Model Type"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    strip.text = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 12)
  ) +
  scale_fill_brewer(palette = "Set2")

# Display the plot
print(heterogeneity_facet_plot)


```{r}
# Create individual plots for each heterogeneity type

################################################################################################################
# Total Heterogeneity
################################################################################################################
total_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "TotalHeterogeneity"),
                     aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", alpha = 0.7) +
  scale_y_break(c(8000, 200000), scales = 0.5) +  # Add y-axis breaks
  labs(
    title = "Total Heterogeneity",
    x = "Response Variable",
    y = "Heterogeneity Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "none"  # Remove legend from this plot
  ) +
  scale_fill_brewer(palette = "Set2")

################################################################################################################
# Explained Heterogeneity
################################################################################################################
explained_plot <- ggplot(
  long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ExplainedHeterogeneity"),
  aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)
) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", alpha = 0.7) +
  scale_y_break(c(5000, 1000000), scales = 0.5) +  # Adjusted breakpoints to match data range
  labs(
    title = "Explained Heterogeneity",
    x = "Response Variable",
    y = "Heterogeneity Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "none"  # Remove legend from this plot
  ) +
  scale_fill_brewer(palette = "Set2")

################################################################################################################
# Residual Heterogeneity
################################################################################################################
residual_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ResidualHeterogeneity"),
                        aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", alpha = 0.7) +
  scale_y_break(c(5000, 100000), scales = 0.5) +
  labs(
    title = "Residual Heterogeneity",
    x = "Response Variable",
    y = "Heterogeneity Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "none"  # Remove legend from this plot
  ) +
  scale_fill_brewer(palette = "Set2")

################################################################################################################
# Combine the plots into a single layout with a shared legend
################################################################################################################


```


```{r}
# Grouped Bar Chart: Total, Explained, and Residual Heterogeneity
heterogeneity_bar_plot <- ggplot(visualization_data, aes(x = ResponseVariable, fill = ModelType)) +
  geom_bar(aes(y = TotalHeterogeneity), stat = "identity", position = position_dodge(width = 0.8), color = "black", alpha = 0.7) +
  geom_bar(aes(y = ExplainedHeterogeneity), stat = "identity", position = position_dodge(width = 0.8), color = "black") +
  labs(
    title = "Total, Explained, and Residual Heterogeneity Across Models",
    x = "Response Variable",
    y = "Heterogeneity Value",
    fill = "Model Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

heterogeneity_bar_plot
```
























#############
# STEP 6
##########################################################################################################################################
PUBLICATION-READY PLOTS OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################

Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.










```{r}
# Load the saved models
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(dir, "fitted_models_null.rds"))
minimal_model_results <- readRDS(file.path(dir, "fitted_models_minimal.rds"))
fixed_effects_model_results <- readRDS(file.path(dir, "fitted_models_fixed_effects.rds"))
simplified_model_results <- readRDS(file.path(dir, "fitted_models_simplified.rds"))
full_model_results <- readRDS(file.path(dir, "fitted_models_full.rds"))

simplified_model_results |> str()
simplified_model_results |> glimpse()
```

```{r}
# WORKING ON THE SIMPLIFIED MODEL
mod_res <- simplified_model_results



# Combine results from all response variables into a single data frame
forest_plot_data <- bind_rows(
  lapply(names(mod_res), function(response) {
    model <- mod_res[[response]]
    
    if (!is.null(model) && !is.null(model$data)) {
      n_effects <- length(model$yi)
      n_studies <- nrow(model$data)
      
      # Ensure the lengths match or skip inconsistent data
      if (n_effects == n_studies) {
        data.frame(
          Study = model$data$id_article,                 # Study IDs
          EffectSize = model$yi,                        # Effect sizes
          CI_Lower = model$yi - 1.96 * sqrt(model$vi),  # Lower CI
          CI_Upper = model$yi + 1.96 * sqrt(model$vi),  # Upper CI
          ResponseVariable = response                   # Response variable
        )
      } else {
        warning(sprintf(
          "Skipping response variable '%s': Mismatched lengths between model$yi (%d) and model$data (%d).",
          response, n_effects, n_studies
        ))
        NULL
      }
    } else {
      warning(sprintf("Skipping response variable '%s': Missing model or data.", response))
      NULL
    }
  })
)

# Check the prepared data
forest_plot_data |> glimpse()
```

```{r}
# Prepare Aggregated Data
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

aggregated_data |> glimpse()
```

```{r}
# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#1b9e77",
  "Crop yield" = "#d95f02",
  "Pest and Disease" = "#7570b3",
  "Product quality" = "#e7298a",
  "Soil quality" = "#66a61e",
  "Water quality" = "#e6ab02"
)

# Create the forest plot with custom colors
forest_plot <- aggregated_data |> 
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank))) +
  # Add points for effect sizes
  geom_point(aes(size = size_category, color = ResponseVariable)) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci, color = ResponseVariable), height = 0.2) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot of Response Variables with Custom Colors",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Number of Studies"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Display the plot
forest_plot
```



```{r}
# Create the ridge plot with custom colors
library(ggridges)
ridge_plot <- forest_plot_data %>% 
  ggplot(aes(x = EffectSize, y = ResponseVariable, fill = ResponseVariable)) +
  geom_density_ridges(alpha = 0.7, scale = 0.8, rel_min_height = 0.02) +
  scale_fill_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  labs(
    title = "Enhanced Ridge Plot of Effect Sizes Across Response Variables",
    subtitle = "Distribution of effect sizes categorized by response variable",
    x = "Effect Size Estimate, Ratio of Means (ROM)",
    y = "Response Variable"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.text = element_text(size = 12),
    panel.grid.major = element_line(color = "gray", linetype = "dashed"),
    legend.position = "none"
  )

# Display the plot
ridge_plot
```

```{r}
simplified_model_results
```

```{r}
# Sample one observation per article per response variable
sampled_data <- imp_dataset %>%
  group_by(id_article, response_variable) %>%
  slice_head(n = 1)

sampled_data
```
```{r}
# Boxplot of effect sizes by response variable (unique articles)
boxplot_effec_size_response_variable <- sampled_data |> 
  ggplot(aes(x = response_variable, 
             y = yi, 
             fill = response_variable)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Boxplot of Effect Sizes (yi) by Response Variable (Unique Articles)",
       x = "Response Variable",
       y = "Effect Size (yi)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

boxplot_effec_size_response_variable
```


```{r}
simplified_model_results |> str()
```


```{r}
# Function to bootstrap effect sizes
bootstrap_effect_sizes <- function(yi, vi, n_bootstrap = 1000) {
  bootstrapped <- replicate(
    n_bootstrap,
    rnorm(length(yi), mean = yi, sd = sqrt(vi)),
    simplify = TRUE
  )
  as.data.frame(t(bootstrapped)) %>% 
    pivot_longer(cols = everything(), names_to = "id", values_to = "yi_boot")
}

# Combine all response variables
bootstrapped_results <- purrr::map_df(
  names(simplified_model_results), 
  function(response_var) {
    model <- simplified_model_results[[response_var]]
    if (!is.null(model)) {
      data.frame(
        response_variable = response_var,
        bootstrap_effect_sizes(
          yi = model$yi,
          vi = model$vi
        )
      )
    }
  }
)

# Summarize bootstrapped results
summary_results <- bootstrapped_results %>%
  group_by(response_variable) %>%
  summarise(
    mean = mean(yi_boot),
    median = median(yi_boot),
    lower_ci = quantile(yi_boot, 0.025),
    upper_ci = quantile(yi_boot, 0.975)
  )


bootstrapped_results |> glimpse()


# Plotting
modelled_res_bootstrapped_plot <- bootstrapped_results |> 
  ggplot(aes(x = response_variable, y = yi_boot, fill = response_variable)) +
  geom_violin() +
  stat_summary(fun = median, geom = "point", shape = 23, size = 2, color = "black") +
  scale_y_log10() + # Apply log scale to the y-axis
  theme_minimal() +
  labs(
    title = "Bootstrapped ROM Effect Sizes by Response Variable (Log Scale)",
    x = "Response Variable",
    y = "Effect Size (yi) - Log Scale"
  ) +
  theme(legend.position = "none")

modelled_res_bootstrapped_plot

```


```{r}
# Load the required library
library(metafor)

# Check data structure (already done in your example)
# str(imp_dataset)

# Define the meta-analysis model
# Here, we assume 'yi' represents the effect size and 'vi' the variance of effect sizes.
# Random effects model with robust variance estimation
robust_model <- rma.mv(
  yi = yi,                # Effect size measure (e.g., Log Response Ratio)
  V = vi,                 # Variance of effect sizes
  random = ~ 1 | exp_id,  # Random effects structure: exp_id
  data = imp_dataset,     # Your dataset
  method = "REML"         # Restricted Maximum Likelihood Estimation
)

# Display a summary of the model
summary(robust_model)

# Conduct tests for heterogeneity
# Q-test for heterogeneity
heterogeneity_test <- anova(robust_model)
print(heterogeneity_test)

# Visualize results: Forest plot
forest(robust_model, 
       slab = paste(imp_dataset$id_article, imp_dataset$response_variable), 
       xlab = "Effect Size", 
       header = "Study (Article) and Response Variable")

# Test for publication bias (e.g., funnel plot asymmetry)
# Funnel plot and publication bias
funnel(robust_model) # Visual inspection for symmetry

# Simplified regression model for publication bias
simpler_model <- rma.uni(yi, 
                         vi, 
                         data = imp_dataset, 
                         method = "REML")

regtest(simpler_model, model = "lm") # Test for asymmetry in simpler model

# Manual influence diagnostics
residuals <- residuals(robust_model, type = "rstandard") # Standardized residuals
leverage <- hatvalues(robust_model)                     # Leverage values
plot(residuals, leverage, main = "Residuals vs Leverage", xlab = "Residuals", ylab = "Leverage")

# Sensitivity analysis
# Define criteria for exclusion (e.g., standardized residuals > 2)
influential_points <- which(abs(residuals) > 2)
sensitivity_model <- update(robust_model, subset = -influential_points)
sensitivity_summary <- summary(sensitivity_model)
print(sensitivity_summary)

# Save outputs
capture.output(summary(sensitivity_model), file = "sensitivity_analysis_summary.txt")

# Leave-One-Out Analysis (Exclude one study at a time)
id_articles <- unique(imp_dataset$id_article) # Get unique study IDs

loo_results <- lapply(id_articles, function(article) {
  temp_data <- subset(imp_dataset, id_article != article) # Exclude one study at a time
  temp_model <- tryCatch(
    rma.mv(
      yi = yi,                # Effect size measure
      V = vi,                 # Variance of effect sizes
      random = ~ 1 | exp_id,  # Adjusted random effects structure
      data = temp_data,       # Subset data excluding the study
      method = "REML"
    ),
    error = function(e) NA
  )
  if (!inherits(temp_model, "try-error") && !is.na(temp_model$beta[1])) {
    data.frame(Study = article, Beta = temp_model$beta[1], SE = temp_model$se[1], pval = temp_model$pval[1])
  } else {
    data.frame(Study = article, Beta = NA, SE = NA, pval = NA)
  }
})

# Combine results into a single data frame
loo_results_df <- do.call(rbind, loo_results)

# Display LOO results
loo_results_df

loo_results_df |> glimpse()

# Save Leave-One-Out results
write.csv(loo_results_df, "leave_one_out_results_quick.csv", row.names = FALSE)

# Visualize LOO results
plot(loo_results_df$Study, loo_results_df$Beta, 
     type = "b", pch = 19, 
     main = "Leave-One-Out Analysis", 
     xlab = "Excluded Study", 
     ylab = "Effect Size", 
     col = "blue")

# Visualize LOO results as a forest plot
forest(loo_results_df$Beta, 
       ci.lb = loo_results_df$Beta - 1.96 * loo_results_df$SE, 
       ci.ub = loo_results_df$Beta + 1.96 * loo_results_df$SE, 
       slab = paste("Study", loo_results_df$Study), 
       xlab = "Effect Size (Hedges's g)", 
       main = "Leave-One-Out Analysis", 
       refline = 0, 
       header = c("Omitted Study", "Effect Size with 95% CI"))
```
```{r}
imp_dataset |> glimpse()
```


```{r}
# Use your dataset (`imp_dataset`) for meta-analysis
# Meta-analysis using the `meta` package

# Load the required library
library(meta)

# Calculate correlation data and add it to the dataset
# Ensure both silvo and control groups are available
imp_dataset$correlation <- with(imp_dataset, {
  # Calculate correlation based on means and standard deviations
  numerator <- (silvo_mean - control_mean)^2
  denominator <- silvo_sd^2 + control_sd^2
  correlation <- numerator / (numerator + denominator)
  # Ensure correlation values are between -1 and 1
  pmax(pmin(correlation, 1), -1)
})

# Meta-analysis using the `meta` package

datcor <- metacor(
  cor = correlation,               # Effect size correlation from calculated column
  n = silvo_n + control_n,          # Total sample size: sum of silvo and control sample sizes
  data = imp_dataset,               # Your dataset
  studlab = paste(id_article),      # Study labels based on `id_article`
  method.tau = "REML",             # Restricted Maximum Likelihood Estimation
  comb.random = TRUE,               # Random-effects model
  comb.fixed = FALSE,               # Do not combine fixed-effects
  sm = "COR"                       # Effect size metric: Correlation
)

# Create forest plot using the `meta` package
meta::forest(datcor, print.I2 = FALSE)

# Display meta-analysis summary
print(datcor)

# Perform a leave-one-out analysis with `metafor`
dat <- escalc(
  measure = "ROM",               # Effect size metric: Ratio of Means
  yi = silvo_mean / control_mean,  # Effect size: ratio of silvo to control means
  vi = (silvo_sd / silvo_mean)^2 + (control_sd / control_mean)^2, # Variance: using delta method for ratios
  data = imp_dataset,              # Your dataset
  slab = paste(id_article, response_variable, sep = ", ")
)


res <- rma(yi, vi, data = dat)    # Random-effects meta-analysis
inf <- influence(res)             # Influence diagnostics

# Save Leave-One-Out results
saveRDS(inf, "influence_diagnostics_quick.rds")

print(inf)

inf |> str()



# Add slab column for influence diagnostics
dat$slab <- paste(dat$id_article, dat$response_variable, sep = ", ")

# Visualize Influence diagnostics 
# Extract diagnostic metrics for plotting
influence_data <- data.frame(
  Study = dat$slab,
  RStudent = inf$inf$rstudent,
  CookD = inf$inf$cook.d,
  Hat = inf$inf$hat
)

# Plot influence diagnostics
influence_plot <- influence_data |> 
  ggplot(aes(x = Study, y = RStudent)) +
  geom_point(aes(size = CookD, color = Hat)) +
  labs(
    title = "Influence Diagnostics for Meta-Analysis",
    x = "Study",
    y = "Studentized Residuals",
    size = "Cook's D",
    color = "Hat Values"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

influence_plot
```






# Influence diagnostics
inf_diag <- influence(robust_model)
print(inf_diag)
plot(inf_diag)

# Save outputs to a file
# Write model summary to a text file
capture.output(summary(robust_model), file = "meta_analysis_summary.txt")

# Optional: Sensitivity analysis
# Exclude influential points and re-run the model if needed
sensitivity_model <- update(robust_model, subset = !inf_diag$is.influential)
sensitivity_summary <- summary(sensitivity_model)
print(sensitivity_summary)

# Save sensitivity analysis summary
capture.output(sensitivity_summary, file = "sensitivity_analysis_summary.txt")

















####################################################################################################################################################
# Fit and Evaluate All Models
####################################################################################################################################################
model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  data_subset <- meta_data[meta_data$response_variable == response, ]
  v_matrix <- v_matrices[[response]]
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  model_results[[response]] <- list(
    null = fit_null_model(data_subset, response),
    minimal = fit_minimal_model(data_subset, response, v_matrix),
    fixed = fit_fixed_effects_model(data_subset, response, v_matrix, moderators),
    simplified = fit_simplified_model(data_subset, response, v_matrix, moderators),
    full = fit_full_model(data_subset, response, v_matrix, moderators)
  )
}

##########################################################################
# Save All Fitted Models
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(model_results, file = file.path(output_dir, "fitted_models_all.rds"))

cat("\nAll models have been saved successfully!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################










```{r}
# Load the library
library(orchaRd)
```

```{r}
simplified_model_results |> str()

# Inspect a sample model
model <- simplified_model_results[["Biodiversity"]]
if (!is.null(model)) {
  print(class(model))      # Should return "rma.mv"
  print(model)             # Summary of the model
} else {
  cat("Model for Biodiversity is NULL\n")
}
```
```{r}
# Generate orchard plots for each response variable
# library(orchaRd)

# Function to preprocess model results
preprocess_models <- function(models_list) {
  valid_models <- list()
  
  for (response in names(models_list)) {
    model <- models_list[[response]]
    if (!is.null(model) && inherits(model, "rma.mv")) {
      tryCatch({
        # Test mod_results function to ensure compatibility
        processed_model <- orchaRd::mod_results(model, mod = "1", group = "exp_id")
        valid_models[[response]] <- model
      }, error = function(e) {
        cat("Excluding model for response variable:", response, "- Error:", e$message, "\n")
      })
    } else {
      cat("Excluding model for response variable:", response, "- Invalid or NULL model.\n")
    }
  }
  
  return(valid_models)
}

# Preprocess models
simplified_model_results_clean <- preprocess_models(simplified_model_results)


for (response in names(simplified_model_results_clean)) {
  model <- simplified_model_results_clean[[response]]
  if (!is.null(model) && inherits(model, "rma.mv")) {
    cat("\nGenerating orchard plot for response variable:", response, "\n")
    tryCatch({
      # Process model results for orchard plotting
      processed_model <- orchaRd::mod_results(
        model,
        mod = "1",             # Overall effect size or specific moderator
        group = "exp_id"       # Grouping variable for random effects
      )
      
      # Generate orchard plot
      orchaRd::orchard_plot(
        processed_model,
        xlab = paste("Effect Size for", response),
        group = "exp_id",      # Grouping variable
        angle = 45             # Rotate labels for better readability
      )
    }, error = function(e) {
      cat("Error generating orchard plot for", response, ":", e$message, "\n")
    })
  } else {
    cat("Skipping response variable:", response, "- Invalid or NULL model.\n")
  }
}
```

```{r}
# Specify the response variable you want to process
response <- "Product quality"  # Replace with the response variable you want to plot

# Check if the model exists and is valid
if (!is.null(simplified_model_results_clean[[response]]) &&
    inherits(simplified_model_results_clean[[response]], "rma.mv")) {
  
  # Retrieve the model
  model <- simplified_model_results_clean[[response]]
  
  # Process the model for orchard plotting
  processed_model <- tryCatch({
    orchaRd::mod_results(
      model,
      mod = "1",             # Overall effect size or specific moderator
      group = "exp_id"       # Grouping variable for random effects
    )
  }, error = function(e) {
    cat("Error processing model for response variable:", response, "-", e$message, "\n")
    return(NULL)
  })
  
  # Check if the processed model is valid
  if (!is.null(processed_model)) {
    # Generate and display the orchard plot
    cat("\nGenerating orchard plot for response variable:", response, "\n")
    orchaRd::orchard_plot(
      processed_model,
      xlab = paste("Effect Size for", response),  # Label for the x-axis
      group = "exp_id",                          # Grouping variable
      angle = 45                                 # Rotate labels for better readability
    )
  } else {
    cat("Unable to generate orchard plot for response variable:", response, "\n")
  }
} else {
  cat("Model for response variable:", response, "is invalid or NULL.\n")
}
```

```{r}
# Filter or prepare the dataset if needed
meta_data <- imp_dataset %>%
  filter(!is.na(yi), !is.na(vi)) %>%   # Remove rows with missing effect size or variance
  mutate(response_variable = as.factor(response_variable)) |>  # Ensure it's a factor
  # Check and convert categorical moderators to factors
  mutate(across(
    c(tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width),
    as.factor
  ))

meta_data
```
```{r}
# Prepare the dataset
meta_data <- imp_dataset %>%
  filter(!is.na(yi), !is.na(vi)) %>%   # Remove rows with missing effect size or variance
  mutate(response_variable = as.factor(response_variable)) |>  # Ensure it's a factor
  mutate(across(
    c(tree_type, crop_type, age_system, season, soil_texture, no_tree_per_m, tree_height, alley_width),
    as.factor
  ))  # Convert categorical variables to factors

# Create a list to store models
models <- list()
response_levels <- unique(meta_data$response_variable)

for (response in response_levels) {
  sub_data <- meta_data %>% filter(response_variable == response)
  
  # Skip subgroups with insufficient levels
  if (any(sapply(sub_data[, c("tree_type", "crop_type", "age_system", "season", 
                              "soil_texture", "no_tree_per_m", "tree_height", "alley_width")], 
                 function(x) length(unique(x))) < 2)) {
    cat("Skipping", response, "- insufficient levels in one or more moderators\n")
    next
  }
  
  # Fit the meta-analytic model
  models[[response]] <- rma.mv(
    yi = yi,
    V = vi,
    mods = ~ tree_type + crop_type,
    random = ~ 1 | exp_id,  # Random effect for studies
    data = sub_data
  )
}


# Generate Orchard Plots
for (response in names(models)) {
  cat("Creating Orchard Plot for:", response, "\n")
  
  # Generate Orchard Plot
  plot <- orchard_plot(
    models[[response]],
    xlab = "Effect Size (Hedges' g)",
    group = "response_variable",
    angle = 45
  ) + ggplot2::ggtitle(paste("Orchard Plot for", response))
  
  # Print the plot
  print(plot)
}
```

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()


#######################################################################################
# Step 1: Check and enforce correct data types
#######################################################################################
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
#######################################################################################
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    #######################################################################################
    # Predictive Mean Matching (pmm)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "pmm",   # Imputed using predictive mean matching
      "control_se" = "pmm", # Imputed using predictive mean matching
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "upper_quartile") {
    #######################################################################################
    # Upper Quartile Imputation (uq)
    #######################################################################################
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
    
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    #######################################################################################
    # Mean Imputation (mean)
    #######################################################################################
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)
    
  } else if (method_name == "linear_imputation") {
    #######################################################################################
    # Linear Regression Imputation (lr)
    #######################################################################################
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), predict(lm(silvo_se ~ control_se + tree_age + crop_type, data = data, na.action = na.exclude)), silvo_se),
        control_se = ifelse(is.na(control_se), predict(lm(control_se ~ silvo_se + tree_age + crop_type, data = data, na.action = na.exclude)), control_se)
      )
    return(data)
    
  } else if (method_name == "rf") {
    #######################################################################################
    # Random Forest Imputation (rf)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "rf",   # Imputed using random forest
      "control_se" = "rf",   # Imputed using random forest
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "bayesian") {
    #######################################################################################
    # Bayesian Imputation (by)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "norm.nob",   # Imputed using Bayesian regression
      "control_se" = "norm.nob",   # Imputed using Bayesian regression
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
#######################################################################################
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation", "rf", "bayesian")
imputed_datasets <- list()

# Iterate through imputation methods
for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  
  tryCatch({
    if (method_name %in% c("pmm", "rf", "bayesian")) {
      imputed_mids <- impute_data(col_for_impute, method_name)
      
      # Save mids objects for diagnostics
      if (method_name == "pmm") imputed_datasets[["pmm"]] <- mice::complete(imputed_mids)
      if (method_name == "rf") imputed_datasets[["rf"]] <- mice::complete(imputed_mids)
      if (method_name == "bayesian") imputed_datasets[["bayesian"]] <- mice::complete(imputed_mids)
    } else {
      # Direct dataset modification for other methods
      imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
    }
  }, error = function(e) {
    cat("Error applying", method_name, "imputation:", e$message, "\n")
  })
}


#######################################################################################
# Step 4: Compare Results
#######################################################################################
# Summary of results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  if (!is.null(imputed_datasets[[method_name]])) {
    print(summary(imputed_datasets[[method_name]]))
  } else {
    cat("No data available for", method_name, "\n")
  }
}


##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed and capped datasets


##########################################################################
# Last run (04/01-25)
# Total time taken: 18.98046 secs

# Last run (05/01-25)
# Total time taken: 2.224936 mins 

# Last run (11/01-25)
# Total time taken: 2.632763 
```




```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
#######################################################################################
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
#######################################################################################
impute_data <- function(data, method_name) {
  if (method_name == "pmm") {
    #######################################################################################
    # Predictive Mean Matching (pmm)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "pmm",   # Imputed using predictive mean matching
      "control_se" = "pmm", # Imputed using predictive mean matching
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "upper_quartile") {
    #######################################################################################
    # Upper Quartile Imputation (uq)
    #######################################################################################
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
    
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "mean_imputation") {
    #######################################################################################
    # Mean Imputation (mean)
    #######################################################################################
    data <- data %>%
      mutate(
        silvo_se = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se),
        silvo_n = ifelse(is.na(silvo_n), mean(silvo_n, na.rm = TRUE), silvo_n),
        control_n = ifelse(is.na(control_n), mean(control_n, na.rm = TRUE), control_n)
      )
    return(data)
    
  } else if (method_name == "linear_imputation") {
    #######################################################################################
    # Linear Regression Imputation (lr)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "norm.nob",   # Imputed using linear regression
      "control_se" = "norm.nob",   # Imputed using linear regression
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "rf") {
    #######################################################################################
    # Random Forest Imputation (rf)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "rf",   # Imputed using random forest
      "control_se" = "rf",   # Imputed using random forest
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else if (method_name == "bayesian") {
    #######################################################################################
    # Bayesian Imputation (by)
    #######################################################################################
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "norm.nob",   # Imputed using Bayesian regression
      "control_se" = "norm.nob",   # Imputed using Bayesian regression
      "silvo_n" = "",            # Not imputed
      "control_n" = "",          # Not imputed
      "tree_age" = "",           # Not imputed
      "crop_type" = "",          # Not imputed
      "tree_type" = "",          # Not imputed
      "bioclim_sub_regions" = "",# Not imputed
      "experiment_year" = "",    # Not imputed
      "alley_width" = "",        # Not imputed
      "id_article" = "",          # Not imputed
      "id_obs" = "",             # Not imputed
      "treat_id" = "",           # Not imputed
      "exp_id" = ""              # Not imputed
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    return(imputed_mids)
    
  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
#######################################################################################
imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation", "rf", "bayesian")
imputed_datasets <- list()

# Separate storage for raw mids objects
imputed_mids_pmm <- NULL
imputed_mids_rf <- NULL
imputed_mids_bayesian <- NULL
imputed_mids_linear <- NULL

# Iterate through imputation methods
for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  
  tryCatch({
    if (method_name %in% c("pmm", "rf", "bayesian")) {
      imputed_mids <- impute_data(col_for_impute, method_name)
      
      # Save mids objects for diagnostics
      if (method_name == "pmm") imputed_mids_pmm <- imputed_mids
      if (method_name == "rf") imputed_mids_rf <- imputed_mids
      if (method_name == "bayesian") imputed_mids_bayesian <- imputed_mids
      if (method_name == "linear_imputation") imputed_mids_linear <- imputed_mids
      
      # Store completed dataset
      imputed_datasets[[method_name]] <- mice::complete(imputed_mids)
    } else {
      # Direct dataset modification for other methods
      imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
    }
  }, error = function(e) {
    cat("Error applying", method_name, "imputation:", e$message, "\n")
  })
}

# Summary of results
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  if (!is.null(imputed_datasets[[method_name]])) {
    print(summary(imputed_datasets[[method_name]]))
  } else {
    cat("No data available for", method_name, "\n")
  }
}

#######################################################################################
# Step 4: Compare Results
#######################################################################################
for (method_name in imputation_methods) {
  cat("\nSummary of Imputed Dataset -", method_name, ":\n")
  print(summary(imputed_datasets[[method_name]]))
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")

##########################################################################
# imputed_mids_pmm and imputed_mids_linear are the raw mids objects for PMM and linear regression respectively
# imputed_datasets contains completed and capped datasets


##########################################################################
# Last run (04/01-25)
# Total time taken: 18.98046 secs

# Last run (05/01-25)
# Total time taken: 2.224936 mins 

# Last run (11/01-25)
# Total time taken: 3.70053 mins
imputed_mids_linear
```


```{r}
# Define thresholds for high and non-high variance
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Create separate datasets
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold | vi < low_variance_threshold)

non_high_variance_data <- imp_data_rom %>%
  filter(!(vi > high_variance_threshold | vi < low_variance_threshold))

# Plot for high variance
high_variance_plot <- ggplot(high_variance_data, aes(x = id_article, y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "red", alpha = 0.7) +
  labs(title = "High Variance Observations", x = "Article ID", y = "Variance (vi)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  # Remove legend for better visualization
  theme(legend.position = "none")

# Plot for non-high variance
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = id_article, y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(title = "Non-High Variance Observations", x = "Article ID", y = "Variance (vi)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine plots side by side
combined_plot <- high_variance_plot + non_high_variance_plot + 
  plot_layout(ncol = 2)

# Display the combined plot
print(combined_plot)
```

```{r}
# Aggregate data by id_article
aggregated_high_variance <- high_variance_data %>%
  group_by(id_article, response_variable) %>%
  summarize(
    mean_vi = mean(vi, na.rm = TRUE),
    .groups = "drop"
  )

aggregated_non_high_variance <- non_high_variance_data %>%
  group_by(id_article, response_variable) %>%
  summarize(
    mean_vi = mean(vi, na.rm = TRUE),
    .groups = "drop"
  )

# Plot for high variance
high_variance_plot <- ggplot(aggregated_high_variance, aes(x = as.factor(id_article), y = mean_vi, fill = response_variable)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  labs(title = "High Variance Observations (Aggregated by Article)", x = "Article ID", y = "Mean Variance (vi)") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "right"
  )

# Plot for non-high variance
non_high_variance_plot <- ggplot(aggregated_non_high_variance, aes(x = as.factor(id_article), y = mean_vi, fill = response_variable)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  labs(title = "Non-High Variance Observations (Aggregated by Article)", x = "Article ID", y = "Mean Variance (vi)") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "right"
  )

# Combine plots side by side
combined_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2) & # Combine plots with patchwork
  theme(
    plot.margin = margin(10, 10, 10, 10),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )

# Display the combined plot
print(combined_plot)
```


```{r}
# Define thresholds for high and non-high variance
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Create separate datasets
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold | vi < low_variance_threshold)

non_high_variance_data <- imp_data_rom %>%
  filter(!(vi > high_variance_threshold | vi < low_variance_threshold))

# Plot for high variance
high_variance_plot <- ggplot(high_variance_data, aes(x = as.factor(id_obs), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "red", alpha = 0.7, outlier.size = 2) +
  geom_text_repel(
    aes(label = id_article), size = 3, color = "black", max.overlaps = Inf, show.legend = FALSE
  ) + # Add article labels only for high variance points
  labs(title = "High Variance Observations", x = "Observation ID", y = "Variance (vi)") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 90, hjust = 1), # Rotate x-axis labels for better visibility
    legend.position = "none"
  )

# Plot for non-high variance
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = as.factor(id_obs), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7, outlier.size = 1.5) +
  labs(title = "Non-High Variance Observations", x = "Observation ID", y = "Variance (vi)") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 90, hjust = 1), # Rotate x-axis labels for better visibility
    legend.position = "right"
  )

# Combine plots side by side
combined_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2) & # Combine plots with patchwork
  theme(
    plot.margin = margin(10, 10, 10, 10),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )

# Display the combined plot
print(combined_plot)
```


```{r}
# Define thresholds for high and non-high variance
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Create separate datasets
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold | vi < low_variance_threshold)

non_high_variance_data <- imp_data_rom %>%
  filter(!(vi > high_variance_threshold | vi < low_variance_threshold))


# Aggregate data at the article level
high_variance_aggregated <- high_variance_data %>%
  group_by(id_article, response_variable) %>%
  summarise(
    vi_median = median(vi, na.rm = TRUE),
    vi_iqr = IQR(vi, na.rm = TRUE),
    .groups = "drop"
  )

non_high_variance_aggregated <- non_high_variance_data %>%
  group_by(id_article, response_variable) %>%
  summarise(
    vi_median = median(vi, na.rm = TRUE),
    vi_iqr = IQR(vi, na.rm = TRUE),
    .groups = "drop"
  )

# Plot for high variance
high_variance_plot <- ggplot(high_variance_aggregated, aes(x = as.factor(id_article), y = vi_median, fill = response_variable)) +
  geom_col(position = "dodge", alpha = 0.8) +
  labs(title = "High Variance Observations (Aggregated)", x = "Article ID", y = "Median Variance (vi)") +
  scale_fill_viridis_d(option = "C", direction = -1) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Plot for non-high variance
non_high_variance_plot <- ggplot(non_high_variance_aggregated, aes(x = as.factor(id_article), y = vi_median, fill = response_variable)) +
  geom_col(position = "dodge", alpha = 0.8) +
  labs(title = "Non-High Variance Observations (Aggregated)", x = "Article ID", y = "Median Variance (vi)") +
  scale_fill_viridis_d(option = "C", direction = -1) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Combine plots side by side
combined_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2) &
  theme(
    plot.margin = margin(10, 10, 10, 10)
  )

# Display the combined plot
print(combined_plot)
```

```{r}
# High variance plot for individual observations
high_variance_plot <- ggplot(high_variance_data, aes(x = as.factor(id_article), y = vi, color = response_variable)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_text_repel(aes(label = id_obs), size = 3, max.overlaps = 10) + # Add labels for observation IDs
  labs(
    title = "High Variance Observations (Individual)",
    x = "Article ID",
    y = "Variance (vi)",
    color = "Response Variable"
  ) +
  scale_color_viridis_d(option = "C", direction = -1) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Non-high variance plot with aggregated boxplots
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = as.factor(id_article), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(
    title = "Non-High Variance Observations (Boxplots)",
    x = "Article ID",
    y = "Variance (vi)",
    fill = "Response Variable"
  ) +
  scale_fill_viridis_d(option = "C", direction = -1) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Combine the two plots
combined_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2) &
  theme(
    plot.margin = margin(10, 10, 10, 10)
  )

# Display the combined plot
print(combined_plot)

```
```{r}
# High variance plot for individual observations with jitter and pseudo-log scale
high_variance_plot <- ggplot(high_variance_data, aes(x = as.factor(id_article), y = vi, color = response_variable)) +
  geom_point(position = position_jitter(width = 0.2, height = 0), size = 3, alpha = 0.7) +
  geom_text_repel(
    aes(label = id_obs),
    position = position_jitter(width = 0.2, height = 0),
    size = 3,
    max.overlaps = Inf # Allow more overlaps to ensure all high variance points are labeled
  ) +
  labs(
    title = "High Variance Observations (Individual, Jittered)",
    x = "Article ID",
    y = "Variance (vi)",
    color = "Response Variable"
  ) +
  scale_color_viridis_d(option = "C", direction = -1) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.1, 1, 10, 30),
    labels = c("0", "0.1", "1", "10", "30")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Non-high variance plot with aggregated boxplots and pseudo-log scale
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = as.factor(id_article), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(
    title = "Non-High Variance Observations (Boxplots, Pseudo-Log)",
    x = "Article ID",
    y = "Variance (vi)",
    fill = "Response Variable"
  ) +
  scale_fill_viridis_d(option = "C", direction = -1) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.1, 1, 10, 30),
    labels = c("0", "0.1", "1", "10", "30")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Combine the two plots
combined_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2) &
  theme(
    plot.margin = margin(10, 10, 10, 10)
  )

# Display the combined plot
print(combined_plot)
```

# Modify the high-variance plot to increase label size
high_variance_plot <- high_variance_data %>%
  ggplot(aes(x = id_article, y = vi, color = response_variable)) +
  geom_jitter(width = 0.2, height = 0.2, alpha = 0.8, size = 3) +  # Jitter for points
  geom_text_repel(
    aes(label = id_obs),
    size = 10,                  # Increase label text size
    max.overlaps = Inf,         # Ensure all labels are shown
    box.padding = 0.5,          # Adjust padding around labels
    point.padding = 0.3
  ) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0.01, 0.1, 1, 10, 30),
    labels = c("0.01", "0.1", "1", "10", "30")
  ) +
  labs(
    title = "High Variance Observations (Individual, Jittered)",
    x = "Article ID",
    y = "Variance (vi) [pseudo-log transformed]"
  ) +
  scale_color_manual(values = global_palette) +
  theme_minimal(base_size = 50) +
  theme(
    plot.title = element_text(size = 150, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 100, face = "bold"),
    axis.text = element_text(size = 70),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 70),
    legend.text = element_text(size = 60),
    legend.title = element_text(size = 70, face = "bold"),
    legend.key.size = unit(2, "cm"),
    legend.position = "top"
  )

# Combine updated plots
combined_plot <- high_variance_plot + non_high_variance_plot + 
  plot_layout(ncol = 2)

# Save the updated plot
ggsave(
  filename = file.path(output_dir, "combined_plot_with_large_labels.png"),
  plot = combined_plot,
  width = 28, height = 16, dpi = 600,
  bg = "white"
)




##########################################################################################################################################
FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################


Protocol with Four Models Fit Meta-Analysis

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Five Suggested Models for Meta-Analysis

##########################################################################
# Model 1: Null Model (Intercept-Only, No Random Effects)
##########################################################################
fit_null_model <- function(data_subset, response_variable) {
  cat("\nFitting null model for response variable:", response_variable, "...\n")
  
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = diag(data_subset$vi),          # Variance matrix: diagonal from vi
      mods = ~ 1,                        # Intercept-only model
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # Restricted Maximum Likelihood estimation
      control = list(
        optimizer = "optim",             # Optimizer function
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum iterations
        rel.tol = 1e-8                   # Convergence tolerance
      )
    )
  }, error = function(e) {
    cat("Error in null model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Null model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 2: Minimal Random Effects Model (Intercept-Only)
##########################################################################
fit_minimal_model <- function(data_subset, response_variable, v_matrix) {
  cat("\nFitting minimal model for response variable:", response_variable, "...\n")
  
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = v_matrix,                      # Variance-covariance matrix
      mods = ~ 1,                        # Intercept-only model
      random = ~ 1 | exp_id,             # Random effect at the experiment level
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # Restricted Maximum Likelihood estimation
      control = list(
        optimizer = "optim",             # Optimizer function
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum iterations
        rel.tol = 1e-8                   # Convergence tolerance
      )
    )
  }, error = function(e) {
    cat("Error in minimal model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Minimal model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 3: Fixed Effects Only Model (With Moderators)
##########################################################################
fit_fixed_effects_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting fixed effects model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in fixed effects model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Fixed effects model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 4: Moderately Simplified Model
##########################################################################
fit_simplified_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting simplified model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = ~ 1 | exp_id,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in simplified model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Simplified model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 5: Comprehensive (Full) Model
##########################################################################
fit_full_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting full model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = list(
        ~ 1 | id_article/response_variable, 
        ~ 1 | exp_id
      ),
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in full model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Full model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

####################################################################################################################################################
# Fit and Evaluate All Models
####################################################################################################################################################
model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  data_subset <- meta_data[meta_data$response_variable == response, ]
  v_matrix <- v_matrices[[response]]
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  model_results[[response]] <- list(
    null = fit_null_model(data_subset, response),
    minimal = fit_minimal_model(data_subset, response, v_matrix),
    fixed = fit_fixed_effects_model(data_subset, response, v_matrix, moderators),
    simplified = fit_simplified_model(data_subset, response, v_matrix, moderators),
    full = fit_full_model(data_subset, response, v_matrix, moderators)
  )
}

##########################################################################
# Save All Fitted Models In One File
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(model_results, file = file.path(output_dir, "fitted_models_all.rds"))

cat("\nAll models have been saved successfully in a single file!\n")

##########################################################################
# Save All Fitted Models In Seperate Files
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(lapply(model_results, `[[`, "null"), file = file.path(output_dir, "fitted_models_null.rds"))
saveRDS(lapply(model_results, `[[`, "minimal"), file = file.path(output_dir, "fitted_models_minimal.rds"))
saveRDS(lapply(model_results, `[[`, "fixed"), file = file.path(output_dir, "fitted_models_fixed_effects.rds"))
saveRDS(lapply(model_results, `[[`, "simplified"), file = file.path(output_dir, "fitted_models_simplified.rds"))
saveRDS(lapply(model_results, `[[`, "full"), file = file.path(output_dir, "fitted_models_full.rds"))

cat("\nAll models have been saved successfully in seperate files!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (04/12-2024)
# Total time taken: 13.93442 secs
# Processing response variable: Biodiversity 
# Fitting null model for response variable: Biodiversity ...
# Null model fitting completed for response variable: Biodiversity .
# Fitting minimal model for response variable: Biodiversity ...
# Minimal model fitting completed for response variable: Biodiversity .
# Fitting fixed effects model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Fixed effects model fitting completed for response variable: Biodiversity .
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Simplified model fitting completed for response variable: Biodiversity .
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Full model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting null model for response variable: Greenhouse gas emission ...
# Null model fitting completed for response variable: Greenhouse gas emission .
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Minimal model fitting completed for response variable: Greenhouse gas emission .
# Fitting fixed effects model for response variable: Greenhouse gas emission ...
# Error in fixed effects model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Error in simplified model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Fitting full model for response variable: Greenhouse gas emission ...
# Error in full model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting null model for response variable: Product quality ...
# Null model fitting completed for response variable: Product quality .
# Fitting minimal model for response variable: Product quality ...
# Minimal model fitting completed for response variable: Product quality .
# Fitting fixed effects model for response variable: Product quality ...
# Fixed effects model fitting completed for response variable: Product quality .
# Fitting simplified model for response variable: Product quality ...
# Simplified model fitting completed for response variable: Product quality .
# Fitting full model for response variable: Product quality ...
# Full model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting null model for response variable: Crop yield ...
# Null model fitting completed for response variable: Crop yield .
# Fitting minimal model for response variable: Crop yield ...
# Minimal model fitting completed for response variable: Crop yield .
# Fitting fixed effects model for response variable: Crop yield ...
# Fixed effects model fitting completed for response variable: Crop yield .
# Fitting simplified model for response variable: Crop yield ...
# Simplified model fitting completed for response variable: Crop yield .
# Fitting full model for response variable: Crop yield ...
# Full model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting null model for response variable: Pest and Disease ...
# Null model fitting completed for response variable: Pest and Disease .
# Fitting minimal model for response variable: Pest and Disease ...
# Minimal model fitting completed for response variable: Pest and Disease .
# Fitting fixed effects model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Pest and Disease .
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Pest and Disease .
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting null model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Soil quality .
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Soil quality .
# Fitting fixed effects model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Soil quality .
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Soil quality .
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting null model for response variable: Water quality ...
# Null model fitting completed for response variable: Water quality .
# Fitting minimal model for response variable: Water quality ...
# Minimal model fitting completed for response variable: Water quality .
# Fitting fixed effects model for response variable: Water quality ...
# Error in fixed effects model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# Fitting simplified model for response variable: Water quality ...
# Error in simplified model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# Fitting full model for response variable: Water quality ...
# Error in full model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models have been saved successfully!
# Total time taken: 13.93442 secs

# Last go (04/12-2024)
# Total time taken: 22.32941 secs
# Processing response variable: Biodiversity 
# Fitting null model for response variable: Biodiversity ...
# Null model fitting completed for response variable: Biodiversity .
# Fitting minimal model for response variable: Biodiversity ...
# Minimal model fitting completed for response variable: Biodiversity .
# Fitting fixed effects model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Fixed effects model fitting completed for response variable: Biodiversity .
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Simplified model fitting completed for response variable: Biodiversity .
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Full model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting null model for response variable: Greenhouse gas emission ...
# Null model fitting completed for response variable: Greenhouse gas emission .
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Minimal model fitting completed for response variable: Greenhouse gas emission .
# Fitting fixed effects model for response variable: Greenhouse gas emission ...
# Error in fixed effects model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Error in simplified model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Fitting full model for response variable: Greenhouse gas emission ...
# Error in full model fitting for Greenhouse gas emission : contrasts can be applied only to factors with 2 or more levels 
# Processing response variable: Product quality 
# Fitting null model for response variable: Product quality ...
# Null model fitting completed for response variable: Product quality .
# Fitting minimal model for response variable: Product quality ...
# Minimal model fitting completed for response variable: Product quality .
# Fitting fixed effects model for response variable: Product quality ...
# Fixed effects model fitting completed for response variable: Product quality .
# Fitting simplified model for response variable: Product quality ...
# Simplified model fitting completed for response variable: Product quality .
# Fitting full model for response variable: Product quality ...
# Full model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting null model for response variable: Crop yield ...
# Null model fitting completed for response variable: Crop yield .
# Fitting minimal model for response variable: Crop yield ...
# Minimal model fitting completed for response variable: Crop yield .
# Fitting fixed effects model for response variable: Crop yield ...
# Fixed effects model fitting completed for response variable: Crop yield .
# Fitting simplified model for response variable: Crop yield ...
# Simplified model fitting completed for response variable: Crop yield .
# Fitting full model for response variable: Crop yield ...
# Full model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting null model for response variable: Pest and Disease ...
# Null model fitting completed for response variable: Pest and Disease .
# Fitting minimal model for response variable: Pest and Disease ...
# Minimal model fitting completed for response variable: Pest and Disease .
# Fitting fixed effects model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Pest and Disease .
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Pest and Disease .
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting null model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Soil quality .
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Soil quality .
# Fitting fixed effects model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Soil quality .
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Soil quality .
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting null model for response variable: Water quality ...
# Null model fitting completed for response variable: Water quality .
# Fitting minimal model for response variable: Water quality ...
# Minimal model fitting completed for response variable: Water quality .
# Fitting fixed effects model for response variable: Water quality ...
# Error in fixed effects model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# Fitting simplified model for response variable: Water quality ...
# Error in simplified model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# Fitting full model for response variable: Water quality ...
# Error in full model fitting for Water quality : contrasts can be applied only to factors with 2 or more levels 
# All models have been saved successfully in a single file!
# All models have been saved successfully in seperate files!
# Total time taken: 22.32941 secs

# Last go (05/12-2024)
# Processing response variable: Biodiversity 
# Fitting null model for response variable: Biodiversity ...
# Null model fitting completed for response variable: Biodiversity .
# Fitting minimal model for response variable: Biodiversity ...
# Minimal model fitting completed for response variable: Biodiversity .
# Fitting fixed effects model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Biodiversity .
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Biodiversity .
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting null model for response variable: Greenhouse gas emission ...
# Null model fitting completed for response variable: Greenhouse gas emission .
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Minimal model fitting completed for response variable: Greenhouse gas emission .
# Fitting fixed effects model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Greenhouse gas emission .
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Greenhouse gas emission .
# Fitting full model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting null model for response variable: Product quality ...
# Null model fitting completed for response variable: Product quality .
# Fitting minimal model for response variable: Product quality ...
# Minimal model fitting completed for response variable: Product quality .
# Fitting fixed effects model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Product quality .
# Fitting simplified model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Product quality .
# Fitting full model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting null model for response variable: Crop yield ...
# Null model fitting completed for response variable: Crop yield .
# Fitting minimal model for response variable: Crop yield ...
# Minimal model fitting completed for response variable: Crop yield .
# Fitting fixed effects model for response variable: Crop yield ...
# Fixed effects model fitting completed for response variable: Crop yield .
# Fitting simplified model for response variable: Crop yield ...
# Simplified model fitting completed for response variable: Crop yield .
# Fitting full model for response variable: Crop yield ...
# Full model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting null model for response variable: Pest and Disease ...
# Null model fitting completed for response variable: Pest and Disease .
# Fitting minimal model for response variable: Pest and Disease ...
# Minimal model fitting completed for response variable: Pest and Disease .
# Fitting fixed effects model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Pest and Disease .
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Pest and Disease .
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting null model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Soil quality .
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Soil quality .
# Fitting fixed effects model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Soil quality .
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Soil quality .
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting null model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Water quality .
# Fitting minimal model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Water quality .
# Fitting fixed effects model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Water quality .
# Fitting simplified model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Water quality
# Fitting full model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Full model fitting completed for response variable: Water quality .
# All models have been saved successfully in a single file!
# All models have been saved successfully in seperate files!
# Total time taken: 19.25503 

# Last go (12/01-2025)
# Processing response variable: Biodiversity 
# Fitting null model for response variable: Biodiversity ...
# Null model fitting completed for response variable: Biodiversity .
# Fitting minimal model for response variable: Biodiversity ...
# Minimal model fitting completed for response variable: Biodiversity .
# Fitting fixed effects model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Biodiversity .
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Biodiversity .
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting null model for response variable: Greenhouse gas emission ...
# Null model fitting completed for response variable: Greenhouse gas emission .
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Minimal model fitting completed for response variable: Greenhouse gas emission .
# Fitting fixed effects model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Greenhouse gas emission .
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Greenhouse gas emission .
# Fitting full model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting null model for response variable: Product quality ...
# Null model fitting completed for response variable: Product quality .
# Fitting minimal model for response variable: Product quality ...
# Minimal model fitting completed for response variable: Product quality .
# Fitting fixed effects model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Product quality .
# Fitting simplified model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Product quality .
# Fitting full model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting null model for response variable: Crop yield ...
# Null model fitting completed for response variable: Crop yield .
# Fitting minimal model for response variable: Crop yield ...
# Minimal model fitting completed for response variable: Crop yield .
# Fitting fixed effects model for response variable: Crop yield ...
# Fixed effects model fitting completed for response variable: Crop yield .
# Fitting simplified model for response variable: Crop yield ...
# Simplified model fitting completed for response variable: Crop yield .
# Fitting full model for response variable: Crop yield ...
# Full model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting null model for response variable: Pest and Disease ...
# Null model fitting completed for response variable: Pest and Disease .
# Fitting minimal model for response variable: Pest and Disease ...
# Minimal model fitting completed for response variable: Pest and Disease .
# Fitting fixed effects model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Pest and Disease .
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Pest and Disease .
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting null model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Soil quality .
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Soil quality .
# Fitting fixed effects model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Soil quality .
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Soil quality .
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting null model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Water quality .
# Fitting minimal model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Water quality .
# Fitting fixed effects model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Water quality .
# Fitting simplified model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Water quality .
# Fitting full model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Full model fitting completed for response variable: Water quality .
# All models have been saved successfully in a single file!
# All models have been saved successfully in seperate files!
# Total time taken: 19.71101 
```



```{r}
ggplot(fit_stats_results, aes(x = Response, y = AIC, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_point(
    data = best_models, 
    aes(x = Response, y = AIC, color = Model), 
    size = 5, shape = 18
  ) +
  facet_wrap(~ Model, scales = "free_y", nrow = 2) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Best Performing Models by AIC",
    x = "Response Variable",
    y = "AIC Value"
  )
```


Calculate standard deviations from standard errors and sample sizes only when _sd is missing

```{r}
# Calculate standard deviations from standard errors and sample sizes only when _sd is missing
database_clean_sd <- database_clean |>
  mutate(
    # Preserve existing silvo_sd and calculate only if missing
    silvo_sd = if_else(is.na(silvo_sd), silvo_se * sqrt(silvo_n), silvo_sd),
    
    # Preserve existing control_sd and calculate only if missing
    control_sd = if_else(is.na(control_sd), control_se * sqrt(control_n), control_sd),
    
    # Create separate columns for calculated standard deviations for better transparency
    silvo_sd_calculated = if_else(is.na(silvo_sd), silvo_se * sqrt(silvo_n), NA_real_),
    control_sd_calculated = if_else(is.na(control_sd), control_se * sqrt(control_n), NA_real_)
  )
```













##########################################################################################################################################
Workflow for Calculating _SD with Comprehensive Conditions
##########################################################################################################################################


# Workflow for Calculating _SD with Comprehensive Conditions













```{r}
# Imputed dataset
imp_pmm_best <- merged_data |> 
  # Modify the imp_pmm_best dataset before saving
  # Remove existing columns
  select(-c(control_se, silvo_se, control_se_original, silvo_se_original)) |>  
  rename(
    # Rename control_se_imputed to control_se
    control_se = control_se_imputed, 
    # Rename silvo_se_imputed to silvo_se
    silvo_se = silvo_se_imputed      
  ) |> 
  as.data.frame()|> 
  # RECALCALCULATE STANDARD DEVIATION FOR IMPUTED DATASET
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )


# Non-imputed dataset (remove geometry if necessary)
non_imp_dataset <- database_clean_sd |> 
  as.data.frame() |> 
  # RECALCALCULATE STANDARD DEVIATION 
  mutate(
    # Calculate standard deviation for silvo group
    silvo_sd = silvo_se * sqrt(silvo_n),
    # Calculate standard deviation for control group
    control_sd = control_se * sqrt(control_n)
  ) |> 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_se, silvo_sd, silvo_n, control_mean, control_se, control_sd, control_n
  )
```




```{r}
# Workflow for Calculating _SD with Comprehensive Conditions

# Step 1: Define the full workflow to calculate _SD with explicit handling of original _SE, imputed _SE, and weighted average CV.

# Load necessary libraries
library(dplyr)

# Step 2: Calculate the CV for studies that report SDs
calculate_cv <- function(data) {
  data %>%
    mutate(
      # Calculate CV for silvo group (when SD and mean are available)
      silvo_cv = ifelse(!is.na(silvo_sd) & silvo_mean > 0, silvo_sd / silvo_mean, NA),
      
      # Calculate CV for control group (when SD and mean are available)
      control_cv = ifelse(!is.na(control_sd) & control_mean > 0, control_sd / control_mean, NA)
    )
}

# Step 3: Calculate the weighted average (pooled) CV
calculate_pooled_cv <- function(data) {
  pooled_values <- data %>%
    filter(!is.na(silvo_cv) & !is.na(control_cv)) %>%
    summarise(
      # Weighted average CV for silvo group
      pooled_silvo_cv = sqrt(sum((silvo_cv^2) * silvo_n, na.rm = TRUE) / sum(silvo_n, na.rm = TRUE)),
      
      # Weighted average CV for control group
      pooled_control_cv = sqrt(sum((control_cv^2) * control_n, na.rm = TRUE) / sum(control_n, na.rm = TRUE))
    )
  return(pooled_values)
}

# Step 4: Apply conditional logic to calculate SD
calculate_sd <- function(data, pooled_cv) {
  data %>%
    mutate(
      # Calculate SD using original _SE values (if available)
      silvo_sd_from_se_original = ifelse(!is.na(silvo_se_original), silvo_se_original * sqrt(silvo_n), NA),
      control_sd_from_se_original = ifelse(!is.na(control_se_original), control_se_original * sqrt(control_n), NA),
      
      # Calculate SD using imputed _SE values (if available)
      silvo_sd_from_se_imputed = ifelse(is.na(silvo_se_original) & !is.na(silvo_se_imputed), silvo_se_imputed * sqrt(silvo_n), NA),
      control_sd_from_se_imputed = ifelse(is.na(control_se_original) & !is.na(control_se_imputed), control_se_imputed * sqrt(control_n), NA),
      
      # Calculate SD using pooled CV (for missing SDs)
      silvo_sd_from_weight_cv = ifelse(is.na(silvo_se_original) & is.na(silvo_se_imputed), pooled_cv$pooled_silvo_cv * silvo_mean, NA),
      control_sd_from_weight_cv = ifelse(is.na(control_se_original) & is.na(control_se_imputed), pooled_cv$pooled_control_cv * control_mean, NA),
      
      # Warning for completely missing SE values
      silvo_warning = ifelse(is.na(silvo_se_original) & is.na(silvo_se_imputed) & is.na(pooled_cv$pooled_silvo_cv), "Warning: Missing silvo SE values for this observation.", NA),
      control_warning = ifelse(is.na(control_se_original) & is.na(control_se_imputed) & is.na(pooled_cv$pooled_control_cv), "Warning: Missing control SE values for this observation.", NA)
    )
}

# Step 5: Workflow implementation
workflow <- function(data) {
  # Step 5a: Calculate CV
  data <- calculate_cv(data)
  
  # Step 5b: Calculate pooled CV
  pooled_cv <- calculate_pooled_cv(data)
  
  # Step 5c: Calculate SD using the comprehensive conditions
  data <- calculate_sd(data, pooled_cv)
  
  # Return the processed dataset
  return(data)
}

# Example usage
# Assuming `imp_pmm_best` is the dataset with necessary columns
processed_data <- workflow(imp_pmm_best)

# Glimpse the processed dataset
processed_data |> glimpse()

# Save the processed dataset
saveRDS(processed_data, file = "processed_data_with_sd.rds")

```









```{r}
# Modify the `imp_pmm_best` dataset to retain all necessary columns for the Workflow for Calculating _SD with Comprehensive Conditions

# Imputed dataset
imp_pmm_best <- merged_data |> 
  # Retain all required columns including original and imputed SE values
  select(
    id_article, id_obs, treat_id, exp_id,
    response_variable, sub_response_variable, location, final_lat, final_lon, exp_site_loc, experiment_year,
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    silvo_mean, silvo_se_original, silvo_se_imputed, silvo_sd,
    silvo_n, control_mean, control_se_original, control_se_imputed, control_sd, control_n
  ) |>  
  # Ensure column names are consistent and informative
  rename(
    control_se_original = control_se_original,
    control_se_imputed = control_se_imputed,
    silvo_se_original = silvo_se_original,
    silvo_se_imputed = silvo_se_imputed
  ) |> 
  # Convert to a data frame for compatibility
  as.data.frame() |> 
  # Rearrange columns for better readability
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative meta-analysis effect size info
    silvo_mean, silvo_se_original, silvo_se_imputed, silvo_sd, silvo_n,
    control_mean, control_se_original, control_se_imputed, control_sd, control_n
  )

# Confirm the structure of the modified dataset
imp_pmm_best |> glimpse()

# The dataset now retains original and imputed _SE values, as well as placeholders for _SD calculations,
# ensuring compatibility with the Workflow for Calculating _SD with Comprehensive Conditions.
```

```{r}
# Workflow for Calculating _SD with Comprehensive Conditions

# Step 1: Define the full workflow to calculate _SD with explicit handling of original _SE, imputed _SE, and weighted average CV.

# Step 2: Calculate the CV for studies that report SDs
calculate_cv <- function(data) {
  data %>%
    mutate(
      # Calculate CV for silvo group (when SD and mean are available)
      silvo_cv = ifelse(!is.na(silvo_sd) & silvo_mean > 0, silvo_sd / silvo_mean, NA),
      
      # Calculate CV for control group (when SD and mean are available)
      control_cv = ifelse(!is.na(control_sd) & control_mean > 0, control_sd / control_mean, NA)
    )
}

# Step 3: Calculate the weighted average (pooled) CV
calculate_pooled_cv <- function(data) {
  pooled_values <- data %>%
    filter(!is.na(silvo_cv) & !is.na(control_cv)) %>%
    summarise(
      # Weighted average CV for silvo group
      pooled_silvo_cv = sqrt(sum((silvo_cv^2) * silvo_n, na.rm = TRUE) / sum(silvo_n, na.rm = TRUE)),
      
      # Weighted average CV for control group
      pooled_control_cv = sqrt(sum((control_cv^2) * control_n, na.rm = TRUE) / sum(control_n, na.rm = TRUE))
    )
  return(pooled_values)
}

# Step 4: Apply conditional logic to calculate SD
calculate_sd <- function(data, pooled_cv) {
  data %>%
    mutate(
      # Calculate SD using original _SE values (if available)
      silvo_sd_from_se_original = ifelse(!is.na(silvo_se_original), silvo_se_original * sqrt(silvo_n), NA),
      control_sd_from_se_original = ifelse(!is.na(control_se_original), control_se_original * sqrt(control_n), NA),
      
      # Calculate SD using imputed _SE values (if available)
      silvo_sd_from_se_imputed = ifelse(is.na(silvo_se_original) & !is.na(silvo_se_imputed), silvo_se_imputed * sqrt(silvo_n), NA),
      control_sd_from_se_imputed = ifelse(is.na(control_se_original) & !is.na(control_se_imputed), control_se_imputed * sqrt(control_n), NA),
      
      # Calculate SD using pooled CV (for missing SDs)
      silvo_sd_from_weight_cv = ifelse(is.na(silvo_se_original) & is.na(silvo_se_imputed), pooled_cv$pooled_silvo_cv * silvo_mean, NA),
      control_sd_from_weight_cv = ifelse(is.na(control_se_original) & is.na(control_se_imputed), pooled_cv$pooled_control_cv * control_mean, NA),
      
      # Warning for completely missing SE values
      silvo_warning = ifelse(is.na(silvo_se_original) & is.na(silvo_se_imputed) & is.na(pooled_cv$pooled_silvo_cv), "Warning: Missing silvo SE values for this observation.", NA),
      control_warning = ifelse(is.na(control_se_original) & is.na(control_se_imputed) & is.na(pooled_cv$pooled_control_cv), "Warning: Missing control SE values for this observation.", NA)
    )
}

# Step 5: Workflow implementation
workflow <- function(data) {
  # Step 5a: Calculate CV
  data <- calculate_cv(data)
  
  # Step 5b: Calculate pooled CV
  pooled_cv <- calculate_pooled_cv(data)
  
  # Step 5c: Calculate SD using the comprehensive conditions
  data <- calculate_sd(data, pooled_cv)
  
  # Return the processed dataset
  return(data)
}

# Example usage
# Assuming `imp_pmm_best` is the dataset with necessary columns
processed_data <- workflow(imp_pmm_best)

# Glimpse the processed dataset
processed_data |> glimpse()
```
```{r}
# Define the column groups
se_columns <- c("silvo_se_original", "silvo_se_imputed", "control_se_original", "control_se_imputed")
# Updated SD columns with weight-based CV
sd_columns <- c(
  "silvo_sd", "control_sd", 
  "silvo_sd_from_se_original", "control_sd_from_se_original", 
  "silvo_sd_from_se_imputed", "control_sd_from_se_imputed", 
  "silvo_sd_from_weight_cv", "control_sd_from_weight_cv"  
)

cv_columns <- c("silvo_cv", "control_cv")

# Function to pivot and plot density plots
plot_density <- function(data, columns, title, x_label) {
  data %>%
    select(all_of(columns)) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
    ggplot(aes(x = value, fill = variable)) +
    geom_density(alpha = 0.5) +
    labs(title = title, x = x_label, y = "Density") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_fill_discrete(name = "Variables")
}

# Density plot for SE variables
plot_density(processed_data, se_columns, "Density Plot of SE Variables", "SE Value")

# Density plot for SD variables
plot_density(processed_data, sd_columns, "Density Plot of SD Variables", "SD Value")

# Density plot for CV variables
plot_density(processed_data, cv_columns, "Density Plot of CV Variables", "CV Value")
```
```{r}
# Function to pivot and plot density plots with pseudo log scaling
plot_density_pseudo <- function(data, columns, title, x_label, y_label) {
  data %>%
    select(all_of(columns)) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
    ggplot(aes(x = value, fill = variable)) +
    geom_density(alpha = 0.5) +
    scale_x_continuous(trans = "pseudo_log") +  # Apply pseudo-log scaling to x-axis
    scale_y_continuous(trans = "pseudo_log") +  # Apply pseudo-log scaling to y-axis
    labs(title = title, x = x_label, y = y_label) +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_fill_discrete(name = "Variables")
}

# Density plot for SE variables with pseudo scaling
plot_density_pseudo(processed_data, se_columns, 
                    "Density Plot of SE Variables (Pseudo Log Scale)", 
                    "SE Value", "Density")

# Density plot for SD variables with pseudo scaling
plot_density_pseudo(processed_data, sd_columns, 
                    "Density Plot of SD Variables (Pseudo Log Scale)", 
                    "SD Value", "Density")

# Density plot for CV variables with pseudo scaling
plot_density_pseudo(processed_data, cv_columns, 
                    "Density Plot of CV Variables (Pseudo Log Scale)", 
                    "CV Value", "Density")

```









```{r}
# Step 4: Map Visualization of Studies
# Simplify the dataset
geo_data <- database_clean %>%
  select(lat, lon, exp_id) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

# Base map
world_map <- map_data("world")

# Plot the map with points
ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.3
  ) +
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = as.factor(exp_id)),
    size = 3, alpha = 0.7
  ) +
  scale_color_viridis_d() +
  labs(
    title = "Geographical Distribution of exp_id",
    x = "Longitude",
    y = "Latitude",
    color = "Experiment ID"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "right"
  )
```


Code for Calculating Meta-Analysis Quantitative Data

```{r}
# Dunctions for converting different measures of variability to SD - (however, in our data we only have SE)

# SE to SD
# SEtoSD <- function(SE, n) {
#   SE * sqrt(n)
# }

# # LSD to SD
# LSDtoSD <- function(LSD, n) {
#   LSD / (qt(0.975, n - 1)) * (sqrt(n) / sqrt(2))
# }
# 
# # CV to SD
# CVtoSD <- function(CV, mean) {
#   (CV / 100) * mean
# }
# 
# # MSE to SD
# MSEtoSD <- function(MSE) {
#   sqrt(MSE)
# }
```

```{r}
##########################################################################################################################################
# CALCULATING STANDARD DEVIATION FROM EXISTING STANDARD ERROR
##########################################################################################################################################

# Calculate standard deviations from standard errors and sample sizes only when _sd is FALSE or NA
# database_clean_sd <- database_clean |>
#   mutate(
#     # Calculate standard deviation for silvo group if silvo_sd is FALSE or NA
#     silvo_sd = if_else(
#       is.na(silvo_sd) | silvo_sd == FALSE,
#       silvo_se * sqrt(silvo_n),
#       silvo_sd  # Keep existing value if silvo_sd is TRUE
#     ),
#     
#     # Calculate standard deviation for control group if control_sd is FALSE or NA
#     control_sd = if_else(
#       is.na(control_sd) | control_sd == FALSE,
#       control_se * sqrt(control_n),
#       control_sd  # Keep existing value if control_sd is TRUE
#     )
#   )

```








```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()

#######################################################################################
# Step 1: Check and enforce correct data types
#######################################################################################
col_for_impute <- database_clean_sd |> 
  as.data.frame() |> 
  select(-geometry) |> 
  select(
    # Columns that need to be imputed
    silvo_se, control_se, 
    # Columns that are used by mice to impute values
    tree_age, crop_type, tree_type, bioclim_sub_regions, experiment_year, alley_width, silvo_n, control_n,
    # IDs that are used to back-link imputed values to the dataset
    id_article, id_obs, treat_id, exp_id
  ) |> 
  mutate(
    silvo_se = as.numeric(silvo_se),
    control_se = as.numeric(control_se),
    silvo_n = as.numeric(silvo_n),
    control_n = as.numeric(control_n),
    tree_age = as.numeric(tree_age),
    crop_type = as.factor(crop_type),
    tree_type = as.factor(tree_type),
    bioclim_sub_regions = as.factor(bioclim_sub_regions),
    alley_width = as.factor(alley_width),
    id_article = as.numeric(id_article),
    id_obs = as.numeric(id_obs),
    treat_id = as.numeric(treat_id),
    exp_id = as.numeric(exp_id)
  )

#######################################################################################
# Step 2: Define the function for each imputation method
#######################################################################################
impute_data <- function(data, method_name) {
  if (method_name == "mean_imputation") {
    data <- data %>%
      mutate(
        silvo_se_imputed = ifelse(is.na(silvo_se), mean(silvo_se, na.rm = TRUE), silvo_se),
        control_se_imputed = ifelse(is.na(control_se), mean(control_se, na.rm = TRUE), control_se)
      )
    return(data)
    
  } else if (method_name == "upper_quartile") {
    upper_quartile_variance <- data %>%
      summarise(across(c(silvo_se, control_se), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
      pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
    
    data <- data %>%
      mutate(
        silvo_se_imputed = ifelse(is.na(silvo_se), sqrt(upper_quartile_variance$upper_quartile[1]), silvo_se),
        control_se_imputed = ifelse(is.na(control_se), sqrt(upper_quartile_variance$upper_quartile[2]), control_se)
      )
    return(data)
    
  } else if (method_name == "linear_imputation") {
    data <- data %>%
      mutate(
        crop_type = as.numeric(as.factor(crop_type)),
        tree_type = as.numeric(as.factor(tree_type)),
        bioclim_sub_regions = as.numeric(as.factor(bioclim_sub_regions)),
        alley_width = as.numeric(as.factor(alley_width))
      )
    
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "norm.predict",   # Imputed using linear regression
      "control_se" = "norm.predict"   # Imputed using linear regression
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    
    completed_data <- mice::complete(imputed_mids)
    completed_data <- completed_data %>%
      mutate(
        silvo_se_imputed = completed_data$silvo_se,
        control_se_imputed = completed_data$control_se
      )
    return(completed_data)
    
  } else if (method_name == "bayesian") {
    data <- data %>%
      mutate(
        silvo_se = ifelse(silvo_se < 0, 0, silvo_se),
        control_se = ifelse(control_se < 0, 0, control_se)
      )
    
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "norm.nob",   # Imputed using Bayesian regression
      "control_se" = "norm.nob"   # Imputed using Bayesian regression
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    
    completed_data <- mice::complete(imputed_mids)
    completed_data <- completed_data %>%
      mutate(
        silvo_se_imputed = completed_data$silvo_se,
        control_se_imputed = completed_data$control_se
      )
    return(completed_data)
    
  } else if (method_name == "pmm") {
    pred_matrix <- mice::make.predictorMatrix(data)
    pred_matrix[, c("tree_age", "crop_type", "tree_type", "bioclim_sub_regions", "experiment_year", "alley_width", 
                    "id_article", "id_obs", "treat_id", "exp_id")] <- 0 
    
    method <- c(
      "silvo_se" = "pmm",   # Imputed using predictive mean matching
      "control_se" = "pmm"  # Imputed using predictive mean matching
    )
    
    imputed_mids <- mice(
      data,
      m = 20,
      maxit = 100,
      method = method,
      predictorMatrix = pred_matrix,
      seed = 1234,
      printFlag = FALSE
    )
    
    completed_data <- mice::complete(imputed_mids)
    completed_data <- completed_data %>%
      mutate(
        silvo_se_imputed = completed_data$silvo_se,
        control_se_imputed = completed_data$control_se
      )
    return(completed_data)
    
  } else {
    stop("Invalid method name.")
  }
}

#######################################################################################
# Step 3: Apply each imputation method
#######################################################################################
imputation_methods <- c("mean_imputation", "upper_quartile", "linear_imputation", "bayesian", "pmm")
imputed_datasets <- list()

# Iterate through imputation methods
for (method_name in imputation_methods) {
  cat("Applying", method_name, "imputation...\n")
  
  tryCatch({
    imputed_datasets[[method_name]] <- impute_data(col_for_impute, method_name)
  }, error = function(e) {
    cat("Error applying", method_name, "imputation:", e$message, "\n")
  })
}

#######################################################################################
# Step 4: Combine and Compare Results
#######################################################################################
imputed_summaries <- lapply(imputed_datasets, function(dataset) {
  dataset %>%
    summarise(
      missing_silvo_se = sum(is.na(silvo_se)),
      missing_control_se = sum(is.na(control_se)),
      imputed_silvo_se = sum(!is.na(silvo_se_imputed)),
      imputed_control_se = sum(!is.na(control_se_imputed))
    )
})

names(imputed_summaries) <- imputation_methods

for (method_name in imputation_methods) {
  cat("\nSummary for", method_name, ":\n")
  print(imputed_summaries[[method_name]])
}

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last run (04/01-25)
# Total time taken: 18.98046 secs

# Last run (05/01-25)
# Total time taken: 2.224936 mins 

# Last run (11/01-25)
# Total time taken: 3.70053 mins

# Last run (16/01-25)
```




















```{r}
##############
# MERGING IMPUTED DATA BACK TO THE ORIGINAL DATASET AND VISUALIZING
############################################################################
# Objective:
# Combine the original dataset (`database_clean_sd`) with the imputed dataset 
# (using the PMM imputation method, selected as the most robust approach).
# This allows for a comparison of original vs. imputed values and ensures clarity by keeping columns distinct.

# Merging process
merged_data <- database_clean_sd_df %>%
  full_join(
    # Selecting the PMM imputed dataset:
    imputed_datasets$upper_quartile %>%
      # Keep only relevant imputed columns
      select(id_article, id_obs, silvo_se, control_se), 
    # Merge based on unique identifiers
    by = c("id_article", "id_obs"), 
    # Suffix to distinguish original vs. imputed columns
    suffix = c("_original", "_imputed") 
  ) |> 
  # Relocate columns to the desired order
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, climate_zone, bioclim_sub_regions, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_sd, silvo_n, silvo_sd_from_se, silvo_sd_merged, 
    control_mean, control_sd, control_n, control_sd_from_se, control_sd_merged
  )

# Preview the structure of the merged dataset to ensure the merge was successful
glimpse(merged_data)

# Dataset Summary:
# - Rows: 1,126
# - Columns: Updated based on merged dataset

# Additional Context:
# - The PMM method was selected as the most robust imputation method based on:
#   - The lowest total relative differences (31.29%) across silvo and control SEs.
#   - Strong performance in Jensen-Shannon Divergence (JSD) values, with 0.08 for control SEs and 0.44 for silvo SEs, 
#     indicating excellent alignment with the original distribution.
#   - Consistent preservation of key metrics like variance, standard deviation, and range.
#   - A balanced approach that minimizes distortion while preserving the statistical properties of the original data.


# Note:
# The evaluations/assessments of the imputation methods, including relative differences, variance comparisons, 
# and Jensen-Shannon Divergence (JSD), are performed in the subsequent sections below. These assessments 
# substantiate the selection of the PMM method as the best approach for this dataset.

# The merged dataset is now ready for further analysis and visualization, allowing for transparent comparisons between the original and imputed values.
```







#############
# STEP 6
##########################################################################################################################################
SAVING TWO VERSIONS OF PREPROCESSED DATA FOR FURTHER ANALYSIS AND VISUALIZATION - RECALCULATION OF _SD WHERE THE ORIGINAL _SD IS MISSING
##########################################################################################################################################

```{r}
# Imputed dataset where _se is imputed and then subsequently used to calculate _sd
imp_pmm_best <- merged_data |> 
  as.data.frame() |> 
  # Modify the imp_pmm_best dataset before saving
  # Remove existing columns
  select(-c(control_se, silvo_se)) |>  
  rename(
    # Rename control_sd to control_sd_original
    control_sd_original = control_sd,
    # Rename silvo_sd to silvo_sd_original
    silvo_sd_original = silvo_sd,
    # Rename control_se_imputed to control_se
    control_se_imputed = control_se_imputed, 
    # Rename silvo_se_imputed to silvo_se
    silvo_se_imputed = silvo_se_imputed      
  ) |> 
  as.data.frame()|> 
  #################################################################################
# RECALCULATE STANDARD DEVIATION FOR IMPUTED DATASET WITH CONDITIONAL RULE
mutate(
  # Calculate standard deviation for silvo group only if silvo_sd_merged is NA
  silvo_sd_from_imputed_se = ifelse(is.na(silvo_sd_merged), silvo_se_imputed * sqrt(silvo_n), NA),
  # Calculate standard deviation for control group only if control_sd_merged is NA
  control_sd_from_imputed_se = ifelse(is.na(control_sd_merged), control_se_imputed * sqrt(control_n), NA),
) |> 
  # COMBINE _sd_final AND _sd_from_imputed_se WITH _sd_final TAKING PRECEDENCE
  mutate(
    silvo_sd_combined = ifelse(is.na(silvo_sd_merged), silvo_sd_from_imputed_se, silvo_sd_merged),
    control_sd_combined = ifelse(is.na(control_sd_merged), control_sd_from_imputed_se, control_sd_merged)
  ) |> 
  # Relocate columns to the desired order 
  relocate(
    # Overall ID info
    id_article, id_obs, treat_id, exp_id,
    # Response variable info
    response_variable, sub_response_variable,
    # Geographic and temporal info
    location, final_lat, final_lon, exp_site_loc, experiment_year,
    # Moderators info
    tree_type, crop_type, age_system, tree_age, season, soil_texture, no_tree_per_m, tree_height, alley_width,
    # Quantitative mata-analysis effect size info
    silvo_mean, silvo_n, silvo_se_original, silvo_sd_original, 
    silvo_sd_from_se, silvo_sd_merged, silvo_sd_from_imputed_se, silvo_sd_combined,
    control_mean, control_n, control_se_original, control_sd_original, 
    control_sd_from_se, control_sd_merged, control_sd_from_imputed_se, control_sd_combined, 
  )
```










##########################################################################################################################################
FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################


Protocol with Four Models Fit Meta-Analysis

```{r}
##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Five Suggested Models for Meta-Analysis

##########################################################################
# Model 1: Null Model (Intercept-Only, No Random Effects)
##########################################################################
fit_null_model <- function(data_subset, response_variable) {
  cat("\nFitting null model for response variable:", response_variable, "...\n")
  
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = diag(data_subset$vi),          # Variance matrix: diagonal from vi
      mods = ~ 1,                        # Intercept-only model
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # Restricted Maximum Likelihood estimation
      control = list(
        optimizer = "optim",             # Optimizer function
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum iterations
        rel.tol = 1e-8                   # Convergence tolerance
      )
    )
  }, error = function(e) {
    cat("Error in null model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Null model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 2: Minimal Random Effects Model (Intercept-Only)
##########################################################################
fit_minimal_model <- function(data_subset, response_variable, v_matrix) {
  cat("\nFitting minimal model for response variable:", response_variable, "...\n")
  
  model <- tryCatch({
    rma.mv(
      yi = yi,                           # Dependent variable: effect size
      V = v_matrix,                      # Variance-covariance matrix
      mods = ~ 1,                        # Intercept-only model
      random = ~ 1 | exp_id,             # Random effect at the experiment level
      data = data_subset,                # Data used for model fitting
      method = "REML",                   # Restricted Maximum Likelihood estimation
      control = list(
        optimizer = "optim",             # Optimizer function
        optim.method = "BFGS",           # Optimization algorithm
        iter.max = 1000,                 # Maximum iterations
        rel.tol = 1e-8                   # Convergence tolerance
      )
    )
  }, error = function(e) {
    cat("Error in minimal model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Minimal model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 3: Fixed Effects Only Model (With Moderators)
##########################################################################
fit_fixed_effects_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting fixed effects model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in fixed effects model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Fixed effects model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 4: Moderately Simplified Model
##########################################################################
fit_simplified_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting simplified model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = ~ 1 | exp_id,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in simplified model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Simplified model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################
# Model 5: Comprehensive (Full) Model
##########################################################################
fit_full_model <- function(data_subset, response_variable, v_matrix, moderators) {
  cat("\nFitting full model for response variable:", response_variable, "...\n")
  
  data_subset <- data_subset %>% mutate(across(all_of(moderators), as.factor)) %>% as.data.frame()
  moderator_formula <- as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
  
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = list(
        ~ 1 | id_article/response_variable, 
        ~ 1 | exp_id
      ),
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in full model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  if (!is.null(model)) {
    cat("Full model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

####################################################################################################################################################
# Fit and Evaluate All Models
####################################################################################################################################################
model_results <- list()
for (response in names(v_matrices)) {
  cat("\nProcessing response variable:", response, "\n")
  
  data_subset <- meta_data[meta_data$response_variable == response, ]
  v_matrix <- v_matrices[[response]]
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  model_results[[response]] <- list(
    null = fit_null_model(data_subset, response),
    minimal = fit_minimal_model(data_subset, response, v_matrix),
    fixed = fit_fixed_effects_model(data_subset, response, v_matrix, moderators),
    simplified = fit_simplified_model(data_subset, response, v_matrix, moderators),
    full = fit_full_model(data_subset, response, v_matrix, moderators)
  )
}

##########################################################################
# Save All Fitted Models In One File
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(model_results, file = file.path(output_dir, "fitted_models_all.rds"))

cat("\nAll models have been saved successfully in a single file!\n")

##########################################################################
# Save All Fitted Models In Seperate Files
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(lapply(model_results, `[[`, "null"), file = file.path(output_dir, "fitted_models_null.rds"))
saveRDS(lapply(model_results, `[[`, "minimal"), file = file.path(output_dir, "fitted_models_minimal.rds"))
saveRDS(lapply(model_results, `[[`, "fixed"), file = file.path(output_dir, "fitted_models_fixed_effects.rds"))
saveRDS(lapply(model_results, `[[`, "simplified"), file = file.path(output_dir, "fitted_models_simplified.rds"))
saveRDS(lapply(model_results, `[[`, "full"), file = file.path(output_dir, "fitted_models_full.rds"))

cat("\nAll models have been saved successfully in seperate files!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (12/01-2025)
# Processing response variable: Biodiversity 
# Fitting null model for response variable: Biodiversity ...
# Null model fitting completed for response variable: Biodiversity .
# Fitting minimal model for response variable: Biodiversity ...
# Minimal model fitting completed for response variable: Biodiversity .
# Fitting fixed effects model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Biodiversity .
# Fitting simplified model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Biodiversity .
# Fitting full model for response variable: Biodiversity ...
# Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Biodiversity .
# Processing response variable: Greenhouse gas emission 
# Fitting null model for response variable: Greenhouse gas emission ...
# Null model fitting completed for response variable: Greenhouse gas emission .
# Fitting minimal model for response variable: Greenhouse gas emission ...
# Minimal model fitting completed for response variable: Greenhouse gas emission .
# Fitting fixed effects model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Greenhouse gas emission .
# Fitting simplified model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Greenhouse gas emission .
# Fitting full model for response variable: Greenhouse gas emission ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Greenhouse gas emission .
# Processing response variable: Product quality 
# Fitting null model for response variable: Product quality ...
# Null model fitting completed for response variable: Product quality .
# Fitting minimal model for response variable: Product quality ...
# Minimal model fitting completed for response variable: Product quality .
# Fitting fixed effects model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Product quality .
# Fitting simplified model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Product quality .
# Fitting full model for response variable: Product quality ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Product quality .
# Processing response variable: Crop yield 
# Fitting null model for response variable: Crop yield ...
# Null model fitting completed for response variable: Crop yield .
# Fitting minimal model for response variable: Crop yield ...
# Minimal model fitting completed for response variable: Crop yield .
# Fitting fixed effects model for response variable: Crop yield ...
# Fixed effects model fitting completed for response variable: Crop yield .
# Fitting simplified model for response variable: Crop yield ...
# Simplified model fitting completed for response variable: Crop yield .
# Fitting full model for response variable: Crop yield ...
# Full model fitting completed for response variable: Crop yield .
# Processing response variable: Pest and Disease 
# Fitting null model for response variable: Pest and Disease ...
# Null model fitting completed for response variable: Pest and Disease .
# Fitting minimal model for response variable: Pest and Disease ...
# Minimal model fitting completed for response variable: Pest and Disease .
# Fitting fixed effects model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Pest and Disease .
# Fitting simplified model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Pest and Disease .
# Fitting full model for response variable: Pest and Disease ...
# Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Pest and Disease .
# Processing response variable: Soil quality 
# Fitting null model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Soil quality .
# Fitting minimal model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Soil quality .
# Fitting fixed effects model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Soil quality .
# Fitting simplified model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Soil quality .
# Fitting full model for response variable: Soil quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Full model fitting completed for response variable: Soil quality .
# Processing response variable: Water quality 
# Fitting null model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Null model fitting completed for response variable: Water quality .
# Fitting minimal model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Minimal model fitting completed for response variable: Water quality .
# Fitting fixed effects model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Fixed effects model fitting completed for response variable: Water quality .
# Fitting simplified model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Simplified model fitting completed for response variable: Water quality .
# Fitting full model for response variable: Water quality ...
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Full model fitting completed for response variable: Water quality .
# All models have been saved successfully in a single file!
# All models have been saved successfully in seperate files!
# Total time taken: 19.71101 
```





```{r}
##########################################################################
# Extract AIC and Fit Statistics for Each Response Variable
##########################################################################

# Initialize a data frame to store fit statistics results
fit_stats_results <- data.frame(
  Response = character(),
  Model = character(),
  AIC = numeric(),
  BIC = numeric(),
  REML = numeric(),
  ML = numeric(),
  LogLikelihood = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each response variable
for (response in names(model_results)) {
  cat("\nExtracting fit statistics for response variable:", response, "\n")
  
  # Retrieve models for the current response variable
  models <- model_results[[response]]
  
  # Loop through each model type
  for (model_type in names(models)) {
    model <- models[[model_type]]
    
    # Skip if the model is NULL
    if (is.null(model)) {
      next
    }
    
    # Extract fit statistics
    if (!is.null(model$fit.stats)) {
      fit_stats <- model$fit.stats
      aic <- fit_stats["AIC", "REML"]
      bic <- fit_stats["BIC", "REML"]
      reml <- fit_stats["REML", "REML"]
      ml <- fit_stats["ML", "REML"]
      
      # Extract log-likelihood
      log_likelihood <- tryCatch({
        if (!is.null(fit_stats["ll", "REML"])) {
          fit_stats["ll", "REML"]
        } else {
          logLik(model)  # Use logLik if not in fit.stats
        }
      }, error = function(e) {
        NA  # Return NA if log-likelihood cannot be extracted
      })
      
      # Append results to the data frame
      fit_stats_results <- rbind(
        fit_stats_results,
        data.frame(
          Response = response,
          Model = model_type,
          AIC = aic,
          BIC = bic,
          REML = reml,
          ML = ml,
          LogLikelihood = log_likelihood,
          stringsAsFactors = FALSE
        )
      )
    } else {
      cat("Fit statistics not available for model:", model_type, "of response:", response, "\n")
    }
  }
}

##########################################################################
# Save Fit Statistics Results to File
##########################################################################
write.csv(fit_stats_results, "fit_stats_results_with_log_likelihood.csv", row.names = FALSE)
cat("Fit statistics with log-likelihood results saved to 'fit_stats_results_with_log_likelihood.csv'\n")

##########################################################################
# Print Fit Statistics Results
##########################################################################
print(fit_stats_results)


print(fit_stats_results)
fit_stats_results |> str()
```







#############
# STEP 4
##########################################################################################################################################
MODEL COMPARISONS, EVALUATION AND DIAGNOSTICS
##########################################################################################################################################

```{r}
# Define a vector of response variables
response_variables <- c(
  "Biodiversity", "Greenhouse gas emission", "Product quality", 
  "Crop yield", "Pest and Disease", "Soil quality", "Water quality"
)

# Step 1: Generic extraction of datasets for each response variable
extract_datasets <- function(model_results, response_vars) {
  datasets <- lapply(response_vars, function(rv) {
    if (!is.null(model_results[[rv]])) {
      model_results[[rv]]$data
    } else {
      NULL
    }
  })
  names(datasets) <- response_vars
  return(datasets)
}

# Extract datasets for simplified and full models
simplified_model_datasets <- extract_datasets(simplified_model_results, response_variables)
full_model_datasets <- extract_datasets(full_model_results, response_variables)
```

```{r}
# Step 2: Combine all response variable datasets into one for each model
combine_datasets <- function(model_datasets) {
  combined_data <- do.call(rbind, model_datasets)
  return(as.data.frame(combined_data))
}

simplified_model_data <- combine_datasets(simplified_model_datasets)
full_model_data <- combine_datasets(full_model_datasets)

# Step 3: Check the structure of the final datasets
str(simplified_model_data)
str(full_model_data)
```
```{r}
# Step 4: Refit the "simplified" model using ML
simplified_ml <- rma.mv(
  yi = yi,
  V = simplified_model_data$vi,  # Variance structure
  mods = ~ tree_type + crop_type + age_system + season + soil_texture,  # Simplified model moderators
  random = ~ 1 | exp_id,          # Random effects structure
  data = simplified_model_data,   # Dataset for simplified model
  method = "ML"                   # Maximum Likelihood method
)

# Step 5: Refit the "full" model using ML
full_ml <- rma.mv(
  yi = yi,
  V = full_model_data$vi,         # Variance structure
  mods = ~ tree_type * crop_type * age_system * season * soil_texture,  # Full model with interactions
  random = ~ 1 | exp_id,          # Random effects structure
  data = full_model_data,         # Dataset for full model
  method = "ML"                   # Maximum Likelihood method
)

# Step 6: Perform a likelihood ratio test to compare the models
lrt <- anova(simplified_ml, full_ml)
print(lrt)

# Step 7: Interpret the results
if (lrt$pval < 0.05) {
  cat("The full model significantly improves the fit compared to the simplified model (p =", lrt$pval, ").\n")
} else {
  cat("The simplified model is sufficient; the full model does not significantly improve the fit (p =", lrt$pval, ").\n")
}
```


```{r}
# Load the saved models
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(dir, "fitted_models_null.rds"))
#fixed_no_intercept_results <- readRDS(file.path(dir, "fitted_models_fixed_no_intercept.rds"))
random_effects_results <- readRDS(file.path(dir, "fitted_models_random_effects.rds"))
#full_no_intercept_results <- readRDS(file.path(dir, "fitted_models_full_no_intercept.rds"))

# Step 1: Extract and Combine Datasets
response_variables <- c(
  "Biodiversity", "Greenhouse gas emission", "Product quality",
  "Crop yield", "Pest and Disease", "Soil quality", "Water quality"
)

#############################################################################################################
# Define a function to extract datasets for each model
extract_datasets <- function(model_results, response_vars) {
  datasets <- lapply(response_vars, function(rv) {
    if (!is.null(model_results[[rv]])) {
      model_results[[rv]]$data
    } else {
      NULL
    }
  })
  names(datasets) <- response_vars
  return(datasets)
}

# Define a function to combine datasets across response variables
combine_datasets <- function(model_datasets) {
  combined_data <- do.call(rbind, model_datasets)
  return(as.data.frame(combined_data))
}

# Step 1: Extract and Combine Datasets
response_variables <- c(
  "Biodiversity", "Greenhouse gas emission", "Product quality",
  "Crop yield", "Pest and Disease", "Soil quality", "Water quality"
)

# Extract datasets for null and random effects models
null_model_datasets <- extract_datasets(null_model_results, response_variables)
random_effects_datasets <- extract_datasets(random_effects_results, response_variables)

# Combine datasets for both models
null_model_data <- combine_datasets(null_model_datasets)
random_effects_data <- combine_datasets(random_effects_datasets)

# Synchronize datasets based on random-effects model rows
synchronized_rows <- intersect(rownames(random_effects_data), rownames(null_model_data))
random_effects_data <- random_effects_data[synchronized_rows, , drop = FALSE]
null_model_data <- null_model_data[synchronized_rows, , drop = FALSE]

# Remove rows with missing `yi` or `vi` values in either dataset
clean_indices <- complete.cases(random_effects_data$yi, random_effects_data$vi,
                                null_model_data$yi, null_model_data$vi)
random_effects_data <- random_effects_data[clean_indices, , drop = FALSE]
null_model_data <- null_model_data[clean_indices, , drop = FALSE]

# Ensure `yi` and `vi` are consistent after synchronization
stopifnot(all(null_model_data$yi == random_effects_data$yi))
stopifnot(all(null_model_data$vi == random_effects_data$vi))

# Construct variance-covariance matrices for overlapping rows
common_vi <- random_effects_data$vi
common_V <- diag(common_vi, nrow = length(common_vi), ncol = length(common_vi))

# Display structure of the synchronized datasets
cat("Number of rows in synchronized datasets:", nrow(random_effects_data), "\n")

# Step 2: Refit Models Using Maximum Likelihood
# Refit the null model using ML
null_ml <- rma.mv(
  yi = null_model_data$yi,
  V = common_V,
  random = ~ 1 | exp_id,
  data = null_model_data,
  method = "ML"
)

# Refit the random effects model using ML
random_effects_ml <- rma.mv(
  yi = random_effects_data$yi,
  V = common_V,
  mods = ~ tree_type + crop_type + age_system + season + soil_texture,
  random = ~ 1 | exp_id,
  data = random_effects_data,
  method = "ML"
)

# Step 3: Perform Likelihood Ratio Test
# Pre-check for consistency between models
cat("Checking consistency of model structures...\n")
cat("Number of observations in null model:", length(null_ml$yi), "\n")
cat("Number of observations in random-effects model:", length(random_effects_ml$yi), "\n")
cat("Dimensions of V in null model:", dim(null_ml$V), "\n")
cat("Dimensions of V in random-effects model:", dim(random_effects_ml$V), "\n")

# Compare the null and random effects models using ANOVA
lrt <- tryCatch({
  anova(null_ml, random_effects_ml)
}, error = function(e) {
  cat("Error during ANOVA: ", e$message, "\n")
  NULL
})

if (!is.null(lrt)) {
  print(lrt)
  if (lrt$pval < 0.05) {
    cat("The random effects model significantly improves the fit compared to the null model (p =", lrt$pval, ").\n")
  } else {
    cat("The null model is sufficient; the random effects model does not significantly improve the fit (p =", lrt$pval, ").\n")
  }
} else {
  cat("Likelihood ratio test could not be completed due to inconsistency.\n")
}

# Checking consistency of model structures...
# Number of observations in null model: 1093 
# Number of observations in random-effects model: 1079 
# Dimensions of V in null model: 1093 1093 
# Dimensions of V in random-effects model: 1079 1079 
# Error during ANOVA:  Observed outcomes and/or sampling variances/covariances not equal in the full and reduced model. 
# Likelihood ratio test could not be completed due to inconsistency.
```



if (!all(dim(null_ml$V) == dim(random_effects_ml$V))) {
  stop("Variance-covariance matrices are not consistent between models.")
}

Number of rows in synchronized datasets: 1093 
Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Checking consistency of model structures...
Number of observations in null model: 1093 
Number of observations in random-effects model: 1079 
Dimensions of V in null model: 1093 1093 
Dimensions of V in random-effects model: 1079 1079 
Fejl: Variance-covariance matrices are not consistent between models.

null_ml |> str()
random_effects_ml |> str()






























The likelihood ratio test compared two key models, the simplified and full, to evaluate how well each explains the variability in effect sizes using moderators such as tree type, crop type, age system, season, and soil texture. The results highlight that the full model, which includes both main effects and interaction terms between moderators, offers a significantly better fit than the simplified model. The full model has a substantially lower AIC (3959.58) and AICc (3966.02) compared to the simplified model’s AIC (4853.19) and AICc (4853.43), demonstrating its ability to explain more variability in the data. Additionally, the full model’s higher log-likelihood value (-1922.79 compared to -2415.59) indicates it is more likely to represent the observed data accurately.

The likelihood ratio test statistic (LRT) was 985.61 with a p-value less than 0.0001, confirming that the improvement in fit observed with the full model is statistically significant and not due to chance. This suggests that the interactions between moderators are essential for capturing the complexity of the relationships in the dataset. Moreover, the residual heterogeneity (QE) is considerably lower in the full model (14476.65) compared to the simplified model (20090.59). This reduction indicates that the full model explains a greater portion of the variability in effect sizes, leaving less unexplained heterogeneity.

Different model setups reveal incremental complexity in addressing the variability in effect sizes. The null model, serving as the simplest baseline, assumes no variability due to moderators or random effects. It has the poorest fit with the highest AIC, failing to capture key variations in the data. The minimal model, which incorporates random effects at the experiment level, captures some variability across experiments but lacks explanatory power since it does not include moderators. The fixed-effects model focuses solely on the impact of moderators, improving fit compared to the minimal model, but it overlooks random variability across experiments. The simplified model strikes a balance, including both fixed and random effects, and performs significantly better than the minimal model by explaining part of the heterogeneity. However, it does not include interaction terms, which limits its ability to fully capture the complexity of relationships among moderators.

The full model incorporates all fixed effects, interaction terms, and nested random effects, offering the most comprehensive explanation of variability in the data. It accounts for dependencies within articles and response variables and captures the combined influence of moderators and their interactions. While the full model’s complexity provides superior explanatory power, it requires careful interpretation, particularly for interaction terms, to ensure the findings are clearly communicated. The simplified model may still be useful in cases where simplicity and interpretability are prioritized, but it sacrifices the nuanced understanding provided by the full model.

Overall, the significant improvement in fit offered by the full model justifies its use in reporting and interpreting results. It highlights the importance of considering interaction effects and nested dependencies to fully capture the complexity of relationships in meta-analytical data. However, balancing clarity and complexity in reporting is crucial to making the findings accessible and actionable.


######################################

This assessment, comparing simplified and full models in the meta-analysis, highlights several important limitations and implications for model fitting, the overall analysis process, and the reporting and communication of results.

One key limitation is the **complexity of the full model**, which includes interaction terms and nested random effects. While this complexity significantly improves the model's fit, it also increases the risk of overfitting, particularly when the sample size or the number of independent observations is limited. Overfitting can lead to inflated estimates of model performance and reduced generalizability, making it crucial to ensure that the dataset supports the level of complexity introduced by the full model.

The reliance on interaction terms in the full model presents another challenge. Interaction terms often provide valuable insights into the combined effects of moderators but can be difficult to interpret, especially in large and complex datasets. For example, explaining the simultaneous influence of tree type, crop type, soil texture, season, and their interactions on the response variable requires careful communication to avoid misinterpretation or oversimplification of results. Additionally, the inclusion of many interaction terms increases the risk of multicollinearity, which may compromise the stability and reliability of model coefficients.

From a model-fitting perspective, the likelihood ratio test (LRT) indicates that the full model significantly improves the fit compared to the simplified model. However, this improvement may come at the cost of interpretability and computational efficiency. The time and resources required to fit and evaluate complex models can be prohibitive, especially when multiple response variables are analyzed across diverse datasets. Simplified models, while less precise, may offer a more practical and interpretable alternative in contexts where resource constraints or audience understanding are priorities.

In terms of reporting and communication, the results emphasize the importance of transparency and clarity. The full model provides the best explanation of variability, but the increased complexity requires careful reporting to ensure that findings are accessible and meaningful to both technical and non-technical audiences. Interaction effects, for example, need to be clearly illustrated, perhaps using visualization tools, to show how different combinations of moderators influence the response variable. The assumptions, limitations, and potential risks of overfitting should also be explicitly discussed to provide context for the results.

Another implication relates to the heterogeneity in the data, as evidenced by residual heterogeneity (QE) values. Even the full model, despite its better fit, leaves a substantial amount of unexplained variability. This highlights the possibility that other moderators or unmeasured factors not included in the analysis may be influencing the response variable. Future meta-analyses should consider incorporating additional covariates or employing alternative modeling frameworks, such as Bayesian approaches, to address this limitation.

Finally, the assessment underscores the need for flexibility in model selection. While the full model is statistically superior, there may be contexts where a simplified model is more appropriate. For example, when communicating findings to stakeholders or decision-makers who value clarity over technical detail, a simplified model may strike a better balance between interpretability and statistical rigor.

In summary, the assessment reveals the trade-offs between complexity, explanatory power, and interpretability in meta-analytic modeling. The results highlight the importance of aligning model choice with research objectives, audience needs, and resource constraints while maintaining transparency and rigor in reporting. Future efforts should focus on addressing unexplained heterogeneity, improving interpretability of complex models, and tailoring communication strategies to diverse audiences.

```{r}
# Load the saved models
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(dir, "fitted_models_null.rds"))
#fixed_no_intercept_results <- readRDS(file.path(dir, "fitted_models_fixed_no_intercept.rds"))
random_effects_results <- readRDS(file.path(dir, "fitted_models_random_effects.rds"))
#full_no_intercept_results <- readRDS(file.path(dir, "fitted_models_full_no_intercept.rds"))

# Step 1: Extract Datasets
# Define the response variables
response_variables <- c(
  "Biodiversity", "Greenhouse gas emission", "Product quality",
  "Crop yield", "Pest and Disease", "Soil quality", "Water quality"
)

# Define a function to refit models with Maximum Likelihood (ML) for each response variable
refit_models_with_ml <- function(response_variable, simplified_data, full_data, v_matrix_simplified, v_matrix_full, moderators) {
  cat("\nRefitting models with ML for response variable:", response_variable, "...\n")

  # Refit the simplified model using ML
  simplified_ml <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix_simplified,
      mods = as.formula(paste("yi ~", paste(moderators, collapse = " + "))),
      random = ~ 1 | exp_id,
      data = simplified_data,
      method = "ML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in simplified model refitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  # Refit the full model using ML
  full_ml <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix_full,
      mods = as.formula(paste("yi ~", paste(moderators, collapse = " * "))),
      random = ~ 1 | exp_id,
      data = full_data,
      method = "ML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in full model refitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  list(simplified_ml = simplified_ml, full_ml = full_ml)
}
```

```{r}
# Perform likelihood ratio test to compare simplified and full models
compare_models <- function(simplified_ml, full_ml, response_variable) {
  if (is.null(simplified_ml) || is.null(full_ml)) {
    cat("\nCannot perform LRT for", response_variable, "due to missing models.\n")
    return(NULL)
  }

  lrt <- tryCatch({
    anova(simplified_ml, full_ml)
  }, error = function(e) {
    cat("Error performing LRT for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  if (!is.null(lrt)) {
    cat("\nLRT for", response_variable, ":\n")
    print(lrt)
  }
  lrt
}
```


```{r}
# Define response variables and moderators
response_variables <- c(
  "Biodiversity", "Greenhouse gas emission", "Product quality", 
  "Crop yield", "Pest and Disease", "Soil quality", "Water quality"
)
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Initialize a list to store results for each response variable
ml_results <- list()

# Loop through each response variable to refit models and perform LRT
for (response in response_variables) {
  cat("\nProcessing response variable:", response, "\n")

  # Extract data and variance matrices for the response variable
  simplified_data <- simplified_model_datasets[[response]]
  full_data <- full_model_datasets[[response]]
  v_matrix_simplified <- simplified_data$vi  # Replace with the actual variance structure
  v_matrix_full <- full_data$vi              # Replace with the actual variance structure

  # Refit models using ML
  models <- refit_models_with_ml(
    response,
    simplified_data,
    full_data,
    v_matrix_simplified,
    v_matrix_full,
    moderators
  )

  # Perform LRT and store results
  lrt_result <- compare_models(models$simplified_ml, models$full_ml, response)
  ml_results[[response]] <- list(models = models, lrt = lrt_result)
}

# Save results as .rds
saveRDS(ml_results, file = "ml_model_comparison_results.rds")
cat("Model comparison results saved to 'ml_model_comparison_results.rds'.\n")

ml_results
```

```{r}
# ml_results |> str()
```


```{r}
# Load results from .rds
#ml_comparison_results <- readRDS("ml_model_comparison_results.rds")
#ml_comparison_results |> str()
# Function to compute AIC if not stored in the model
get_aic <- function(model) {
  tryCatch({
    if (!is.null(model)) {
      return(as.numeric(AIC(model)))
    } else {
      return(NA)
    }
  }, error = function(e) {
    return(NA)
  })
}

# Function to manually extract LRT stats
get_lrt_manual <- function(result) {
  tryCatch({
    if (!is.null(result$lrt)) {
      list(
        statistic = if (!is.null(result$lrt$LRT)) as.numeric(result$lrt$LRT) else NA,
        pval = if (!is.null(result$lrt$pval)) as.numeric(result$lrt$pval) else NA
      )
    } else {
      list(statistic = NA, pval = NA)
    }
  }, error = function(e) {
    list(statistic = NA, pval = NA)
  })
}

# Updated loop
comparison_df <- data.frame(
  Response = character(),
  Simplified_AIC = numeric(),
  Full_AIC = numeric(),
  LRT_Statistic = numeric(),
  LRT_pval = numeric(),
  stringsAsFactors = FALSE
)

for (response in names(ml_results)) {
  result <- ml_results[[response]]

  if (!is.null(result) && !is.null(result$models)) {
    # Compute AIC values
    simplified_aic <- get_aic(result$models$simplified_ml)
    full_aic <- get_aic(result$models$full_ml)

    # Manually extract LRT values
    lrt_values <- get_lrt_manual(result)
    lrt_stat <- lrt_values$statistic
    lrt_pval <- lrt_values$pval

    # Append to the dataframe
    comparison_df <- rbind(
      comparison_df,
      data.frame(
        Response = response,
        Simplified_AIC = simplified_aic,
        Full_AIC = full_aic,
        LRT_Statistic = lrt_stat,
        LRT_pval = lrt_pval,
        stringsAsFactors = FALSE
      )
    )
  } else {
    cat("Skipping response variable:", response, "- Missing or invalid result structure.\n")
  }
}

# Display the final dataframe
print(comparison_df)
```


```{r}
# str(ml_results[[1]]$models$simplified_ml)
```


```{r}
# Visualization 1: AIC Comparison


# Transform the data for AIC comparison
aic_long <- comparison_df %>%
  pivot_longer(cols = c(Simplified_AIC, Full_AIC),
               names_to = "Model",
               values_to = "AIC") %>%
  mutate(Model = factor(Model, levels = c("Simplified_AIC", "Full_AIC"),
                        labels = c("Simplified", "Full")))

# AIC Comparison Plot (without x-axis labels and text)
aic_plot <- ggplot(aic_long, aes(x = Response, y = AIC, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = c("Simplified" = "#FF9999", "Full" = "#66C2A5")) +
  labs(
    title = "AIC Comparison for Simplified and Full Models",
    y = "AIC Value",
    fill = "Model Type"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),   # Remove x-axis text
    axis.ticks.x = element_blank(), # Remove x-axis ticks
    plot.title = element_text(size = 14, face = "bold")
  )



# LRT Statistics Plot
lrt_plot <- ggplot(comparison_df, aes(x = Response, y = LRT_Statistic)) +
  geom_bar(stat = "identity", fill = "#8DA0CB") +
  geom_text(aes(label = ifelse(!is.na(LRT_pval) & LRT_pval < 0.05, paste0("p=", signif(LRT_pval, 2)), "")),
            vjust = -0.5, color = "black", size = 3) +
  labs(title = "Likelihood Ratio Test Statistics",
       x = "Response Variable",
       y = "LRT Statistic") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold")
  )

# Combine the updated plots without the "Response" label
final_plot <- aic_plot / lrt_plot + 
  plot_layout(heights = c(1, 1), guides = "collect") + # Equal heights for both plots
  plot_annotation(
    title = "Model Comparison: AIC and LRT Statistics",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
    )
  ) & 
  theme(axis.title.x = element_blank()) # Remove shared x-axis label

# Display the combined plot
print(final_plot)

```


The comparison of Simplified and Full models provides valuable insights into the performance, validity, and implications of the meta-analysis results. The AIC values and Likelihood Ratio Test (LRT) statistics reveal that the Full model often provides a better fit for the data, as evidenced by lower AIC values and significant LRT results for key response variables. For outcomes such as greenhouse gas emissions, crop yield, product quality, and soil quality, the Full model captures more variance and provides statistically significant improvements in fit. This suggests that these outcomes benefit from the additional complexity and parameters of the Full model, which better account for heterogeneity and underlying moderators.

However, for certain response variables such as biodiversity, pest and disease, and water quality, the results indicate that the Simplified model is sufficient. In these cases, the differences in AIC between the Simplified and Full models are negligible, and the LRT results are not statistically significant. This implies that adding complexity does not substantially improve the model’s explanatory power and that a simpler, more parsimonious approach is adequate. 

These findings highlight the importance of tailoring model selection to each response variable. While the Full model is necessary for outcomes with significant LRT results, simpler models should be favored when they provide comparable fit, as this avoids overfitting and enhances interpretability. For variables where the residual variance remains high and AIC values are identical, such as pest and disease and water quality, the models may not fully capture the variability in the data. This could reflect limitations in the data, such as insufficient moderator information or high heterogeneity among studies.

In reporting these findings, it is essential to emphasize the response-specific approach to model selection, balancing simplicity and performance. The significant improvements observed for some variables underscore the importance of including additional parameters when warranted, while non-significant results for others highlight the value of parsimony. Addressing the limitations of unexplained variance and the assumptions underlying the models will further strengthen the transparency and robustness of the meta-analysis. Overall, this approach ensures a nuanced interpretation of the results, aligning the modeling strategy with the unique characteristics of each response variable.

The results provide a strong justification for the use of the Simplified model in certain contexts within the meta-analysis, particularly for response variables where the Full model does not significantly improve the model fit. For response variables such as biodiversity, pest and disease, and water quality, the Simplified model performs comparably to the Full model. This is evident from the minimal differences in AIC values and non-significant Likelihood Ratio Test (LRT) results, indicating that the additional complexity of the Full model does not enhance explanatory power or reduce residual variance. 

The justification for the Simplified model lies in its parsimony. Simpler models are easier to interpret, less prone to overfitting, and require fewer assumptions. In cases where the Full model does not yield statistically significant improvements, the Simplified model strikes a balance between analytical rigor and practical interpretability, ensuring that the findings remain accessible and communicable to a broader audience.

This approach also aligns with the principles of meta-analysis, where heterogeneity is a key concern. For variables like pest and disease or water quality, where residual variance remains high and the models fail to fully explain the variability, the Simplified model avoids unnecessary complexity while acknowledging the limitations of the available data. It allows for a transparent presentation of findings without overcomplicating the narrative or misrepresenting the robustness of the analysis.

In summary, the Simplified model is justified when it performs as well as the Full model, as seen in specific response variables. Its use ensures clarity, avoids overfitting, and maintains interpretability, all of which are essential for drawing actionable conclusions from the meta-analysis while recognizing and communicating the inherent limitations of the data.



The variability in Likelihood Ratio Test (LRT) p-values across response variables reflects differences in how well the Full model improves model fit compared to the Simplified model. The LRT evaluates whether the additional parameters in the Full model explain significantly more variation, with a significant p-value supporting the use of the more complex model. Non-significant p-values suggest that the added complexity does not yield meaningful improvement, indicating that the Simplified model may suffice.

This variability can be attributed to several factors. The characteristics of the response variables themselves play a role. For instance, response variables such as "Greenhouse Gas Emission" and "Crop Yield" may involve complex relationships with covariates that the Full model captures more effectively, resulting in significant LRT p-values. In contrast, variables like "Pest and Disease" or "Water Quality" may either have simpler underlying processes or exhibit greater uncertainty, leading to non-significant results.

Data structure and sample size are also critical. Response variables with larger, higher-quality datasets, as seen with "Greenhouse Gas Emission," provide greater statistical power for detecting improvements in model fit. Conversely, sparse or highly variable datasets may limit the ability to discern meaningful differences between the Simplified and Full models.

Residual variance, or heterogeneity, further influences LRT outcomes. When unexplained variance remains high, even the Full model may fail to significantly outperform the Simplified model. This highlights cases where neither model fully captures the complexity of the underlying processes. Additionally, the degree of complexity introduced by the Full model affects its ability to improve model fit. If the increase in parameters is marginal, the difference between models may not be pronounced, while substantial additions to the model's structure can lead to more significant improvements.

The variability in LRT p-values underscores the importance of context and domain-specific insights in interpreting these results. It reveals the interplay between the nature of the response variables, the quality of the data, the extent of residual heterogeneity, and the complexity of the models being compared. This highlights the need for careful consideration of whether the Full model's complexity is justified in each case.








```{r}
##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES WITH `-1` INTERCEPT REMOVAL APPROACH
##########################################################################################################################################

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Models for Meta-Analysis with Intercept Removal Approach

##########################################################################
# Function: Fit Models with Custom Moderator Formulas (Including `-1` for Intercept Removal)
##########################################################################
fit_model <- function(data_subset, response_variable, v_matrix, moderators, random_effects = NULL, intercept = TRUE) {
  cat("\nFitting model for response variable:", response_variable, "...\n")
  
  # Ensure moderators are valid
  if (is.null(moderators) || length(moderators) == 0) {
    moderator_formula <- ~ 1  # Intercept-only model
  } else {
    # Build moderator formula with or without intercept
    moderator_formula <- if (intercept) {
      as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
    } else {
      as.formula(paste("yi ~", paste(moderators, collapse = " + "), "- 1"))
    }
  }

# Fit the model
model <- tryCatch({
  rma.mv(
    yi = data_subset$yi,  # Explicitly reference the 'yi' column from the data_subset
    V = v_matrix,
    mods = moderator_formula,
    random = random_effects,
    data = data_subset,
    method = "ML",  # Use Maximum Likelihood (ML) for direct comparisons
    control = list(
      optimizer = "optim",
      optim.method = "BFGS",
      iter.max = 1000,
      rel.tol = 1e-8
    )
  )
}, error = function(e) {
  cat("Error in model fitting for", response_variable, ":", e$message, "\n")
  return(NULL)
})


  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}


##########################################################################################################################################
# Fit Models with Detailed Comments
##########################################################################################################################################

model_results_ml <- list()

for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")

  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]

  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit various models for the response variable and store results in a nested list
  model_results_ml[[response]] <- list(
    
    # Null model with ML: Intercept-only model, no random effects, no moderators
    null_ml = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = NULL,       # No moderators
      intercept = TRUE         # Include intercept
    ),

    # Fixed effects model with no intercept and ML: Fits a model with specified moderators but excludes the intercept
    fixed_no_intercept_ml = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators, # Include specified moderators
      intercept = FALSE        # Exclude intercept
    ),

    # Random effects model with ML: Includes specified moderators and a random effect at the experiment level
    random_effects_ml = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,             # Include specified moderators
      random_effects = ~ 1 | exp_id,       # Random effect at the experiment level
      intercept = TRUE                     # Include intercept
    ),

    # Full model without intercept and ML: Includes specified moderators and multiple random effects
    full_no_intercept_ml = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = list(                # Include multiple random effects
        ~ 1 | id_article/response_variable, # Random effect for nested structure of articles and variables
        ~ 1 | exp_id                        # Random effect at the experiment level
      ),
      intercept = FALSE                     # Exclude intercept
    )
  )
}

##########################################################################
# Save All Fitted Models In One File
##########################################################################
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

saveRDS(model_results_ml, file = file.path(output_dir, "fitted_models_ml_all.rds"))

cat("\nAll models fitted with ML have been saved successfully in a single file!\n")

##########################################################################
# Save All Fitted Models In Separate Files
##########################################################################
saveRDS(lapply(model_results_ml, `[[`, "null_ml"), file = file.path(output_dir, "fitted_models_null_ml.rds"))
saveRDS(lapply(model_results_ml, `[[`, "fixed_no_intercept_ml"), file = file.path(output_dir, "fitted_models_fixed_no_intercept_ml.rds"))
saveRDS(lapply(model_results_ml, `[[`, "random_effects_ml"), file = file.path(output_dir, "fitted_models_random_effects_ml.rds"))
saveRDS(lapply(model_results_ml, `[[`, "full_no_intercept_ml"), file = file.path(output_dir, "fitted_models_full_no_intercept_ml.rds"))

cat("\nAll models fitted with ML have been saved successfully in separate files!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (18/01-2025)
# Total time taken: 29.79783
```
 






```{r}

##########################################################################
# Visualization
##########################################################################


# AIC Bar Plot with Broken Axis
ggplot(comparison_long |> dplyr::filter(Metric == "Akaike Information Criterion (AIC)"), 
       aes(x = Response, y = Value, fill = Comparison)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_break(c(5000, 750000), scales = 0.5) +  # Add a break in the y-axis
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal() +
  labs(
    title = "AIC Comparison Across Models",
    x = "Response Variable",
    y = "AIC Value",
    fill = "Comparison"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )


# Facetted BIC Bar Plot by Comparison
ggplot(comparison_long |> dplyr::filter(Metric == "Bayesian Information Criterion (BIC)"), 
       aes(x = Response, y = Value, fill = Comparison)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_break(c(5000, 750000), scales = 0.5) +  # Add a break in the y-axis
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal() +
  labs(
    title = "BIC Comparison Across Models",
    x = "Response Variable",
    y = "BIC Value",
    fill = "Comparison"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  )

# Facetted Log-Likelihood Line Plot by Comparison
# (Omitted here)



# Facetted P-Value Plot by Comparison
ggplot(comparison_long |>
filter(Metric == "P-Value"), 
       aes(x = Response, y = Value, group = Comparison, color = Comparison)) +
  geom_point(size = 3, shape = 17) +  # Use triangle shapes for points
  geom_line(size = 1, linetype = "dotted") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  facet_wrap(~ Metric, scales = "free_y") +
  theme_classic(base_size = 14) +
  labs(
    title = "P-Value Comparison Across Models",
    subtitle = "Facetted view for different response variables",
    x = "Response Variable",
    y = "P-Value",
    color = "Model Comparison"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

```



# AIC Differences with Facets
aic_plot_facet <- ggplot(aic_diff_plot_data, aes(x = Comparison_Type, y = AIC_Difference, fill = Comparison_Type)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ Response, scales = "free_y") +
  labs(
    title = "Differences in AIC Across Comparison Types (Faceted)",
    x = "Comparison Type",
    y = "AIC Difference",
    fill = "Comparison Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot
aic_plot_facet





Model comparison based on the AIC
The AIC Comparison plot illustrates the Akaike Information Criterion (AIC) values for three model comparisons: "Full vs Null," "Full vs Random," and "Random vs Fixed," across various response variables. AIC balances model fit and complexity, with lower values indicating a better trade-off between these factors.

### Key Observations:

1. **High AIC Values for "Soil Quality" in "Full vs Null"**:
   - Similar to the BIC analysis, "Full vs Null" for "Soil quality" has extremely high AIC values, reflecting the poor fit of the null model relative to the full model. The broken y-axis highlights this disparity while retaining readability for other response variables.

2. **Comparable AIC Values for "Full vs Random" and "Random vs Fixed"**:
   - Across most response variables, the AIC values for "Full vs Random" and "Random vs Fixed" are closely aligned, especially for "Crop yield," "Product quality," and "Pest and Disease." This suggests that the additional complexity introduced by the full model offers minimal improvements in fit over the random effects model.

3. **"Full vs Null" Dominates for All Response Variables**:
   - For all response variables, "Full vs Null" shows consistently higher AIC values compared to other comparisons, indicating that while the full model improves fit compared to the null model, the penalty for its complexity is significant.

4. **Relatively Low AIC for "Product Quality"**:
   - AIC values for "Product quality" are much smaller across all comparisons, reflecting either low variability in the data or that simpler models provide adequate fit for this response variable.

### Conclusion:
The AIC plot indicates that intermediate models like the random effects model (used in "Full vs Random" comparisons) often strike a better balance between fit and complexity. The significant penalties for complexity in "Full vs Null" comparisons, particularly for "Soil quality," emphasize the need to avoid overfitting by selecting simpler, yet robust, models. Overall, this plot reinforces the importance of tailoring model selection to the specific characteristics of each response variable.


Model comparison based on the BIC
The BIC Comparison plot shows the Bayesian Information Criterion (BIC) values for the three model comparisons ("Full vs Null," "Full vs Random," and "Random vs Fixed") across different response variables. BIC penalizes model complexity more strongly than AIC, emphasizing the balance between goodness-of-fit and simplicity.

### Key Observations:
1. **"Full vs Null" Shows the Largest BIC Values**:
   - For all response variables, the "Full vs Null" comparison has consistently higher BIC values. This suggests that while the full model improves fit, the additional complexity it introduces is heavily penalized, particularly for complex variables like "Soil quality."
   - The extreme BIC values for "Soil quality" highlight that the null model is insufficient but that the complexity of the full model may be excessive for this dataset.

2. **Smaller BIC Differences Between "Full vs Random" and "Random vs Fixed"**:
   - Comparisons between the full model and the random effects model ("Full vs Random") and the random effects model versus the fixed effects model ("Random vs Fixed") show much smaller differences in BIC values. This indicates that these models balance fit and complexity more evenly.
   - For variables such as "Crop yield" and "Product quality," the full and random effects models have nearly identical BIC values, suggesting minimal gains in fit from the full model despite its added complexity.

3. **Response Variable Variability**:
   - Variables like "Greenhouse gas emission" and "Pest and Disease" exhibit moderate BIC values across all comparisons, indicating a reasonable balance between model fit and complexity.
   - In contrast, "Product quality" shows very small BIC values across all comparisons, suggesting low variability in this response variable or that simpler models suffice.

### Conclusion:
The BIC results suggest that for many response variables, simpler models (random or fixed effects) may be sufficient, as they balance fit and complexity better than the full model. However, for variables like "Soil quality," the null model's poor fit drives up the full model's BIC, implying a need for intermediate models with reduced complexity. Overall, these insights align with the notion that model selection should prioritize simplicity unless justified by substantial gains in fit.

Model comparison based on the anova
The P-Value Comparison plot illustrates the statistical significance of the model comparisons for different response variables across three pairings: "Full vs Null," "Full vs Random," and "Random vs Fixed." 

Key observations include:

1. **Significant Differences in "Full vs Null"**: For most response variables, the P-values for the "Full vs Null" comparison are near zero. This indicates that the full model significantly outperforms the null model, highlighting the importance of including both random effects and moderators.

2. **Non-Significant "Random vs Fixed" Comparisons**: The P-values for "Random vs Fixed" are consistently above 0.05 for all response variables, suggesting that including random effects does not provide a statistically significant improvement over a fixed-effects model alone in these cases.

3. **Mixed Results for "Full vs Random"**: The "Full vs Random" comparisons show variability across response variables. For some responses, the P-values approach significance (e.g., close to 0.05), while for others, the difference between the full model and random effects model is not statistically significant.

Overall, this analysis underscores that the choice of model depends heavily on the response variable and the specific comparison being tested. While the full model generally offers improvements over the null model, its advantages over simpler models vary, suggesting that careful consideration of the data structure and research context is necessary when selecting the most appropriate model.



#############
# STEP 5
##########################################################################################################################################
MODEL DIAGNOSTICS ON EACH SUBSET MODEL FITTING 
##########################################################################################################################################

```{r}
# Load the saved models
full_model_results <- readRDS(file.path(dir, "fitted_models_full.rds"))
simplified_model_results <- readRDS(file.path(dir, "fitted_models_simplified.rds"))
minimal_model_results <- readRDS(file.path(dir, "fitted_models_minimal.rds"))
fixed_effects_model_results <- readRDS(file.path(dir, "fitted_models_fixed_effects.rds"))

```

```{r}
summary(full_model_results$Biodiversity)
```

```{r}
# Function to extract key diagnostics from a fitted model
extract_model_diagnostics <- function(model, response_variable) {
  if (is.null(model)) {
    return(data.frame(
      ResponseVariable = response_variable,
      AIC = NA,
      BIC = NA,
      LogLikelihood = NA,
      Tau2 = NA,
      I2 = NA,
      QM = NA,
      QMp = NA
    ))
  }

  # Extract diagnostics
  aic <- AIC(model)
  bic <- BIC(model)
  log_likelihood <- as.numeric(logLik(model))
  tau2 <- sum(model$sigma2)
  i2 <- round((tau2 / (tau2 + mean(model$vi))) * 100, 1)
  qm <- model$QM
  qmp <- model$QMp

  data.frame(
    ResponseVariable = response_variable,
    AIC = aic,
    BIC = bic,
    LogLikelihood = log_likelihood,
    Tau2 = tau2,
    I2 = i2,
    QM = qm,
    QMp = qmp
  )
}

# Function to systematically extract diagnostics for all models in a set
extract_diagnostics_for_all <- function(model_set, model_type) {
  bind_rows(
    lapply(names(model_set), function(response) {
      extract_model_diagnostics(model_set[[response]], response)
    })
  ) %>%
    mutate(ModelType = model_type)
}

# Extract diagnostics for each model set
model_diagnostics_full <- extract_diagnostics_for_all(full_model_results, "Full")
model_diagnostics_simplified <- extract_diagnostics_for_all(simplified_model_results, "Simplified")
model_diagnostics_minimal <- extract_diagnostics_for_all(minimal_model_results, "Minimal")
model_diagnostics_fixed <- extract_diagnostics_for_all(fixed_effects_model_results, "Fixed")

# Combine diagnostics into a single data frame
all_meta_analysis_model_diagnostics <- bind_rows(
  model_diagnostics_full,
  model_diagnostics_simplified,
  model_diagnostics_minimal,
  model_diagnostics_fixed
)

# Save the combined diagnostics for future use
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
saveRDS(all_meta_analysis_model_diagnostics, file.path(output_dir, "all_meta_analysis_model_diagnostics.rds"))

# View summary of diagnostics
all_meta_analysis_model_diagnostics
all_meta_analysis_model_diagnostics |> str()
```


```{r}
# Visualize AIC, BIC, and Log-Likelihood
diag_plot <- all_meta_analysis_model_diagnostics %>%
  pivot_longer(cols = c(AIC, BIC, LogLikelihood), names_to = "Metric", values_to = "Value") %>%
  ggplot(aes(x = ResponseVariable, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ ModelType, scales = "free_y") +
  labs(
    title = "Model Fit Diagnostics by Metric and Model Type",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the diagnostics plot
print(diag_plot)

```

AIC (Red) and BIC (Green): Lower values generally indicate a better fit while penalizing model complexity. Models with significantly lower AIC/BIC for a response variable are likely better.
Log-Likelihood (Blue): Higher values indicate a better likelihood of the model fitting the observed data. A noticeable difference in this metric across models could highlight where certain models are less suitable.
Trends and Insights:

If the bars for a specific response variable are much higher in one model type (e.g., "Full") compared to others, it suggests that model type may overfit or is not optimal for that variable.
The presence of similar AIC, BIC, and Log-Likelihood values across models for a variable may suggest that the simpler models (e.g., "Simplified" or "Minimal") are sufficient, avoiding unnecessary complexity.

```{r}
all_meta_analysis_model_diagnostics |> str()
all_meta_analysis_model_diagnostics |> glimpse()
```



```{r}
# export the data as excel to this path output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
# library(writexl)
# write_xlsx(all_meta_analysis_model_diagnostics, path = file.path(output_dir, "all_meta_analysis_model_diagnostics_uq.xlsx"))
```


```{r}
# Load the saved model diagnostics using the relative path
# library(readxl)
all_meta_analysis_model_diagnostics_relative_to_full_model <- read_excel(
  here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "all_meta_analysis_model_diagnostics_uq.xlsx"), 
  sheet = "relative_model_metrics"
)

# Verify the loaded data
str(all_meta_analysis_model_diagnostics_relative_to_full_model)
```

```{r}
# Prepare the data for plotting with ModelType
plot_data_metrics_relative_to_full_model <- all_meta_analysis_model_diagnostics_relative_to_full_model %>%
  select(ResponseVariable, ModelType, AIC_Relative_Difference_To_Full_Model,
         BIC_Relative_Difference_To_Full_Model, LogLikelihood_Relative_Difference_To_Full_Model) %>%
  pivot_longer(
    cols = starts_with("AIC_Relative_Difference_To_Full_Model") : starts_with("LogLikelihood_Relative_Difference_To_Full_Model"),
    names_to = "Metric",
    values_to = "RelativeValue"
  ) %>%
  mutate(
    Metric = case_when(
      grepl("AIC", Metric) ~ "AIC",
      grepl("BIC", Metric) ~ "BIC",
      grepl("LogLikelihood", Metric) ~ "Log-Likelihood"
    ),
    RelativeValue = as.numeric(RelativeValue) # Convert to numeric for plotting
  ) %>%
  filter(!is.na(RelativeValue)) # Remove rows with NA values

# Separate the data for Soil Quality and other response variables
soil_quality_data <- plot_data_metrics_relative_to_full_model %>% filter(ResponseVariable == "Soil quality")
other_response_variables_data <- plot_data_metrics_relative_to_full_model %>% filter(ResponseVariable != "Soil quality")

# Plot for all response variables except Soil Quality
other_response_variables_metrics_plot <- other_response_variables_data |> 
  ggplot(aes(x = ResponseVariable, y = RelativeValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_log10() +  # Use log scale for y-axis
  facet_wrap(~ Metric, scales = "free_y") +
  labs(
    title = "Relative Metrics (AIC, BIC, Log-Likelihood) Compared to Full Model (Excluding Soil Quality)",
    x = "Response Variable",
    y = "Relative Difference (%)",
    fill = "Model Type"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    #axis.text.x = element_blank(),   # Remove x-axis text
    #axis.ticks.x = element_blank(), # Remove x-axis ticks
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) 

# Plot for Soil Quality
soil_quality_metrics_plot <- soil_quality_data |> 
  ggplot(aes(x = Metric, y = RelativeValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_log10() +  # Use log scale for y-axis
  labs(
    title = "Relative Metrics (AIC, BIC, Log-Likelihood) for Soil Quality",
    x = "Metric",
    y = "Relative Difference (%)",
    fill = "Model Type"
  ) +
  theme_minimal(base_size = 14)

# Combine the plots into a stylish multiplot
combined_plot <- other_response_variables_metrics_plot / soil_quality_metrics_plot +
  plot_layout(heights = c(3, 1)) + # Adjust layout heights
  plot_annotation(
    title = "Comparison of Model Performance Across Metrics",
    caption = "Metrics relative to the Full Model for each Response Variable"
  )

# Print the combined plot
print(combined_plot)
```

```{r}
# Identify the model with the overall lowest relative difference for each metric and select best performing model based on AIC

# Convert relevant columns to numeric
summary_data_for_best_model_aic <- all_meta_analysis_model_diagnostics_relative_to_full_model %>%
  mutate(
    AIC_Relative_Difference_To_Full_Model = as.numeric(AIC_Relative_Difference_To_Full_Model),
    BIC_Relative_Difference_To_Full_Model = as.numeric(BIC_Relative_Difference_To_Full_Model),
    LogLikelihood_Relative_Difference_To_Full_Model = as.numeric(LogLikelihood_Relative_Difference_To_Full_Model)
  )

# Identify the model with the lowest relative difference for AIC
best_model_by_aic <- summary_data_for_best_model_aic %>%
  group_by(ResponseVariable) %>%
  filter(AIC_Relative_Difference_To_Full_Model == min(AIC_Relative_Difference_To_Full_Model, na.rm = TRUE)) %>%
  select(ResponseVariable, ModelType, AIC_Relative_Difference_To_Full_Model, AIC)

# Display the best models
best_model_by_aic
```

```{r}
# Calculate the average relative AIC difference for each model type across all response variables
best_overall_model <- summary_data_for_best_model_aic %>%
  group_by(ModelType) %>%
  summarize(
    Avg_AIC_Relative_Difference = mean(AIC_Relative_Difference_To_Full_Model, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(Avg_AIC_Relative_Difference) 

# Display the best overall model
best_best_best <- best_overall_model %>%
  slice(1)  # Select the model with the lowest average relative AIC difference

best_overall_model
best_best_best
```




##########################################################################
# Model 1: Comprehensive (Full) Model
# Incorporates the most complex structure with nested random effects and all moderators.
##########################################################################
##########################################################################
# Model 2: Moderately Simplified Model
# Simplifies random effects and removes interaction terms.
##########################################################################
##########################################################################
# Model 3: Minimal Random Effects Model
# Focuses on intercept-only model with minimal random effects.
##########################################################################
##########################################################################
# Model 4: Fixed Effects Only Model
# Removes random effects entirely and focuses solely on fixed effects.
##########################################################################


Based on the model evaluations provided, the **Simplified model** stands out as the most effective choice for your meta-analysis, balancing performance and simplicity. It consistently demonstrated the lowest average relative AIC difference across response variables, indicating strong explanatory power while avoiding overfitting. This aligns with the principles of parsimony, which advocate for selecting models that are as simple as possible while adequately capturing the essential data patterns.

The use of **relative AIC** in this context is valid, as it facilitates direct comparisons of model performance. However, it is crucial to emphasize that AIC measures relative model fit and does not provide absolute information about model adequacy. Supplementary evaluation metrics, such as residual diagnostics and likelihood ratio tests, can help confirm the robustness of the Simplified model and ensure that its reduced complexity does not compromise its ability to represent the data accurately.

The Simplified model is particularly noteworthy for its generalizability across diverse response variables, including biodiversity, greenhouse gas emissions, and crop yield. These results suggest that the Simplified model effectively balances essential patterns while excluding excessive complexity, making it well-suited for both interpretation and communication of meta-analytic findings. However, for specific response variables like "Pest and Disease" and "Soil Quality," where alternative models occasionally performed better, further investigation into variable-specific dynamics might be warranted.

In reporting these findings, it is essential to underscore that the Simplified model's selection reflects a deliberate effort to streamline analysis without sacrificing explanatory power. This choice enhances the interpretability of the meta-analysis results and supports meaningful insights for broader audiences, including researchers and policymakers. Nonetheless, the limitations inherent in relying solely on relative AIC should be transparently communicated, highlighting the importance of complementary evaluations to ensure the robustness and validity of the conclusions drawn.





##########################################################################################################################################
Testing model assumptions using diagnostic plots such as:
  * QQ plots for normality 
* Residuals vs. fitted values for homoscedasticity. 
##########################################################################################################################################

































```{r}
# null_model_results <- readRDS(file.path(dir, "fitted_models_null.rds"))
# minimal_model_results <- readRDS(file.path(dir, "fitted_models_minimal.rds"))
# fixed_effects_model_results <- readRDS(file.path(dir, "fitted_models_fixed_effects.rds"))
# simplified_model_results <- readRDS(file.path(dir, "fitted_models_simplified.rds"))
# full_model_results <- readRDS(file.path(dir, "fitted_models_full.rds"))

```



```{r}
# Prepare the data for plotting heterogeneity (I²)
i2_plot_data <- all_meta_analysis_model_diagnostics_relative_to_full_model %>%
  select(ResponseVariable, ModelType, I2) %>%
  mutate(I2 = as.numeric(I2)) %>%
  filter(!is.na(I2)) |>
  # Adjust the data for pseudo-logarithmic transformation
  # Replace zeros with small positive values
  mutate(I2_transformed = ifelse(I2 == 0, 0.01, I2))  |> 
  # Adjust the data for visualization with offsets
  # Add a small offset to prevent zero distortion
  mutate(I2_offset = I2 + 0.01)  |>
  # Adjust the data for better visualization
  # Scale and offset I² values for better visibility
  mutate(I2_scaled = I2 * 10 + 0.1)  |> 
  as.data.frame()



i2_plot_data |> glimpse()

# Create the plot with a linear scale for better bar visualization
i2_comparison_plot_linear <- i2_plot_data |> 
  ggplot(aes(x = ResponseVariable, y = I2_transformed, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_log10() +
  labs(
    title = "Heterogeneity (I²) Comparison Across Models (Linear Scale)",
    x = "Response Variable",
    y = "I² (%)",
    fill = "Model Type"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

# Print the plot
i2_comparison_plot_linear
```
```{r}
# Prepare the data for plotting heterogeneity (I²)
i2_plot_data <- all_meta_analysis_model_diagnostics_relative_to_full_model %>%
  select(ResponseVariable, ModelType, I2) %>%
  mutate(I2 = as.numeric(I2)) %>%
  filter(!is.na(I2)) |>
  # Adjust the data for pseudo-logarithmic transformation
  # Replace zeros with small positive values
  mutate(I2_transformed = ifelse(I2 == 0, 0.01, I2))  |> 
  # Adjust the data for visualization with offsets
  # Add a small offset to prevent zero distortion
  mutate(I2_offset = I2 + 0.01)  |>
  # Adjust the data for better visualization
  # Scale and offset I² values for better visibility
  mutate(I2_scaled = I2 * 10 + 0.1)  |> 
  as.data.frame()


# Plot for lower range (0–1%)
p1 <- ggplot(data = i2_plot_data, aes(x = ResponseVariable, y = I2_transformed, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_cartesian(ylim = c(0, 0.1)) +
  labs(
    x = "Response Variable",
    y = "I² (%)",
    title = NULL
  ) +
  theme_classic() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "right"
  )

# Plot for upper range (>5%)
p2 <- ggplot(data = i2_plot_data, aes(x = ResponseVariable, y = I2_transformed, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_cartesian(ylim = c(5, 10)) +
  labs(
    x = NULL,
    y = NULL,
    title = "Heterogeneity (I²) Comparison Across Models (Broken Y-Axis)"
  ) +
  theme_classic() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "none"
  )

# Add whitespace between the plots
p_combined <- p2 / plot_spacer() / p1 + 
  plot_layout(heights = c(2, 0.3, 4))

# Display the combined plot
p_combined
```



```{r}
##########################################################################
# Extract heterogeneity from the model_results object
##########################################################################

model_results |> str()

```

```{r}
# Define a function to compute heterogeneity partitioning
compute_heterogeneity_partitioning <- function(model_results, response_variable, model_type) {
  # Extract QE and QM, handle cases where they might be missing
  QE <- model_results$QE
  QM <- model_results$QM
  
  if (!is.null(QE) && !is.null(QM)) {
    residual_heterogeneity <- QE - QM  # Calculate Residual Heterogeneity
    explained_proportion <- (QM / QE) * 100  # Proportion Explained
    residual_proportion <- (residual_heterogeneity / QE) * 100  # Residual Proportion
    
    return(data.frame(
      ResponseVariable = response_variable,
      ModelType = model_type,
      TotalHeterogeneity = QE,
      ExplainedHeterogeneity = QM,
      ResidualHeterogeneity = residual_heterogeneity,
      ExplainedProportion = explained_proportion,
      ResidualProportion = residual_proportion
    ))
  } else {
    return(data.frame(
      ResponseVariable = response_variable,
      ModelType = model_type,
      TotalHeterogeneity = NA,
      ExplainedHeterogeneity = NA,
      ResidualHeterogeneity = NA,
      ExplainedProportion = NA,
      ResidualProportion = NA
    ))
  }
}
```

```{r}
# Combine all models into a named list for iteration
all_models <- list(
  null = null_model_results,
  minimal = minimal_model_results,
  fixed = fixed_effects_model_results,
  simplified = simplified_model_results,
  full = full_model_results
)

# Initialize an empty data frame to store results
heterogeneity_results <- data.frame()

# Loop through each model type and response variable
for (model_type in names(all_models)) {
  model_data <- all_models[[model_type]]
  
  for (response_variable in response_variables) {
    # Check if model exists for the response variable
    if (!is.null(model_data[[response_variable]])) {
      model <- model_data[[response_variable]]
      
      # Compute heterogeneity partitioning for this model-response pair
      result <- compute_heterogeneity_partitioning(model, response_variable, model_type)
      
      # Append to results
      heterogeneity_results <- rbind(heterogeneity_results, result)
    }
  }
}

# View final heterogeneity results
print(heterogeneity_results)
heterogeneity_results |> str()
```

```{r}
# Step 1: Correct negative ResidualHeterogeneity and recalculate proportions in the original data
heterogeneity_results_fix <- heterogeneity_results %>%
  mutate(
    # Set negative ResidualHeterogeneity values to 0 as negative heterogeneity is not meaningful
    ResidualHeterogeneity = ifelse(ResidualHeterogeneity < 0, 0, ResidualHeterogeneity),
    # Recalculate ExplainedProportion as a percentage of TotalHeterogeneity
    ExplainedProportion = ifelse(
      TotalHeterogeneity > 0,
      100 * ExplainedHeterogeneity / TotalHeterogeneity,
      NA  # Assign NA if TotalHeterogeneity is zero or missing
    ),
    # Recalculate ResidualProportion similarly
    ResidualProportion = ifelse(
      TotalHeterogeneity > 0,
      100 * ResidualHeterogeneity / TotalHeterogeneity,
      NA  # Assign NA if TotalHeterogeneity is zero or missing
    )
  )

# Step 2: Reshape the data to a long format for visualization
long_heterogeneity_data <- heterogeneity_results_fix %>%
  pivot_longer(
    cols = c(TotalHeterogeneity, ExplainedHeterogeneity, ResidualHeterogeneity),
    names_to = "HeterogeneityType",
    values_to = "HeterogeneityValue"
  )

# Step 3: Join back TotalHeterogeneity for recalculations
adjusted_heterogeneity_data <- long_heterogeneity_data %>%
  left_join(
    heterogeneity_results_fix %>% select(ResponseVariable, ModelType, TotalHeterogeneity),
    by = c("ResponseVariable", "ModelType")
  ) %>%
  mutate(
    # Adjust negative values only for ResidualHeterogeneity
    HeterogeneityValue = ifelse(
      HeterogeneityType == "ResidualHeterogeneity" & HeterogeneityValue < 0,
      0,  # Set negative values to 0
      HeterogeneityValue
    ),
    # Recalculate ExplainedProportion and ResidualProportion
    ExplainedProportion = ifelse(
      HeterogeneityType == "ExplainedHeterogeneity" & TotalHeterogeneity > 0,
      100 * HeterogeneityValue / TotalHeterogeneity,
      NA
    ),
    ResidualProportion = ifelse(
      HeterogeneityType == "ResidualHeterogeneity" & TotalHeterogeneity > 0,
      100 * HeterogeneityValue / TotalHeterogeneity,
      NA
    )
  )

# Step 4: Validate the adjusted data
adjusted_heterogeneity_data |> glimpse()
```



```{r}
# Prepare the data for visualization
visualization_data <- adjusted_heterogeneity_data %>%
  filter(!is.na(TotalHeterogeneity) & !is.na(ExplainedHeterogeneity)) %>%  # Remove rows with NA values
  mutate(
    ResponseVariable = factor(ResponseVariable, levels = unique(ResponseVariable)),  # Order response variables
    ModelType = factor(ModelType, levels = c("null", "minimal", "fixed", "simplified", "full"))  # Order models
  )

visualization_data |> str()
```


```{r}
# Prepare the data for visualization
visualization_data <- heterogeneity_results %>%
  filter(!is.na(TotalHeterogeneity) & !is.na(ExplainedHeterogeneity)) %>%  # Remove rows with NA values
  mutate(
    ResponseVariable = factor(ResponseVariable, levels = unique(ResponseVariable)),  # Order response variables
    ModelType = factor(ModelType, levels = c("null", "minimal", "fixed", "simplified", "full"))  # Order models
  )

visualization_data |> str()

# Reshape data to include HeterogeneityType
long_heterogeneity_data <- visualization_data %>%
  pivot_longer(
    cols = c(TotalHeterogeneity, ExplainedHeterogeneity, ResidualHeterogeneity),
    names_to = "HeterogeneityType",
    values_to = "HeterogeneityValue"
  )

# Correct negative residual heterogeneity values
long_heterogeneity_data_adjusted <- long_heterogeneity_data %>%
  mutate(
    HeterogeneityValue = ifelse(
      HeterogeneityType == "ResidualHeterogeneity" & HeterogeneityValue < 0,
      0,  # Replace negative values with 0
      HeterogeneityValue
    )
  )
```





```{r}
# Total Heterogeneity Plot
total_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "TotalHeterogeneity"),
                     aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  scale_y_break(c(8000, 200000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Total Heterogeneity", x = NULL, y = "Heterogeneity Value") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.y = element_text(size = 12),
    legend.position = "none"
  )

# Explained Heterogeneity Plot
explained_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ExplainedHeterogeneity"),
                         aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  scale_y_break(c(4000, 1000000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Explained Heterogeneity", x = NULL, y = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

# Residual Heterogeneity Plot
residual_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ResidualHeterogeneity"),
                        aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  scale_y_break(c(10000, 300000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Residual Heterogeneity", x = NULL, y = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.y = element_blank(),
    legend.position = "right"
  )

# Combine the plots with a shared legend
combined_plot <- (total_plot | explained_plot | residual_plot) +  # Arrange side-by-side
  plot_layout(guides = "collect", widths = c(500, 500, 5)) +  # Collect legend, ensure equal widths
  plot_annotation(
    title = "Partitioned Heterogeneity Across Models",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold"),
      legend.position = "top"  # Shared legend at the top
    )
  )

# Display the plot
print(combined_plot)
```

```{r}
# Base theme
base_theme <- theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold"),
    axis.title.y = element_text(size = 12)
  )

# Total Heterogeneity Plot
total_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "TotalHeterogeneity"),
                     aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.6, alpha = 0.7) +
  scale_y_break(c(7500, 200000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Total Heterogeneity", x = NULL, y = "Heterogeneity Value") +
  base_theme +
  theme(legend.position = "none")

# Explained Heterogeneity Plot
explained_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ExplainedHeterogeneity"),
                         aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.6, alpha = 0.7) +
  scale_y_break(c(4000, 1000000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Explained Heterogeneity", x = NULL, y = NULL) +
  base_theme +
  theme(legend.position = "none")

explained_plot

# Residual Heterogeneity Plot
residual_plot <- ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "ResidualHeterogeneity"),
                        aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.5), width = 0.4, alpha = 0.7) +
  scale_y_break(c(10000, 300000), scales = 0.5) +  # Add y-axis breaks
  labs(title = "Residual Heterogeneity", x = NULL, y = NULL) +
  base_theme +
  theme(legend.position = "none")


# Extract the legend from one of the actual plots
legend_plot <- ggpubr::get_legend(
  ggplot(long_heterogeneity_data_adjusted %>% filter(HeterogeneityType == "TotalHeterogeneity"),
         aes(x = ResponseVariable, y = HeterogeneityValue, fill = ModelType)) +
    geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
    theme_minimal(base_size = 14) +
    theme(legend.position = "top", legend.title = element_text(size = 12, face = "bold"))
)


# Combine the plots into a single layout with equal widths
combined_plot <- wrap_plots(
  legend_plot,
  total_plot + explained_plot + residual_plot,
  ncol = 1, # Set three plots in one row for equal widths
  heights = c(0.1, 1) # Allocate height for the legend and plots
) +
  plot_annotation(
    title = "Partitioned Heterogeneity Across Models",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold")
    )
  )

# Display the plot
print(combined_plot)
```









```{r}
visualization_data |> str()
```



```{r}
# Remove problematic rows (negative/infinite values)
visualization_data_cleaned <- visualization_data %>%
  filter(
    !is.na(ExplainedProportion),
    !is.na(ResidualProportion),
    ExplainedProportion > 0,
    ResidualProportion > 0
  )

# Scatter Plot: Explained vs. Residual Proportion
heterogeneity_scatter_plot <- ggplot(visualization_data_cleaned, aes(x = ExplainedProportion, y = ResidualProportion, color = ModelType)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed") +
  scale_y_continuous(trans = "pseudo_log", breaks = c(10, 50, 100)) +  # Adjust y-scale
  scale_x_continuous(trans = "pseudo_log", breaks = c(1, 10, 100, 1000)) +  # Adjust x-scale
  labs(
    title = "Explained vs. Residual Proportion by Model Type",
    x = "Explained Proportion (%)",
    y = "Residual Proportion (%)",
    color = "Model Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    legend.title = element_text(size = 12, face = "bold")
  )

# Display the refined plot
print(heterogeneity_scatter_plot)

```



This plot illustrates the relationship between the explained proportion and residual proportion of heterogeneity across different meta-analysis models. Explained heterogeneity represents the proportion of total variance in effect sizes that can be attributed to predictors or moderators in the model, while residual heterogeneity reflects the remaining unexplained variance after accounting for these predictors. These two metrics are essential for assessing the performance of different models, as they help quantify how effectively a model explains patterns in the data while minimizing unaccounted variation.

In the plot, each model type is represented by a unique color, with dashed trend lines indicating general patterns for each model. The null model, which includes no random or fixed effects, has consistently high residual heterogeneity, showing that it fails to explain much of the variability in effect sizes. In contrast, models with additional complexity, such as the minimal, fixed, simplified, and full models, show varied performance in balancing explained and residual proportions.

The fixed model shows a steep decline in residual heterogeneity as explained proportion increases, indicating strong performance in capturing variance through fixed effects. The minimal model strikes a balance between explained and residual proportions, suggesting that its inclusion of random effects improves its capacity to explain heterogeneity compared to the null model. The simplified and full models demonstrate better explained proportions in some cases, but their performance is more variable, reflecting potential challenges in overfitting or the inclusion of unnecessary complexity.

This plot provides valuable insights into the trade-offs between model complexity and explanatory power. Models like the null model, while simple, do not perform well in capturing heterogeneity. More complex models, such as the full or simplified models, may improve explained heterogeneity but at the cost of consistency or interpretability.

One limitation of this analysis is that it does not address how these differences in heterogeneity metrics affect the broader goals of the meta-analysis, such as reporting effect sizes or testing specific hypotheses. Additionally, model performance may vary depending on the specific response variables being studied, and the choice of the best model should align with the overarching research questions and practical considerations for reporting. These results highlight the importance of transparently communicating the rationale for model selection and acknowledging the trade-offs involved in balancing explanatory power, complexity, and interpretability in meta-analysis.


This plot illustrates the relationship between the explained proportion and residual proportion of heterogeneity across different meta-analysis models. Explained heterogeneity represents the proportion of total variance in effect sizes that can be attributed to predictors or moderators in the model, while residual heterogeneity reflects the remaining unexplained variance after accounting for these predictors. These two metrics are essential for assessing the performance of different models, as they help quantify how effectively a model explains patterns in the data while minimizing unaccounted variation.

In the plot, each model type is represented by a unique color, with dashed trend lines indicating general patterns for each model. The null model, which includes no random or fixed effects, has consistently high residual heterogeneity, showing that it fails to explain much of the variability in effect sizes. In contrast, models with additional complexity, such as the minimal, fixed, simplified, and full models, show varied performance in balancing explained and residual proportions.

The fixed model shows a steep decline in residual heterogeneity as explained proportion increases, indicating strong performance in capturing variance through fixed effects. The minimal model strikes a balance between explained and residual proportions, suggesting that its inclusion of random effects improves its capacity to explain heterogeneity compared to the null model. The simplified and full models demonstrate better explained proportions in some cases, but their performance is more variable, reflecting potential challenges in overfitting or the inclusion of unnecessary complexity.

This plot provides valuable insights into the trade-offs between model complexity and explanatory power. Models like the null model, while simple, do not perform well in capturing heterogeneity. More complex models, such as the full or simplified models, may improve explained heterogeneity but at the cost of consistency or interpretability.

One limitation of this analysis is that it does not address how these differences in heterogeneity metrics affect the broader goals of the meta-analysis, such as reporting effect sizes or testing specific hypotheses. Additionally, model performance may vary depending on the specific response variables being studied, and the choice of the best model should align with the overarching research questions and practical considerations for reporting. These results highlight the importance of transparently communicating the rationale for model selection and acknowledging the trade-offs involved in balancing explanatory power, complexity, and interpretability in meta-analysis.



#############
# STEP 5
##########################################################################################################################################
KEY INFLUENCE DIAGNOSTICS ON EACH SUBSET - SIMPLIFIED MODEL FITTING 
##########################################################################################################################################

```{r}

```


Understanding the overall effect of each response variable on the overall effect size: Quantifying how the outcomes (response variables) studied in our meta-analysis contribute to the aggregated measure of effect sizes. The Null and Minimal Random Effects Models are particularly good in this.

What Does "Overall Effect of Each Response Variable" Mean?
  Effect Size (yi):
  
  Each study reports an effect size (yi) for a specific response variable (e.g., biodiversity, crop yield). The effect size quantifies the magnitude of the observed effect, such as the improvement in biodiversity or crop yield due to an intervention.
Overall Effect:
  
  The overall effect is a weighted average of the effect sizes across studies for a given response variable. It answers the question: "On average, how much does this response variable change across studies?"
Weighting accounts for the precision (variance, vi) of each study, giving more weight to studies with lower variance.

```{r}
# The Null Model is the simplest form of meta-analysis. It estimates an overall effect size (intercept-only) without accounting for moderators or random effects, making # it ideal for quickly summarizing the overall effect.

# Fit the null model for each response variable
overall_effects <- lapply(names(v_matrices), function(response) {
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Fit the null model
  fit_null_model(data_subset, response)
})

# Extract results
overall_effects_summary <- lapply(overall_effects, function(model) {
  if (!is.null(model)) {
    summary(model)$b[, "Estimate"] # Extract the overall effect size (intercept)
  } else {
    NA
  }
})

# Combine results into a data frame
overall_effects_df <- data.frame(
  ResponseVariable = names(v_matrices),
  OverallEffect = unlist(overall_effects_summary)
)

# Fit the minimal random effects model for each response variable
minimal_effects <- lapply(names(v_matrices), function(response) {
  data_subset <- meta_data[meta_data$response_variable == response, ]
  v_matrix <- v_matrices[[response]]
  
  # Fit the minimal random effects model
  fit_minimal_model(data_subset, response, v_matrix)
})

# Extract results
minimal_effects_summary <- lapply(minimal_effects, function(model) {
  if (!is.null(model)) {
    summary(model)$b[, "Estimate"] # Extract the overall effect size (intercept)
  } else {
    NA
  }
})

# Combine results into a data frame
minimal_effects_df <- data.frame(
  ResponseVariable = names(v_matrices),
  OverallEffect = unlist(minimal_effects_summary)
)

# Compare results from both models
comparison_df <- merge(
  overall_effects_df,
  minimal_effects_df,
  by = "ResponseVariable",
  suffixes = c("_NullModel", "_MinimalModel")
)
```

```{r}
# Combine results for visualization
effects_plot_data <- overall_effects_df %>%
  rename(EffectSize = OverallEffect) %>%
  mutate(Model = "Null Model") %>%
  bind_rows(
    minimal_effects_df %>%
      rename(EffectSize = OverallEffect) %>%
      mutate(Model = "Minimal Random Effects Model")
  )

# Plot
ggplot(effects_plot_data, aes(x = ResponseVariable, y = EffectSize, color = Model)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = EffectSize - 0.1, ymax = EffectSize + 0.1), # Adjust error range as needed
                width = 0.2, position = position_dodge(width = 0.5)) +
  labs(
    title = "Overall Effect Sizes by Response Variable",
    x = "Response Variable",
    y = "Overall Effect Size",
    color = "Model"
  ) +
  theme_minimal(base_size = 14)

```





#############
# STEP 7
##########################################################################################################################################
PUBLICATION-READY PLOTS AND TABLES OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################



Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.



```{r}
# Load the saved models
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(dir, "fitted_models_null.rds"))
minimal_model_results <- readRDS(file.path(dir, "fitted_models_minimal.rds"))
fixed_effects_model_results <- readRDS(file.path(dir, "fitted_models_fixed_effects.rds"))
simplified_model_results <- readRDS(file.path(dir, "fitted_models_simplified.rds"))
full_model_results <- readRDS(file.path(dir, "fitted_models_full.rds"))

# simplified_model_results |> str()
# simplified_model_results |> glimpse()
```

```{r}
# WORKING ON THE SIMPLIFIED MODEL
mod_res <- simplified_model_results
```

```{r}
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)
```

```{r}
# Combine results from all response variables into a single data frame
forest_plot_data <- bind_rows(
  lapply(names(mod_res), function(response) {
    # Access the model for the current response variable
    model <- mod_res[[response]]
    
    # Check if the model and its associated data are available
    if (!is.null(model) && !is.null(model$data)) {
      # Align the data with fitted effect sizes (model$yi) by subsetting
      aligned_data <- model$data[seq_along(model$yi), ]
      
      # Ensure alignment resolves potential mismatch in lengths
      if (nrow(aligned_data) == length(model$yi)) {
        # Construct a data frame with necessary details for the forest plot
        data.frame(
          Study = aligned_data$id_article,              # Study identifiers
          EffectSize = model$yi,                       # Fitted effect sizes
          CI_Lower = model$yi - 1.96 * sqrt(model$vi), # Lower confidence interval (95%)
          CI_Upper = model$yi + 1.96 * sqrt(model$vi), # Upper confidence interval (95%)
          ResponseVariable = response                  # Corresponding response variable
        )
      } else {
        # Warn if alignment did not resolve mismatched lengths
        warning(sprintf(
          "Still mismatched lengths for '%s': model$yi (%d) vs. aligned_data (%d).",
          response, length(model$yi), nrow(aligned_data)
        ))
        NULL # Skip this response variable
      }
    } else {
      # Warn if the model or its data is unavailable
      warning(sprintf("Skipping response variable '%s': Missing model or data.", response))
      NULL # Skip this response variable
    }
  })
)
```

```{r}
# Summarize the data by response variable
aggregated_data <- forest_plot_data %>%
  group_by(ResponseVariable) %>% # Group data by response variable
  summarise(
    # Calculate the overall mean effect size
    overall_effect = mean(EffectSize, na.rm = TRUE),
    # Calculate mean confidence interval bounds
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    # Count total observations and unique studies
    num_observations = n(),
    num_studies = n_distinct(Study), # Assuming 'Study' represents unique studies
    # Categorize studies into size groups based on the number of studies
    size_category = case_when(
      num_studies <= 2 ~ "1-2",  # Small sample size category
      num_studies <= 4 ~ "3-4",  # Medium sample size category
      num_studies > 4 ~ "5+"     # Large sample size category
    ),
    .groups = "drop" # Drop grouping after summarization
  ) %>%
  # Add response rank based on the overall effect size
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")), # Order size categories
    response_rank = rank(overall_effect) # Rank responses by overall effect
  )

# Reorder response variables based on the mean effect size (descending order)
aggregated_data <- aggregated_data %>%
  arrange(desc(overall_effect)) %>%
  mutate(ResponseVariable = factor(ResponseVariable, levels = ResponseVariable))

# View the aggregated data
aggregated_data |> 
  ```


```{r}
# Create the forest plot
forest_plot <- aggregated_data %>%
  ggplot(aes(x = overall_effect, y = reorder(ResponseVariable, response_rank), color = ResponseVariable)) +
  # Add points for effect sizes
  geom_point(aes(size = size_category), shape = 19, alpha = 0.8) +
  # Add horizontal error bars for confidence intervals
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2, size = 1, alpha = 0.7) +
  # Add vertical line at zero
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey40", size = 0.8) +
  # Customize point size scale
  scale_size_manual(
    values = c("1-2" = 3, "3-4" = 5, "5+" = 7),
    name = "Number of Studies"
  ) +
  # Customize color scale
  scale_color_manual(
    values = custom_colors,
    name = "Response Variable"
  ) +
  # Customize plot labels and appearance
  labs(
    title = "Forest Plot: Effect Sizes and Confidence Intervals",
    subtitle = "Meta-analysis of Temperate Silvoarable Agroforestry",
    x = "Effect Size (Overall)",
    y = "Response Variable",
    size = "Study Size Category"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(size = 10),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, face = "italic", hjust = 0.5),
    legend.position = "right",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )

# Display the plot
print(forest_plot)
```



```{r}
# Updated forest plot dataset extraction
detailed_forest_data <- bind_rows(
  lapply(names(mod_res), function(response) {
    model <- mod_res[[response]]
    
    if (!is.null(model) && !is.null(model$data)) {
      aligned_data <- model$data[seq_along(model$yi), ]  # Align data with fitted values
      
      if (nrow(aligned_data) == length(model$yi)) {
        data.frame(
          Study = aligned_data$id_article,
          EffectSize = model$yi,
          CI_Lower = model$yi - 1.96 * sqrt(model$vi),
          CI_Upper = model$yi + 1.96 * sqrt(model$vi),
          Variance = model$vi,
          Precision = 1 / sqrt(model$vi),
          ResponseVariable = response
        )
      } else {
        NULL
      }
    } else {
      NULL
    }
  })
)


# Apply the same order to the detailed data
# detailed_forest_data <- detailed_forest_data %>%
#   mutate(ResponseVariable = factor(ResponseVariable, levels = levels(aggregated_data$ResponseVariable)))



# Verify the prepared dataset
detailed_forest_data |> glimpse()
```

```{r}
# Update aggregated_data to include Variance
aggregated_data <- detailed_forest_data %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    num_observations = n(),
    num_studies = n_distinct(Study),
    mean_variance = mean(Variance, na.rm = TRUE),  # Include average variance
    size_category = case_when(
      num_studies <= 2 ~ "1-2",
      num_studies <= 4 ~ "3-4",
      num_studies > 4 ~ "5+"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    size_category = factor(size_category, levels = c("1-2", "3-4", "5+")),
    response_rank = rank(overall_effect)
  )

# Verify updated aggregated_data
aggregated_data |> glimpse()
```

```{r}
# Beeswarm Forest Plot with Scaled X-Axis
beeswarm_forest_plot_scaled <- ggplot(detailed_forest_data, 
                                      aes(
                                        x = EffectSize, 
                                        y = reorder(ResponseVariable, aggregated_data$overall_effect[match(ResponseVariable, aggregated_data$ResponseVariable)]), 
                                        color = ResponseVariable
                                      )
) +
  # Beeswarm for individual points with more jitter
  ggbeeswarm::geom_quasirandom(alpha = 0.7, size = 2, width = 0.2) +
  # Add summary points for aggregated data with larger size and bold outline
  geom_point(
    data = aggregated_data, 
    aes(
      x = overall_effect, 
      y = reorder(ResponseVariable, overall_effect), 
      size = num_studies
    ), 
    shape = 21, fill = "white", color = "black", stroke = 1.5, inherit.aes = FALSE
  ) + 
  # Add error bars for confidence intervals with thicker lines
  geom_errorbarh(
    data = aggregated_data, 
    aes(
      xmin = lower_ci, 
      xmax = upper_ci, 
      y = reorder(ResponseVariable, overall_effect)
    ), 
    height = 0.2, 
    color = "black", 
    linewidth = 1.2, 
    inherit.aes = FALSE
  ) + 
  # Add secondary, smaller error bars for 75% CI
  # geom_errorbarh(
  #   data = aggregated_data, 
  #   aes(
  #     xmin = overall_effect - 0.674 * sqrt(mean_variance),  # 50% CI lower bound
  #     xmax = overall_effect + 0.674 * sqrt(mean_variance),  # 50% CI upper bound
  #     y = reorder(ResponseVariable, overall_effect)
  #   ), 
  #   height = 0.1, 
  #   color = "blue", 
  #   linewidth = 1, 
  #   inherit.aes = FALSE
  # ) + 
  # Vertical line at 0 with increased thickness
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  # Custom color scale (hides legend for response variables)
  scale_color_manual(values = custom_colors, guide = "none") +
  # Custom size scale for number of studies
  scale_size_continuous(range = c(2, 6), name = "Number of Studies") +
  # # Scaled x-axis for relative differences
  # scale_x_continuous(
  #   breaks = c(-1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5), 
  #   labels = c("-1", "-0.5", "0", "0.5", "1", "1.5", "2", "2.5"),
  #   expand = c(0.01, 0.01)
  # ) +
  # Pseudo-logarithmic x-axis scale
  scale_x_continuous(
    trans = scales::pseudo_log_trans(sigma = 0.1),  # Pseudo-log transformation
    breaks = c(-1, -0.5, 0, 0.5, 1, 2),  # Custom breakpoints
    labels = scales::label_number(accuracy = 0.1)  # Custom label formatting
  ) +
  # Customize plot labels and title
  labs(
    title = "Beeswarm Forest Plot of Effect Sizes (Scaled X-Axis)",
    x = "Effect Size (Ratio of Means, ROM)",
    y = "Response Variable"
  ) +
  # Minimal theme with increased text size
  theme_minimal(base_size = 16) +
  theme(
    axis.text.y = element_text(size = 14),
    axis.text.x = element_text(size = 14),
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )

# Display the updated plot
print(beeswarm_forest_plot_scaled)

```

```{r}
# Add heterogeneity measures as annotations
annotated_forest_plot <- beeswarm_forest_plot_scaled +
  geom_text(
    data = aggregated_data,
    aes(
      x = 2.5,  # Position on the right-hand side of the plot
      y = reorder(ResponseVariable, overall_effect),
      label = paste0("I²: ", round(heterogeneity$I2, 1), "%, ", "Tau²: ", round(heterogeneity$Tau2, 2))
    ),
    size = 4, 
    hjust = 1
  )
print(annotated_forest_plot)
```
Additional elements that can be added to the forrestplot


Publication Bias Tests
Results of Egger’s test, Begg’s test, or funnel plot asymmetry tests could be summarized and shown.




Model Fit Statistics (AIC, BIC, Log-Likelihood)
Report the model's Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or log-likelihood as a measure of fit.
These metrics can be added to the plot title or as a subtitle.
Example Subtitle:

subtitle = "Simplified Model: AIC = 1423.6, BIC = 1450.8, Log-Likelihood = -704.8"

Outlier Influence Diagnostics
Include results from influence diagnostics (e.g., Cook’s distance, leave-one-out analysis).
Highlight influential studies or response variables in the plot, perhaps with special shapes or colors.


. Cumulative Meta-Analysis Results
Show cumulative effect sizes calculated by adding one study at a time in order of publication year.
This helps assess temporal trends in the evidence base.
Implementation:
Use a separate plot or integrate cumulative trends as a line within the current plot.


Subgroup Analysis or Moderator Effects
Display subgroup analyses or moderator effects (e.g., based on study design, location, crop type).
Include these as a secondary legend or overlay on the forest plot.


Table to accompany the Forest Plot

```{r}

```







Funnel Plot

```{r}
# Example meta-analysis dataset
funnel_data <- data.frame(
  yi = detailed_forest_data$EffectSize,
  vi = detailed_forest_data$Variance,
  study = detailed_forest_data$Study,
  response = detailed_forest_data$ResponseVariable
)
```











To produce a table similar to the one in the "cabbage paper," summarizing the effect of moderator variables on response variables, you can follow these steps. The table will display the significance (p-values) of the moderators for different response variables based on your simplified meta-analysis model.

```{r}
# Fit simplified models (assuming this has already been done)
model_results <- lapply(names(v_matrices), function(response) {
  data_subset <- meta_data[meta_data$response_variable == response, ]
  v_matrix <- v_matrices[[response]]
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture") # Define moderators
  fit_simplified_model(data_subset, response, v_matrix, moderators)
})
names(model_results) <- names(v_matrices)
```

```{r}
# Extract p-values for each model and moderator
extract_p_values <- function(model, response_variable) {
  if (is.null(model)) return(NULL)
  
  summary_data <- summary(model)
  p_values <- summary_data$coefficients[-1, "pval"]  # Exclude the intercept
  moderators <- rownames(summary_data$coefficients)[-1]
  
  data.frame(
    ResponseVariable = response_variable,
    Moderator = moderators,
    P_Value = p_values,
    stringsAsFactors = FALSE
  )
}

# Combine p-values across all response variables
p_values_table <- do.call(
  rbind,
  lapply(names(model_results), function(response) {
    extract_p_values(model_results[[response]], response)
  })
)
```

```{r}
# Reshape the table for final output

formatted_table <- p_values_table %>%
  pivot_wider(
    names_from = ResponseVariable,
    values_from = P_Value
  ) %>%
  arrange(Moderator)
```

```{r}
# Add aspects manually (example mapping)
aspects <- c(
  "tree_type" = "Genetic",
  "crop_type" = "Genetic",
  "age_system" = "Temporal",
  "season" = "Temporal",
  "soil_texture" = "Spatial"
)

formatted_table <- formatted_table %>%
  mutate(Aspect = aspects[Moderator]) %>%
  relocate(Aspect, .before = Moderator)
```

```{r}
# Highlight significant p-values
formatted_table <- formatted_table %>%
  mutate(across(where(is.numeric), ~ ifelse(. < 0.05, sprintf("**%.3f**", .), sprintf("%.3f", .))))
```

```{r}
# Render as Publication-Ready Table
formatted_table %>%
  gt() %>%
  tab_header(
    title = "Effect of Moderator Variables on Ecosystem Services",
    subtitle = "Significant effects (P < 0.05) are highlighted."
  )
```




Protocol with Four Models Fit Meta-Analysis

```{r}
##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES WITH `-1` INTERCEPT REMOVAL APPROACH
##########################################################################################################################################

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Models for Meta-Analysis with Intercept Removal Approach

##########################################################################################################################################
# Fit Models with Increasing Complexity
##########################################################################################################################################

# Function to fit models
fit_model <- function(data_subset, response_variable, v_matrix, moderators, random_effects = NULL, intercept = TRUE, include_interaction = FALSE) {
  cat("\nFitting model for response variable:", response_variable, "...\n")

  # Ensure moderators are valid
  if (is.null(moderators) || length(moderators) == 0) {
    moderator_formula <- ~ 1  # Intercept-only model
  } else {
    # Build moderator formula with or without interactions
    if (include_interaction) {
      # Include all interactions among moderators
      moderator_formula <- if (intercept) {
        as.formula(paste("yi ~", paste(moderators, collapse = " * ")))
      } else {
        as.formula(paste("yi ~", paste(moderators, collapse = " * "), "- 1"))
      }
    } else {
      # Build moderator formula with or without intercept
      moderator_formula <- if (intercept) {
        as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
      } else {
        as.formula(paste("yi ~", paste(moderators, collapse = " + "), "- 1"))
      }
    }
  }

  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################################################################################
# Fit Models with Hierarchical Complexity
##########################################################################################################################################

model_results <- list()

for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")

  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]

  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit various models for the response variable and store results in a nested list
  model_results[[response]] <- list(
    
    # Null model: Intercept-only model, no random effects, no moderators
    A_null = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = NULL,                    # No moderators
      intercept = TRUE                      # Include intercept
    ),

    # Minimal random effects model: Includes random effect at the experiment level, no moderators
    B_minimal_random = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = NULL,                    # No moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),

    # Fixed effects model: Includes specified moderators, no random effects
    C_fixed_effects = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      intercept = TRUE                      # Include intercept
    ),

    # Random effects model: Includes specified moderators and a random effect at the experiment level
    D_random_effects = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),

    # Random effects model with interaction: Includes interactions among moderators and a random effect at the experiment level
    E_random_effects_interaction = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE,                     # Include intercept
      include_interaction = TRUE            # Include all interactions
    ),

    # Full model: Includes specified moderators and multiple random effects
    F_full = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = list(                # Include multiple random effects
        ~ 1 | id_article/response_variable, # Random effect for nested structure of articles and variables
        ~ 1 | exp_id                        # Random effect at the experiment level
      ),
      intercept = TRUE                      # Include intercept
    ),

    # Full interaction model: Includes all interactions among moderators and multiple random effects
    G_full_interaction = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = list(                # Include multiple random effects
        ~ 1 | id_article/response_variable, # Random effect for nested structure of articles and variables
        ~ 1 | exp_id                        # Random effect at the experiment level
      ),
      intercept = TRUE,                     # Include intercept
      include_interaction = TRUE            # Include all interactions
    )
  )
}

##########################################################################################################################################
# Save All Fitted Models
##########################################################################################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in one file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all.rds"))
cat("\nAll models have been saved successfully in a single file!\n")

# Save individual models in separate files
saveRDS(lapply(model_results, `[[`, "A_null"), file = file.path(output_dir, "fitted_models_A_null.rds"))
saveRDS(lapply(model_results, `[[`, "B_minimal_random"), file = file.path(output_dir, "fitted_models_B_minimal_random.rds"))
saveRDS(lapply(model_results, `[[`, "C_fixed_effects"), file = file.path(output_dir, "fitted_models_C_fixed_effects.rds"))
saveRDS(lapply(model_results, `[[`, "D_random_effects"), file = file.path(output_dir, "fitted_models_D_random_effects.rds"))
saveRDS(lapply(model_results, `[[`, "E_random_effects_interaction"), file = file.path(output_dir, "fitted_models_E_random_effects_interaction.rds"))
saveRDS(lapply(model_results, `[[`, "F_full"), file = file.path(output_dir, "fitted_models_F_full.rds"))
saveRDS(lapply(model_results, `[[`, "G_full_interaction"), file = file.path(output_dir, "fitted_models_G_full_interaction.rds"))

cat("\nAll models have been saved successfully in separate files!\n")
##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################

# Last go (18/01-2025)
# Total time taken: 23.19483 secs
```

The code functions as intended, effectively fitting multiple models for each response variable using precomputed variance matrices and handling variations in intercept inclusion and random effects specifications. The use of parallel processing enhances efficiency, while error handling ensures robustness during model fitting. Output messages indicate some expected issues, such as missing data leading to omitted rows, multicollinearity resulting in redundant predictors, and high variance ratios that could compromise stability in certain cases. Warnings about single-level factors in random effects suggest limited variability, which may require adjustments to the random-effects structure.

To improve the analysis, it is important to address missing data, potentially through imputation, and to examine multicollinearity among moderators using diagnostic measures like variance inflation factors. Stabilizing large variance ratios with transformations or alternative modeling approaches could enhance result reliability. Single-level random effects should be removed or reorganized to avoid redundancy.

Performance-wise, the total runtime of approximately 23 seconds is reasonable, and the successful saving of models in both single and separate files ensures accessibility for further analysis. Future efforts should prioritize debugging high variance ratios and redundant predictors, document the handling of omitted rows, and thoroughly evaluate the fitted models for each response variable. Diagnostic plots can provide additional insights into model fit and variability, ensuring the outputs are robust and interpretable.





This approach ensures systematic, transparent evaluation of moderators, random effects, and their combined influence on effect sizes.

```{r}
##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

# Redefining the workflow to integrate the 'cabbage approach' for incremental inclusion of moderators

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Models for Meta-Analysis with Stepwise Moderator Inclusion

##########################################################################################################################################
# Function to fit models with one moderator at a time
fit_model_cabbage <- function(data_subset, response_variable, v_matrix, moderator, random_effects = NULL, intercept = TRUE) {
  cat("\nFitting model for response variable:", response_variable, "with moderator:", moderator, "...\n")

  # Build the formula for the moderator
  moderator_formula <- if (!is.null(moderator)) {
    if (intercept) {
      as.formula(paste("yi ~", moderator))
    } else {
      as.formula(paste("yi ~", moderator, "- 1"))
    }
  } else {
    ~ 1  # Intercept-only model
  }

  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, "with moderator", moderator, ":", e$message, "\n")
    return(NULL)
  })

  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, "with moderator:", moderator, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################################################################################
# Fit Models for Each Response Variable with Incremental Moderator Inclusion
##########################################################################################################################################

model_results <- list()

for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")

  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]

  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit models step-by-step for the response variable
  model_results[[response]] <- list(
    
    # Null model: Intercept-only model, no random effects, no moderators
    A_null = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                     # No moderators
      intercept = TRUE                      # Include intercept
    ),

    # Minimal random effects model: Includes random effect at the experiment level, no moderators
    B_minimal_random_incremental = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                     # No moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),

    # Incremental model without random effects: Adds moderators incrementally
    C_incremental_no_random_incremental = lapply(moderators, function(moderator) {
      fit_model_incremental(
        data_subset = data_subset,
        response_variable = response,
        v_matrix = v_matrix,
        moderator = moderator,             # Add one moderator
        random_effects = NULL,             # No random effects
        intercept = TRUE                   # Include intercept
      )
    }),

    # Incremental model with random effects: Adds moderators incrementally
    D_incremental_random_incremental = lapply(moderators, function(moderator) {
      fit_model_incremental(
        data_subset = data_subset,
        response_variable = response,
        v_matrix = v_matrix,
        moderator = moderator,             # Add one moderator
        random_effects = ~ 1 | exp_id,     # Random effect at the experiment level
        intercept = TRUE                   # Include intercept
      )
    }),

    # Base intercept-only model with both fixed and random effects (new model for testing)
    E_intercept_fixed_random_incremental = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                    # No moderators
      random_effects = ~ 1 | exp_id,       # Random effect at the experiment level
      intercept = TRUE                     # Include intercept
    )
  )
}

##########################################################################################################################################
# Save All Fitted Models
##########################################################################################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in one file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all_incremental.rds"))
cat("\nAll models have been saved successfully in a single file!\n")

# Save individual models in separate files
for (response in names(model_results)) {
  saveRDS(model_results[[response]], file = file.path(output_dir, paste0("fitted_models_", response, "_incremental.rds")))
}

cat("\nAll models have been saved successfully in separate files!\n")
##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################

```

The updated workflow of model structures and fitting, incorporates the "cabbage approach" for meta-analysis by incrementally adding individual moderators to assess
their impact systematically. This allows for testing the contributions of each moderator while retaining flexibility to compare models with and without random effects.

### Key Features of the Workflow
1. **Null Model**:
   - A baseline intercept-only model estimating the global average effect size without moderators or random effects.
   - Useful for understanding the unadjusted overall effect size for each response variable.

2. **Minimal Random Effects Model**:
   - Includes a random effect at the experiment level but no moderators.
   - Captures between-experiment variability, providing a robust estimate of heterogeneity in the dataset.

3. **Incremental Models**:
   - Moderators are added one at a time to the models.
   - **Without Random Effects**: Examines the isolated impact of each moderator without accounting for between-experiment variability.
   - **With Random Effects**: Adds a random effect to capture variability across experiments while systematically evaluating each moderator’s contribution.

4. **Base Model with Fixed and Random Effects**:
   - Includes both fixed and random effects with no moderators.
   - Serves as a benchmark to assess the importance of random effects alongside the intercept.

### Benefits of This Workflow
- **Incremental Moderator Assessment**: Aligns with the cabbage approach by isolating and evaluating the contribution of each moderator individually.
- **Heterogeneity Testing**: Provides flexibility to assess variance explained by random effects versus moderators.
- **Scalability**: Accommodates models with increasing complexity, enabling comparisons across response variables.

In this updated workflow, the intercept represents the global average effect size, providing a baseline measurement when no moderators are included in the model. This is equivalent to an intercept-only model, capturing the overall mean effect across studies for a given response variable.

Clarifications:
Intercept Inclusion (intercept = TRUE):

This ensures that the model estimates a global average (mean effect size) alongside the effects of moderators, where applicable.
For example, in the model intercrop_des_pr <- update(ma_base_pr, mods = ~ Intercropping.design), the global intercept is included by default unless explicitly removed with -1. This means the model estimates the overall effect and the incremental effects of levels within the "Intercropping.design" variable.
Intercept Removal (intercept = FALSE or -1):

Removing the intercept isolates the effects of individual moderators. For example, in intercrop_des_pr <- update(ma_base_pr, mods = ~ Intercropping.design - 1), the model estimates the effects of each level of "Intercropping.design" directly, without an overall mean effect.
Relevance to the 'Cabbage Approach':
The workflow supports the cabbage approach by allowing for incremental addition of moderators to assess their individual contributions to explaining heterogeneity. Including the intercept (intercept = TRUE) ensures that models test both the global effect and the incremental moderator effects, aligning with the philosophy of stepwise moderator assessment.

Importance:
Including or excluding the intercept depends on the research question. If the goal is to understand deviations from the global mean, the intercept is necessary. For comparisons purely within levels of a moderator, the intercept can be omitted to focus solely on those levels.

When deciding whether to include or omit the intercept in a meta-analysis or regression model, several considerations should guide the choice. These include the research question, the nature of the data, and the potential impact on model interpretation and performance. Here are key factors:

---

### **1. The Role of the Intercept**
- **Including the Intercept**:
  - Estimates the global mean effect size (baseline effect) across all studies when no moderators are included or after accounting for moderators.
  - Provides a reference point against which moderator effects are measured (e.g., deviations from the global mean).
  - Useful when comparing levels of moderators to the overall mean or understanding the general trend in the data.

- **Omitting the Intercept**:
  - Forces the model to estimate individual effects for each level of the moderator without reference to a baseline (global average).
  - Useful in models with categorical moderators, where the focus is on comparing the levels of a factor directly.

---

### **2. Model Interpretation**
- **Including the Intercept**:
  - Moderator effects are interpreted as deviations from the global mean. This is often easier to understand, particularly in applied contexts.
  - Allows for direct interpretation of the baseline effect when moderators are not significant.

- **Omitting the Intercept**:
  - Moderator effects are interpreted independently, which may be appropriate for categorical moderators without a natural reference category.
  - The model may lack a global perspective, making it less intuitive for general conclusions.

---

### **3. Limitations and Bias**
- **Including the Intercept**:
  - May introduce **collinearity** when categorical moderators are included, particularly if all levels are included without centering or adjustment.
  - Can obscure the direct effects of moderators when the global mean absorbs much of the explained variance.

- **Omitting the Intercept**:
  - Leads to **biased estimates** if the reference effect (global mean) is meaningful but excluded.
  - May result in overfitting when too many parameters are estimated without a baseline for comparison.
  - Interpretation may become less straightforward, especially if multiple moderators interact.

---

### **4. Model Performance**
- **Including the Intercept**:
  - Generally improves stability, particularly for small or imbalanced datasets, by anchoring the model with a baseline.
  - Reduces the risk of overfitting, especially in complex models with many parameters.

- **Omitting the Intercept**:
  - May lead to better fit for specific comparisons (e.g., among levels of a categorical moderator) but at the cost of generalizability.
  - Requires more data to estimate individual effects accurately, as the baseline information is excluded.

---

### **5. Research Question and Context**
- **When to Include the Intercept**:
  - When seeking to estimate an overall effect size across studies or understand the impact of moderators relative to a global baseline.
  - When the global mean is meaningful and relevant for the research context.

- **When to Omit the Intercept**:
  - When comparing effects within levels of a categorical moderator without regard to a global reference.
  - When the baseline (global mean) is not of interest or is not meaningful in the study context.

---

### **Practical Recommendations**
- If unsure, start by including the intercept. Test its significance and assess its impact on model performance and interpretation.
- For categorical moderators, consider the natural reference level and interpretability when deciding whether to remove the intercept.
- Regularly check for collinearity and overfitting, especially in models with numerous moderators or interactions.
- Use model diagnostics (e.g., AIC, BIC, R²) to compare the fit and performance of models with and without the intercept.

By aligning the decision with the study’s objectives and the nature of the data, you can make informed choices that balance interpretability and model performance.















```{r}
####################################################################################################################################################
# Load the Saved Models and Inspect Results
####################################################################################################################################################

# Load the saved models
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(output_dir, "fitted_models_A_null.rds"))
minimal_random_results <- readRDS(file.path(output_dir, "fitted_models_B_minimal_random.rds"))
fixed_effects_results <- readRDS(file.path(output_dir, "fitted_models_C_fixed_effects.rds"))
random_effects_results <- readRDS(file.path(output_dir, "fitted_models_D_random_effects.rds"))
random_effects_interaction_results <- readRDS(file.path(output_dir, "fitted_models_E_random_effects_interaction.rds"))
full_results <- readRDS(file.path(output_dir, "fitted_models_F_full.rds"))
full_interaction_results <- readRDS(file.path(output_dir, "fitted_models_G_full_interaction.rds"))

# Inspect the names of the response variables available
names(full_results)

# Check the structure of the model results for a specific response variable (e.g., "Biodiversity")
full_results[["Biodiversity"]] |> str()

##########################################################################################################################################
# Identify Response Variables with Failed Model Fits
##########################################################################################################################################

# Combine all model results into a single list for checking
failed_fits <- list(
  A_null = sapply(null_model_results, is.null),
  B_minimal_random = sapply(minimal_random_results, is.null),
  C_fixed_effects = sapply(fixed_effects_results, is.null),
  D_random_effects = sapply(random_effects_results, is.null),
  E_random_effects_interaction = sapply(random_effects_interaction_results, is.null),
  F_full = sapply(full_results, is.null),
  G_full_interaction = sapply(full_interaction_results, is.null)
)

# Response variables with any failed models
failed_responses <- unique(unlist(lapply(failed_fits, function(x) names(x)[x])))
if (length(failed_responses) > 0) {
  cat("\nFailed model fits detected for the following response variables:", failed_responses, "\n")
} else {
  cat("\nNo failed model fits detected.\n")
}

##########################################################################################################################################
# Analyze Successful Models (Example)
##########################################################################################################################################

# Extract successful Full Models
successful_full_models <- full_results[!sapply(full_results, is.null)]
names(successful_full_models)

# Check the structure of the model results for a specific response variable (e.g., "Crop yield")
successful_full_models[["Crop yield"]] |> str()

```































```{r}
####################################################################################################################################################
# Inspect Model Results
####################################################################################################################################################

# Display available response variables in the Incremental Random Model (as an example)
cat("\nAvailable response variables in Incremental Random Model:\n")
names(model_results[["D_incremental_random"]]) |> print()

# Check the structure of a specific model for a response variable (e.g., "Biodiversity")
cat("\nStructure of the Incremental Random Model for 'Biodiversity':\n")
model_results[["D_incremental_random"]][["Biodiversity"]] |> str()

####################################################################################################################################################
# Identify Failed Model Fits
####################################################################################################################################################

# Check for failed fits across all models
failed_fits <- lapply(model_results, function(models) {
  sapply(models, is.null)
})

# Extract response variables with any failed fits
failed_responses <- unique(unlist(lapply(failed_fits, function(fit_status) {
  names(fit_status)[fit_status]
})))

# Report failed model fits
if (length(failed_responses) > 0) {
  cat("\nFailed model fits detected for the following response variables:\n")
  print(failed_responses)
} else {
  cat("\nNo failed model fits detected.\n")
}

####################################################################################################################################################
# Extract Successful Models for Diagnostics
####################################################################################################################################################

# Example: Extract successful Incremental Random Models
successful_incremental_random_models <- model_results[["D_incremental_random"]][!sapply(model_results[["D_incremental_random"]], is.null)]

# List available response variables in successful Incremental Random Models
cat("\nResponse variables with successful Incremental Random Model fits:\n")
names(successful_incremental_random_models) |> print()

# Inspect the structure of a successful Incremental Random Model for a specific response variable (e.g., "Crop yield")
cat("\nStructure of the Incremental Random Model for 'Crop yield':\n")
successful_incremental_random_models[["Crop yield"]] |> str()

####################################################################################################################################################
# Next Steps: Diagnostics and Model Comparisons
####################################################################################################################################################

# This code prepares the loaded models for further diagnostics and comparisons, such as:
# 1. Comparing AIC, BIC, and other criteria across models.
# 2. Quantifying variance and heterogeneity components.
# 3. Analyzing the contribution of moderators to the effect sizes.

```



```{r}
##########################################################################
# Extract AIC and Fit Statistics for Each Response Variable
##########################################################################

# Initialize a data frame to store fit statistics results
fit_stats_results <- data.frame(
  Response = character(),
  Model = character(),
  AIC = numeric(),
  BIC = numeric(),
  REML = numeric(),
  ML = numeric(),
  LogLikelihood = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each response variable
for (response in names(model_results)) {
  cat("\nExtracting fit statistics for response variable:", response, "\n")

  # Retrieve models for the current response variable
  models <- model_results[[response]]

  # Loop through each model type
  for (model_type in names(models)) {
    model <- models[[model_type]]

    # Skip if the model is NULL
    if (is.null(model)) {
      next
    }

    # Compute fit statistics
    log_likelihood <- tryCatch({
      logLik(model)
    }, error = function(e) {
      NA
    })

    aic <- tryCatch({
      AIC(model)
    }, error = function(e) {
      NA
    })

    bic <- tryCatch({
      BIC(model)
    }, error = function(e) {
      NA
    })

    reml <- tryCatch({
      model$fit.stats["REML", "REML"]
    }, error = function(e) {
      NA
    })

    ml <- tryCatch({
      model$fit.stats["ML", "ML"]
    }, error = function(e) {
      NA
    })

    # Append results to the data frame
    fit_stats_results <- rbind(
      fit_stats_results,
      data.frame(
        Response = response,
        Model = model_type,
        AIC = aic,
        BIC = bic,
        REML = reml,
        ML = ml,
        LogLikelihood = log_likelihood,
        stringsAsFactors = FALSE
      )
    )
  }
}

##########################################################################
# Save and Inspect Results
##########################################################################
write.csv(fit_stats_results, "fit_stats_results_with_aic_bic.csv", row.names = FALSE)
cat("Fit statistics with AIC and BIC saved to 'fit_stats_results_with_aic_bic.csv'\n")
print(fit_stats_results)
```

```{r}
#######################################################################################################
# Organize Nested Meta-Analysis Results into a Condensed Data Frame
#######################################################################################################

extract_model_metrics <- function(model, response, model_type) {
  if (is.null(model)) {
    return(data.frame(
      Response = response,
      Model_Type = model_type,
      k = NA,
      tau2 = NA,
      QE = NA,
      QEp = NA,
      QM = NA,
      QMp = NA,
      AIC = NA,
      BIC = NA,
      LogLik = NA,
      Coefficients = NA,
      stringsAsFactors = FALSE
    ))
  }
  
 # Modify the coefficients extraction to remove the "=" sign
coefficients <- if (!is.null(model$b) && is.numeric(model$b)) {
  # Remove '=' sign and format
  coeffs <- paste0(names(model$b), "=", round(model$b, 4), collapse = "; ")
  gsub("=", "", coeffs)  # Remove all '=' signs
} else {
  NA
  }
  
  # Extract AIC, BIC, LogLik
  aic <- tryCatch(if (!is.null(model$fit.stats)) model$fit.stats[1, "REML"] else NA, error = function(e) NA)
  bic <- tryCatch(if (!is.null(model$fit.stats)) model$fit.stats[2, "REML"] else NA, error = function(e) NA)
  loglik <- tryCatch(if (!is.null(model$fit.stats)) model$fit.stats[3, "REML"] else NA, error = function(e) NA)

  # Extract metrics
  data.frame(
    Response = response,
    Model_Type = model_type,
    k = if (!is.null(model$k)) model$k else NA,
    tau2 = if (!is.null(model$tau2)) model$tau2 else NA,
    QE = if (!is.null(model$QE)) model$QE else NA,
    QEp = if (!is.null(model$QEp)) model$QEp else NA,
    QM = if (!is.null(model$QM)) model$QM else NA,
    QMp = if (!is.null(model$QMp)) model$QMp else NA,
    AIC = aic,
    BIC = bic,
    LogLik = loglik,
    Coefficients = coefficients,
    stringsAsFactors = FALSE
  )
}

# Initialize an empty data frame
results_summary <- data.frame()

# Loop through each response variable and model
for (response in names(model_results)) {
  for (model_type in names(model_results[[response]])) {
    model <- model_results[[response]][[model_type]]
    
    # Check if sub-models exist (e.g., [[1]], [[2]])
    if (is.list(model) && all(sapply(model, function(x) inherits(x, "rma")))) {
      sub_model_metrics <- do.call(rbind, lapply(seq_along(model), function(i) {
        extract_model_metrics(model[[i]], response, paste0(model_type, "_", i))
      }))
      # Aggregate sub-model metrics (e.g., calculate mean tau2, QM, etc.)
      aggregated_metrics <- data.frame(
        Response = response,
        Model_Type = model_type,
        k = mean(sub_model_metrics$k, na.rm = TRUE),
        tau2 = mean(sub_model_metrics$tau2, na.rm = TRUE),
        QE = mean(sub_model_metrics$QE, na.rm = TRUE),
        QEp = mean(sub_model_metrics$QEp, na.rm = TRUE),
        QM = mean(sub_model_metrics$QM, na.rm = TRUE),
        QMp = mean(sub_model_metrics$QMp, na.rm = TRUE),
        AIC = mean(sub_model_metrics$AIC, na.rm = TRUE),
        BIC = mean(sub_model_metrics$BIC, na.rm = TRUE),
        LogLik = mean(sub_model_metrics$LogLik, na.rm = TRUE),
        Coefficients = paste(unique(sub_model_metrics$Coefficients), collapse = " | "),
        stringsAsFactors = FALSE
      )
      results_summary <- rbind(results_summary, aggregated_metrics)
    } else {
      # Extract metrics for single models
      results_summary <- rbind(
        results_summary,
        extract_model_metrics(model, response, model_type)
      )
    }
  }
}

# Filter for models with complete outputs (all required metrics available)
results_summary <- results_summary[complete.cases(results_summary[, c("k", "tau2", "QE", "QEp")]), ]

# Diagnostic Outputs: Flag incomplete or missing results
results_summary$Diagnostic_Flag <- ifelse(is.na(results_summary$k) | is.na(results_summary$tau2) | 
                                          is.na(results_summary$QE) | is.na(results_summary$QEp), "Incomplete", "Complete")

# View the condensed dataset
print(results_summary)
results_summary |> glimpse()

# Save the summary
# summary_file <- file.path(output_dir, "meta_analysis_results_summary_with_AIC_BIC_LogLik.csv")
# write.csv(results_summary, summary_file, row.names = FALSE)
# cat("\nSummary saved successfully at:", summary_file, "\n")

```


```{r}
# Assuming your data frame is named 'results_summary'
df_split <- results_summary %>%
  # Remove the '=' sign if it exists in the Coefficients column
  mutate(Coefficients = gsub("=", "", Coefficients)) %>%
  
  # Split the coefficients by '|' into separate columns (up to 10 columns)
  separate(Coefficients, 
           into = paste0("Coeff_", seq(1, 10)), 
           sep = " \\| ", 
           extra = "merge", 
           fill = "right") %>%
  
  # Unnest the coefficients from each 'Coeff_' column (create rows for each)
  pivot_longer(cols = starts_with("Coeff_"),
               names_to = "Coefficient_Group",
               values_to = "Coefficient") %>%
  
  # Now split each coefficient by ';' into separate rows (unnest them)
  separate_rows(Coefficient, sep = ";") %>%
  
  # Remove any rows where Coefficients are NA
  filter(!is.na(Coefficient)) %>%
  
  # Optionally, trim any leading or trailing spaces from the coefficients
  mutate(Coefficient = trimws(Coefficient)) |> 
  
  mutate(Coefficient = as.numeric(Coefficient))

# View the modified data frame
print(df_split)

df_split |> str()

# Convert the data back to wide format
df_wide <- df_split %>%
  pivot_wider(
    names_from = Coefficient_Group,   # The column that will become the new columns
    values_from = Coefficient,        # The values to fill in these new columns
    values_fn = list(Coefficient = ~ paste(. , collapse = "; "))  # If there are multiple coefficients, combine them
  )

# View the modified data frame in wide format
print(df_wide)

df_wide |> str()

# Optional: View structure of the new wide format
df_wide |> str()

```

```{r}
# Inspecting the levels of 'exp_id'
unique_levels <- length(unique(model_results$`Crop yield`$B_minimal_random_incremental$data$exp_id))
print(unique_levels)
```



























```{r}
# Ensure data is in the correct format
fit_stats_results$Model <- factor(
  fit_stats_results$Model,
  levels = c(
    "A_null", 
    "B_minimal_random", 
    "C_fixed_effects", 
    "D_random_effects", 
    "E_random_effects_interaction", 
    "F_full", 
    "G_full_interaction"
  )
)

# Pivot the data to long format for ggplot
fit_stats_long <- fit_stats_results |> 
  pivot_longer(
    cols = c("AIC", "BIC", "LogLikelihood"),
    names_to = "Metric",
    values_to = "Value"
  )

# Create the plot
all_model_fit_stats_aic_bic_loglik_plot <- fit_stats_long |> 
  ggplot(aes(x = Response, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Model, scales = "free_y", nrow = 3) +  # Free y-axis scaling for each model
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    panel.grid.minor = element_blank(),
    strip.text = element_text(size = 12, face = "bold")
  ) +
  labs(
    title = "Model Fit Statistics by Response Variable and Model Type",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Metric"
  )

# Display the plot
print(all_model_fit_stats_aic_bic_loglik_plot)
```

Save plot of all fitted models with diagnostics (AIC, BIC, LogLikelihood)

```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 120),        # Increase title size
    plot.subtitle = element_text(size = 70),
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 100),       # Increase axis title size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 100),
    legend.title = element_text(size = 50),
    legend.position = "top",
    legend.text = element_text(size = 50),
    axis.text.x = element_text(size = 100,
                               angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
all_model_fit_stats_aic_bic_loglik_plot <- all_model_fit_stats_aic_bic_loglik_plot + theme_custom


# Save the enhanced plot
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")
ggsave(
  filename = file.path(output_dir, "all_model_fit_stats_aic_bic_loglik_plot.png"),
  plot = all_model_fit_stats_aic_bic_loglik_plot,
  width = 16, height = 10, dpi = 600,
  bg = "white"
)
```

Grouped Bar Chart for Model Fit Metrics
```{r}
# Ensure Model is a factor with the correct order
fit_stats_results$Model <- factor(
  fit_stats_results$Model,
  levels = c(
    "A_null", 
    "B_minimal_random", 
    "C_fixed_effects", 
    "D_random_effects", 
    "E_random_effects_interaction", 
    "F_full", 
    "G_full_interaction"
  )
)

# Pivot the data to long format for ggplot
fit_stats_long <- fit_stats_results |> 
  pivot_longer(
    cols = c("AIC", "BIC", "LogLikelihood"),
    names_to = "Metric",
    values_to = "Value"
  )

# Create grouped bar chart
fit_stats_plot <- ggplot(fit_stats_long, aes(x = Response, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric, scales = "free", nrow = 1) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top"
  ) +
  labs(
    title = "Model Fit Metrics by Response Variable and Model",
    x = "Response Variable",
    y = "Metric Value",
    fill = "Model"
  )

# Display the plot
fit_stats_plot
```

Combined Lower and Upper Range Bar Charts
```{r}
# Create lower range plot
p1 <- ggplot(fit_stats_long, aes(x = Response, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric, scales = "free", nrow = 1) +
  coord_cartesian(ylim = c(0, 15000)) +  # Adjust lower range
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.text = element_blank()
  ) +
  labs(
    x = NULL,
    y = "Metric Value (Lower Range)",
    fill = "Model"
  )

# Create upper range plot
p2 <- ggplot(fit_stats_long, aes(x = Response, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric, scales = "free", nrow = 1) +
  coord_cartesian(ylim = c(200000, 250000)) +  # Adjust upper range
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "none"
  ) +
  labs(
    title = "Model Fit Metrics by Response Variable and Model",
    x = NULL,
    y = "Metric Value (Upper Range)"
  )

# Combine plots with a gap
p_combined <- p2 / plot_spacer() / p1 + 
  plot_layout(heights = c(1, 0.1, 2))

# Display the combined plot
p_combined
```

Average AIC and Best Models
```{r}
# Calculate average AIC for each model type
average_aic <- fit_stats_results %>%
  group_by(Model) %>%
  summarise(Average_AIC = mean(AIC, na.rm = TRUE)) %>%
  arrange(Average_AIC)

# Identify the model type with the lowest average AIC
best_model_type <- average_aic %>%
  slice(1) %>%
  pull(Model)

cat("The best model type based on average AIC is:", best_model_type, "\n")

# Identify the best model for each response variable based on AIC
best_models <- fit_stats_results %>%
  group_by(Response) %>%
  filter(AIC == min(AIC, na.rm = TRUE)) %>%
  ungroup()

# Display results
average_aic
best_models
```

Create a gt table for average AIC by model
```{r}
# Create a gt table for average AIC by model
average_aic_table <- average_aic %>%
  gt() %>%
  tab_header(
    title = "Average AIC by Model Type",
    subtitle = "Comparison of Model Performance Across All Response Variables"
  ) %>%
  cols_label(
    Model = "Model Type",
    Average_AIC = "Average AIC"
  ) %>%
  fmt_number(
    columns = vars(Average_AIC),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  )

# Save the table as an HTML file
gtsave(
  data = average_aic_table,
  filename = file.path(here::here("DATA", "OUTPUT_FROM_R", "TABLES"), "average_aic_table.html")
)

# Create a gt table for the best models by response variable
best_models_gt_table <- best_models %>%
  gt() %>%
  tab_header(
    title = "Best Models by Response Variable",
    subtitle = "Model Performance Comparison Based on AIC"
  ) %>%
  cols_label(
    Response = "Response Variable",
    Model = "Best Model",
    AIC = "AIC",
    BIC = "BIC",
    REML = "REML",
    ML = "ML",
    LogLikelihood = "Log-Likelihood"
  ) %>%
  fmt_number(
    columns = vars(AIC, BIC, REML, ML, LogLikelihood),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  )

best_models_gt_table
```

```{r}
# Save the table as an HTML file
gtsave(
  data = best_models_gt_table,
  filename = file.path(here::here("DATA", "OUTPUT_FROM_R", "TABLES"), "best_models_gt_table.html")
)
```





```{r}
# Calculate the average AIC for each model type across all response variables
average_aic <- fit_stats_results %>%
  group_by(Model) %>%
  summarise(Average_AIC = mean(AIC, na.rm = TRUE)) %>%
  arrange(Average_AIC)


average_aic |> str()

################################################################################################

# Calculate average metrics for each model type
average_metrics <- fit_stats_results %>%
  group_by(Model) %>%
  summarise(
    Average_AIC = mean(AIC, na.rm = TRUE),
    Average_BIC = mean(BIC, na.rm = TRUE),
    Average_LogLikelihood = mean(LogLikelihood, na.rm = TRUE)
  ) %>%
  arrange(Average_AIC)

# Combine metrics for each response variable-model combination
metrics_table <- fit_stats_results %>%
  select(Response, Model, AIC, BIC, LogLikelihood) %>%
  arrange(Response, Model) %>%
  left_join(average_metrics, by = "Model")

# Add a column to indicate the best model for each response variable
metrics_table <- metrics_table %>%
  group_by(Response) %>%
  mutate(Best_Model = ifelse(AIC == min(AIC, na.rm = TRUE), "✓", "")) %>%
  ungroup()

# Add a column to indicate the overall best model based on average AIC
metrics_table <- metrics_table %>%
  mutate(Overall_Best = ifelse(Model == average_metrics$Model[1], "✓", ""))

# Create the gt table
model_comparison_gt_table <- metrics_table %>%
  gt(groupname_col = "Response") %>%
  tab_header(
    title = "Model Comparison by Response Variable and Average Metrics",
    subtitle = "Summary of AIC, BIC, and Log-Likelihood values for each model-response combination and averaged across all responses"
  ) %>%
  cols_label(
    Model = "Model Type",
    AIC = "AIC",
    BIC = "BIC",
    LogLikelihood = "Log-Likelihood",
    Average_AIC = "Average AIC (All Responses)",
    Average_BIC = "Average BIC (All Responses)",
    Average_LogLikelihood = "Average Log-Likelihood (All Responses)",
    Best_Model = "Best Model (Per Response)",
    Overall_Best = "Overall Best Model"
  ) %>%
  fmt_number(
    columns = vars(AIC, BIC, LogLikelihood, Average_AIC, Average_BIC, Average_LogLikelihood),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", color = "green"),
    locations = cells_body(columns = vars(Best_Model, Overall_Best), rows = Best_Model == "✓" | Overall_Best == "✓")
  ) %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = vars(Average_AIC, Average_BIC, Average_LogLikelihood),
      rows = Overall_Best == "✓"
    )
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16,
    row_group.font.size = 16
  ) %>%
  opt_table_font(
    font = list(
      google_font("Lato"),
      default_fonts()
    )
  )


model_comparison_gt_table
```

Save the model_comparison_gt_table table as an HTML file

```{r}
# Save the table as an HTML file
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "TABLES")
gtsave(
  data = model_comparison_gt_table,
  filename = file.path(output_dir, "overall_model_comparison_gt_table.html")
)
```



A more concise version

```{r}
# Summarize data by model type
concise_gt_table_data <- average_metrics %>%
  left_join(
    fit_stats_results %>%
      group_by(Model) %>%
      summarise(
        Best_Per_Response = sum(AIC == min(AIC, na.rm = TRUE)), # Count how many responses selected this model
        .groups = "drop"
      ),
    by = "Model"
  ) %>%
  mutate(
    Overall_Best = ifelse(Model == average_metrics$Model[1], "✓", "") # Mark the overall best model
  ) %>%
  select(Model, Average_AIC, Average_BIC, Average_LogLikelihood, Best_Per_Response, Overall_Best)

# Create a concise gt table
model_comparison_gt_table_concise <- concise_gt_table_data %>%
  gt() %>%
  tab_header(
    title = "Model Comparison: Summary of Metrics Across Responses",
    subtitle = "Average AIC, BIC, and Log-Likelihood for each model type and their performance across response variables"
  ) %>%
  cols_label(
    Model = "Model Type",
    Average_AIC = "Avg. AIC",
    Average_BIC = "Avg. BIC",
    Average_LogLikelihood = "Avg. Log-Likelihood",
    Best_Per_Response = "Best Fit (Per Response)",
    Overall_Best = "Overall Best"
  ) %>%
  fmt_number(
    columns = vars(Average_AIC, Average_BIC, Average_LogLikelihood),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", color = "green"),
    locations = cells_body(columns = vars(Overall_Best), rows = Overall_Best == "✓")
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 18,
    heading.subtitle.font.size = 14
  ) %>%
  opt_table_font(
    font = list(
      google_font("Lato"),
      default_fonts()
    )
  )

model_comparison_gt_table_concise
```

```{r}
# Save the concise table
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "TABLES")
gtsave(
  data = model_comparison_gt_table_concise,
  filename = file.path(output_dir, "model_comparison_gt_table_concise.html")
)
```









```{r}
#######################################################################################
# Step 5: Extract Fitted Data and Prepare for Visualization
#######################################################################################

# Ensure consistent data types across datasets
consistent_col_types <- function(data) {
  data %>%
    mutate(
      crop_type = as.factor(crop_type),
      tree_type = as.factor(tree_type),
      bioclim_sub_regions = as.factor(bioclim_sub_regions),
      alley_width = as.factor(alley_width)
    )
}

# Apply the function to ensure consistency
col_for_impute <- consistent_col_types(col_for_impute)

for (method_name in names(imputed_datasets)) {
  imputed_datasets[[method_name]] <- consistent_col_types(imputed_datasets[[method_name]])
}

# Combine original and imputed datasets into one
visualization_data <- col_for_impute %>%
  mutate(source = "Original") %>%
  bind_rows(
    imputed_datasets[["linear_imputation"]] %>% mutate(source = "Linear Imputation"),
    imputed_datasets[["pmm"]] %>% mutate(source = "PMM"),
    imputed_datasets[["rf"]] %>% mutate(source = "Random Forest"),
    imputed_datasets[["bayesian"]] %>% mutate(source = "Bayesian"),
    imputed_datasets[["upper_quartile"]] %>% mutate(source = "Upper Quartile"),
    imputed_datasets[["mean_imputation"]] %>% mutate(source = "Mean Imputation")
  )

# Visualize imputed values against original

# Scatterplot for silvo_sd
ggplot(visualization_data, aes(x = source, y = silvo_sd_merged, color = source)) +
  geom_jitter(width = 0.2, alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Comparison of Imputed Values for silvo_sd",
    x = "Data Source",
    y = "silvo_sd"
  ) +
  theme(
    # Rotate x-axis text
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

# Scatterplot for control_se
ggplot(visualization_data, aes(x = source, y = control_sd_merged, color = source)) +
  geom_jitter(width = 0.2, alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Comparison of Imputed Values for control_sd",
    x = "Data Source",
    y = "control_sd"
  ) +
  theme(
    # Rotate x-axis text
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```


```{r}
# Extract completed dataset for the PMM method
completed_data_pmm <- mice::complete(imputed_mids_pmm, action = "long")

# View the distribution of `silvo_sd_merged` and `control_sd_merged`
summary(completed_data_pmm$silvo_sd_merged)
summary(completed_data_pmm$control_sd_merged)
```

```{r}
# Add a column to track imputed rows
completed_data_pmm <- mice::complete(imputed_mids_pmm, action = "long")

# Identify imputed rows
completed_data_pmm <- completed_data_pmm %>%
  mutate(imputed = is.na(col_for_impute$silvo_sd_merged[rep(1:nrow(col_for_impute), imputed_mids_pmm$m)]))

# Summarize by observed and imputed
observed_vs_imputed <- completed_data_pmm %>%
  group_by(imputed) %>%
  summarise(
    silvo_sd_mean = mean(silvo_sd_merged, na.rm = TRUE),
    control_sd_mean = mean(control_sd_merged, na.rm = TRUE),
    .groups = "drop"
  )

print(observed_vs_imputed)
```

```{r}
# Visualize imputed values
# Create a column for 'imputed' based on the rows in `completed_data_pmm`
completed_data_pmm <- completed_data_pmm %>%
  mutate(imputed = is.na(silvo_sd_merged))

# Ensure `imputed` column is logically assigned
imputed_status <- is.na(col_for_impute$silvo_sd_merged)
completed_data_pmm$imputed <- imputed_status

# Create the plot
ggplot(completed_data_pmm, aes(x = silvo_sd_merged, fill = imputed)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Density of Imputed vs. Observed Silvo SD",
    x = "Silvo SD",
    fill = "Imputed"
  ) +
  theme_minimal()

```

```{r}
# Evaluate the mice::mice() imputed datasets
# Check convergence diagnostics
imputed_mids |> str()
plot(imputed_mids$data$silvo_sd_merged)
```



```{r}
# Use stripplot to compare observed and imputed values
stripplot(imputed_datasets$pmm, silvo_sd_merged + control_sd_merged ~ .imp,
  cex = c(1, 2), pch = c(20, 20), jitter = TRUE, alpha = 0.4, scales = "free")
```














```{r}
# Step 1: Summarize each imputed dataset
# Quantitative Assessment:
# Calculate summary statistics for each imputed dataset, focusing on proximity to medians (mean proximity for silvo_se and control_se).
# Incorporate additional metrics like variance, range, and RMSE for better decision-making.

imputed_summaries <- list()

for (i in 1:20) {
  data <- mice::complete(imputed_mids_pmm, i) # Extract the i-th imputed dataset

  # Calculate summary statistics for each imputation
  summary <- data %>%
    summarise(
      mean_silvo_sd = mean(silvo_sd_merged, na.rm = TRUE),
      sd_silvo_sd = sd(silvo_sd_merged, na.rm = TRUE),
      mean_control_sd = mean(control_sd_merged, na.rm = TRUE),
      sd_control_sd = sd(control_sd_merged, na.rm = TRUE),
      range_silvo_sd = max(silvo_sd_merged, na.rm = TRUE) - min(silvo_sd_merged, na.rm = TRUE),
      range_control_sd = max(control_sd_merged, na.rm = TRUE) - min(control_sd_merged, na.rm = TRUE)
    )

  imputed_summaries[[i]] <- summary
}

# Combine all summaries into a single data frame
imputed_summaries_df <- bind_rows(imputed_summaries, .id = "imputation")

# Calculate medians for silvo_se and control_se
median_silvo_sd <- median(imputed_summaries_df$mean_silvo_sd)
median_control_sd <- median(imputed_summaries_df$mean_control_sd)

# Add a column calculating Euclidean distance to medians
imputed_summaries_df <- imputed_summaries_df %>%
  mutate(
    distance_from_median = sqrt(
      (mean_silvo_sd - median_silvo_sd)^2 + (mean_control_sd - median_control_sd)^2
    )
  )
```

```{r}
# Step 2: Advanced Quantitative Metrics
# Root Mean Squared Error (RMSE):
# Add RMSE comparison between observed and imputed values for silvo_se and control_se.

# RMSE calculation function
calculate_rmse <- function(observed, imputed) {
  sqrt(mean((observed - imputed)^2, na.rm = TRUE))
}

# Add RMSE to imputed summaries
imputed_summaries_df <- imputed_summaries_df %>%
  rowwise() %>%
  mutate(
    rmse_silvo_sd = calculate_rmse(
      col_for_impute$silvo_sd_merged[!is.na(col_for_impute$silvo_sd_merged)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$silvo_sd_merged[is.na(col_for_impute$silvo_sd_merged)]
    ),
    rmse_control_sd = calculate_rmse(
      col_for_impute$control_sd_merged[!is.na(col_for_impute$control_sd_merged)],
      mice::complete(imputed_mids_pmm, as.numeric(imputation))$control_sd_merged[is.na(col_for_impute$control_sd_merged)]
    )
  )

imputed_summaries_df
```



```{r}
# Step 4: Visual Assessment
# Density Plots for Each Method and Variable:
# Compare observed and imputed distributions for silvo_se and control_se across all imputation methods.

# Prepare data for visualization
observed_values <- list(
  silvo_se = col_for_impute$silvo_se[!is.na(col_for_impute$silvo_se)],
  control_se = col_for_impute$control_se[!is.na(col_for_impute$control_se)]
)

# Combine observed and imputed values for plotting
combined_plot_data <- list()

for (variable in c("silvo_se", "control_se")) {
  for (method_name in names(imputed_datasets)) {
    imputed_values <- imputed_datasets[[method_name]][[variable]][is.na(col_for_impute[[variable]])]

    combined_plot_data[[paste(variable, method_name, sep = "_")]] <- data.frame(
      value = c(observed_values[[variable]], imputed_values),
      type = c(rep("Original", length(observed_values[[variable]])),
               rep("Imputed", length(imputed_values))),
      method = method_name,
      variable = variable
    )
  }
}

###############################################################################
# Combine all data into one frame
imputation_plot_data_all <- bind_rows(combined_plot_data)

###############################################################################
# Remove linear imputation for better visualization
imputation_plot_data_no_linear <- 
  imputation_plot_data_all |> 
  filter(method != "linear_imputation")
```

```{r}
# Generic function to generate density plots
generate_density_plot <- function(data, title_suffix, scale_type = "linear") {
  plot <- ggplot(data, aes(x = value, fill = type)) +
    geom_density(alpha = 0.5) +
    facet_grid(variable ~ method) +
    labs(
      title = paste("Density Comparison:", title_suffix),
      x = ifelse(scale_type == "linear", "Value", "Value (Pseudo-Log Scale)"),
      y = "Density"
    ) +
    theme_minimal() +
    scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
    theme(strip.text = element_text(size = 10, face = "bold"))
  
  if (scale_type == "pseudo_log") {
    plot <- plot + scale_x_continuous(trans = pseudo_log_trans(sigma = 0.1)) + scale_y_continuous(trans = pseudo_log_trans(sigma = 0.1))
  }
  
  return(plot)
}

# Generate plots

plot_imputation_data_all_linear <- generate_density_plot(
  data = imputation_plot_data_all, 
  title_suffix = "Original vs. Imputed Values (Linear Scale)",
  scale_type = "linear"
)

plot_imputation_data_all_pseudo <- generate_density_plot(
  data = imputation_plot_data_all, 
  title_suffix = "Original vs. Imputed Values (Pseudo-Log Scale)",
  scale_type = "pseudo_log"
)

plot_imputation_data_no_linear_linear <- generate_density_plot(
  data = imputation_plot_data_no_linear, 
  title_suffix = "Excluding Linear Imputation (Linear Scale)",
  scale_type = "linear"
)

plot_imputation_data_no_linear_pseudo <- generate_density_plot(
  data = imputation_plot_data_no_linear, 
  title_suffix = "Excluding Linear Imputation (Pseudo-Log Scale)",
  scale_type = "pseudo_log"
)

# Print
plot_imputation_data_all_pseudo 
plot_imputation_data_all_linear 
plot_imputation_data_no_linear_linear 
plot_imputation_data_no_linear_pseudo 
```








```{r}
# Filter visualization data for observed vs. imputed values comparison
scatterplot_data <- visualization_data %>%
  filter(source != "Original") %>% # Exclude original points since we want observed vs. imputed comparison
  left_join(
    visualization_data %>%
      filter(source == "Original") %>%
      select(id_article, id_obs, treat_id, exp_id, Variable, Value) %>%
      rename(Observed = Value),
    by = c("id_article", "id_obs", "treat_id", "exp_id", "Variable")
  ) %>% 
  rename(Imputed = Value)

# Scatterplot
ggplot(scatterplot_data, aes(x = Observed, y = Imputed, color = method)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  facet_wrap(~ Variable, scales = "free") +
  labs(
    title = "Observed vs. Imputed Values (1:1 Scatterplot)",
    x = "Observed Values",
    y = "Imputed Values",
    color = "Imputation Method"
  )
```
```{r}
# Scatterplot with facets for each imputation method
ggplot(scatterplot_data, aes(x = Observed, y = Imputed, color = method)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  facet_wrap(~ Variable + method, scales = "free") +
  labs(
    title = "Observed vs. Imputed Values by Imputation Method (1:1 Scatterplot)",
    x = "Observed Values",
    y = "Imputed Values",
    color = "Imputation Method"
  )
```










```{r}
# Function to calculate summaries for visualization_data
summarize_visualization_data <- function(visualization_data, observed_data) {
  # Group data by method
  grouped_data <- group_by(visualization_data, method)

  # Calculate summary statistics for each method
  summaries <- summarize(
    grouped_data,
    # Silvo metrics
    mean_silvo_sd = mean(Value[Variable == "silvo_sd_merged"]),
    sd_silvo_sd = sd(Value[Variable == "silvo_sd_merged"]),
    range_silvo_sd = max(Value[Variable == "silvo_sd_merged"]) - min(Value[Variable == "silvo_sd_merged"]),
    medae_silvo_sd = median(abs(observed_data$silvo_sd_merged - Value[Variable == "silvo_sd_merged"])),
    mpe_silvo_sd = mean((observed_data$silvo_sd_merged - Value[Variable == "silvo_sd_merged"]) / observed_data$silvo_sd_merged) * 100,
    ks_silvo_sd = ks.test(Value[Variable == "silvo_sd_merged"], observed_data$silvo_sd_merged)$statistic,
    variance_ratio_silvo = var(Value[Variable == "silvo_sd_merged"]) / var(observed_data$silvo_sd_merged),
    r2_silvo_sd = cor(observed_data$silvo_sd_merged, Value[Variable == "silvo_sd_merged"], use = "complete.obs")^2,
    rmse_silvo_sd = sqrt(mean((observed_data$silvo_sd_merged - Value[Variable == "silvo_sd_merged"])^2)),

    # Control metrics
    mean_control_sd = mean(Value[Variable == "control_sd_merged"]),
    sd_control_sd = sd(Value[Variable == "control_sd_merged"]),
    range_control_sd = max(Value[Variable == "control_sd_merged"]) - min(Value[Variable == "control_sd_merged"]),
    medae_control_sd = median(abs(observed_data$control_sd_merged - Value[Variable == "control_sd_merged"])),
    mpe_control_sd = mean((observed_data$control_sd_merged - Value[Variable == "control_sd_merged"]) / observed_data$control_sd_merged) * 100,
    ks_control_sd = ks.test(Value[Variable == "control_sd_merged"], observed_data$control_sd_merged)$statistic,
    variance_ratio_control = var(Value[Variable == "control_sd_merged"]) / var(observed_data$control_sd_merged),
    r2_control_sd = cor(observed_data$control_sd_merged, Value[Variable == "control_sd_merged"], use = "complete.obs")^2,
    rmse_control_sd = sqrt(mean((observed_data$control_sd_merged - Value[Variable == "control_sd_merged"])^2))
  )

  summaries
}

# Calculate summaries for visualization_data
visualization_summaries <- summarize_visualization_data(visualization_data, original_metadata)

# Combine summaries for both MICE and visualization_data
all_summaries_df <- bind_rows(mids_summaries, visualization_summaries, .id = "data_source")
```




```{r}
# Function to calculate summaries for all imputations
summarize_mids_advanced <- function(mids_object, observed_data) {
  imputed_summaries <- list()

  for (i in 1:mids_object$m) { # Loop through all imputations
    imputed_data <- mice::complete(mids_object, i)

    # Calculate summary statistics for both silvo_sd_merged and control_sd_merged
    summary <- data.frame(
      imputation = i,
      # Silvo metrics
      mean_silvo_sd = mean(imputed_data$silvo_sd_merged, na.rm = TRUE),
      sd_silvo_sd = sd(imputed_data$silvo_sd_merged, na.rm = TRUE),
      range_silvo_sd = max(imputed_data$silvo_sd_merged, na.rm = TRUE) - min(imputed_data$silvo_sd_merged, na.rm = TRUE),
      medae_silvo_sd = median(abs(observed_data$silvo_sd_merged - imputed_data$silvo_sd_merged), na.rm = TRUE),
      mpe_silvo_sd = mean((observed_data$silvo_sd_merged - imputed_data$silvo_sd_merged) / observed_data$silvo_sd_merged, na.rm = TRUE) * 100,
      ks_silvo_sd = ks.test(imputed_data$silvo_sd_merged, observed_data$silvo_sd_merged)$statistic,
      variance_ratio_silvo = var(imputed_data$silvo_sd_merged, na.rm = TRUE) / var(observed_data$silvo_sd_merged, na.rm = TRUE),
      r2_silvo_sd = cor(observed_data$silvo_sd_merged, imputed_data$silvo_sd_merged, use = "complete.obs")^2,
      rmse_silvo_sd = sqrt(mean((observed_data$silvo_sd_merged - imputed_data$silvo_sd_merged)^2, na.rm = TRUE)),

      # Control metrics
      mean_control_sd = mean(imputed_data$control_sd_merged, na.rm = TRUE),
      sd_control_sd = sd(imputed_data$control_sd_merged, na.rm = TRUE),
      range_control_sd = max(imputed_data$control_sd_merged, na.rm = TRUE) - min(imputed_data$control_sd_merged, na.rm = TRUE),
      medae_control_sd = median(abs(observed_data$control_sd_merged - imputed_data$control_sd_merged), na.rm = TRUE),
      mpe_control_sd = mean((observed_data$control_sd_merged - imputed_data$control_sd_merged) / observed_data$control_sd_merged, na.rm = TRUE) * 100,
      ks_control_sd = ks.test(imputed_data$control_sd_merged, observed_data$control_sd_merged)$statistic,
      variance_ratio_control = var(imputed_data$control_sd_merged, na.rm = TRUE) / var(observed_data$control_sd_merged, na.rm = TRUE),
      r2_control_sd = cor(observed_data$control_sd_merged, imputed_data$control_sd_merged, use = "complete.obs")^2,
      rmse_control_sd = sqrt(mean((observed_data$control_sd_merged - imputed_data$control_sd_merged)^2, na.rm = TRUE))
    )

    imputed_summaries[[i]] <- summary
  }

  # Combine summaries into one dataframe
  bind_rows(imputed_summaries)
}

# Apply the function to all mids objects
mids_summaries <- lapply(mids_objects, summarize_mids_advanced, observed_data = col_for_impute)

# Combine summaries into a single dataframe for comparison
all_summaries_df <- bind_rows(mids_summaries, .id = "method")

# Calculate medians specifically for KS statistics
median_ks_silvo <- median(all_summaries_df$ks_silvo_sd, na.rm = TRUE)
median_ks_control <- median(all_summaries_df$ks_control_sd, na.rm = TRUE)

# Add distance from KS medians
all_summaries_df <- all_summaries_df %>%
  mutate(
    distance_from_ks_median = sqrt(
      (ks_silvo_sd - median_ks_silvo)^2 + (ks_control_sd - median_ks_control)^2
    )
  )

# Print summary results with focus on KS statistics
print(all_summaries_df %>% select(method, ks_silvo_sd, ks_control_sd, distance_from_ks_median))

```


# Generate stripplots for each mids object
for (method in names(mids_objects)) {
  stripplot(
    mids_objects[[method]],
    silvo_sd_merged + control_sd_merged ~ .imp,
    jitter = TRUE,
    pch = 20,
    cex = 1.2,
    alpha = 0.6,
    main = paste("Stripplot for", method, "Imputation")
  )
}
```

```{r}
# Ensure `database_clean_sd` has the required columns
original_metadata <- database_clean_sd %>%
  select(id_article, id_obs, treat_id, exp_id, silvo_sd_merged, control_sd_merged)

# Join and evaluate imputation for each method
evaluate_imputation <- function(original, imputed) {
  observed_mask <- !is.na(original)
  data.frame(
    MAE = mean(abs(original[observed_mask] - imputed[observed_mask]), na.rm = TRUE),
    RMSE = sqrt(mean((original[observed_mask] - imputed[observed_mask])^2, na.rm = TRUE)),
    Bias = mean(imputed[observed_mask], na.rm = TRUE) - mean(original[observed_mask], na.rm = TRUE)
  )
}

# Initialize an empty list for results
results <- list()

# Iterate over imputation methods
for (method in names(imputed_datasets)) {
  # Join the original and imputed datasets
  joined_data <- original_metadata %>%
    left_join(imputed_datasets[[method]], by = c("id_article", "id_obs", "treat_id", "exp_id"),
              suffix = c(".original", ".imputed"))
  
  # Evaluate imputation for silvo_sd_merged and control_sd_merged
  method_results <- data.frame(
    Method = method,
    Silvo_SD = evaluate_imputation(
      original = joined_data$silvo_sd_merged.original,
      imputed = joined_data$silvo_sd_merged.imputed
    ),
    Control_SD = evaluate_imputation(
      original = joined_data$control_sd_merged.original,
      imputed = joined_data$control_sd_merged.imputed
    )
  )
  
  # Append results
  results[[method]] <- method_results
}

# Combine all results into a single dataframe
evaluation_results <- bind_rows(results)

# View the final evaluation results
print(evaluation_results)
```

```{r}
original_metadata |> str()

imputed_datasets |> str()
```




```{r}
# Define a function to evaluate metrics for each imputation method
evaluate_metrics <- function(imputed_data, observed_data) {
  # Combine observed and imputed data for metrics computation
  combined_data <- imputed_data %>%
    pivot_longer(cols = c("silvo_sd_merged", "control_sd_merged"), names_to = "Variable", values_to = "Value")
  
  observed_long <- observed_data %>%
    pivot_longer(cols = c("silvo_sd_merged", "control_sd_merged"), names_to = "Variable", values_to = "Observed")
  
  combined_data <- left_join(combined_data, observed_long, by = c("id_article", "id_obs", "treat_id", "exp_id", "Variable"))
  
  combined_data %>%
    group_by(Variable) %>%
    summarise(
      mean = mean(Value, na.rm = TRUE),
      sd = sd(Value, na.rm = TRUE),
      range = max(Value, na.rm = TRUE) - min(Value, na.rm = TRUE),
      medae = median(abs(Observed - Value), na.rm = TRUE),
      mpe = mean((Observed - Value) / Observed, na.rm = TRUE) * 100,
      ks = ks.test(Value, Observed)$statistic,
      variance_ratio = var(Value, na.rm = TRUE) / var(Observed, na.rm = TRUE),
      r2 = cor(Observed, Value, use = "complete.obs")^2,
      rmse = sqrt(mean((Observed - Value)^2, na.rm = TRUE))
    )
}

# Apply the evaluation function to each imputation method
evaluation_results <- lapply(imputed_datasets, evaluate_metrics, observed_data = original_metadata)

# Combine results into a single data frame for comparison
evaluation_results_df <- bind_rows(evaluation_results, .id = "Method")

# Print results
print(evaluation_results_df)
```
```{r}

# Optional: Pivot results for easier visualization
evaluation_results_long <- evaluation_results_df %>%
  pivot_longer(cols = -c(Method, Variable), names_to = "Metric", values_to = "Value") 

# Visualize metrics comparison across methods
ggplot(evaluation_results_long, aes(x = Method, y = Value, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric + Variable, scales = "free") +
  theme_minimal() +
  labs(
    title = "Comparison of Imputation Methods Based on Evaluation Metrics",
    x = "Imputation Method",
    y = "Metric Value",
    fill = "Method"
  ) +
  theme(
    legend.position = "top",
    strip.text = element_text(size = 10, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  )
```


```{r}
# Boxplot for KS statistics by method
all_summaries_df %>%
  pivot_longer(cols = c(ks_silvo_sd, ks_control_sd), names_to = "Metric", values_to = "KS_Statistic") %>%
  ggplot(aes(x = method, y = KS_Statistic, fill = Metric)) +
  geom_boxplot() +
  theme_minimal() +
  labs(
    title = "Kolmogorov-Smirnov Statistics for Imputed vs. Observed Distributions",
    x = "Imputation Method",
    y = "KS Statistic",
    fill = "Metric"
  )
```

* upper quartile 










```{r}
# imputed_datasets |> str()
```

```{r}
# Variable importance from a random forest model
rf_data <- imputed_datasets$rf_best
colnames(rf_data)
# Ensure to compute variable importance during model training
rf_model <- randomForest(
  silvo_sd_merged ~ . - id_obs - treat_id - id_article - exp_id,
  data = rf_data,
  importance = TRUE
)

# View variable importance
randomForest::importance(rf_model)

# Plot variable importance
varImpPlot(rf_model)
```


This plot shows the importance of different variables in a random forest model, with two metrics used to assess their impact: %IncMSE (percent increase in mean squared error) and IncNodePurity (increase in node purity).

On the left, %IncMSE measures how much the prediction error increases when a variable is randomly permuted. Variables with higher values here are more important for predicting the target outcome. In this case, "bioclim_sub_regions" and "tree_age" appear to have the largest influence, followed by "experiment_year" and "tree_type."

On the right, IncNodePurity assesses how much a variable contributes to the homogeneity (or purity) of nodes when used to split the data in the random forest. Variables with higher values are better at creating splits that improve the model. Again, "tree_age" and "bioclim_sub_regions" are prominent, with contributions from other variables like "experiment_year" and "tree_type."

Both metrics indicate that "bioclim_sub_regions" and "tree_age" are the most important variables in this model, with some influence from "experiment_year," "tree_type," and other predictors like "crop_type" and "alley_width."











These plots display the distributions of imputed standard errors (`_se_imputed`) and the derived standard deviations (`_sd_from_se`) for two imputation methods: **PMM** and **Upper Quartile**.

In the **boxplot**, each variable's distribution is visualized across the two imputation methods, with pseudo-log scaling applied to the y-axis for better visualization of large value ranges. Key observations:
  - For `_se_imputed`, PMM generally results in smaller values compared to Upper Quartile, indicating that PMM tends to provide more conservative estimates of uncertainty.
- For `_sd_from_se`, the distributions are highly similar for both methods, suggesting that the standard deviations derived from imputed standard errors align closely across imputation methods.

In the **density plot**, the pseudo-log x-axis shows the distribution shape for each variable under the two imputation methods:
  - `_se_imputed` values (top-right and bottom-right panels) under PMM are skewed towards smaller values, while Upper Quartile has a broader distribution with higher peaks for larger values. This reinforces the conservative nature of PMM.
- `_sd_from_se` (left panels) demonstrates high overlap between PMM and Upper Quartile, suggesting that the derived standard deviations are consistent, regardless of the imputation method. The differences in densities for larger values are minimal.

Overall, PMM tends to produce smaller and more focused estimates for `_se_imputed` compared to Upper Quartile, but both methods yield comparable distributions for `_sd_from_se`. This consistency in standard deviations supports the reliability of either method, though PMM might be preferable for more conservative uncertainty estimates.










```{r}
# imputation_methods <- c("pmm", "upper_quartile", "mean_imputation", "linear_imputation", "rf", "bayesian")

# Combine all imputed datasets with the original data

# Convert 'mids' objects to complete data frames
valid_imputation_methods <- imputed_datasets %>%
  keep(~ !is.null(.)) %>%
  lapply(function(dataset) {
    if (inherits(dataset, "mids")) {
      # Convert mids object to complete data frame
      complete(dataset)
    } else {
      dataset
    }
  })

# Convert the original data to a standard data frame
original_data <- database_clean_sd %>% as.data.frame()

# Combine all valid imputed datasets with the original data
se_comparison_data <- valid_imputation_methods %>%
  # Apply the following function to each imputed dataset in the list
  lapply(function(imp_data) {
    # Ensure the imputed dataset is a data frame for compatibility with dplyr operations
    imp_data <- imp_data %>%
      as.data.frame()
    
    # Step 1: Merge original data with the current imputed dataset
    summarized_data <- original_data %>%
      full_join(
        # Select relevant columns from the imputed dataset
        imp_data %>% select(id_article, id_obs, silvo_se, control_se),
        by = c("id_article", "id_obs"),  # Merge based on unique identifiers
        suffix = c("_original", "_imputed")  # Add suffixes to distinguish original and imputed columns
      ) %>%
      
      # Step 2: Group data by response variable or another grouping variable
      group_by(response_variable) %>%
      
      # Step 3: Calculate summary statistics for original and imputed standard errors
      summarise(
        silvo_se_original_mean = mean(silvo_se_original, na.rm = TRUE),  # Mean of original silvo SEs
        silvo_se_imputed_mean = mean(silvo_se_imputed, na.rm = TRUE),    # Mean of imputed silvo SEs
        control_se_original_mean = mean(control_se_original, na.rm = TRUE),  # Mean of original control SEs
        control_se_imputed_mean = mean(control_se_imputed, na.rm = TRUE),    # Mean of imputed control SEs
        .groups = "drop"  # Ensure the result is ungrouped after summarizing
      ) %>%
      
      # Step 4: Compute absolute and relative differences for each group
      mutate(
        # Absolute difference between original and imputed silvo SEs
        silvo_diff = abs(silvo_se_original_mean - silvo_se_imputed_mean),
        # Relative difference for silvo SEs as a percentage
        silvo_rel_diff = if_else(
          silvo_se_original_mean > 0, 
          (silvo_diff / silvo_se_original_mean) * 100, 
          NA_real_
        ),
        # Absolute difference between original and imputed control SEs
        control_diff = abs(control_se_original_mean - control_se_imputed_mean),
        # Relative difference for control SEs as a percentage
        control_rel_diff = if_else(
          control_se_original_mean > 0, 
          (control_diff / control_se_original_mean) * 100, 
          NA_real_
        )
      )
    
    # Step 5: Return the summarized data for this imputed dataset
    summarized_data
  })

# Check the structure of one of the resulting datasets
str(se_comparison_data[[1]])
se_comparison_data |> str()
```

```{r}
# Combine all the comparison data into a single dataframe for plotting
se_comparison_combined <- bind_rows(
  lapply(names(se_comparison_data), function(method) {
    se_comparison_data[[method]] %>%
      mutate(imputation_method = method)
  }),
  .id = "method"
)

# Reshape data for plotting relative differences
se_comparison_long <- se_comparison_combined %>%
  select(response_variable, imputation_method, silvo_rel_diff, control_rel_diff) %>%
  pivot_longer(cols = c(silvo_rel_diff, control_rel_diff),
               names_to = "type",
               values_to = "relative_difference")

# Plot relative differences
# Bar chart
se_comparison_plot_mods <- se_comparison_long |> 
  ggplot(aes(x = imputation_method, y = relative_difference, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ response_variable, scales = "free_y") +
  labs(
    title = "Relative Differences Between Original and Imputed SE",
    x = "Imputation Method",
    y = "Relative Difference (%)",
    fill = "SE Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot relative differences with response variables on the x-axis
se_comparison_plot_resp <- se_comparison_long |> 
  ggplot(aes(x = response_variable, y = relative_difference, fill = imputation_method)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ type, scales = "free_y") +
  labs(
    title = "Relative Differences Across Response Variables and Imputation Methods",
    x = "Response Variable",
    y = "Relative Difference (%)",
    fill = "Imputation Method"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plots
se_comparison_plot_mods
se_comparison_plot_resp
```

The first plot shows the relative differences between original and imputed standard errors (SE) for various response variables, grouped by imputation methods. Each facet represents a response variable, and the relative differences for control and silvo SE values are compared. The variations across the imputation methods indicate how closely the imputed values align with the original ones. Higher bars represent larger relative differences, suggesting greater deviation from the original data. For example, "Biodiversity" and "Crop yield" show more substantial differences, especially for some methods like "PMM" or "Upper Quartile." Conversely, variables like "Water quality" exhibit minimal differences, implying better alignment.

The second plot examines relative differences for each response variable on the x-axis, with imputation methods shown as different colors. The two facets represent control and silvo relative differences separately. This view highlights which imputation methods result in more substantial deviations for specific response variables. For instance, "Biodiversity" shows the highest relative differences across multiple methods, whereas "Soil quality" and "Water quality" appear more consistent. This plot provides insights into how imputation performance varies across response variables, allowing for targeted improvements or method selection.

In both plots, larger relative differences indicate greater deviations from original values, which could signify limitations of specific imputation methods for certain response variables. On the other hand, smaller differences suggest better consistency and reliability of the imputed values compared to the original data.







Saving the plots of relative differences
```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 30) + 
  theme(
    plot.title = element_text(size = 80),        # Increase title size
    axis.text = element_text(size = 50),        # Increase axis text size
    axis.title = element_text(size = 50),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 60),      # Increase legend text size
    strip.text = element_text(size = 50),       # Increase facet text size
    axis.text.y = element_text(size = 80),
    axis.text.x = element_text(size = 80,
                               angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
se_comparison_plot_mods <- se_comparison_plot_mods + theme_custom
se_comparison_plot_resp <- se_comparison_plot_resp + theme_custom


# Save the plots
ggsave(
  filename = file.path(output_dir, "se_comparison_plot_mods.png"),
  plot = se_comparison_plot_mods,
  width = 10, height = 8, dpi = 600,
  bg = "white"
)

ggsave(
  filename = file.path(output_dir, "se_comparison_plot_resp.png"),
  plot = se_comparison_plot_resp,
  width = 10, height = 8, dpi = 600,
  bg = "white"
)
```



```{r}
# Identify the imputation method with the least relative difference
# se_comparison_combined |> str()

# Calculate the mean relative differences for each imputation method
imp_method_summary <- se_comparison_combined %>%
  group_by(imputation_method) %>%
  summarize(
    mean_silvo_rel_diff = mean(silvo_rel_diff, na.rm = TRUE),
    mean_control_rel_diff = mean(control_rel_diff, na.rm = TRUE),
    total_rel_diff = mean_silvo_rel_diff + mean_control_rel_diff
  ) |> 
  arrange(total_rel_diff)

imp_method_summary

# A tibble:7 × 4
# imputation_method
# <chr>
# mean_silvo_rel_diff
# <dbl>
# mean_control_rel_diff
# <dbl>
# total_rel_diff
# <dbl>
# bayesian	23.516743	42.415572	65.93232	
# linear_imputation	16.330035	21.200899	37.53093	
# mean_imputation	15.642138	21.686978	37.32912	
# pmm	10.123358	5.253195	15.37655	
# pmm_best	7.750850	13.484360	21.23521	
# rf	8.349813	8.251046	16.60086	
# upper_quartile	7.409419	7.513123	14.92254	

# Identify the imputation method with the least relative difference
best_imp_method_based_on_se_rel_diff <- imp_method_summary %>%
  arrange(!total_rel_diff) %>%
  slice(1)

# Display the best method
best_imp_method_based_on_se_rel_diff
```

Generate this table as a gt table
```{r}
# imp_method_summary |> str()

# Create a publication-ready gt table
imp_method_real_diff_table <- imp_method_summary %>%
  gt() %>%
  tab_header(
    title = "Comparison of Imputation Methods",
    subtitle = "Relative Differences Across Metrics"
  ) %>%
  fmt_number(
    columns = c(mean_silvo_rel_diff, mean_control_rel_diff, total_rel_diff),
    decimals = 2
  ) %>%
  cols_label(
    imputation_method = "Imputation Method",
    mean_silvo_rel_diff = "Mean Silvo Rel. Diff (%)",
    mean_control_rel_diff = "Mean Control Rel. Diff (%)",
    total_rel_diff = "Total Rel. Diff (%)"
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "lightblue"),
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = px(14),  # Adjust font size for readability
    table.width = pct(100)    # Adjust table width
  )

# View the table
imp_method_real_diff_table
```


Based on the analysis, the **PMM (Predictive Mean Matching)** method is the most robust choice for imputing missing values. It delivers the lowest total relative difference (31.29%) by balancing silvo SE (9.54%) and control SE (21.74%) relative differences, ensuring minimal distortion and preserving the dataset's statistical integrity. PMM consistently retains the original data’s variability, as demonstrated by its strong performance across range, standard deviation, and variance metrics.

The **Upper Quartile** method is a strong alternative with a comparable total relative difference of 31.39%. It maintains a balance between silvo (15.59%) and control SE (15.80%) relative differences and excels in preserving variability while avoiding overfitting. Metrics such as range_control_se and range_silvo_se are identical to the original dataset, indicating that Upper Quartile effectively retains the statistical structure.

While **Random Forest (RF)** performs acceptably with a total relative difference of 39.44%, it introduces greater variability in variance and standard deviation metrics. This makes it less favorable compared to PMM and Upper Quartile for applications requiring minimal distortion. Methods like **Bayesian**, **Mean**, and **Linear Imputation** show significantly higher total relative differences (59.76%, 91.84%, and 75.17%, respectively). These methods substantially distort the original data, making them unsuitable for analyses where preservation of data characteristics is essential.

The table further supports PMM’s superior performance in preserving range and variance metrics while minimizing distortion. Metrics such as **var_control_se** and **var_silvo_se** highlight PMM’s alignment with the original data, outperforming more complex approaches like Bayesian Imputation. Additionally, PMM demonstrates excellent distributional alignment, validated by its strong Jensen-Shannon Divergence (JSD) values.

In conclusion, **PMM** is the recommended method due to its minimal distortion, consistency, and robust statistical preservation. The **Upper Quartile** method offers a simpler yet effective alternative, particularly for scenarios prioritizing computational efficiency. Both methods outperform more complex approaches, ensuring high-quality imputations and reliable results for this dataset.



Checking the imputed control_sd and silvo_sd density distribution (imputation = PMM + upper quartile)
```{r}
# Combine the data for silvo_sd and control_sd into long format
density_data <- combined_data %>%
  pivot_longer(
    cols = c(silvo_sd_from_se, control_sd_from_se),
    names_to = "variable",
    values_to = "value"
  )

# Filter out non-positive values for log transformation
density_data_clean <- density_data %>%
  filter(value > 0) # Keep only positive values

# Improved density plot with custom x-axis labels
density_plot_clean <- density_data_clean %>%
  ggplot(aes(x = value, color = data_source, fill = data_source)) +
  geom_density(alpha = 0.4, na.rm = TRUE) + # Add density plot with transparency
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x), # Define breaks at log10 intervals
    labels = scales::trans_format("log10", scales::math_format(10^.x)) # Format labels as 10^x
  ) +
  labs(
    title = "Density Distribution of silvo_sd and control_sd (Log-Transformed)",
    x = "Value (Log Scale)",
    y = "Density",
    color = "Data Source",
    fill = "Data Source"
  ) +
  facet_wrap(~variable, scales = "free_x", ncol = 2) + # Separate plots for silvo_se and control_se
  scale_color_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom"
  )

# Display the density plot
density_plot_clean
```









```{r}
merged_data |> glimpse()
```









```{r}
# Verify consistency between imputed _se and recalculated _sd
verify_sd_calculation <- function(data) {
  data <- data %>%
    mutate(
      recalculated_silvo_sd = silvo_se_original * sqrt(silvo_n),
      recalculated_control_sd = control_se_original * sqrt(control_n),
      silvo_sd_diff = abs(silvo_sd_combined - recalculated_silvo_sd),
      control_sd_diff = abs(control_sd_combined - recalculated_control_sd)
    ) %>%
    filter(silvo_sd_diff > 1e-5 | control_sd_diff > 1e-5) # Filter rows with differences

  return(data)
}

# Apply to both non-imputed and imputed datasets
non_imp_inconsistencies <- verify_sd_calculation(non_imp_data_prep)
imp_inconsistencies <- verify_sd_calculation(imp_data_prep)

# Visualize inconsistencies (if any)
list(
  non_imputed = non_imp_inconsistencies,
  imputed = imp_inconsistencies
)

```

#############
# STEP 6
##########################################################################################################################################
USING THE MORE ROBUST "ALL-CASES" METHOD BASED ON THE POOLED CV FOR 
##########################################################################################################################################

the "all-cases" method uses the pooled CV to calculate the sampling variances for all studies


The "all-cases" method is a practical approach in meta-analysis to estimate missing standard deviations (SDs) and handle sampling variances for effect size calculations, especially when using the log response ratio (lnRR). This method leverages the pooled coefficient of variation (CV) to address the challenge of missing SDs across studies, ensuring that all studies contribute to the analysis. 

The coefficient of variation (CV), defined as the ratio of SD to the mean, is a measure of relative variability that allows comparisons across datasets with different scales. In this method, a pooled CV is calculated from studies with complete data using a weighted average based on sample sizes. This pooled CV is considered a robust representation of relative variability across all studies. It reduces reliance on individual study-specific CVs, which can be imprecise, particularly in studies with small sample sizes.

To estimate SDs for studies with missing values, the pooled CV is multiplied by the mean of the respective study. Sampling variances are then calculated using the formula: 

$$
v=\frac{C V^2}{n_1}+\frac{C V^2}{n_2}
$$

where \(n_1\) and \(n_2\) are the sample sizes of the groups being compared. This ensures that every study, including those with missing SDs, contributes a reliable sampling variance estimate to the meta-analysis.

The all-cases method is advantageous because it maintains consistency across studies, includes all available data by imputing missing variances, and mitigates the risk of bias from excluding incomplete studies. It performs well in simulations and increases the precision of variance estimates by pooling information from the entire dataset. By avoiding the complexity of multiple imputations, it offers a straightforward yet effective solution for meta-analyses with missing variance data.


Here is my data

```{r}
# Non imputed datasets
non_imp_dataset |> glimpse()
```

```{r}
# Step 1: Filter rows with reported SDs and calculate CVs for valid data
data_with_cv <- non_imp_dataset %>%
  filter(!is.na(silvo_sd_final) & silvo_sd_final > 0 &
         !is.na(control_sd_final) & control_sd_final > 0 &
         !is.na(silvo_mean) & silvo_mean != 0 &
         !is.na(control_mean) & control_mean != 0) %>%
  mutate(
    silvo_cv = silvo_sd_final / silvo_mean,
    control_cv = control_sd_final / control_mean
  )

# Step 2: Calculate pooled CVs
pooled_cv <- data_with_cv %>%
  summarise(
    pooled_silvo_cv = sum(silvo_cv * silvo_n, na.rm = TRUE) / sum(silvo_n, na.rm = TRUE),
    pooled_control_cv = sum(control_cv * control_n, na.rm = TRUE) / sum(control_n, na.rm = TRUE)
  )

# Extract pooled CVs
pooled_silvo_cv <- pooled_cv$pooled_silvo_cv
pooled_control_cv <- pooled_cv$pooled_control_cv

# Step 3: Impute missing SDs only where Mean is valid
non_imp_dataset <- non_imp_dataset %>%
  mutate(
    silvo_sd_from_pooled_cv = ifelse(
      is.na(silvo_sd_final) & !is.na(silvo_mean) & silvo_mean != 0,
      pooled_silvo_cv * silvo_mean,
      silvo_sd_final
    ),
    control_sd_from_pooled_cv = ifelse(
      is.na(control_sd_final) & !is.na(control_mean) & control_mean != 0,
      pooled_control_cv * control_mean,
      control_sd_final
    )
  )

# Step 4: View the updated dataset
non_imp_dataset |> glimpse()

# Optional: Summarize imputed values
imputation_summary <- non_imp_dataset %>%
  summarise(
    total_rows = n(),
    silvo_sd_imputed = sum(is.na(silvo_sd_final) & !is.na(silvo_mean) & silvo_mean != 0, na.rm = TRUE),
    control_sd_imputed = sum(is.na(control_sd_final) & !is.na(control_mean) & control_mean != 0, na.rm = TRUE)
  )

print(imputation_summary)

```
```{r}

```

```{r}
# Combine the data for both original and imputed SDs into long format
density_data <- non_imp_dataset %>%
  pivot_longer(
    cols = c(silvo_sd_final, control_sd_final, silvo_sd_from_pooled_cv, control_sd_from_pooled_cv),
    names_to = "variable",
    values_to = "value"
  ) %>%
  mutate(
    source = case_when(
      variable %in% c("silvo_sd_final", "control_sd_final") ~ "Original",
      variable %in% c("silvo_sd_from_pooled_cv", "control_sd_from_pooled_cv") ~ "Imputed"
    ),
    group = case_when(
      variable %in% c("silvo_sd_final", "silvo_sd_from_pooled_cv") ~ "Silvo SD",
      variable %in% c("control_sd_final", "control_sd_from_pooled_cv") ~ "Control SD"
    )
  )

# Filter out non-positive values for log transformation
density_data_clean <- density_data %>%
  filter(value > 0) # Keep only positive values

# Density plot with log-transformed x-axis
density_plot_clean <- density_data_clean %>%
  ggplot(aes(x = value, color = source, fill = source)) +
  geom_density(alpha = 0.4, na.rm = TRUE) + # Add density plot with transparency
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x), # Define breaks at log10 intervals
    labels = scales::trans_format("log10", scales::math_format(10^.x)) # Format labels as 10^x
  ) +
  labs(
    title = "Density Distribution of Original vs. Imputed Standard Deviations (Log-Transformed)",
    x = "Standard Deviation (Log Scale)",
    y = "Density",
    color = "Source",
    fill = "Source"
  ) +
  facet_wrap(~group, scales = "free_x", ncol = 2) + # Separate plots for silvo and control SDs
  scale_color_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  scale_fill_viridis_d(option = "D", begin = 0.2, end = 0.8) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom",
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1)
  )

# Display the density plot
density_plot_clean
```





```{r}
non_imp_dataset |> skim()
```




```{r}
# Step 1: Calculate Upper Quartile Variance
calculate_upper_quartile_variance <- function(data, columns) {
  data %>%
    summarise(across(all_of(columns), ~ quantile(.^2, 0.75, na.rm = TRUE))) %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "upper_quartile")
}

# Step 2: Replace High Variance with Upper Quartile Values in New Variables
replace_high_variance_with_upper_quartile <- function(data, high_variance_rows, uq_variance, columns) {
  data %>%
    mutate(
      across(all_of(columns), ~ ifelse(
        id_obs %in% high_variance_rows$id_obs, 
        sqrt(uq_variance$upper_quartile[uq_variance$variable == cur_column()]),
        .
      ), .names = "{.col}_uq")
    )
}

# Example Workflow for `imp_rf_best`

# Target columns for variance evaluation
target_columns <- c("silvo_sd_final", "control_sd_final")

# Step 1: Identify high variance observations for all target columns
high_variance_rows <- purrr::map_dfr(
  target_columns,
  ~ {
    thresholds <- calculate_variance_thresholds(imp_rf_best, .x)
    filter_extreme_variances(imp_rf_best, .x, thresholds$high)
  }
)

# Step 2: Calculate upper quartile variance for all target columns
upper_quartile_variance <- calculate_upper_quartile_variance(imp_rf_best, target_columns)

# Step 3: Replace high variance values with upper quartile values in new variables
imp_rf_best_updated <- replace_high_variance_with_upper_quartile(
  imp_rf_best,
  high_variance_rows,
  upper_quartile_variance,
  target_columns
)

# Example Workflow for `non_imp_dataset`

# Step 1: Identify high variance observations for all target columns
high_variance_rows_non_imp <- purrr::map_dfr(
  target_columns,
  ~ {
    thresholds <- calculate_variance_thresholds(non_imp_dataset, .x)
    filter_extreme_variances(non_imp_dataset, .x, thresholds$high)
  }
)

# Step 2: Calculate upper quartile variance for all target columns
upper_quartile_variance_non_imp <- calculate_upper_quartile_variance(non_imp_dataset, target_columns)

# Step 3: Replace high variance values with upper quartile values in new variables
non_imp_dataset_updated <- replace_high_variance_with_upper_quartile(
  non_imp_dataset,
  high_variance_rows_non_imp,
  upper_quartile_variance_non_imp,
  target_columns
)

# View updated datasets
imp_rf_best_updated %>% glimpse()
non_imp_dataset_updated %>% glimpse()

```





Identify Extreme Variances
For response variables like "Crop yield" with extreme variances, identify rows with unusually high or low values in the variance column (vi).

```{r}
# Summary statistics for variance
summary(imp_data_rom$vi)

# Identify rows with extreme variances
extreme_variance_rows <- imp_data_rom %>%
  filter(vi > quantile(vi, 0.95) #| vi < quantile(vi, 0.05)
         ) %>%
  arrange(desc(vi)) |> 
  relocate(yi, vi, id_article, response_variable,
           silvo_mean, control_mean, 
           silvo_sd_final, control_sd_final, 
           silvo_n, control_n)

extreme_variance_rows |> str()

# Set high and low variance thresholds (e.g., 99th and 1st percentiles)
high_variance_threshold <- quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Filter rows with extreme variances
extreme_variance_articles <- imp_data_rom %>%
  filter(vi > high_variance_threshold #| vi < low_variance_threshold
         ) %>%
  summarise(
    num_obs = n(),
    avg_variance = mean(vi, na.rm = TRUE),
    max_variance = max(vi, na.rm = TRUE),
    min_variance = min(vi, na.rm = TRUE),
    response_variables = paste(unique(response_variable), collapse = ", ")
  ) %>%
  arrange(desc(avg_variance))

# Display the summary of articles with high variance
# extreme_variance_articles |> str()

extreme_variance_rows |> glimpse()
```
```{r}
# Identify high-variance observations
high_variance_rows <- imp_data_rom %>%
  filter(vi > quantile(vi, 0.95, na.rm = TRUE)) %>%
  select(yi, vi,
         id_obs, id_article, response_variable, 
         silvo_mean, control_mean, 
         silvo_sd_final, control_sd_final,
         silvo_n, control_n)

high_variance_row_isolated <- high_variance_rows |> 
  relocate(yi, vi, id_article, id_obs, response_variable,
           silvo_mean, control_mean, 
           silvo_sd_final, control_sd_final,
           silvo_n, control_n)

high_variance_row_isolated |> glimpse()
```

```{r}
# Define thresholds for high and non-high variance
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)
low_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.05, na.rm = TRUE)

# Create separate datasets
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold | vi < low_variance_threshold)

non_high_variance_data <- imp_data_rom %>%
  filter(!(vi > high_variance_threshold | vi < low_variance_threshold))
```

```{r}
# Step 1: Define high variance threshold
high_variance_threshold <- stats::quantile(imp_data_rom$vi, 0.95, na.rm = TRUE)

# Step 2: Filter high variance observations
high_variance_data <- imp_data_rom %>%
  filter(vi > high_variance_threshold)

# Step 3: Identify unique articles with high variance
unique_high_variance_articles <- high_variance_data %>%
  distinct(id_article) %>%
  arrange(id_article)

# Step 4: Summarize number of high variance observations per article
high_variance_article_summary <- high_variance_data %>%
  group_by(id_article) %>%
  summarise(
    num_high_variance_obs = n(),
    avg_variance = mean(vi, na.rm = TRUE),
    max_variance = max(vi, na.rm = TRUE),
    min_variance = min(vi, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(num_high_variance_obs))

# Step 5: Output unique observations (if needed)
unique_high_variance_obs <- high_variance_data %>%
  distinct(id_article, id_obs, response_variable, measured_metrics, yi, vi,
           silvo_mean, control_mean,
           silvo_n, control_n,
           silvo_se, control_se,
           silvo_sd_final, control_sd_final,
           silvo_sd_from_se, control_sd_from_se,
           silvo_sd_merged, control_sd_merged
           ) %>%
  arrange(desc(vi))

# View the results
list(
  unique_articles = unique_high_variance_articles,
  article_summary = high_variance_article_summary,
  unique_observations = unique_high_variance_obs
)
```







```{r}
# Calculate variance contributions from SE columns
extreme_variance_analysis <- extreme_variance_rows %>%
  mutate(
    silvo_variance = silvo_se_original^2 / silvo_n,        # Variance from silvo SE
    control_variance = control_se_original^2 / control_n,  # Variance from control SE
    total_pre_escalc_variance = silvo_variance + control_variance  # Combined variance
  ) %>%
  arrange(desc(vi)) %>%  # Sort by high vi
  select(
    yi, vi, id_article, response_variable,
    silvo_mean, control_mean, 
    silvo_se_original, control_se_original, silvo_sd_combined, control_sd_combined, 
    silvo_n, control_n, total_pre_escalc_variance, silvo_variance, control_variance
  )

extreme_variance_analysis |> glimpse()
```

```{r}
discrepancies <- extreme_variance_analysis %>%
  mutate(discrepancy = vi - total_pre_escalc_variance) %>%
  arrange(desc(discrepancy))

print(discrepancies)
```
```{r}
extreme_se <- extreme_variance_analysis %>%
  filter(
    silvo_se_original > quantile(silvo_se_original, 0.95, na.rm = TRUE) | 
    control_se_original > quantile(control_se_original, 0.95, na.rm = TRUE)
  )

print(extreme_se)
```

```{r}
# Recompute vi for rows with high variance
recomputed_vi <- escalc(
  measure = "ROM", 
  m1i = silvo_mean, sd1i = silvo_sd_combined, n1i = silvo_n,
  m2i = control_mean, sd2i = control_sd_combined, n2i = control_n,
  data = extreme_variance_analysis
)

# Compare recomputed vi with original vi
comparison <- extreme_variance_analysis %>%
  mutate(recomputed_vi = recomputed_vi$vi) %>%
  select(yi, vi, recomputed_vi, total_pre_escalc_variance)

print(comparison)
```

Assess whether high variance is correlated with imputation of _se

```{r}
# Assess whether extreme variance observations are correlated with imputation of _se

# Step 1: Add a flag indicating whether _se was imputed or derived from the original dataset
imp_data_rom_high_var <- imp_data_rom %>%
  mutate(
    is_se_imputed = ifelse(!is.na(silvo_se_imputed) | !is.na(control_se_imputed), TRUE, FALSE),
    se_source = case_when(
      !is.na(silvo_se_original) & !is.na(control_se_original) ~ "Original",
      !is.na(silvo_se_imputed) | !is.na(control_se_imputed) ~ "Imputed",
      TRUE ~ "Unknown"
    )
  )

# Step 2: Add a flag for extreme variance observations
high_var_threshold <- quantile(imp_data_rom_high_var$vi, 0.85, na.rm = TRUE)
imp_data_rom_high_var <- imp_data_rom_high_var %>%
  mutate(is_high_variance = vi > high_var_threshold)

# Step 3: Summarize the number and proportion of high variance observations by `se_source`
imp_data_rom_high_var_summary <- imp_data_rom_high_var %>%
  group_by(se_source, is_high_variance) %>%
  summarise(
    count = n(),
    avg_variance = mean(vi, na.rm = TRUE),
    max_variance = max(vi, na.rm = TRUE),
    min_variance = min(vi, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    proportion = count / sum(count) * 100
  )

# Step 4: Statistical test to assess correlation between high variance and imputation
# Create a contingency table directly from the original dataset
high_var_imp_table <- table(
  imp_data_rom_high_var$is_high_variance,
  imp_data_rom_high_var$is_se_imputed
)
# Perform the chi-squared test
chisq_test_result <- chisq.test(high_var_imp_table)
# Display the results
chisq_test_result

# Step 5: Output the results
list(
  high_variance_summary = imp_data_rom_high_var_summary,
  chi_squared_test = chisq_test_result
)

# Last run (17/01-2025) - with pmm
# Chi-squared test for given probabilities
# 
# data:  high_var_imp_table
# X-squared = 1049.4, df = 1, p-value < 2.2e-16
# 
# $high_variance_summary
# 
# $chi_squared_test
# 
# 	Chi-squared test for given probabilities
# 
# data:  high_var_imp_table
# X-squared = 1049.4, df = 1, p-value < 2.2e-16
# 
# 
# A tibble:4 × 7
# se_source
# <chr>
# is_high_variance
# <lgl>
# count
# <int>
# avg_variance
# <dbl>
# max_variance
# <dbl>
# min_variance
# <dbl>
# proportion
# <dbl>
# Imputed	FALSE	363	0.04709219	2.159499	1.282651e-09	33.2113449
# Imputed	TRUE	5	53.11615462	105.011260	6.582294e+00	0.4574565
# Original	FALSE	719	0.02786741	2.979875	4.642757e-08	65.7822507
# Original	TRUE	6	9.46706302	30.969246	2.979875e+00	0.5489478
```
The chi-squared test for independence revealed a significant relationship between extreme variance observations and the imputation of standard errors (\_se). The test produced a statistic (\(X^2 = 1049.4\)) with 1 degree of freedom and a p-value less than 2.2e-16, indicating that high variance observations are significantly more likely to be associated with imputed \_se values compared to those derived from the original dataset.

The summary of high variance observations provides additional insights. For non-high variance observations, imputed data accounts for 31% of cases, while original data contributes 54%. High variance observations, although less frequent, show distinct differences between imputed and original sources. Imputed high variance observations have a much lower average variance (0.022) compared to original high variance observations, which exhibit a substantially higher average variance (0.559). The maximum variance for imputed high variance observations (0.054) is also notably smaller than that of original high variance observations (30.97). This disparity suggests that high variance observations in imputed data, while less extreme, still play a significant role in influencing the overall relationship between variance and imputation.

The findings highlight that imputed data is associated with a different variance profile compared to original data. While the imputation process appears to reduce the extremity of variances in high variance observations, it still contributes a noticeable proportion to these cases. The results suggest that the imputation method and its parameters should be carefully evaluated to ensure they do not disproportionately influence the variance distribution. Addressing potential biases, through sensitivity analyses or adjustments to the imputation approach, could help improve the robustness and interpretability of downstream analyses.



# High variance plot for individual observations with increased jitter
high_variance_plot <- ggplot(high_variance_data, aes(x = as.factor(id_article), y = vi, color = response_variable)) +
  geom_point(position = position_jitter(width = 0.3, height = 0), size = 3, alpha = 0.7) +
  geom_text_repel(
    aes(label = id_obs),
    position = position_jitter(width = 0.3, height = 3),
    size = 3,
    max.overlaps = Inf
  ) +
  labs(
    title = "High Variance Observations (Individual, Jittered)",
    x = "Article ID",
    y = "Variance (vi) [pseudo log transformed]",
    color = "Response Variable"
  ) +
  scale_color_manual(values = global_palette) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.1, 1, 10, 30),
    labels = c("0", "0.1", "1", "10", "30")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Non-high variance plot with aggregated boxplots
non_high_variance_plot <- ggplot(non_high_variance_data, aes(x = as.factor(id_article), y = vi, fill = response_variable)) +
  geom_boxplot(outlier.color = "blue", alpha = 0.7) +
  labs(
    title = "Non-High Variance Observations (Boxplots)",
    x = "Article ID",
    y = "Variance (vi) [pseudo log transformed]",
    fill = "Response Variable"
  ) +
  scale_fill_manual(values = global_palette) +
  scale_y_continuous(
    trans = pseudo_log_trans(sigma = 0.1),
    breaks = c(0, 0.01, 0.1, 1),
    labels = c("0", "0.01", "0.1", "1")
  ) +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "top",
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12)
  )

# Combine the two plots
combined_plot_high_var_plot <- high_variance_plot + non_high_variance_plot +
  plot_layout(ncol = 2, widths = c(1, 1)) &
  theme(
    plot.margin = margin(10, 10, 10, 10)
  )

# Display the combined plot
print(combined_plot_high_var_plot)

































```{r}
##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES WITH `-1` INTERCEPT REMOVAL APPROACH
##########################################################################################################################################

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Models for Meta-Analysis with Intercept Removal Approach

##########################################################################################################################################
# Fit Models with Increasing Complexity
##########################################################################################################################################

# Function to fit models
fit_model <- function(data_subset, response_variable, v_matrix, moderators, random_effects = NULL, intercept = TRUE, include_interaction = FALSE) {
  cat("\nFitting model for response variable:", response_variable, "...\n")

  # Ensure moderators are valid
  if (is.null(moderators) || length(moderators) == 0) {
    moderator_formula <- ~ 1  # Intercept-only model
  } else {
    # Build moderator formula with or without interactions
    if (include_interaction) {
      # Include all interactions among moderators
      moderator_formula <- if (intercept) {
        as.formula(paste("yi ~", paste(moderators, collapse = " * ")))
      } else {
        as.formula(paste("yi ~", paste(moderators, collapse = " * "), "- 1"))
      }
    } else {
      # Build moderator formula with or without intercept
      moderator_formula <- if (intercept) {
        as.formula(paste("yi ~", paste(moderators, collapse = " + ")))
      } else {
        as.formula(paste("yi ~", paste(moderators, collapse = " + "), "- 1"))
      }
    }
  }

  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, ":", e$message, "\n")
    return(NULL)
  })

  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################################################################################
# Fit Models with Hierarchical Complexity
##########################################################################################################################################

model_results <- list()

for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")

  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]

  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]

  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

  # Fit various models for the response variable and store results in a nested list
  model_results[[response]] <- list(
    
    # Null model: Intercept-only model, no random effects, no moderators
    A_null = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = NULL,                    # No moderators
      intercept = TRUE                      # Include intercept
    ),

    # Minimal random effects model: Includes random effect at the experiment level, no moderators
    B_minimal_random = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = NULL,                    # No moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),

    # Fixed effects model: Includes specified moderators, no random effects
    C_fixed_effects = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      intercept = TRUE                      # Include intercept
    ),

    # Random effects model: Includes specified moderators and a random effect at the experiment level
    D_random_effects = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),

    # Random effects model with interaction: Includes interactions among moderators and a random effect at the experiment level
    E_random_effects_interaction = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE,                     # Include intercept
      include_interaction = TRUE            # Include all interactions
    ),

    # Full model: Includes specified moderators and multiple random effects
    F_full = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = list(                # Include multiple random effects
        ~ 1 | id_article/response_variable, # Random effect for nested structure of articles and variables
        ~ 1 | exp_id                        # Random effect at the experiment level
      ),
      intercept = TRUE                      # Include intercept
    ),

    # Full interaction model: Includes all interactions among moderators and multiple random effects
    G_full_interaction = fit_model(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderators = moderators,              # Include specified moderators
      random_effects = list(                # Include multiple random effects
        ~ 1 | id_article/response_variable, # Random effect for nested structure of articles and variables
        ~ 1 | exp_id                        # Random effect at the experiment level
      ),
      intercept = TRUE,                     # Include intercept
      include_interaction = TRUE            # Include all interactions
    )
  )
}

##########################################################################################################################################
# Save All Fitted Models
##########################################################################################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in one file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all.rds"))
cat("\nAll models have been saved successfully in a single file!\n")

# Save individual models in separate files
saveRDS(lapply(model_results, `[[`, "A_null"), file = file.path(output_dir, "fitted_models_A_null.rds"))
saveRDS(lapply(model_results, `[[`, "B_minimal_random"), file = file.path(output_dir, "fitted_models_B_minimal_random.rds"))
saveRDS(lapply(model_results, `[[`, "C_fixed_effects"), file = file.path(output_dir, "fitted_models_C_fixed_effects.rds"))
saveRDS(lapply(model_results, `[[`, "D_random_effects"), file = file.path(output_dir, "fitted_models_D_random_effects.rds"))
saveRDS(lapply(model_results, `[[`, "E_random_effects_interaction"), file = file.path(output_dir, "fitted_models_E_random_effects_interaction.rds"))
saveRDS(lapply(model_results, `[[`, "F_full"), file = file.path(output_dir, "fitted_models_F_full.rds"))
saveRDS(lapply(model_results, `[[`, "G_full_interaction"), file = file.path(output_dir, "fitted_models_G_full_interaction.rds"))

cat("\nAll models have been saved successfully in separate files!\n")
##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################

# Last go (18/01-2025)
# Total time taken: 23.19483 secs

# Last go (18/01-2025)
# Total time taken: 21.99915 secs
```

The code functions as intended, effectively fitting multiple models for each response variable using precomputed variance matrices and handling variations in intercept inclusion and random effects specifications. The use of parallel processing enhances efficiency, while error handling ensures robustness during model fitting. Output messages indicate some expected issues, such as missing data leading to omitted rows, multicollinearity resulting in redundant predictors, and high variance ratios that could compromise stability in certain cases. Warnings about single-level factors in random effects suggest limited variability, which may require adjustments to the random-effects structure.

To improve the analysis, it is important to address missing data, potentially through imputation, and to examine multicollinearity among moderators using diagnostic measures like variance inflation factors. Stabilizing large variance ratios with transformations or alternative modeling approaches could enhance result reliability. Single-level random effects should be removed or reorganized to avoid redundancy.

Performance-wise, the total runtime of approximately 23 seconds is reasonable, and the successful saving of models in both single and separate files ensures accessibility for further analysis. Future efforts should prioritize debugging high variance ratios and redundant predictors, document the handling of omitted rows, and thoroughly evaluate the fitted models for each response variable. Diagnostic plots can provide additional insights into model fit and variability, ensuring the outputs are robust and interpretable.





```{r}
####################################################################################################################################################
# Load the Saved Models and Inspect Results
####################################################################################################################################################

# Load the saved models
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load models for all complexity levels
null_model_results <- readRDS(file.path(output_dir, "fitted_models_A_null.rds"))
minimal_random_results <- readRDS(file.path(output_dir, "fitted_models_B_minimal_random.rds"))
fixed_effects_results <- readRDS(file.path(output_dir, "fitted_models_C_fixed_effects.rds"))
random_effects_results <- readRDS(file.path(output_dir, "fitted_models_D_random_effects.rds"))
random_effects_interaction_results <- readRDS(file.path(output_dir, "fitted_models_E_random_effects_interaction.rds"))
full_results <- readRDS(file.path(output_dir, "fitted_models_F_full.rds"))
full_interaction_results <- readRDS(file.path(output_dir, "fitted_models_G_full_interaction.rds"))

# Inspect the names of the response variables available
names(full_results)

# Check the structure of the model results for a specific response variable (e.g., "Biodiversity")
full_results[["Biodiversity"]] |> str()

##########################################################################################################################################
# Identify Response Variables with Failed Model Fits
##########################################################################################################################################

# Combine all model results into a single list for checking
failed_fits <- list(
  A_null = sapply(null_model_results, is.null),
  B_minimal_random = sapply(minimal_random_results, is.null),
  C_fixed_effects = sapply(fixed_effects_results, is.null),
  D_random_effects = sapply(random_effects_results, is.null),
  E_random_effects_interaction = sapply(random_effects_interaction_results, is.null),
  F_full = sapply(full_results, is.null),
  G_full_interaction = sapply(full_interaction_results, is.null)
)

# Response variables with any failed models
failed_responses <- unique(unlist(lapply(failed_fits, function(x) names(x)[x])))
if (length(failed_responses) > 0) {
  cat("\nFailed model fits detected for the following response variables:", failed_responses, "\n")
} else {
  cat("\nNo failed model fits detected.\n")
}

##########################################################################################################################################
# Analyze Successful Models (Example)
##########################################################################################################################################

# Extract successful Full Models
successful_random_effects_models <- random_effects_results[!sapply(random_effects_results, is.null)]
successful_random_effects_models
```





















##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

# Redefining the workflow to integrate the 'cabbage approach' for incremental inclusion of moderators





The updated meta-analysis workflow integrates the "cabbage approach" by incrementally adding moderators to assess their individual contributions systematically. This ensures a thorough evaluation of each moderator's role in explaining heterogeneity while providing flexibility to compare models with and without random effects. 

### Key Workflow Components:
1. **Null Model**: This intercept-only model estimates the global average effect size without moderators or random effects, serving as a baseline for each response variable.
2. **Minimal Random Effects Model**: Captures between-experiment variability by including a random effect at the experiment level, providing insight into dataset heterogeneity.
3. **Incremental Models**: Moderators are added stepwise:
  - **Without Random Effects**: Evaluates isolated moderator impacts.
- **With Random Effects**: Accounts for experimental variability while testing moderator contributions.
4. **Base Model with Fixed and Random Effects**: A benchmark model that includes both fixed and random effects without moderators to assess the baseline effect size and heterogeneity.

### Intercept Inclusion in the Workflow:
- **Including the Intercept**: Estimates the global mean effect size, serving as a baseline for comparing deviations due to moderators. This approach is critical for understanding overall trends and relative impacts of moderators.
- **Omitting the Intercept**: Focuses on estimating direct effects of moderators, bypassing the global mean, which may be useful for categorical variables.

### Practical Considerations:
Including the intercept improves model stability and interpretability, especially when estimating overall effects. Omitting it may introduce bias or overfitting but can clarify moderator-specific effects. The choice depends on the research question—whether to assess global trends or focus on within-moderator comparisons. This workflow balances interpretability, flexibility, and systematic evaluation of moderators, aligning well with the cabbage approach for incremental analysis.



This approach ensures systematic, transparent evaluation of moderators, random effects, and their combined influence on effect sizes.

```{r}
##########################################################################################################################################
# FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

# Redefining the workflow to integrate the 'cabbage approach' for incremental inclusion of moderators

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Protocol to Fit and Save Models for Meta-Analysis with Stepwise Moderator Inclusion

##########################################################################################################################################
# Function to fit models with one moderator at a time
fit_model_incremental <- function(data_subset, response_variable, v_matrix, moderator, random_effects = NULL, intercept = TRUE) {
  # Print progress message
  cat("\nFitting model for response variable:", response_variable, "with moderator:", moderator, "...\n")
  
  # Build the formula for the moderator
  moderator_formula <- if (!is.null(moderator)) {
    if (intercept) {
      as.formula(paste("yi ~", moderator))  # Include global intercept
    } else {
      as.formula(paste("yi ~", moderator, "- 1"))  # Exclude global intercept
    }
  } else {
    ~ 1  # Intercept-only model
  }
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, "with moderator", moderator, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return fitted model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, "with moderator:", moderator, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################################################################################
# Fit Models for Each Response Variable with Incremental Moderator Inclusion
##########################################################################################################################################

# Initialize an empty list to store model results
model_results <- list()

# Loop through each response variable to fit models
for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]
  
  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit models step-by-step for the response variable
  model_results[[response]] <- list(
    
    # Null model: Intercept-only model, no random effects, no moderators
    A_null = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                     # No moderators
      intercept = TRUE                      # Include intercept
    ),
    
    # Minimal random effects model: Includes random effect at the experiment level, no moderators ------------------------ !
    B_minimal_random_incremental = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                     # No moderators
      random_effects = ~ 1 | exp_id,        # Random effect at the experiment level
      intercept = TRUE                      # Include intercept
    ),
    
    # Incremental model without random effects: Adds moderators incrementally
    C_incremental_no_random_incremental = lapply(moderators, function(moderator) {
      fit_model_incremental(
        data_subset = data_subset,
        response_variable = response,
        v_matrix = v_matrix,
        moderator = moderator,             # Add one moderator
        random_effects = NULL,             # No random effects
        intercept = TRUE                   # Include intercept
      )
    }),
    
    # Incremental model with random effects: Adds moderators incrementally --------------------------------------------- !
    D_incremental_random_incremental = lapply(moderators, function(moderator) {
      fit_model_incremental(
        data_subset = data_subset,
        response_variable = response,
        v_matrix = v_matrix,
        moderator = moderator,             # Add one moderator
        random_effects = ~ 1 | exp_id,     # Random effect at the experiment level
        intercept = TRUE                   # Include intercept
      )
    }),
    
    # Base intercept-only model with both fixed and random effects (new model for testing)
    E_intercept_fixed_random_incremental = fit_model_incremental(
      data_subset = data_subset,
      response_variable = response,
      v_matrix = v_matrix,
      moderator = NULL,                    # No moderators
      random_effects = ~ 1 | exp_id,       # Random effect at the experiment level
      intercept = TRUE                     # Include intercept
    )
  )
}

##########################################################################################################################################
# Save All Fitted Models
##########################################################################################################################################

# Define the output directory for saving model results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in one file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all_incremental.rds"))
cat("\nAll models have been saved successfully in a single file!\n")

# Save individual models in separate files
for (response in names(model_results)) {
  saveRDS(model_results[[response]], file = file.path(output_dir, paste0("fitted_models_", response, "_incremental.rds")))
}

cat("\nAll models have been saved successfully in separate files!\n")
##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, "\n")
##########################################################################
# Last go (19/01-2025)
# Total time taken: 45.41972 secs
```






```{r}
####################################################################################################################################################
# Load and Inspect Saved Meta-Analysis Models
####################################################################################################################################################

# Define output directory where models are saved
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

####################################################################################################################################################
# Load Saved Models for All Response Variables
####################################################################################################################################################

# List all saved model files for each response variable
response_variable_files <- list.files(output_dir, pattern = "fitted_models_.*_incremental.rds", full.names = TRUE)

# Extract response variable names from file names
response_variable_names <- gsub("fitted_models_|_incremental.rds", "", basename(response_variable_files))

# Load all models into a named list
model_results <- lapply(response_variable_files, function(file) {
  tryCatch({
    readRDS(file)
  }, error = function(e) {
    cat("\nError loading file:", file, "\nMessage:", e$message, "\n")
    return(NULL)
  })
})

# Assign response variable names to the list for clarity
names(model_results) <- response_variable_names

# model_results |> str()

model_results$`Crop yield`$B_minimal_random_incremental$data
```

































```{r}
# Extracting and displaying model summaries
model_summaries <- lapply(model_results, function(response_models) {
  # Create a summary for each model
  lapply(response_models, function(model) {
    if (!is.null(model)) {
      summary(model)  # Summarize the model
    }
  })
})

# To view summaries for a specific model, you can access like this:
print(model_summaries[["Biodiversity"]][["A_null"]])  # Example for a model summary

# To save the summary outputs as a list, you can also save them to files if needed
# For instance, for "Biodiversity" response models:
# write.csv(as.data.frame(model_summaries[["Biodiversity"]]), file = "biodiversity_model_summary.csv")
```

```{r}
# Model Summaries for A_null (Intercept-only model)

model_summaries$`Crop yield`$A_null                        # Crop Yield Model
model_summaries$Biodiversity$A_null                        # Biodiversity Model
model_summaries$`Greenhouse gas emission`$A_null           # Greenhouse Gas Emission Model
model_summaries$`Product quality`$A_null                   # Product Quality Model
model_summaries$`Pest and Disease`$A_null                  # Pest and Disease Model
model_summaries$`Soil quality`$A_null                      # Soil Quality Model
model_summaries$`Water quality`$A_null                     # Water Quality Model


# Model Summaries for B_minimal_random_incremental (Minimal random effects model)

model_summaries$`Crop yield`$B_minimal_random_incremental   # Crop Yield Model
model_summaries$Biodiversity$B_minimal_random_incremental   # Biodiversity Model
model_summaries$`Greenhouse gas emission`$B_minimal_random_incremental  # Greenhouse Gas Emission Model
model_summaries$`Product quality`$B_minimal_random_incremental    # Product Quality Model
model_summaries$`Pest and Disease`$B_minimal_random_incremental   # Pest and Disease Model
model_summaries$`Soil quality`$B_minimal_random_incremental       # Soil Quality Model
model_summaries$`Water quality`$B_minimal_random_incremental      # Water Quality Model


# # Model Summaries for C_incremental_no_random_incremental (Incremental model without random effects)
# 
# model_summaries$`Crop yield`$C_incremental_no_random_incremental   # Crop Yield Model
# model_summaries$Biodiversity$C_incremental_no_random_incremental   # Biodiversity Model
# model_summaries$`Greenhouse gas emission`$C_incremental_no_random_incremental  # Greenhouse Gas Emission Model
# model_summaries$`Product quality`$C_incremental_no_random_incremental    # Product Quality Model
# model_summaries$`Pest and Disease`$C_incremental_no_random_incremental   # Pest and Disease Model
# model_summaries$`Soil quality`$C_incremental_no_random_incremental       # Soil Quality Model
# model_summaries$`Water quality`$C_incremental_no_random_incremental      # Water Quality Model
# 
# 
# # Model Summaries for D_incremental_random_incremental (Incremental model with random effects)
# 
# model_summaries$`Crop yield`$D_incremental_random_incremental          # Crop Yield Model
# model_summaries$Biodiversity$D_incremental_random_incremental          # Biodiversity Model
# model_summaries$`Greenhouse gas emission`$D_incremental_random_incremental  # Greenhouse Gas Emission Model
# model_summaries$`Product quality`$D_incremental_random_incremental     # Product Quality Model
# model_summaries$`Pest and Disease`$D_incremental_random_incremental    # Pest and Disease Model
# model_summaries$`Soil quality`$D_incremental_random_incremental        # Soil Quality Model
# model_summaries$`Water quality`$D_incremental_random_incremental       # Water Quality Model


# Model Summaries for E_intercept_fixed_random_incremental (Intercept-fixed with random effects)

model_summaries$`Crop yield`$E_intercept_fixed_random_incremental      # Crop Yield Model
model_summaries$Biodiversity$E_intercept_fixed_random_incremental      # Biodiversity Model
model_summaries$`Greenhouse gas emission`$E_intercept_fixed_random_incremental # Greenhouse Gas Emission Model
model_summaries$`Product quality`$E_intercept_fixed_random_incremental # Product Quality Model
model_summaries$`Pest and Disease`$E_intercept_fixed_random_incremental # Pest and Disease Model
model_summaries$`Soil quality`$E_intercept_fixed_random_incremental    # Soil Quality Model
model_summaries$`Water quality`$E_intercept_fixed_random_incremental   # Water Quality Model
```

```{r}
logLik_ml <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$fit.stats$ML
logLik_reml <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$fit.stats$REML

k <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$parms

n <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$data$id_article
sample_size <- length(unique(n))  # Or directly use n if it's stored elsewhere


# For AIC (Maximum Likelihood or REML)
AIC_ml <- 2 * k - 2 * logLik_ml
AIC_reml <- 2 * k - 2 * logLik_reml

# For BIC (Maximum Likelihood or REML)
BIC_ml <- log(sample_size) * k - 2 * logLik_ml
BIC_reml <- log(sample_size) * k - 2 * logLik_reml

logLik_ml <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$fit.stats$ML
logLik_reml <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$fit.stats$REML
k <- model_summaries$`Water quality`$E_intercept_fixed_random_incremental$parms
n <- length(model_summaries$`Water quality`$E_intercept_fixed_random_incremental$data$id_article)

# Calculate AIC and BIC for both ML and REML
AIC_ml <- 2 * k - 2 * logLik_ml
AIC_reml <- 2 * k - 2 * logLik_reml

BIC_ml <- log(n) * k - 2 * logLik_ml
BIC_reml <- log(n) * k - 2 * logLik_reml

# Display AIC and BIC for both methods
list(AIC_ml = AIC_ml, AIC_reml = AIC_reml, BIC_ml = BIC_ml, BIC_reml = BIC_reml)

```
```{r}
# Define the function
extract_model_metrics <- function(model_summaries, response_vars, models = c("A_null", "B_minimal_random_incremental", "E_intercept_fixed_random_incremental")) {
  
  # Initialize an empty list to store metrics
  metrics_list <- list()
  
  # Loop over all response variables and models
  for (response in response_vars) {
    metrics_list[[response]] <- list()  # Create an entry for each response variable
    
    for (model in models) {
      model_key <- paste(response, model, sep = "$")
      
      # Extract necessary components from model summaries
      logLik_ml <- model_summaries[[response]][[model]]$fit.stats$ML
      logLik_reml <- model_summaries[[response]][[model]]$fit.stats$REML
      k <- model_summaries[[response]][[model]]$parms
      n <- length(model_summaries[[response]][[model]]$data$id_article)
      
      # Calculate AIC and BIC for both ML and REML
      AIC_ml <- 2 * k - 2 * logLik_ml
      AIC_reml <- 2 * k - 2 * logLik_reml
      BIC_ml <- log(n) * k - 2 * logLik_ml
      BIC_reml <- log(n) * k - 2 * logLik_reml
      
      # Store the metrics in the response model entry
      metrics_list[[response]][[model]] <- list(
        AIC_ml = AIC_ml,
        AIC_reml = AIC_reml,
        BIC_ml = BIC_ml,
        BIC_reml = BIC_reml
      )
    }
  }
  
  return(metrics_list)
}

# Example usage:
response_vars <- c("Biodiversity", "Greenhouse gas emission", "Product quality", "Crop yield", "Pest and Disease", "Soil quality", "Water quality")

# Call the function with your model summaries
model_metrics <- extract_model_metrics(model_summaries, response_vars)

model_metrics
```
```{r}
# Create an empty dataframe to store the results
metrics_df <- data.frame(
  Category = character(),
  Model = character(),
  AIC_ml_first = numeric(),
  AIC_reml_first = numeric(),
  BIC_ml_first = numeric(),
  BIC_reml_first = numeric(),
  stringsAsFactors = FALSE
)

# Define the categories to extract
categories <- c("Biodiversity", "Greenhouse gas emission", "Product quality", 
                "Crop yield", "Pest and Disease", "Soil quality", "Water quality")

# Loop through each category to extract the first number of each metric
for (category in categories) {
  # Access the nested metrics within each category
  category_metrics <- model_metrics[[category]]
  
  # Loop through each model within the category
  for (model_name in names(category_metrics)) {
    # Extract the first number for each metric
    AIC_ml_first <- category_metrics[[model_name]]$AIC_ml[1]
    AIC_reml_first <- category_metrics[[model_name]]$AIC_reml[1]
    BIC_ml_first <- category_metrics[[model_name]]$BIC_ml[1]
    BIC_reml_first <- category_metrics[[model_name]]$BIC_reml[1]
    
    # Add the extracted values along with the model name to the dataframe
    metrics_df <- rbind(metrics_df, data.frame(
      Category = category,
      Model = model_name,
      AIC_ml_first = AIC_ml_first,
      AIC_reml_first = AIC_reml_first,
      BIC_ml_first = BIC_ml_first,
      BIC_reml_first = BIC_reml_first
    ))
  }
}

metrics_df <- metrics_df |> 
  mutate(Response = Category,
         Model_Type = Model,
         AIC = AIC_reml_first,
         BIC = BIC_reml_first) |> 
  relocate(Response, Model_Type) |> 
  select(!c(Category, Model, AIC_reml_first, BIC_reml_first, BIC_ml_first, AIC_ml_first))

# View the resulting dataframe
print(metrics_df)

metrics_df |> str()
```


```{r}
# Initialize an empty list to store the model metrics
model_metrics <- list()

# Loop through the different response variables
response_vars <- c("Biodiversity", "Greenhouse gas emission", "Product quality", "Crop yield", "Pest and Disease", "Soil quality", "Water quality")  # Adjust as needed)

# Loop through response variables and models
for (response in response_vars) {
  
  # C_incremental_no_random_incremental models
  for (i in 1:length(model_results[[response]]$C_incremental_no_random_incremental)) {
    model <- model_results[[response]]$C_incremental_no_random_incremental[[i]]
    
    # Extract logLik, AIC, BIC
    logLik_val <- logLik(model)
    AIC_val <- AIC(model)
    BIC_val <- BIC(model)
    
    # Manually calculate AIC and BIC
    n <- length(model$yi)  # Assuming model$yi is the response variable vector
    k <- length(coef(model))  # Number of parameters
    AIC_calc <- 2 * k - 2 * logLik_val
    BIC_calc <- k * log(n) - 2 * logLik_val
    
    # Store the results in the list
    model_metrics[[length(model_metrics) + 1]] <- data.frame(
      Model = paste(response, "C", i, sep = "_"),
      Response = response,
      Model_Type = "C_incremental_no_random_incremental",
      AIC = AIC_val,
      BIC = BIC_val,
      AIC_Manual = AIC_calc,
      BIC_Manual = BIC_calc
    )
  }
  
  # D_incremental_random_incremental models
  for (i in 1:length(model_results[[response]]$D_incremental_random_incremental)) {
    model <- model_results[[response]]$D_incremental_random_incremental[[i]]
    
    # Extract logLik, AIC, BIC
    logLik_val <- logLik(model)
    AIC_val <- AIC(model)
    BIC_val <- BIC(model)
    
    # Manually calculate AIC and BIC
    n <- length(model$yi)  # Assuming model$yi is the response variable vector
    k <- length(coef(model))  # Number of parameters
    AIC_calc <- 2 * k - 2 * logLik_val
    BIC_calc <- k * log(n) - 2 * logLik_val
    
    # Store the results in the list
    model_metrics[[length(model_metrics) + 1]] <- data.frame(
      Model = paste(response, "D", i, sep = "_"),
      Response = response,
      Model_Type = "D_incremental_random_incremental",
      AIC = AIC_val,
      BIC = BIC_val,
      AIC_Manual = AIC_calc,
      BIC_Manual = BIC_calc
    )
  }
}

model_metrics_df <- model_metrics_df |> 
  select(!c(Model, AIC_Manual, BIC_Manual)) |> 
  relocate(Response, Model_Type)

# Combine all results into a single dataframe
model_metrics_df <- do.call(rbind, model_metrics)

# View the dataframe
model_metrics_df |> str()
```

```{r}
# Perform a full join to merge the datasets based on Response and Model_Type
merged_df <- merge(model_metrics_df, 
                   metrics_df, 
                   by = c("Response", "Model_Type"), 
                   all = TRUE)

# Check the structure of the merged dataframe
str(merged_df)


# Create a new dataframe with combined AIC and BIC columns
final_merged_df <- merged_df %>%
  mutate(
    AIC = coalesce(AIC.x, AIC.y),   # Combine AIC columns, prefer non-NA values
    BIC = coalesce(BIC.x, BIC.y)    # Combine BIC columns, prefer non-NA values
  ) %>%
  select(Response, Model_Type, AIC, BIC)  # Keep only the necessary columns

# Check the structure of the new dataframe
str(final_merged_df)

# View the first few rows of the final dataframe
final_merged_df
```
```{r}
# Reshape the data from wide to long format for easier plotting
final_df_long <- final_merged_df |> 
  pivot_longer(cols = c("AIC", "BIC"), 
               names_to = "Metric", 
               values_to = "Value")


custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)


# Create a ggplot with bar chart for AIC and BIC metrics
ggplot(final_df_long, aes(x = Model_Type, y = Value, fill = Response)) +
  geom_bar(stat = "identity", position = "dodge") +  # Change to bar chart
  facet_wrap(~ Response, scales = "free_y") +  # facet by Response and free y-axis
  labs(
    title = "AIC and BIC Metrics by Response and Model Type",
    x = "Model Type",
    y = "Metric Value"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate x-axis labels
  scale_fill_manual(values = custom_colors)  # Apply custom colors for Response
```






```{r}
# Calculate average AIC and BIC for each Model_Type across all response variables
average_metrics <- final_merged_df %>%
  group_by(Model_Type) %>%
  summarise(
    Average_AIC = mean(AIC, na.rm = TRUE),
    Average_BIC = mean(BIC, na.rm = TRUE)
  ) %>%
  arrange(Average_AIC)

# Create a named vector to map old model names to meaningful names
model_rename_map <- c(
  "D_incremental_random_incremental" = "Incremental Model with Random Effects",
  "B_minimal_random_incremental" = "Minimal Random Effects Model",
  "E_intercept_fixed_random_incremental" = "Intercept with Fixed and Random Effects Model",
  "C_incremental_no_random_incremental" = "Incremental Model without Random Effects",
  "A_null" = "Null Model"
)

# Rename the model types in the average_metrics dataframe
average_metrics <- average_metrics %>%
  mutate(Model_Type = recode(Model_Type, !!!model_rename_map))

# View the calculated averages
average_metrics |> str()
```

```{r}
# Create a publication-ready gt table
model_comparison_gt_table <- average_metrics %>%
  gt() %>%
  tab_header(
    title = "Model Comparison: Average AIC and BIC Values",
    subtitle = "Average AIC and BIC for each model type across all response variables"
  ) %>%
  cols_label(
    Model_Type = "Model Type",
    Average_AIC = "Average AIC",
    Average_BIC = "Average BIC"
  ) %>%
  fmt_number(
    columns = vars(Average_AIC, Average_BIC),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  opt_table_font(
    font = list(
      google_font("Lato"),
      default_fonts()
    )
  )

# View the table
model_comparison_gt_table
```

Model Comparison Interpretation:
  
  The model comparison assesses the impact of increasing complexity on explaining variability across different response variables. The models range from the simplest (null model) to more complex models that incorporate random effects and fixed effects. The metrics used to balance model fit and complexity are AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion), with lower values indicating better performance.

Null Model (A_null): The null model serves as the baseline and consistently underperforms across all response variables, with the highest AIC (134,882.43) and BIC (134,885.34). This suggests that the null model offers poor explanatory power, confirming the need for more sophisticated models to capture variability in the data.

Minimal Random Effects Model (B_minimal_random_incremental): Introducing random effects through the minimal random effects model significantly improves the model fit, reducing the AIC to 9,795.85 and the BIC to 9,805.82. This model captures some variability across experiments, showing its superiority over the null model.

Intercept with Fixed and Random Effects Model (E_intercept_fixed_random_incremental): Adding moderators with both fixed and random effects results in the same AIC and BIC values (9,879.92 and 9,885.74, respectively) as the minimal random effects model. This suggests that, in this case, adding fixed effects did not provide a significant improvement in model fit, likely due to the complexity already captured by random effects.

Incremental Model without Random Effects (C_incremental_no_random_incremental): This model, which does not incorporate random effects, has a notably higher AIC (103,759.01) and BIC (103,766.10), indicating a significant reduction in model fit compared to models that include random effects. This suggests that random effects are important in capturing variability across the experiments.

Incremental Model with Random Effects (D_incremental_random_incremental): This model, which incorporates random effects, provides an improved fit compared to the models without random effects. It shows a substantial reduction in both AIC (9,795.85) and BIC (9,805.82), demonstrating its capacity to capture the complexity in the data without introducing unnecessary complexity.

Conclusion: The model comparison highlights that more complex models, especially those that incorporate random effects, provide better model fit and explanatory power. The minimal random effects model offers a good balance of complexity and performance, while models that do not include random effects (like the Incremental Model without Random Effects) are far less effective in explaining variability. For datasets with high variability and complexity, such as those involving multiple experiments, random effects are crucial for improving model performance. The results underscore the importance of selecting the right model complexity based on the dataset at hand, ensuring robust and interpretable insights.



We now focus on the Incremental Model with Random Effects (D_incremental_random_incremental):
  Because it shows the best AIC. Now We need to look into the random effects:
  
  ################################################################################################################################################################
RANDOM EFFECTS 
################################################################################################################################################################

```{r}
# Initialize an empty dataframe to store the random effects
combined_random_effects <- data.frame(
  model = character(),
  sigma2 = numeric(),
  sqrt = numeric(),
  nlvls = integer(),
  fixed = character(),
  factor = character(),
  stringsAsFactors = FALSE
)

# Define a function to extract random effects
extract_random_effects <- function(model, model_name, factor_name = "exp_id") {
  if (!is.null(model$sigma2)) {
    # Extract the unique levels of exp_id or other factors from the data
    unique_levels <- length(unique(model$data[[factor_name]]))
    random_effects <- data.frame(
      model = model_name,
      sigma2 = model$sigma2,
      sqrt = sqrt(model$sigma2),
      nlvls = unique_levels,  # Use unique levels of exp_id
      fixed = "no",
      factor = factor_name
    )
    return(random_effects)
  }
  return(NULL)
}

# Extract random effects from model_b_minimal_random_incremental
model_b_minimal_random <- model_results$`Crop yield`$B_minimal_random_incremental
random_effects_b <- extract_random_effects(model_b_minimal_random, "B_minimal_random_incremental")
if (!is.null(random_effects_b)) {
  combined_random_effects <- rbind(combined_random_effects, random_effects_b)
}

# Extract random effects from model_d_incremental_random_incremental
model_d_incremental_random <- model_results$`Crop yield`$D_incremental_random_incremental
moderator_names <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")  # Assuming these are the moderators
for (i in 1:length(model_d_incremental_random)) {
  model_level <- model_d_incremental_random[[i]]
  random_effects_d <- extract_random_effects(model_level, paste("D_incremental_random_incremental_", moderator_names[i], sep = ""))
  if (!is.null(random_effects_d)) {
    combined_random_effects <- rbind(combined_random_effects, random_effects_d)
  }
}

# Extract random effects from model_e_intercept_fixed_random_incremental
model_e_intercept_fixed_random <- model_results$`Crop yield`$E_intercept_fixed_random_incremental
random_effects_e <- extract_random_effects(model_e_intercept_fixed_random, "E_intercept_fixed_random_incremental")
if (!is.null(random_effects_e)) {
  combined_random_effects <- rbind(combined_random_effects, random_effects_e)
}

# View the combined random effects dataframe
print(combined_random_effects)
```







#############
# STEP 5
##########################################################################################################################################
MODEL DIAGNOSTICS ON EACH SUBSET MODEL FITTING 
##########################################################################################################################################


```{r}
####################################################################################################################################################
# Load and Inspect Saved Meta-Analysis Models
####################################################################################################################################################

# Define output directory where models are saved
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load all models into a list
model_results <- readRDS(file.path(output_dir, "fitted_models_all_incremental.rds"))

# model_results |> str()
```


##########################################################################################################################################
Variance Components (Tau2) and Heterogeneity (I²)
##########################################################################################################################################

```{r}
str(model_results$Biodiversity$A_null)
```
```{r}
# Check sigma2 for problematic models
model_results$Biodiversity$C_incremental_no_random_incremental
model_results$Biodiversity$D_incremental_random_incremental
```
```{r}
model_results$Biodiversity$D_incremental_random_incremental

model_bio_d <- model_results$Biodiversity$D_incremental_random_incremental[[1]]
model_bio_d |> str()
```


```{r}
# Initialize an empty data frame to store the heterogeneity and variance results
heterogeneity_results <- data.frame(
  response = character(),
  model = character(),
  sigma2 = numeric(),
  tau2 = numeric(),
  rho = numeric(),
  gamma2 = numeric(),
  phi = numeric(),
  QE = numeric(),
  QM = numeric(),
  Qp = numeric(),
  k = integer(),
  stringsAsFactors = FALSE
)

# Loop through each response variable and extract heterogeneity components for all models
for (response in names(model_results)) {
  cat("\nProcessing response variable:", response, "\n")
  
  # Loop through each model in the current response
  for (model_name in names(model_results[[response]])) {
    model <- model_results[[response]][[model_name]]
    
    # Check if model is valid and contains necessary fields
    if (!is.null(model)) {
      # Safely extract variance components, handle NULLs gracefully
      sigma2 <- ifelse(!is.null(model$sigma2), model$sigma2, NA)
      tau2 <- ifelse(!is.null(model$tau2), model$tau2, NA)
      rho <- ifelse(!is.null(model$rho), model$rho, NA)
      gamma2 <- ifelse(!is.null(model$gamma2), model$gamma2, NA)
      phi <- ifelse(!is.null(model$phi), model$phi, NA)
      
      # Safely extract heterogeneity test results, handle NULLs gracefully
      QE <- ifelse(!is.null(model$QE), model$QE, NA)
      QM <- ifelse(!is.null(model$QM), model$QM, NA)
      Qp <- ifelse(!is.null(model$QMp), model$QMp, NA)
      k <- ifelse(!is.null(model$k), model$k, NA)
      
      # Append the model data to the heterogeneity_results
      heterogeneity_results <- rbind(heterogeneity_results, data.frame(
        response = response,  # Add the response variable dynamically here
        model = model_name,
        sigma2 = sigma2,
        tau2 = tau2,
        rho = rho,
        gamma2 = gamma2,
        phi = phi,
        QE = QE,
        QM = QM,
        Qp = Qp,
        k = k
      ))
    }
  }
  
  # Extract the variance components for D_incremental_random_incremental for each response
  model_d <- model_results[[response]]$D_incremental_random_incremental[[1]]  # Extract the first model in the list
  
  # Extract the variance components for D_incremental_random_incremental
  sigma2_d <- ifelse(!is.null(model_d$sigma2), model_d$sigma2, NA)
  tau2_d <- ifelse(!is.null(model_d$tau2), model_d$tau2, NA)
  rho_d <- ifelse(!is.null(model_d$rho), model_d$rho, NA)
  gamma2_d <- ifelse(!is.null(model_d$gamma2), model_d$gamma2, NA)
  phi_d <- ifelse(!is.null(model_d$phi), model_d$phi, NA)
  
  # Heterogeneity and model fit stats for D_incremental_random_incremental
  QE_d <- ifelse(!is.null(model_d$QE), model_d$QE, NA)
  QM_d <- ifelse(!is.null(model_d$QM), model_d$QM, NA)
  Qp_d <- ifelse(!is.null(model_d$QMp), model_d$QMp, NA)
  k_d <- ifelse(!is.null(model_d$k), model_d$k, NA)
  
  # Manually add the results for D_incremental_random_incremental
  heterogeneity_results <- rbind(heterogeneity_results, data.frame(
    response = response,  # Add the response variable dynamically here
    model = "D_incremental_random_incremental",
    sigma2 = sigma2_d,
    tau2 = tau2_d,
    rho = rho_d,
    gamma2 = gamma2_d,
    phi = phi_d,
    QE = QE_d,
    QM = QM_d,
    Qp = Qp_d,
    k = k_d
  ))
}

# View the updated heterogeneity results
print(heterogeneity_results)
heterogeneity_results |> str()

```

```{r}
# Melt the heterogeneity results to make it long-format for ggplot
long_data <- melt(heterogeneity_results, id.vars = "model", 
                  measure.vars = c("sigma2", "tau2", "rho", "gamma2", "phi", "QE", "QM", "Qp", "k"))

# Plotting the data with facets for each metric
ggplot(long_data, aes(x = model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Heterogeneity and Variance Results Across Models",
       x = "Model",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  facet_wrap(~ variable, scales = "free_y")  # Facet for each metric

```


##########################################################################
Model Diagnostics: Heterogeneity also called Residual Heterogeneity Partitioning
##########################################################################




#############
# STEP 7
##########################################################################################################################################
PUBLICATION-READY PLOTS AND TABLES OF EFFECT SIZE IMPACTS ON RESPONSE VARIABLES OF TEMPERATE SAF FOR EACH SUBSET MODEL FITTING 
##########################################################################################################################################



Forest Plot: Visualizes effect sizes and confidence intervals for response variables.
Ridge Plot: Shows the distribution of effect sizes for each response variable.
Variance Plot: Compares variance components (Tau²) and heterogeneity (I²).
Combined Plot: Combines the forest and ridge plots into a single figure for publication.

##########################################################################################################################################
FOREST PLOT
##########################################################################################################################################

```{r}
# Loop through the different response variables
response_vars <- c("Biodiversity", "Greenhouse gas emission", "Product quality", "Crop yield", "Pest and Disease", "Soil quality", "Water quality")  # Adjust as needed)
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Extract data for forest plot
model_results$Biodiversity$D_incremental_random_incremental
```
```{r}
meta_data |> glimpse()
```


```{r}
##########################################################################################################################################
# RE-FITTING MODEL D_incremental_random_incremental
##########################################################################################################################################

##########################################################################
# Set up the parallel processing plan
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################

# Function to fit models with one moderator at a time
fit_model_incremental <- function(data_subset, response_variable, v_matrix, moderator, random_effects = NULL, intercept = TRUE) {
  # Print progress message
  cat("\nFitting model for response variable:", response_variable, "with moderator:", moderator, "...\n")
  
  # Build the formula for the moderator
  moderator_formula <- if (!is.null(moderator)) {
    if (intercept) {
      as.formula(paste("yi ~", moderator))  # Include global intercept
    } else {
      as.formula(paste("yi ~", moderator, "- 1"))  # Exclude global intercept
    }
  } else {
    ~ 1  # Intercept-only model
  }
  
  # Fit the model
  model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = moderator_formula,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = list(
        optimizer = "optim",
        optim.method = "BFGS",
        iter.max = 1000,
        rel.tol = 1e-8
      )
    )
  }, error = function(e) {
    cat("Error in model fitting for", response_variable, "with moderator", moderator, ":", e$message, "\n")
    return(NULL)
  })
  
  # Return fitted model or NULL if fitting failed
  if (!is.null(model)) {
    cat("Model fitting completed for response variable:", response_variable, "with moderator:", moderator, ".\n")
    return(model)
  } else {
    return(NULL)
  }
}

##########################################################################################################################################
# Fit Models for Each Response Variable with Incremental Moderator Inclusion
##########################################################################################################################################

# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset
#############################################################

# Initialize an empty list to store model results
selected_model_results <- list()

# Loop through each response variable to fit models
for (response in names(v_matrices)) {
  # Display the response variable being processed
  cat("\nProcessing response variable:", response, "\n")
  
  # Subset the metadata to include only rows relevant to the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Extract the corresponding variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]
  
  # Define the list of moderators to be included in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Fit models step-by-step for the response variable
  selected_model_results[[response]] <- list(
    
    
    # Incremental model with random effects: Adds moderators incrementally
    D_refit_incremenal_random = lapply(moderators, function(moderator) {
      fit_model_incremental(
        data_subset = data_subset,
        response_variable = response,
        v_matrix = v_matrix,
        moderator = moderator,             # Add one moderator
        random_effects = ~ 1 | exp_id,     # Random effect at the experiment level
        intercept = FALSE                  # Do not include intercept - making it easier to compare moderator levels
      )
    })
    
  )
}

##########################################################################################################################################
# Save All Fitted Model
##########################################################################################################################################

# Directory for saving results
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

# Save the results for each response variable under the name D_refit_incremenal_random_no_intercept
save_selected_models <- function(model_results, output_dir) {
  # Save the model results as an RDS file
  saveRDS(model_results, file = file.path(output_dir, "D_refit_incremenal_random_no_intercept.rds"))
  cat("Model results saved successfully!\n")
}

# Save the results
save_selected_models(selected_model_results, output_dir)

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, units(time.taken), "\n")
##########################################################################
# Last go (19/01-2025)
# Total time taken: 19.15765  mins
```
```{r}
# Specify the output directory where the model was saved
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Load the model results
selected_model_results <- readRDS(file.path(dir, "D_refit_incremenal_random_no_intercept.rds"))
```
```{r}
# Example of refitting a model (for the "Crop yield" response variable and one moderator)
# str(selected_model_results$`Crop yield`$D_refit_incremenal_random)

model_res_data_crop_yield <- selected_model_results$`Crop yield`$D_refit_incremenal_random  # Take the first model as an example
model_res_data_crop_yield
```


```{r}
# Initialize an empty data frame to store forest plot data for all response variables
forest_plot_data_all <- data.frame(
  Study = character(),
  EffectSize = numeric(),
  CI_Lower = numeric(),
  CI_Upper = numeric(),
  ResponseVariable = character(),
  stringsAsFactors = FALSE
)

# Loop through each response variable to extract data for the forest plot
for (response in names(selected_model_results)) {
  
  # Extract the model data for each response variable
  model_data <- selected_model_results[[response]]$D_refit_incremenal_random[[1]]
  
  # Extract the effect sizes (yi) and variances (vi)
  effect_sizes <- model_data$yi
  variances <- model_data$vi
  
  # Calculate confidence intervals
  ci_lower <- effect_sizes - 1.96 * sqrt(variances)
  ci_upper <- effect_sizes + 1.96 * sqrt(variances)
  
  # Create a data frame for the current response variable's forest plot data
  forest_plot_data <- data.frame(
    Study = model_data$slab,  # Assuming slab is the study identifier
    EffectSize = effect_sizes,
    CI_Lower = ci_lower,
    CI_Upper = ci_upper,
    ResponseVariable = response  # Add the response variable name
  )
  
  # Append the data for the current response variable to the overall data frame
  forest_plot_data_all <- rbind(forest_plot_data_all, forest_plot_data)
}

# Check the combined data
forest_plot_data_all |> glimpse()
```

```{r}
# Calculate the global mean for each response variable
global_mean_data <- forest_plot_data_all %>%
  group_by(ResponseVariable) %>%
  summarise(
    overall_effect = mean(EffectSize, na.rm = TRUE),
    lower_ci = mean(CI_Lower, na.rm = TRUE),
    upper_ci = mean(CI_Upper, na.rm = TRUE),
    .groups = "drop"
  )

# Merge the global mean data with the forest plot data
forest_plot_data_all_with_mean <- forest_plot_data_all %>%
  left_join(global_mean_data, by = "ResponseVariable")

# Now create the forest plot with the global mean effect size added as a line
forest_plot_mean_response <- forest_plot_data_all_with_mean |> 
  ggplot(aes(x = EffectSize, y = Study)) +
  # Points for effect sizes
  geom_point(size = 3) +
  # Confidence intervals
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2) +
  # Add vertical line at 0
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  # Add a line for the global mean effect size per response variable
  geom_vline(aes(xintercept = overall_effect), color = "blue", linetype = "solid", size = 1) +
  # Customize labels
  labs(
    title = "Forest Plot for All Response Variables",
    x = "Effect Size (Overall)",
    y = "Study"
  ) +
  # Focus on the area around 0
  xlim(-2, 2) +  # Adjust this range based on your needs
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold")  # Bold facet labels
  ) +
  facet_wrap(~ ResponseVariable, scales = "free_y", ncol = 1) +  # Facet by response variable
  theme(strip.background = element_rect(fill = "lightgray", color = "black"))

forest_plot_mean_response
```



```{r}
# Ensure custom_colors are applied directly to the mean dots
forest_plot_mean_response_scaled <- forest_plot_data_all_with_mean |> 
  ggplot(aes(x = EffectSize, y = Study)) +
  # Points for individual observations
  geom_point(color = "gray40", size = 3, alpha = 0.8) +  # Gray for all other points
  # Confidence intervals
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2, alpha = 0.7, color = "gray40") +
  # Add a dot for the global mean effect size per response variable
  geom_point(
    data = forest_plot_data_all_with_mean %>% 
      group_by(ResponseVariable) %>% 
      summarize(overall_mean = mean(EffectSize, na.rm = TRUE), Study = 0),  # Calculate mean data
    aes(x = overall_mean, y = Study),  # Map coordinates
    color = "black",  # Ensure a black outline for visibility
    size = 5, shape = 21,  # Filled circle
    fill = forest_plot_data_all_with_mean %>%
      pull(ResponseVariable) %>%
      unique() %>%
      purrr::map_chr(~ custom_colors[.])  # Map custom colors directly
  ) +
  # Customize labels
  labs(
    title = "Improved Forest Plot with Robust Mean Dot Colors",
    x = "Effect Size (Pseudo-Log Scale)",
    y = "Observation",
    fill = "Response Variable"
  ) +
  # Apply pseudo-log scale transformation to x-axis
  scale_x_continuous(
    trans = pseudo_log_scale, 
    breaks = c(-2, -1, 0, 1, 2), 
    labels = scales::number_format(accuracy = 0.01)
  ) +
  # Facet by response variable with synchronized strip background colors
  ggh4x::facet_wrap2(
    ~ ResponseVariable, 
    scales = "free_y", 
    ncol = 1, 
    strip = ggh4x::strip_themed(
      background_x = list(
        Biodiversity = element_rect(fill = custom_colors["Biodiversity"], color = "black"),
        `Greenhouse gas emission` = element_rect(fill = custom_colors["Greenhouse gas emission"], color = "black"),
        `Product quality` = element_rect(fill = custom_colors["Product quality"], color = "black"),
        `Crop yield` = element_rect(fill = custom_colors["Crop yield"], color = "black"),
        `Pest and Disease` = element_rect(fill = custom_colors["Pest and Disease"], color = "black"),
        `Soil quality` = element_rect(fill = custom_colors["Soil quality"], color = "black"),
        `Water quality` = element_rect(fill = custom_colors["Water quality"], color = "black")
      )
    )
  ) +
  # Add slim dotted vertical line at x = 0 for all facets
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", alpha = 0.6, size = 0.8) +
  # Enhance theme for readability
  theme_minimal(base_size = 14) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "right",
    panel.border = element_rect(color = "black", fill = NA, size = 1)  # Black border for each panel
  )

# Display the corrected plot
print(forest_plot_mean_response_scaled)
```

Saving the forest_plot_mean_response_scaled 
```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 10) + 
  theme(
    plot.title = element_text(size = 10),        # Increase title size
    axis.text = element_text(size = 10),        # Increase axis text size
    axis.title = element_text(size = 10),       # Increase axis title size
    legend.position = "top",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 10),      # Increase legend text size
    strip.text = element_text(size = 10),       # Increase facet text size
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10,
                               angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
forest_plot_mean_response_scaled <- forest_plot_mean_response_scaled + theme_custom


# Specify the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists (optional step)
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}


# Save the plots
ggsave(
  filename = file.path(output_dir, "forest_plot_mean_response_scaled.png"),
  plot = forest_plot_mean_response_scaled,
  width = 16, height = 8, dpi = 600,
  bg = "white"
)
```

Further enhanced plot
```{r}
forest_plot_data_all_with_mean |> glimpse()

# Calculate the variance and confidence intervals for overall means
mean_summary <- forest_plot_data_all_with_mean %>%
  group_by(ResponseVariable) %>%
  summarize(
    overall_mean = mean(EffectSize, na.rm = TRUE),
    mean_variance = mean(VarEffectSize, na.rm = TRUE),
    central_y = mean(range(Study)),  # Center y-coordinate
    lower_ci = overall_mean - 1.96 * sqrt(mean_variance),  # Lower confidence interval
    upper_ci = overall_mean + 1.96 * sqrt(mean_variance)   # Upper confidence interval
  )

mean_summary |> glimpse()
```

```{r}
# Updated plot with error bars for mean dots
forest_plot_mean_response_scaled_v4 <- ggplot(forest_plot_data_all_with_mean, aes(x = EffectSize, y = Study)) +
  # Points for individual observations
  geom_point(color = "gray40", size = 3, alpha = 0.8) +  
  # Confidence intervals for individual observations
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2, alpha = 0.7, color = "gray40") +
  # Add a dot for the global mean effect size per response variable, centered vertically
  geom_point(
    data = mean_summary,
    mapping = aes(x = overall_mean, y = central_y, color = ResponseVariable),
    size = 5, shape = 16, inherit.aes = FALSE
  ) +
  # Add error bars for the global mean dots
  geom_errorbarh(
    data = mean_summary,
    mapping = aes(xmin = lower_ci, xmax = upper_ci, y = central_y, color = ResponseVariable),
    height = 0.5, size = 0.8, alpha = 0.8, inherit.aes = FALSE
  ) +
  # Add slim dotted vertical line at x = 0 for all facets
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", alpha = 0.6, size = 0.8) +
  # Customize labels
  labs(
    title = "Improved Forest Plot with Centered Mean Dots and Error Bars per Response Variable",
    x = "Effect Size (Pseudo-Log Scale)",
    y = "Observation",
    color = "Response Variable"
  ) +
  # Apply pseudo-log scale transformation to x-axis
  scale_x_continuous(
    trans = pseudo_log_scale, 
    breaks = c(-2, -1, 0, 1, 2), 
    labels = scales::number_format(accuracy = 0.01)
  ) +
  # Apply custom colors to ResponseVariable for mean dots
  scale_color_manual(values = custom_colors) +
  # Facet by response variable with custom strip background colors
  facet_wrap(~ ResponseVariable, scales = "free_y", ncol = 1) +
  theme_minimal(base_size = 14) +
  # Enhance theme for readability and custom facet colors
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    strip.background = element_rect(fill = "white", color = "black"),
    legend.position = "none",
    panel.border = element_rect(color = "black", fill = NA, size = 1)
  )

# Display the updated plot
forest_plot_mean_response_scaled_v4
```
Saving the forest_plot_mean_response_scaled_v4 
```{r}
# Increase base text size and adjust all styling for the plots
theme_custom <- theme_minimal(base_size = 10) + 
  theme(
    plot.title = element_text(size = 10),        # Increase title size
    axis.text = element_text(size = 10),        # Increase axis text size
    axis.title = element_text(size = 10),       # Increase axis title size
    legend.position = "none",                    # Place legend at the top
    legend.title = element_blank(),             # Remove legend title
    legend.text = element_text(size = 10),      # Increase legend text size
    strip.text = element_text(size = 10),       # Increase facet text size
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10)
    # angle = 45, hjust = 1) # Rotate x-axis text
  )

# Apply theme modifications to each plot
forest_plot_mean_response_scaled_v4 <- forest_plot_mean_response_scaled_v4 + theme_custom


# Specify the output directory
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists (optional step)
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}


# Save the plots
ggsave(
  filename = file.path(output_dir, "forest_plot_mean_response_scaled_v4.png"),
  plot = forest_plot_mean_response_scaled_v4,
  width = 20, height = 8, dpi = 600,
  bg = "white"
)
```


#############
# STEP 6
##########################################################################################################################################
KEY VARIANCE EXPLANATION FOR EACH RESPONSE VARIABLE AND MODERATOR - MODEL FITTING 
##########################################################################################################################################

```{r}
# selected_model_results |> str()
```
```{r}
print(selected_model_results[[1]]$D_refit_incremenal_random[[1]]) 
```


```{r}
# Initialize an empty data frame to store the summary table results
summary_table <- data.frame(
  Aspect = character(),
  Moderator = character(),
  Estimate = numeric(),
  SE = numeric(),
  Z_value = numeric(),
  P_value = numeric(),
  CI_Lower = numeric(),
  CI_Upper = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each response variable to extract and summarize results
for (response in names(selected_model_results)) {
  
  # Extract the model data for each response variable
  model_data_list <- selected_model_results[[response]]$D_refit_incremenal_random
  
  # Loop through each moderator for the current response variable
  for (moderator_index in seq_along(model_data_list)) {
    
    # Extract the model fit for the current moderator
    model_data <- model_data_list[[moderator_index]]
    
    # Check if the model fitting was successful and contains necessary data
    if (!is.null(model_data) && !is.null(model_data$beta)) {
      
      # Extract the relevant statistics for each moderator (e.g., 'beta', 'se', 'zval', 'pval', etc.)
      estimates <- model_data$beta  # The effect estimates (beta coefficients)
      standard_errors <- model_data$se  # Standard errors of the estimates
      z_values <- model_data$zval  # Z-values
      p_values <- model_data$pval  # P-values
      ci_lower <- model_data$ci.lb  # Lower bound of the confidence intervals
      ci_upper <- model_data$ci.ub  # Upper bound of the confidence intervals
      
      # Check if estimates have more than 0 entries
      if (length(estimates) > 0) {
        # Loop through each effect size estimate for the current moderator
        for (i in 1:length(estimates)) {
          # Ensure the current moderator has valid data
          if (!is.na(estimates[i]) && !is.na(standard_errors[i]) && !is.na(z_values[i]) && !is.na(p_values[i])) {
            # Store each result in the summary table
            summary_table <- rbind(summary_table, data.frame(
              Aspect = response,  # Use response variable as the aspect
              Moderator = rownames(estimates)[i],  # Moderator name (from row names)
              Estimate = estimates[i],
              SE = standard_errors[i],
              Z_value = z_values[i],
              P_value = p_values[i],
              CI_Lower = ci_lower[i],
              CI_Upper = ci_upper[i]
            ))
          }
        }
      }
    }
  }
}

summary_table |> glimpse()
```

```{r}
# Format the p-values for significance with appropriate asterisks
summary_table$P_value_formatted <- ifelse(summary_table$P_value < 0.001, 
                                          paste0("< 0.001", "***"),  # Three asterisks for p < 0.001
                                          ifelse(summary_table$P_value < 0.01, 
                                                 paste0(round(summary_table$P_value, 3), "**"),  # Two asterisks for p < 0.01
                                                 ifelse(summary_table$P_value < 0.05, 
                                                        paste0(round(summary_table$P_value, 3), "*"),  # One asterisk for p < 0.05
                                                        ifelse(summary_table$P_value < 0.10, 
                                                               paste0(round(summary_table$P_value, 3), "·"),  # One dot for p < 0.10
                                                               round(summary_table$P_value, 3)))))  # No asterisks for p ≥ 0.05

# Create a new column for neatly rounded p-values without extra characters
summary_table <- summary_table %>%
  mutate(
    P_value_simple = round(P_value, 3)  # Round p-values to 3 decimals
  )

# Step 1: Format the p-values and create significance columns
summary_table <- summary_table %>%
  mutate(
    P_value_simple = round(P_value, 3),  # Rounded p-values without any special symbols
    # Create formatted p-values based on P_value
    P_value_formatted = case_when(
      P_value < 0.001 ~ paste0("< 0.001", "***"),  # Three asterisks for p < 0.001
      P_value < 0.01  ~ paste0(round(P_value, 3), "**"),  # Two asterisks for p < 0.01
      P_value < 0.05  ~ paste0(round(P_value, 3), "*"),   # One asterisk for p < 0.05
      P_value < 0.10  ~ paste0(round(P_value, 3), "·"),   # One dot for p < 0.10
      TRUE ~ paste0(round(P_value, 3))  # No asterisks for p ≥ 0.05 (ensure it's treated as a string)
    ),
    
    # Create significance level column for each response variable
    significance = case_when(
      P_value < 0.001 ~ "***",  # Three asterisks for p < 0.001
      P_value < 0.01  ~ "**",   # Two asterisks for p < 0.01
      P_value < 0.05  ~ "*",    # One asterisk for p < 0.05
      P_value < 0.10  ~ "·",   # One dot for p < 0.10
      TRUE ~ "NS"  # Non-significant for p ≥ 0.05
    )
  ) |> 
  # Convert P_value_simple = 0.000 to < 0.001
  mutate(P_value_simple = ifelse(P_value_simple == 0, "< 0.001", P_value_simple))


# Check the result to ensure columns are correctly formatted
summary_table |> glimpse()
```

```{r}
summary_table_split <- summary_table %>%
  mutate(Response_variable = Aspect) %>%  # Copy 'Aspect' to 'Response_variable' for clarity
  # Separate "Moderator" using a regular expression to handle cases like "seasonSummer"
  mutate(Moderator_type = gsub("(.*?)([A-Z].*)", "\\1", Moderator),  # Capture the first part before the capitalized word
         Moderator_type_level = gsub(".*?([A-Z].*)", "\\1", Moderator)) %>%  # Capture the second part after the first capitalized word
  # Clean the "Moderator_type_level" column by removing unnecessary prefixes (e.g., "type", "system")
  mutate(Moderator_type_level = gsub("type", "", Moderator_type_level),
         Moderator_type_level = gsub("system", "", Moderator_type_level)) %>%
  # Format 'Moderator_type' and 'Moderator_type_level' with appropriate typography
  mutate(Moderator_type = gsub("_", " ", Moderator_type),  # Replace underscores with spaces
         Moderator_type = tools::toTitleCase(Moderator_type),  # Capitalize each word
         Moderator_type_level = gsub("_", " ", Moderator_type_level),  # Replace underscores with spaces
         Moderator_type_level = tools::toTitleCase(Moderator_type_level)) %>%  # Capitalize each word
  # Modify the specific case of "Tuber,root and Other" & "Fruit,nut & Other"
  mutate(Moderator_type_level = gsub("Tuber,root and Other", "Tuber, root and other crops", Moderator_type_level),
         Moderator_type_level = gsub("Fruit,nut & Other", "Fruit, nut and other trees", Moderator_type_level)) %>%
  # Filter out rows where Moderator = "intrcpt"
  filter(Moderator != "intrcpt") %>%
  relocate(Response_variable, Moderator, Moderator_type, Moderator_type_level)  # Reorder columns for better readability


# Check the result
summary_table_split |> glimpse()
```


```{r}
# Prepare the GT table using the split Moderator structure
summary_table_gt <- summary_table_split %>%
  mutate(row_id = row_number()) %>%  # Add row_id column for alternating row colors
  gt() %>%
  tab_header(
    title = "Summary of Moderator Effects on Key Response Variables"
  ) %>%
  fmt_number(
    columns = c("Estimate", "P_value_simple"),
    decimals = 3
  ) %>%
  tab_spanner(
    label = "Statistics",
    columns = c("Estimate", "P_value_simple", "significance")
  ) %>%
  # Apply bold for significant values in the 'significance' column
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = "significance",
      rows = significance %in% c("***", "**", "*")  # Bold for significant results
    )
  ) %>%
  # Apply normal weight for non-significant results in the 'significance' column
  tab_style(
    style = cell_text(weight = "normal"),
    locations = cells_body(
      columns = "significance",
      rows = significance == "NS"  # Normal weight for non-significant results
    )
  ) %>%
  # Apply alternating row colors for readability (odd/even rows)
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(color = "black")
    ),
    locations = cells_body(
      columns = everything(),
      rows = row_id %% 2 != 0  # Odd rows
    )
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "#f2f2f2"),  # Light gray for alternating rows
      cell_text(color = "black")
    ),
    locations = cells_body(
      columns = everything(),
      rows = row_id %% 2 == 0  # Even rows
    )
  ) %>%
  # Add column spacing for better readability
  tab_options(
    table.width = pct(100),  # Set width to 100% of the container
    column_labels.font.size = 12,  # Adjust font size of column labels
    row.striping.include_table_body = TRUE,  # Enable row striping in the table body
    data_row.padding = px(5)  # Padding between data rows for readability
  ) %>%
  # Remove row_id column for the final table display
  cols_hide(columns = "row_id") |> 
  cols_hide(columns = "Moderator") |> 
  cols_hide(columns = "Aspect") |> 
  cols_hide(columns = "P_value_formatted") |> 
  cols_hide(columns = "P_value") 

# Print the GT table without the row_id column
summary_table_gt
```


```{r}
summary_table_selected_condenced <- summary_table_split |> 
  select(Response_variable, Moderator_type, Moderator_type_level, Estimate, P_value_simple, significance)

summary_table_selected_condenced |> glimpse() 
```

```{r}
selected_model_results$Biodiversity$D_refit_incremenal_random[[3]]$k.eff
```


Study level data to add to the summary table

```{r}
################################################################################
# Number of observations for each 

# Define response variables and moderators
response_vars <- c("Biodiversity", "Greenhouse gas emission", "Product quality", "Crop yield", "Pest and Disease", "Soil quality", "Water quality")
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Initialize an empty data frame to store the summary of number of observations
study_summary <- data.frame(
  Response_variable = character(),
  Moderator = character(),
  Num_observations = integer(),
  stringsAsFactors = FALSE
)

# Loop through each response variable
for (response in response_vars) {
  
  # Extract the model data for each response variable
  model_data_list <- selected_model_results[[response]]$D_refit_incremenal_random
  
  # Loop through each moderator for the current response variable
  for (moderator_index in seq_along(moderators)) {
    
    # Check if the current moderator exists in the model data
    moderator_name <- moderators[moderator_index]
    moderator_data <- model_data_list[[moderator_index]]
    
    # Check if the moderator data contains the necessary field (k.eff) and is not null or empty
    if (!is.null(moderator_data$k.eff)) {
      num_observations <- moderator_data$k.eff  # Extract the number of observations (k.eff)
      
      # Append the result to the summary table
      study_summary <- rbind(study_summary, data.frame(
        Response_variable = response,
        Moderator = moderator_name,
        Num_observations = num_observations
      ))
    } else {
      # If k.eff is missing, print a debug message
      cat("Missing data for:", response, "Moderator:", moderator_name, "\n")
    }
  }
}

# Check the resulting summary of observations
study_summary |> glimpse()
```


```{r}
# meta_data |> glimpse()

# Define response variables and moderators
response_vars <- c("Biodiversity", "Greenhouse gas emission", "Product quality", "Crop yield", "Pest and Disease", "Soil quality", "Water quality")
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Initialize an empty list to store the summary data
study_obs_summary <- data.frame()

# Loop through each response variable
for (response in unique(meta_data$response_variable)) {
  # Loop through each moderator
  for (moderator in moderators) {
    # Filter data for the specific response and moderator
    filtered_data <- meta_data %>%
      filter(response_variable == response, !is.na(get(moderator)))
    
    # Calculate the number of unique observations and studies
    num_observations <- filtered_data %>%
      select(id_obs) %>%
      distinct() %>%
      nrow()
    
    num_studies <- filtered_data %>%
      select(id_article) %>%
      distinct() %>%
      nrow()
    
    # Calculate the average yi and vi for the specific combination
    avg_yi <- filtered_data %>%
      summarise(mean_yi = mean(yi, na.rm = TRUE)) %>%
      pull(mean_yi)
    
    avg_vi <- filtered_data %>%
      summarise(mean_vi = mean(vi, na.rm = TRUE)) %>%
      pull(mean_vi)
    
    # Get the list of unique studies (id_article) for this combination
    unique_studies <- filtered_data %>%
      select(id_article) %>%
      distinct() %>%
      pull(id_article)
    
    # Store the result in the summary data frame
    study_obs_summary <- rbind(study_obs_summary, data.frame(
      Response_variable = response,
      Moderator = moderator,
      Num_observations = num_observations,
      Num_studies = num_studies,
      Avg_yi = avg_yi,
      Avg_vi = avg_vi,
      Unique_studies = I(list(unique_studies))  # Storing the list of unique studies
    ))
  }
}

# Standardize column names in dataframe

# In study_obs_summary, rename 'Moderator' to 'Moderator_type' 
# and 'Unique_studies' column to be more descriptive
study_obs_summary <- study_obs_summary %>%
  rename(
    Moderator_type = Moderator,
    n_obs = Num_observations,
    n_studies = Num_studies
  ) |> 
  # Modify the specific case of "Tuber,root and Other" & "Fruit,nut & Other"
  mutate(Moderator_type = gsub("tree_type", "Tree Type", Moderator_type),
         Moderator_type = gsub("crop_type", "Crop Type", Moderator_type),
         Moderator_type = gsub("age_system", "Age System", Moderator_type),
         Moderator_type = gsub("season", "Season", Moderator_type),
         Moderator_type = gsub("soil_texture", "Soil Texture", Moderator_type)
  )


# Display the summary with additional information
study_obs_summary |> glimpse()

# study_obs_summary$Unique_studies
```



Merging with the model summary statistics

```{r}
# summary_table_selected_condenced |> glimpse() 


# Perform the left join
merged_summary <- study_obs_summary %>%
  full_join(
    summary_table_selected_condenced,
    by = c("Response_variable", "Moderator_type")
  ) |> 
  # Reorder columns for better readability
  relocate(Response_variable, Moderator_type, Moderator_type_level, n_studies, n_obs, Estimate, P_value_simple, significance, Avg_yi, Avg_vi, Unique_studies) 

# View the merged dataset
merged_summary |>  glimpse()
```

```{r}
# Prepare the GT table from merged_summary
merged_summary_gt <- merged_summary %>%
  mutate(row_id = row_number()) %>%  # Add row_id column for alternating row colors
  gt() %>%
  tab_header(
    title = "Summary of Moderator Effects on Key Response Variables"
  ) %>%
  fmt_number(
    columns = c("Estimate", "Avg_yi", "Avg_vi"),
    decimals = 3
  ) %>%
  fmt_number(
    columns = "P_value_simple",
    decimals = 3
  ) %>%
  tab_spanner(
    label = "Statistics",
    columns = c("Estimate", "P_value_simple", "significance", "Avg_yi", "Avg_vi")
  ) %>%
  cols_label(
    Response_variable = "Response Variable",
    Moderator_type = "Moderator Type",
    Moderator_type_level = "Moderator Level",
    n_studies = "n studies",
    n_obs = "n observations",
    Estimate = "Estimate",
    P_value_simple = "P-Value",
    significance = "Significance",
    Avg_yi = "Avg. Effect Size (yi)",
    Avg_vi = "Avg. Variance (vi)"
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = "significance",
      rows = significance %in% c("***", "**", "*")  # Bold for significant results
    )
  ) %>%
  tab_style(
    style = cell_text(weight = "normal"),
    locations = cells_body(
      columns = "significance",
      rows = significance == "NS"  # Normal weight for non-significant results
    )
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(color = "black")
    ),
    locations = cells_body(
      rows = row_id %% 2 != 0  # Odd rows
    )
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "#f2f2f2"),  # Light gray for alternating rows
      cell_text(color = "black")
    ),
    locations = cells_body(
      rows = row_id %% 2 == 0  # Even rows
    )
  ) %>%
  tab_options(
    table.width = pct(100),  # Full-width table
    column_labels.font.size = 12,  # Font size for column labels
    data_row.padding = px(5)  # Padding for rows
  ) %>%
  cols_hide(columns = "row_id") %>%  # Hide row_id column
  cols_hide(columns = "Avg_yi") |>   # Hide Avg_yi column
  cols_hide(columns = "Avg_vi")      # Hide Avg_vi column

# Print the table
merged_summary_gt
```




























```{r}
# Step 2: Select the relevant columns and pivot the table
summary_table_condensed <- summary_table_split %>%
  select(Response_variable, Moderator_type, Moderator_type_level, Estimate, P_value_simple, significance)  # Ensure 'significance' is included

# Pivot the table with response variables as columns
summary_table_pivoted <- summary_table_condensed %>%
  pivot_wider(names_from = Moderator_type_level, values_from = c(Estimate, P_value_simple, significance))

# Step 3: Rename the columns for clarity
summary_table_pivoted <- summary_table_pivoted %>%
  rename_with(~ gsub("Estimate_", "Effect Estimate: ", .), starts_with("Estimate")) %>%
  rename_with(~ gsub("P_value_simple_", "P-value (simple): ", .), starts_with("P_value_simple")) %>%
  rename_with(~ gsub("significance_", "Significance: ", .), starts_with("significance"))

summary_table_pivoted |> glimpse()
```























non_imp_dataset 
imp_dataset 



Performing a Leave-One-Out (LOO) sensitivity analysis on the full datasets and full model can be justified. This analysis would complement the LOO performed on each individual meta-regression model and provide additional insights. Here’s a detailed explanation:
  
  Justification for LOO Sensitivity Analysis on Full Model
Assess Overall Model Robustness:
  
  Applying LOO sensitivity analysis to the full model allows you to evaluate the robustness of the overall meta-analytic findings. It helps identify if specific studies have a disproportionate influence on the aggregated results.
This is particularly useful when you have concerns about the quality or heterogeneity of the included studies. If the removal of a single study drastically changes the overall effect size or model fit, this study might need further investigation.
Comparison Between Datasets (Imputed vs Non-Imputed):
  
  Conducting LOO on both non_imp_dataset and imp_dataset allows you to compare the stability of the results between datasets with and without imputation.
It helps validate the imputation process by checking whether imputed data introduces or mitigates influential observations.
Insights Across All Moderators:
  
  The LOO analysis on individual meta-regression models examines the impact of leaving out observations within each moderator level. However, performing LOO on the full model assesses the overall influence of each study across all moderators combined.
This provides a broader view of the dataset and highlights influential studies that might not be detected when focusing only on specific moderators.
Pros and Cons of LOO on the Full Model
Pros:
  Comprehensive Sensitivity Check: It provides a complete picture of the influence of each study across the entire dataset.
Validation of Imputed Data: It allows you to test the robustness of results obtained from imputed datasets, helping ensure that imputation did not introduce bias.
Identification of Influential Studies: It can detect influential studies that have a large impact on the overall meta-analytic effect size or variance components.
Cons:
  Computational Intensity: The LOO analysis on the full model can be computationally expensive, especially with large datasets and complex random-effects structures.
Potential Overlap: If individual meta-regression models are already analyzed with LOO, the full-model LOO might yield redundant information. However, this risk is mitigated by focusing on the broader dataset perspective.
Complex Interpretation: If a study is found to be influential in the full model LOO but not in specific meta-regression models, interpretation can be challenging. It may indicate complex interactions or issues with the overall model specification.
Implementation Plan
You can implement the LOO sensitivity analysis on the full model using both imputed and non-imputed datasets. Here’s how to proceed:
  
  Fit the Full Model on each dataset (non_imp_dataset, imp_dataset, non_imp_dataset_imputed, imp_dataset_imputed).
Run LOO Sensitivity Analysis using a function that iteratively removes one study at a time and refits the model.
Summarize the Results: Report changes in the effect size, heterogeneity statistics (I²), and model fit metrics (AIC, BIC, LogLik).

Why Split by Response Variable?
  Distinct Moderation Effects: Response variables (e.g., ecosystem services like greenhouse gas emissions, soil quality, etc.) likely differ in how moderators (e.g., tree type, crop type, season) influence them. Splitting allows the model to isolate the unique relationships within each response variable.

Interpretability: Without splitting, estimates may conflate effects across distinct response variables, making it difficult to attribute results to a specific ecosystem service.

Appropriate LOO Sensitivity: Running LOO sensitivity on mixed-response datasets can be misleading, as the omitted studies might affect multiple response variables differently, introducing biases or artifacts.

Fits the Cabbage Approach: This methodology inherently groups analyses by discrete categories, and splitting by response variable mirrors that logic.

```{r}

```


post_process_moderators <- function(summary) {
  # Separate higher-level and lower-level moderators
  higher_level <- summary %>%
    filter(!grepl("[a-zA-Z]+[0-9]+", Moderator))
  
  lower_level <- summary %>%
    filter(grepl("[a-zA-Z]+[0-9]+", Moderator))
  
  # Calculate proportion of heterogeneity explained by higher-level moderators
  heterogeneity_explained <- higher_level %>%
    group_by(Moderator) %>%
    summarise(
      ProportionExplained = mean(Estimate^2 / SE^2, na.rm = TRUE),
      MeanPValue = mean(Pval, na.rm = TRUE)
    )
  
  list(
    HigherLevel = higher_level,
    # LowerLevel = lower_level,
    HeterogeneityExplained = heterogeneity_explained
  )
}

# Post-process results
post_processed_results <- post_process_moderators(moderator_influence_summary)


```{r}
# Custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Function to plot individual funnel plots for each response variable
plot_individual_funnel <- function(forest_plot_data) {
  # Loop through each response variable and create a plot
  lapply(names(forest_plot_data), function(response) {
    data <- forest_plot_data[[response]]
    
    # Check if data is valid
    if (is.null(data) || nrow(data) == 0) {
      message(paste("No valid data for funnel plot for", response))
      return(NULL)
    }
    
    # Calculate precision (1 / sqrt(Variance))
    data$Precision <- 1 / sqrt(data$Variance)
    
    # Calculate overall effect size for reference line
    overall_effect <- mean(data$EffectSize, na.rm = TRUE)
    
    # Create the funnel plot
    plot <- ggplot(data, aes(x = EffectSize, y = Precision)) +
      geom_point(size = 3, alpha = 0.8, color = custom_colors[response]) +
      geom_vline(xintercept = overall_effect, linetype = "dashed", color = "red", size = 1) +
      geom_smooth(method = "loess", se = FALSE, linetype = "dotted", size = 0.8, color = "black") +
      labs(
        title = paste("Funnel Plot for", response),
        x = "Effect Size",
        y = "Precision (1 / SE)"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 12),
        legend.position = "none",
        panel.grid = element_blank()
      )
    
    # Save the plot as a separate file for each response variable
    ggsave(filename = paste0("funnel_plot_", gsub(" ", "_", response), ".png"), plot = plot, width = 8, height = 6)
    
    # Print the plot to the console
    print(plot)
  })
}

# Example usage
plot_individual_funnel(forest_plot_data)

```


# Modernized Funnel Plot Visualization

# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Modernized Funnel Plot Function
visualize_funnel_plots <- function(model_results) {
  funnel_plots <- lapply(names(model_results), function(response) {
    model <- model_results[[response]]$full_model
    if (is.null(model)) return(NULL)
    
    # Generate funnel plot data
    funnel_data <- tryCatch({
      funnel(model, yaxis = "sei", plot = FALSE)
    }, error = function(e) {
      message(paste("Error generating funnel plot for", response, ":", e$message))
      return(NULL)
    })
    
    if (is.null(funnel_data) || is.null(funnel_data$sei) || length(funnel_data$sei) == 0) {
      message(paste("No valid data for funnel plot for", response))
      return(NULL)
    }
    
    funnel_df <- data.frame(
      EffectSize = funnel_data$yi,
      SE = funnel_data$sei,
      ResponseVariable = response
    )
    
    # Calculate overall effect size for reference line
    overall_effect <- mean(funnel_df$EffectSize, na.rm = TRUE)
    
    # Create the enhanced funnel plot
    ggplot(funnel_df, aes(x = EffectSize, y = 1 / SE, color = ResponseVariable)) +
      geom_point(size = 3, alpha = 0.8) +
      geom_vline(xintercept = overall_effect, linetype = "dashed", color = "red", size = 1) +
      geom_smooth(method = "loess", se = FALSE, linetype = "dotted", size = 0.8, color = "black") +
      scale_color_manual(values = custom_colors) +
      labs(
        title = paste("Funnel Plot for", response),
        x = "Effect Size",
        y = "Precision (1 / SE)",
        color = "Response Variable"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 12),
        legend.position = "bottom",
        panel.grid = element_blank()
      )
  })
  
  # Return the list of plots
  funnel_plots
}

# Generate Funnel Plots
funnel_plots <- visualize_funnel_plots(model_results)

# Display Funnel Plots
for (plot in funnel_plots) {
  if (!is.null(plot)) print(plot)
}

```


```{r}
##########################################################################################################################################
# MODERATOR ANALYSIS - INFLUENCE OF SILVOARABLE AGROFORESTRY CHARACTERISTICS (MODERATORS)
##########################################################################################################################################


# Define a function to extract and summarize moderator model results for each response variable
analyze_moderator_influence <- function(model_results) {
  # Extract response variables from the results
  response_variables <- names(model_results)
  
  # Iterate over each response variable to analyze moderators
  moderator_summaries <- lapply(response_variables, function(response) {
    cat("Analyzing response variable:", response, "\n")
    
    # Extract moderator models for the current response
    moderator_models <- model_results[[response]]$moderator_models
    
    # Summarize the moderator influence
    summaries <- lapply(names(moderator_models), function(moderator) {
      cat("  Moderator:", moderator, "\n")
      
      # Extract the model for the current moderator
      mod_model <- moderator_models[[moderator]]
      
      # Extract coefficients, standard errors, and p-values
      coef_summary <- data.frame(
        Term = rownames(mod_model$b),
        Estimate = mod_model$b[, 1],
        SE = mod_model$se,
        Zval = mod_model$zval,
        Pval = mod_model$pval,
        CI.Lower = mod_model$ci.lb,
        CI.Upper = mod_model$ci.ub
      )
      
      return(coef_summary)
    })
    
    # Combine summaries for all moderators into a single data frame
    combined_summary <- do.call(rbind, summaries)
    combined_summary$Moderator <- rep(names(moderator_models), sapply(summaries, nrow))
    combined_summary$ResponseVariable <- response
    
    return(combined_summary)
  })
  
  # Combine summaries for all response variables into a single data frame
  full_summary <- do.call(rbind, moderator_summaries)
  return(full_summary)
}

# Call the function and store the results
moderator_influence_summary <- analyze_moderator_influence(model_results)

# Post-process the summarized influence of moderators
# Update the post-process function to calculate heterogeneity explained for each response variable
post_process_moderators_by_response <- function(summary) {
  # Separate higher-level and lower-level moderators
  higher_level <- summary %>%
    filter(!grepl("[a-zA-Z]+[0-9]+", Moderator))
  
  # Calculate proportion of heterogeneity explained by higher-level moderators, grouped by ResponseVariable
  heterogeneity_explained <- higher_level %>%
    group_by(ResponseVariable, Moderator) %>%
    summarise(
      ProportionExplained = mean(Estimate^2 / SE^2, na.rm = TRUE),
      MeanPValue = mean(Pval, na.rm = TRUE),
      .groups = "drop"
    )
  
  list(
    HigherLevel = higher_level,
    HeterogeneityExplainedByResponse = heterogeneity_explained
  )
}

# Call the updated function
post_processed_results_by_response <- post_process_moderators_by_response(moderator_influence_summary)

# View the proportion of heterogeneity explained by response variable and moderator
post_processed_results_by_response$HeterogeneityExplainedByResponse


post_processed_results_higher_level <- post_processed_results_by_response$HigherLevel |> relocate(
  ResponseVariable,
  Moderator,
  Term,
  Estimate,
  SE,
  Pval,
  Zval,
  CI.Lower,
  CI.Lower
)

# Define the file path for saving
output_file <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "post_processed_results_higher_level.xlsx")
# Save the dataframe to an Excel file
write.xlsx(post_processed_results_higher_level, file = output_file, row.names = FALSE)
cat("The post_processed_results_higher_level data has been saved to:", output_file, "\n")


# View summaries
post_processed_results_by_response$HeterogeneityExplained


post_processed_results_higher_level |> glimpse()

post_processed_results_by_response |> glimpse()
```




# Function to calculate proportion of heterogeneity explained using tau^2
calculate_heterogeneity_explained <- function(model_results) {
  results <- list()
  
  for (response in names(model_results)) {
    mod_models <- model_results[[response]]$moderator_models
    full_model <- model_results[[response]]$full_model
    
    if (is.null(mod_models) || is.null(full_model)) next
    
    # Extract tau^2 from the full model (without moderators)
    tau2_null <- full_model$tau2
    
    for (moderator in names(mod_models)) {
      mod_model <- mod_models[[moderator]]
      
      # Extract tau^2 from the model with the moderator
      tau2_mod <- mod_model$tau2
      
      # Calculate proportion of heterogeneity explained
      proportion_explained <- (tau2_null - tau2_mod) / tau2_null * 100
      
      # Ensure the proportion does not exceed 100%
      proportion_explained <- min(proportion_explained, 100)
      
      # Store results
      results[[length(results) + 1]] <- data.frame(
        ResponseVariable = response,
        Moderator = moderator,
        Tau2_Null = tau2_null,
        Tau2_Moderated = tau2_mod,
        ProportionExplained = proportion_explained
      )
    }
  }
  
  return(do.call(rbind, results))
}

# Run the corrected heterogeneity calculation
heterogeneity_results <- calculate_heterogeneity_explained(model_results)

# Print results
print(heterogeneity_results)


# Define the file path for saving
# output_file <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R", "post_processed_results_higher_level.xlsx")
# Save the dataframe to an Excel file
# write.xlsx(post_processed_results_higher_level, file = output_file, row.names = FALSE)
# cat("The post_processed_results_higher_level data has been saved to:", output_file, "\n")


# View summaries
post_processed_results_by_response$HeterogeneityExplained

post_processed_results_higher_level |> glimpse()
post_processed_results_by_response |> glimpse()

post_processed_results_by_response


```{r}
##########################################################################################################################################
# MODERATOR ANALYSIS - INFLUENCE OF SILVOARABLE AGROFORESTRY CHARACTERISTICS (MODERATORS)
##########################################################################################################################################

# Define a function to extract and summarize moderator model results for each response variable
analyze_moderator_influence <- function(model_results) {
  # Extract response variables from the results
  response_variables <- names(model_results)
  
  # Iterate over each response variable to analyze moderators
  moderator_summaries <- lapply(response_variables, function(response) {
    cat("Analyzing response variable:", response, "\n")
    
    # Extract moderator models for the current response
    moderator_models <- model_results[[response]]$moderator_models
    
    # Summarize the moderator influence
    summaries <- lapply(names(moderator_models), function(moderator) {
      cat("  Moderator:", moderator, "\n")
      
      # Extract the model for the current moderator
      mod_model <- moderator_models[[moderator]]
      
      # Extract coefficients, standard errors, and p-values
      coef_summary <- data.frame(
        Term = rownames(mod_model$b),
        Estimate = mod_model$b[, 1],
        SE = mod_model$se,
        Zval = mod_model$zval,
        Pval = mod_model$pval,
        CI.Lower = mod_model$ci.lb,
        CI.Upper = mod_model$ci.ub
      )
      
      return(coef_summary)
    })
    
    # Combine summaries for all moderators into a single data frame
    combined_summary <- do.call(rbind, summaries)
    combined_summary$Moderator <- rep(names(moderator_models), sapply(summaries, nrow))
    combined_summary$ResponseVariable <- response
    
    return(combined_summary)
  })
  
  # Combine summaries for all response variables into a single data frame
  full_summary <- do.call(rbind, moderator_summaries)
  return(full_summary)
}

# Call the function and store the results
moderator_influence_summary <- analyze_moderator_influence(model_results)

# Post-process the summarized influence of moderators
# Update the post-process function to calculate heterogeneity explained for each response variable
post_process_moderators_by_response <- function(summary) {
  # Separate higher-level and lower-level moderators
  higher_level <- summary %>%
    filter(!grepl("[a-zA-Z]+[0-9]+", Moderator))
  
  # Calculate proportion of heterogeneity explained by higher-level moderators, grouped by ResponseVariable
  heterogeneity_explained <- higher_level %>%
    group_by(ResponseVariable, Moderator) %>%
    summarise(
      ProportionExplained = mean(Estimate^2 / SE^2, na.rm = TRUE),
      MeanPValue = mean(Pval, na.rm = TRUE),
      .groups = "drop"
    )
  
  list(
    HigherLevel = higher_level,
    HeterogeneityExplainedByResponse = heterogeneity_explained
  )
}

# Call the updated function
post_processed_results_by_response <- post_process_moderators_by_response(moderator_influence_summary)

# View the proportion of heterogeneity explained by response variable and moderator
post_processed_results_by_response$HeterogeneityExplainedByResponse


post_processed_results_higher_level <- post_processed_results_by_response$HigherLevel |> relocate(
  ResponseVariable,
  Moderator,
  Term,
  Estimate,
  SE,
  Pval,
  Zval,
  CI.Lower,
  CI.Lower
)


# View summaries
post_processed_results_by_response$HeterogeneityExplained
post_processed_results_higher_level |> glimpse()
post_processed_results_by_response |> glimpse()

post_processed_results_by_response
```



####################################################################################################
# DEBUGGING: EXPLAINED HETEROGENEITY ANALYSIS
####################################################################################################


# Function to extract τ² values from fitted models and calculate explained heterogeneity
calculate_explained_heterogeneity <- function(model_results) {
  
  heterogeneity_results <- list()  # Store results
  
  for (response in names(model_results)) {
    # Extract the full (null) model (without moderators)
    full_model <- model_results[[response]]$full_model
    
    # Skip if the full model does not exist
    if (is.null(full_model)) {
      message(paste("Skipping", response, "as full model is missing."))
      next
    }
    
    # Extract τ² from the full model (null model without moderators)
    tau2_null <- if ("tau2" %in% names(full_model)) full_model$tau2 else NA
    if (length(tau2_null) > 1) tau2_null <- sum(tau2_null)  # Handle cases where tau2 is a vector
    
    # Print τ² to debug if it's being extracted correctly
    cat("\nResponse Variable:", response, " | τ² (Null Model):", tau2_null, "\n")
    
    # Extract moderator models for this response variable
    moderator_models <- model_results[[response]]$moderator_models
    
    if (is.null(moderator_models)) {
      message(paste("No moderator models found for", response))
      next
    }
    
    # Store results for each moderator
    moderator_results <- lapply(names(moderator_models), function(moderator) {
      mod_model <- moderator_models[[moderator]]  # Get the model for this moderator
      
      # Extract τ² for the moderated model
      tau2_moderated <- if ("tau2" %in% names(mod_model)) mod_model$tau2 else NA
      if (length(tau2_moderated) > 1) tau2_moderated <- sum(tau2_moderated)  # Handle vector τ²
      
      # Print τ² to debug
      cat("  Moderator:", moderator, "| τ² (Moderated Model):", tau2_moderated, "\n")
      
      # Calculate the proportion of explained heterogeneity
      proportion_explained <- ifelse(!is.na(tau2_null) && tau2_null > 0, 
                                     ((tau2_null - tau2_moderated) / tau2_null) * 100, 
                                     NA)  # Prevent division by zero
      
      # Ensure proportions are not negative
      proportion_explained <- max(proportion_explained, 0, na.rm = TRUE)
      
      # Return as a data frame
      return(data.frame(
        ResponseVariable = response,
        Moderator = moderator,
        Tau2_Null = tau2_null,
        Tau2_Moderated = tau2_moderated,
        ProportionExplained = proportion_explained
      ))
    })
    
    # Combine results for all moderators of this response variable
    heterogeneity_results[[response]] <- do.call(rbind, moderator_results)
  }
  
  # Convert results list into a single data frame
  results_df <- do.call(rbind, heterogeneity_results)
  
  # Print debugging output
  print(results_df)
  
  return(results_df)
}

# Run the function to debug τ² extraction
heterogeneity_results <- calculate_explained_heterogeneity(model_results)




####################################################################################################
# FUNCTION TO FIT INCREMENTAL MODELS AND ASSESS MODERATOR EFFECT ON HETEROGENEITY
####################################################################################################


# Function to fit models and assess heterogeneity explained by moderators
fit_and_evaluate_moderators <- function(data, response_variable, v_matrix, moderators, random_effects) {
  results <- list()
  
  cat("\nProcessing response variable:", response_variable, "\n")
  
  #############################################################################################
  # Null model: Global average without moderators (Baseline τ²)
  results$null_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      random = random_effects,
      data = data,
      method = "REML",
      control = list(iter.max = 2000, rel.tol = 1e-9)
    )
  }, error = function(e) {
    cat("Error in null model:", e$message, "\n")
    return(NULL)
  })
  
  if (is.null(results$null_model)) return(NULL)
  
  # Extract τ² from the null model
  tau2_null <- sum(results$null_model$tau2)  # Handles cases where tau2 is a vector
  
  # Print τ² for debugging
  cat("τ² (Null Model):", tau2_null, "\n")
  
  #############################################################################################
  # Fit separate models for each moderator
  moderator_results <- map(moderators, function(moderator) {
    cat("Fitting model for moderator:", moderator, "\n")
    
    mod_model <- tryCatch({
      rma.mv(
        yi = yi,
        V = v_matrix,
        mods = as.formula(paste("~", moderator)),
        random = random_effects,
        data = data,
        method = "REML",
        control = list(iter.max = 2000, rel.tol = 1e-9)
      )
    }, error = function(e) {
      cat("Error in moderator model for", moderator, ":", e$message, "\n")
      return(NULL)
    })
    
    if (is.null(mod_model)) return(NULL)
    
    # Extract τ² from the moderated model
    tau2_moderated <- sum(mod_model$tau2)
    
    # Print τ² for debugging
    cat("  τ² (Moderated Model -", moderator, "):", tau2_moderated, "\n")
    
    # Calculate explained heterogeneity
    proportion_explained <- ifelse(tau2_null > 0, ((tau2_null - tau2_moderated) / tau2_null) * 100, NA)
    
    # Ensure proportion is between 0 and 100
    proportion_explained <- max(min(proportion_explained, 100), 0, na.rm = TRUE)
    
    return(data.frame(
      ResponseVariable = response_variable,
      Moderator = moderator,
      Tau2_Null = tau2_null,
      Tau2_Moderated = tau2_moderated,
      ProportionExplained = proportion_explained
    ))
  })
  
  # Combine results into a dataframe
  results_df <- do.call(rbind, moderator_results)
  
  return(results_df)
}

##########################################################################
# Fit Models and Calculate Heterogeneity Explained for Each Response Variable
##########################################################################

# Load the saved v_matrices
dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(dir, "v_matrices_by_response_variable.rds"))


# Initialize an empty list to store model results
heterogeneity_results <- list()

# Loop through each response variable to fit models and assess heterogeneity
for (response in names(v_matrices)) {
  cat("\n-------------------------\nAnalyzing:", response, "\n-------------------------\n")
  
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Extract the variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]
  
  # Define the moderators to include in the model
  moderators <- c("tree_type", "crop_type") # , "age_system", "season", "soil_texture"
  
  # Define random effects structure
  random_effects <- ~ 1 | exp_id
  
  # Fit models and assess heterogeneity explained
  heterogeneity_results[[response]] <- fit_and_evaluate_moderators(
    data = data_subset,
    response_variable = response,
    v_matrix = v_matrix,
    moderators = moderators,
    random_effects = random_effects
  )
}

# Convert list to dataframe
heterogeneity_results_df <- do.call(rbind, heterogeneity_results)

# Print and return results
print(heterogeneity_results_df)
heterogeneity_results_df


```{r}
for (response in names(model_results)) {
  full_model <- model_results[[response]]$full_model
  if (!is.null(full_model)) {
    cat("Response:", response, " - Full model tau2 =", full_model$tau2, "\n")
  }
  
  moderator_models <- model_results[[response]]$moderator_models
  if (!is.null(moderator_models)) {
    for (moderator in names(moderator_models)) {
      mod_model <- moderator_models[[moderator]]
      cat("  Moderator:", moderator, " - tau2 =", mod_model$tau2, "\n")
    }
  }
}
```
# Calculate the Proportion of Explained Heterogeneity

if (!is.null(null_model) && !is.null(full_model)) {
  proportion_explained <- ((tau2_null - tau2_full) / tau2_null) * 100
  proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure within 0-100%
  
  cat("\nProportion of Explained Heterogeneity:", proportion_explained, "%\n")
} else {
  cat("\nCannot compute explained heterogeneity due to missing models.\n")
}



```{r}
# Define the moderators
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Initialize a data frame to store results
heterogeneity_results <- data.frame()

# Fit the Null Model (NO RANDOM EFFECTS)
null_model <- tryCatch({
  rma(
    yi = yi,
    vi = diag(v_matrix),  # Use diagonal for within-study variance
    data = data_subset,
    method = "REML",
    control = list(iter.max = 2000, rel.tol = 1e-9)
  )
}, error = function(e) {
  message("Error in null model: ", e$message)
  return(NULL)
})

# Extract tau² from the null model
if (!is.null(null_model)) {
  tau2_null <- null_model$tau2
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
  tau2_null <- NA
}

# Fit separate models for each moderator
for (moderator in moderators) {
  cat("\nFitting model for moderator:", moderator, "\n")
  
  mod_model <- tryCatch({
    rma(
      yi = yi,
      vi = diag(v_matrix),
      mods = as.formula(paste("~", moderator)),  # Single moderator model
      data = data_subset,
      method = "REML",
      control = list(iter.max = 2000, rel.tol = 1e-9)
    )
  }, error = function(e) {
    message("Error in model for", moderator, ":", e$message)
    return(NULL)
  })
  
  # Extract tau² from the moderator model
  if (!is.null(mod_model)) {
    tau2_moderated <- mod_model$tau2
    cat("  τ² (Model with", moderator, "):", tau2_moderated, "\n")
    
    # Calculate proportion of explained heterogeneity
    proportion_explained <- ifelse(tau2_null > 0, ((tau2_null - tau2_moderated) / tau2_null) * 100, NA)
    proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure between 0-100%
    
    # Store results
    heterogeneity_results <- rbind(heterogeneity_results, 
                                   data.frame(ResponseVariable = response_variable,
                                              Moderator = moderator,
                                              Tau2_Null = tau2_null,
                                              Tau2_Moderated = tau2_moderated,
                                              ProportionExplained = proportion_explained))
  } else {
    cat("⚠ Warning: Model for", moderator, "failed.\n")
  }
}

# Print results
heterogeneity_results
```





```{r}
# Visualize Proportion Explained for Overall and Response Variable-Specific Situations

# Function to visualize ProportionExplained
visualize_proportion_explained <- function(post_processed_results) {
  # Extract data for overall situation
  overall_data <- post_processed_results$HeterogeneityExplained
  
  # Extract data for response variable-specific situation
  response_specific_data <- post_processed_results$HigherLevel %>%
    group_by(ResponseVariable, Moderator) %>%
    summarise(
      ProportionExplained = mean(Estimate^2 / SE^2, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Plot overall situation
  overall_plot <- ggplot(overall_data, aes(x = reorder(Moderator, ProportionExplained), y = ProportionExplained)) +
    geom_bar(stat = "identity", fill = "skyblue") +
    # scale_y_continuous(breaks = c(0, 100, 200, 300, 400), limits = c(0, 450)) +
    coord_flip() +
    labs(
      title = "Proportion of Explained Heterogeneity (Overall)",
      x = "Moderator",
      y = "Proportion Explained (%)"
    ) +
    theme_minimal()
  
  # Plot response variable-specific situation
  response_plot <- ggplot(response_specific_data, aes(x = reorder(Moderator, ProportionExplained), y = ProportionExplained, fill = ResponseVariable)) +
    geom_bar(stat = "identity", position = "dodge") +
    ggbreak::scale_y_break(c(15, 300), scales = 0.5) +
    ggbreak::scale_y_break(c(400, 450), scales = 0.5) + # Add two breaks for the y-axis
    coord_flip() +
    labs(
      title = "Proportion of Explained Heterogeneity (By Response Variable)",
      x = "Moderator",
      y = "Proportion Explained (%)",
      fill = "Response Variable"
    ) +
    theme_minimal()
  
  # Return both plots
  list(
    OverallPlot = overall_plot,
    ResponseSpecificPlot = response_plot
  )
}

# Generate plots
plots <- visualize_proportion_explained(post_processed_results_by_response)

# Print plots
print(plots$OverallPlot)
print(plots$ResponseSpecificPlot)
```


Updated Interpretation of Moderators' Effects on Model Heterogeneity

1. Significance of Moderators (P-value Interpretation):
- **P-values** reflect whether the moderator has a statistically significant effect on the response variable. 
  - A **P-value < 0.05** suggests strong evidence that the moderator significantly affects the heterogeneity in effect sizes.
  - Higher P-values indicate weaker evidence for significant effects, meaning the moderator’s influence may not be consistent across the response variables.

2. Magnitude of Effects (Estimate and SE):
- The **Estimate** represents the effect size or influence of each moderator term.
  - Positive values suggest that the moderator increases heterogeneity or variability.
  - Negative values suggest it reduces variability or explains heterogeneity in a systematic way.
- The **Standard Error (SE)** indicates the precision of the estimate.
  - Lower SE values imply higher confidence in the estimates, whereas higher SEs indicate less precise estimates.

3. Confidence Intervals (CI.Lower and CI.Upper):
- The confidence intervals provide the range within which the true effect size is likely to fall.
  - Moderators with confidence intervals that **exclude zero** are more likely to have consistent and meaningful effects.

4. Specific Moderators' Contributions:
  1. **Tree Type**:
  - While some terms (e.g., "tree_typeTimber") have moderate P-values (e.g., ~0.19), the confidence intervals often include zero, indicating less consistent influence.
- Moderators under "tree_type" show mixed evidence, with high variability between terms.
2. **Crop Type**:
  - Some subcategories (e.g., "crop_typeLegume") exhibit low P-values (e.g., < 0.001) and confidence intervals excluding zero, suggesting significant effects.
- This moderator likely contributes moderately to explaining heterogeneity in effect sizes.
3. **Age System**:
  - Terms such as "age_systemYoung" consistently show low P-values (<0.01) and confidence intervals excluding zero, highlighting a strong and significant influence on heterogeneity.
- This moderator seems to have one of the most consistent and impactful effects.
4. **Season**:
  - The influence of "season" varies, with subcategories like "seasonWinter" having moderate P-values (~0.13) and wider confidence intervals.
- Its effects are less consistent compared to age system or crop type.
5. **Soil Texture**:
  - Subcategories like "soil_textureSand" often show significant effects (P-values < 0.05), with narrow confidence intervals excluding zero.
- This suggests that soil texture is a critical moderator for explaining heterogeneity.

5. Proportion of Heterogeneity Explained:
  - Combining the above data with the proportion of heterogeneity explained reveals:
  - **Age System** consistently explains the highest proportion of heterogeneity (e.g., 95.47%).
- **Soil Texture** follows closely, explaining ~82.4%.
- **Crop Type** and **Season** contribute moderately (64.5% and 45.2%, respectively).
- **Tree Type** has a lower overall contribution (~50.6%).

Summary:
  - **Key Moderators:** "Age System" and "Soil Texture" are the most impactful, with consistently significant effects and the highest proportions of heterogeneity explained.
- **Moderate Contributors:** "Crop Type" and "Season" show variable but important contributions to explaining heterogeneity.
- **Least Impactful:** "Tree Type" appears to have inconsistent effects, with many subcategories not significantly contributing to heterogeneity.





Publication-ready map that visualizes the ecosystem services (response variables) reported in each study (id_article),
```{r}
# Step 1: Simplify the dataset for visualization
geo_data <- imp_dataset %>%
  group_by(lat = final_lat, lon = final_lon, response_variable) %>%
  summarize(
    n_studies = n_distinct(id_article),
    .groups = "drop"
  ) %>%
  filter(!is.na(lat) & !is.na(lon)) # Remove rows with missing coordinates

# Step 2: Base world map
world_map <- map_data("world")
# Step 3: Create the enhanced map
geo_distribution_of_studies_map <- ggplot() +
  # Add base map polygons
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add jittered points for studies
  geom_point(
    data = geo_data,
    aes(x = lon, y = lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 1, height = 0.8)
  ) +
  # Apply custom colors
  scale_color_manual(values = custom_colors, name = "Ecosystem Service") +
  scale_size_continuous(
    name = "Number of Studies",
    range = c(2, 5),  # Adjust size range for better visibility
    breaks = c(1, 2, 5),  # Customize breaks based on study count
    labels = c("1", "2", "5+")
  ) +
  # Add labels and enhance the theme
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry Studies",
    subtitle = "Larger Map for Clearer Visualization",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_quickmap() +  # Quick world map projection
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
geo_distribution_of_studies_map
```

```{r}
imp_dataset |> glimpse()
```


```{r}
# Count unique observations per location and response variable
location_counts_obs <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_studies = n_distinct(id_obs), .groups = "drop")

# Count unique studies per location and response variable
location_counts_article <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_studies = n_distinct(id_article), .groups = "drop")

# Check the result
glimpse(location_counts_obs)
location_counts_article
```
```{r}
# Ensure unique lat/lon for each location
geo_data <- imp_dataset %>%
  select(location, final_lat, final_lon) %>%
  distinct(location, .keep_all = TRUE) %>%
  left_join(location_counts_obs, by = "location") %>%
  filter(!is.na(final_lat) & !is.na(final_lon))  # Remove missing coordinates

# Check the merged dataset
glimpse(geo_data)
```

```{r}
# Load world map
world_map <- map_data("world")

# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)


# Fix longitude sign issue for USA and Canada
geo_data <- geo_data %>%
  mutate(final_lon = ifelse(location %in% c("USA", "Canada") & final_lon > 0, -final_lon, final_lon))

# Create the updated map
geo_distribution_of_studies_map <- ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  geom_point(
    data = geo_data,
    aes(x = final_lon, y = final_lat, color = response_variable, size = n_studies),
    alpha = 0.8,
    position = position_jitter(width = 10, height = 5)
  ) +
  scale_color_manual(values = custom_colors, name = "Ecosystem Service") +
  scale_size_continuous(
    name = "Number of Observations",
    range = c(1, 8),
    breaks = c(1, 5, 10, 20, 30, 50), 
    labels = c("1", "5", "10", "20", "30", "50+")
  ) +
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry",
    subtitle = "Point size indicates the number of observations per location",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(-180, 180), ylim = c(-60, 90)) + # Ensure USA/Canada appear
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
print(geo_distribution_of_studies_map)
```

```{r}
# Count unique articles per location and response variable
location_counts_article <- imp_dataset %>%
  group_by(location, response_variable) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop")

# Merge with geographical coordinates while retaining unique locations
geo_article_data <- imp_dataset %>%
  select(location, final_lat, final_lon) %>%
  distinct() %>%  # Ensure one row per location
  left_join(location_counts_article, by = "location") %>%
  # Remove rows with missing coordinates
  filter(!is.na(final_lat) & !is.na(final_lon)) |> 
  # Fix longitude sign issue for USA and Canada
  mutate(final_lon = ifelse(location %in% c("USA", "Canada") & final_lon > 0, -final_lon, final_lon))


geo_article_data
```


```{r}
# Create the updated map
geo_distribution_of_articles_map <- ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  geom_point(
    data = geo_article_data,
    aes(x = final_lon, y = final_lat, color = response_variable, size = n_articles),
    alpha = 0.8,
    position = position_jitter(width = 5, height = 2)
  ) +
  scale_color_manual(values = custom_colors, name = "Ecosystem Service") +
  scale_size_continuous(
    name = "Number of Articles",
    range = c(2, 4),  # Adjust for better readability
    breaks = c(1, 2, 3), 
    labels = c("1", "2", "3+")
  ) +
  labs(
    title = "Geographical Distribution of Published Articles on Ecosystem Services",
    subtitle = "Point size indicates the number of unique articles per location",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(-180, 180), ylim = c(-60, 90)) +  # Ensure USA/Canada appear
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
geo_distribution_of_articles_map
```

```{r}
# Instead I want to show the number of response variables per article

# Step 1: Count the number of unique response variables per article
article_response_counts <- imp_dataset %>%
  group_by(id_article) %>%
  summarize(n_response_variables = n_distinct(response_variable), .groups = "drop")

# Step 2: Aggregate at the location level (average number of response variables per article)
location_response_counts <- imp_dataset %>%
  select(location, final_lat, final_lon, id_article) %>%
  distinct() %>%  # Keep unique article-location pairs
  left_join(article_response_counts, by = "id_article") %>%
  group_by(location, final_lat, final_lon) %>%
  summarize(mean_response_variables = mean(n_response_variables), .groups = "drop")

# Fix longitude sign issue for USA and Canada
location_response_counts <- location_response_counts %>%
  mutate(final_lon = ifelse(location %in% c("USA", "Canada") & final_lon > 0, -final_lon, final_lon))

# Step 3: Create the updated map
geo_distribution_response_variables_map <- ggplot() +
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  geom_point(
    data = location_response_counts,
    aes(x = final_lon, y = final_lat, size = mean_response_variables),
    color = "darkred", alpha = 0.8,
    position = position_jitter(width = 5, height = 3)
  ) +
  scale_size_continuous(
    name = "Mean Response Variables per Article",
    range = c(2, 10),  # Adjust size range for better visibility
    breaks = c(1, 2, 3, 4, 5, 6), 
    labels = c("1", "2", "3", "4", "5", "6+")
  ) +
  labs(
    title = "Mean Number of Response Variables per Article",
    subtitle = "Point size represents the mean number of response variables per article at each location",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(-180, 180), ylim = c(-60, 90)) +  # Ensure all locations appear
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the map
print(geo_distribution_response_variables_map)
```


```{r}
# Step 1: Define European Bounding Box
europe_bbox <- c(xmin = -15, xmax = 35, ymin = 34, ymax = 72)

# Step 2: Ensure `n_studies` is numeric (Fixing viridis error)
world_data <- world_data %>%
  mutate(n_studies = as.numeric(n_studies))  # Convert study count to numeric

# Step 3: Extract Centroids for Each Country
world_data <- world_data %>%
  mutate(
    centroid = st_centroid(geometry),  # Compute centroid
    lon = st_coordinates(centroid)[,1],  # Extract longitude
    lat = st_coordinates(centroid)[,2]   # Extract latitude
  ) %>%
  select(-centroid)  # Remove geometry-based centroids

# Step 4: Filter Dataset for Europe Only
europe_data <- world_data %>%
  filter(!is.na(n_studies)) %>%  # Ensure only numeric study counts
  filter(lon >= europe_bbox["xmin"], lon <= europe_bbox["xmax"],
         lat >= europe_bbox["ymin"], lat <= europe_bbox["ymax"])

# Step 5: Ensure `n_studies` is not a categorical variable (Double-check)
europe_data$n_studies <- as.numeric(europe_data$n_studies)
```


```{r}
# Load world map
world_map <- map_data("world")

# Define custom colors for response variables
custom_colors <- c(
  "Biodiversity" = "#FF9999",
  "Greenhouse gas emission" = "#66C266",
  "Product quality" = "#FFC000",
  "Crop yield" = "#FF9933",
  "Pest and Disease" = "#33CCCC",
  "Soil quality" = "#9966CC",
  "Water quality" = "#9999FF"
)

# Step 1: Count unique articles per location
location_counts_article <- imp_dataset %>%
  group_by(location) %>%
  summarize(n_articles = n_distinct(id_article), .groups = "drop")

# Step 2: Count distinct response variables per article
article_response_counts <- imp_dataset %>%
  group_by(id_article) %>%
  summarize(n_response_variables = n_distinct(response_variable), .groups = "drop")

# Step 3: Aggregate at the location level (mean response variables per article)
location_response_counts <- imp_dataset %>%
  select(location, final_lat, final_lon, id_article) %>%
  distinct() %>%  # Keep unique article-location pairs
  left_join(article_response_counts, by = "id_article") %>%
  group_by(location, final_lat, final_lon) %>%
  summarize(mean_response_variables = mean(n_response_variables), .groups = "drop")

# Step 4: Merge datasets to retain latitude, longitude, and study counts
geo_data <- imp_dataset %>%
  select(location, final_lat, final_lon) %>%
  distinct() %>%
  left_join(location_counts_article, by = "location") %>%
  left_join(location_response_counts, by = c("location", "final_lat", "final_lon")) %>%
  filter(!is.na(final_lat) & !is.na(final_lon))  # Remove missing coordinates

# Fix longitude sign issue for USA and Canada
geo_data <- geo_data %>%
  mutate(final_lon = ifelse(location %in% c("USA", "Canada") & final_lon > 0, -final_lon, final_lon))

geo_data <- geo_data %>%
  mutate(
    response_variable_category = case_when(
      mean_response_variables <= 1 ~ "1 Response",
      mean_response_variables <= 2 ~ "2 Responses",
      mean_response_variables <= 3 ~ "3 Responses",
      mean_response_variables <= 4 ~ "4 Responses",
      mean_response_variables <= 5 ~ "5 Responses",
      TRUE ~ "6+ Responses"
    )
  )

custom_colors <- c(
  "1 Response" = "#FF9999",
  "2 Responses" = "#66C266",
  "3 Responses" = "#33CCCC",
  "4 Responses" = "#FFC000",
  "5 Responses" = "#FF9933",
  "6+ Responses" = "#9966CC"
)

# Step 5: Create the final geographic map
# Step 5: Create the final geographic map
geo_distribution_map <- ggplot() +
  # Add world map background
  geom_polygon(
    data = world_map,
    aes(x = long, y = lat, group = group),
    fill = "gray90", color = "gray70", size = 0.4
  ) +
  # Add study locations with jittered points
  geom_point(
    data = geo_data,
    aes(x = final_lon, y = final_lat, color = response_variable_category, size = n_articles),
    alpha = 0.9,
    position = position_jitter(width = 2, height = 2)
  ) +
  # Define manual colors for response variable diversity
  scale_color_manual(values = custom_colors, name = "Response Variables per Article") +
  scale_size_continuous(
    name = "Number of Articles",
    range = c(2, 4),
    breaks = c(1, 3, 5),
    labels = c("1", "3", "5+")
  ) +
  # Labels and layout
  labs(
    title = "Geographical Distribution of Ecosystem Services in Silvoarable Agroforestry",
    subtitle = "Point size represents the number of articles; color represents response variable diversity",
    x = "Longitude",
    y = "Latitude"
  ) +
  coord_cartesian(xlim = c(-180, 180), ylim = c(-60, 90)) +  # Ensure all locations are shown
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 18),
    plot.subtitle = element_text(hjust = 0.5, size = 14),
    legend.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid = element_blank(),
    axis.line = element_line(color = "gray50"),
    legend.box = "vertical"
  )

# Display the final map
geo_distribution_map
```








# Generating combined map

# Step 1: Define Bounding Box for Europe
europe_bbox <- c(xmin = -10, xmax = 30, ymin = 35, ymax = 65)

# Step 2: Generate the Global Map
global_map <- ggplot() +
  # Base map: Show all countries
  geom_sf(data = world_data, aes(fill = n_studies), color = "black", size = 0.3) +
  
  # Use "Blues" scale for study counts
  scale_fill_distiller(
    palette = "Blues",
    direction = 1,
    na.value = "gray90",
    limits = c(0, max(world_data$n_studies, na.rm = TRUE))
  ) + 
  
  # Separate scales for pie chart colors
  new_scale_fill() +
  
  # Add pie charts at country centroids
  geom_arc_bar(
    data = pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 3,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(values = response_colors) +  
  
  # Remove all text, legends, and axes
  theme_void() +
  theme(legend.position = "none")

# Step 3: Generate the Zoomed-in Europe Map using BBOX
europe_map <- ggplot() +
  # Base map for all European countries
  geom_sf(data = europe_all_countries, aes(fill = as.numeric(n_studies)), color = "black", size = 0.3) +
  
  # Use "Blues" scale for study counts in Europe
  scale_fill_distiller(
    palette = "Blues",
    direction = 1,
    na.value = "gray90",
    limits = c(1, max(europe_all_countries$n_studies, na.rm = TRUE))
  ) + 
  
  new_scale_fill() +
  
  # Add pie charts at country centroids
  geom_arc_bar(
    data = europe_pie_chart_data,
    aes(
      x0 = st_coordinates(geometry)[,1],
      y0 = st_coordinates(geometry)[,2],
      r0 = 0, r = 2,
      fill = response_variable,
      amount = proportion
    ),
    stat = "pie",
    inherit.aes = FALSE
  ) +
  scale_fill_manual(values = response_colors) +  
  
  # Apply bounding box limits
  coord_sf(xlim = c(europe_bbox["xmin"], europe_bbox["xmax"]), ylim = c(europe_bbox["ymin"], europe_bbox["ymax"])) +  
  
  # Remove all text, legends, and axes
  theme_void() +
  theme(legend.position = "none")

# Step 4: Combine the Global and Europe Maps using Patchwork
combined_map <- global_map + 
  inset_element(europe_map, 
                left = 0.20, 
                bottom = 0.25, 
                right = 0.85, 
                top = 0.75)

# Step 5: Display the Final Merged Map
print(combined_map)









```{r}
# Define the output folder path
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "FIGURES")

# Ensure the directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Define the file paths for saving
output_path_png <- file.path(output_dir, "forest_plot_density.png")
output_path_pdf <- file.path(output_dir, "forest_plot_density.pdf")

# Save the plot as PNG
ggsave(
  filename = output_path_png,
  plot = forest_plot_density,
  width = 16, height = 8, dpi = 600
)

# Save the plot as PDF
ggsave(
  filename = output_path_pdf,
  plot = forest_plot_density,
  width = 16, height = 8
)
```


```{r}
# Step 1: Ensure response variables are ordered by mean effect size
imp_data_rom_violin <- imp_data_rom %>%
  mutate(response_variable = fct_reorder(response_variable, yi, .fun = mean, na.rm = TRUE))

# Step 1: Summarize CI & Study Counts
summary_data <- imp_data_rom_violin %>%
  group_by(response_variable) %>%
  summarise(
    mean_effect = mean(yi, na.rm = TRUE),
    lower_CI = mean_effect - 1.96 * (sd(yi, na.rm = TRUE) / sqrt(n())),
    upper_CI = mean_effect + 1.96 * (sd(yi, na.rm = TRUE) / sqrt(n())),
    study_count = n_distinct(id_article)
  ) %>%
  ungroup()

# Step 2: Define scale limits (exclude extreme outliers)
yi_limits <- quantile(imp_data_rom_violin$yi, probs = c(0.025, 0.975), na.rm = TRUE)  # 95% range

# Step 2: Define fixed x-axis limits
x_min <- -0.25
x_max <- 0.65

# Step 3: Create the violin plot with CI & study count annotations
forest_violin_plot <- imp_data_rom_violin |> 
  ggplot(aes(x = yi, y = response_variable, fill = response_variable)) +
  
  # Violin plot to show effect size distribution
  geom_violin(trim = FALSE, alpha = 0.5, color = "black") +
  
  # Boxplot to show mean and IQR
  geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0.8, color = "black") +
  
  # Mean points for emphasis
  stat_summary(fun = mean, geom = "point", shape = 21, size = 3, fill = "white", stroke = 1.5) +
  
  # Apply custom colors
  scale_fill_manual(values = custom_colors) +
  
  # Labels and theme
  labs(
    title = "Overall Effects of SAF on Agroecosystem Services Relative to Monocrop",
    x = "log-ROM Relative to Monocrop",
    y = "",
    fill = "Response Variable"
  ) +
  
  # Adjust x-axis limits to remove extreme values
  scale_x_continuous(limits = c(x_min, x_max),
                     breaks = seq(x_min, x_max, by = 0.1)  # Add breaks every 0.1
  ) +
  
  # Theme adjustments
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    legend.position = "none",
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),
    panel.grid.minor = element_blank(),
    axis.line = element_line(linewidth = 0.5)
  ) +
  
  # Add vertical red dotted reference line at x = 0 (neutral effect line)
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", linewidth = 1) +
  
  # Add CI & Study Counts as Text Labels on the Right Side
  geom_text(
    data = summary_data,
    aes(
      x = 0.55,  # Position slightly outside the max x-axis
      y = response_variable,
      label = paste0("CI: [", round(lower_CI, 2), ", ", round(upper_CI, 2), "]\nN=", study_count)
    ),
    hjust = 0,  # Left-aligned text
    size = 4.5
  )

# Display the plot
forest_violin_plot
```

```{r}
# Step 1: Define fixed x-axis limits
x_min <- -0.25
x_max <- 0.65

# Step 2: Create the Forest Violin Plot (without text clutter)
forest_violin_plot <- ggplot(imp_data_rom_violin, aes(x = yi, y = response_variable, fill = response_variable)) +
  # Violin plot to show effect size distribution
  geom_violin(trim = FALSE, alpha = 0.5, color = "black") +
  # Boxplot to show mean and IQR
  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.8, color = "black") +
  # Mean points for emphasis
  stat_summary(fun = mean, geom = "point", shape = 21, size = 3, fill = "white", stroke = 1.5) +
  # Confidence Interval Line (Horizontal Error Bars)
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2, color = "black", linewidth = 1) +
  # Apply custom colors
  scale_fill_manual(values = custom_colors) +
  # Labels and theme
  labs(
    title = "Overall Effects of SAF on Agroecosystem Services Relative to Monocrop",
    x = "log RR Relative to Monocrop",
    y = "",
    fill = "Response Variable"
  ) +
  # Adjust x-axis limits and add more breaks
  scale_x_continuous(
    limits = c(x_min, x_max),  
    breaks = seq(x_min, x_max, by = 0.05)  # Add breaks every 0.05
  ) +
  # Theme adjustments
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    legend.position = "none",
    panel.grid.major.y = element_line(color = "gray", linewidth = 0.5),
    panel.grid.minor = element_blank(),
    axis.line = element_line(linewidth = 0.5)
  ) +
  # Add vertical red dotted reference line at x = 0 (neutral effect line)
  geom_vline(xintercept = 0, linetype = "dotted", color = "red", linewidth = 1.5)

# Step 3: Create the CI & Study Count Text Plot
text_plot <- ggplot(structured_summary, aes(y = ResponseVariable)) +
  geom_text(
    aes(
      x = 1,  # Arbitrary x-axis position (we will remove x-axis later)
      label = paste0("CI: [", round(CI_Lower, 2), ", ", round(CI_Upper, 2), "]\n",
                     "Back-Transformed: [", round(ROM_Lower_Percent, 2), "%, ", 
                     round(ROM_Upper_Percent, 2), "%]\nN=", study_count)
    ),
    hjust = 0, size = 5
  ) +
  labs(title = "Effect Size & Study Counts") +
  theme_void() +  # Remove grid, axis lines, and background
  theme(
    plot.title = element_text(size = 14, hjust = 0.5, face = "bold"),
    axis.text.y = element_blank(),  # Remove y-axis labels
    axis.ticks.y = element_blank()
  )

# Step 4: Arrange both plots side by side
final_plot <- forest_violin_plot + text_plot + plot_layout(widths = c(3, 1))  # Give more space to violin plot

# Display the plot
print(final_plot)

```












##########################################################################################################################################
FITTING MODELS (SUB-GROUP) FOR EACH RESPONSE VARIABLE USING PRECOMPUTED V_MATRICES
##########################################################################################################################################

```{r}
##########################################################################################################################################
# HIERARCHICAL COMPLEXITY APPROACH ALIGNED WITH THE CABBAGE APPROACH
##########################################################################################################################################

# Start time tracking
start.time <- Sys.time()
##########################################################################
# Set up parallel processing
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################

# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-10) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy.
  rel.tol = 1e-5
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)


# Function to fit models incrementally for a response variable
fit_models_all <- function(data_subset, response_variable, v_matrix, moderators, random_effects) {
  results <- list()
  
  cat("\nProcessing response variable:", response_variable, "\n")
  
  #############################################################################################
  # Null model: Global average without moderators
  results$null_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in null model:", e$message, "\n")
    return(NULL)
  })
  ############################################################################################# <-------------- ! (chosen model) !
  # Minimal random effects model: Intercept-only model
  results$minimal_random_effects <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = ~ 1,  # Intercept-only model
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in minimal random effects model:", e$message, "\n")
    return(NULL)
  })
  
  #############################################################################################
  # Incremental models for each moderator (no interaction):
  results$moderator_models <- map(moderators, ~ {
    moderator <- .x
    tryCatch({
      rma.mv(
        yi = yi,
        V = v_matrix,
        mods = as.formula(paste("~", moderator)),
        random = random_effects,
        data = data_subset,
        method = "REML",
        control = control_params
      )
    }, error = function(e) {
      cat("Error in moderator model for", moderator, ":", e$message, "\n")
      return(NULL)
    })
  })
  names(results$moderator_models) <- moderators
  
  ############################################################################################# 
  # Full model with all moderators (no interaction):
  results$full_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = as.formula(paste("~", paste(moderators, collapse = " + "))),
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in full model:", e$message, "\n")
    return(NULL)
  })
  
  #############################################################################################
  # Full interaction model with all moderators: 
  results$interaction_model <- tryCatch({
    rma.mv(
      yi = yi,
      V = v_matrix,
      mods = as.formula(paste("~", paste(moderators, collapse = " * "))),
      random = random_effects,
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    cat("Error in interaction model:", e$message, "\n")
    return(NULL)
  })
  
  return(results)
}

##########################################################################
# Fit Models for Each Response Variable
##########################################################################

# Initialize an empty list to store model results
model_results <- list()

# Loop through each response variable to fit models
for (response in names(v_matrices)) {
  # Subset the data for the current response variable
  data_subset <- meta_data[meta_data$response_variable == response, ]
  
  # Extract the variance-covariance matrix for the response variable
  v_matrix <- v_matrices[[response]]
  
  # Define the moderators to include in the model
  moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")
  
  # Define random effects structure 
  # Previously defined as [~ 1 | exp_id], 
  # but now defined as ~ 1 | id_article/location*experiment_year where location-year is included as a crossed random effect
  # Actually the random effects structure is now [~ 1 | id_article/location]
  
  random_effects <- list(~ 1 | id_article/exp_id)
  
  # random_effects <- list(~ 1 | id_article)
  
  # random_effects <- list(~ 1 | exp_id)
  
  # random_effects <- list(~ 1 | id_article/location * experiment_year)
  
  #   random_effects <- list(
  #   ~ 1 | id_article,                  # Study-level variance
  #   ~ 1 | location,                    # Location variance
  #   ~ 1 | experiment_year,             # Year variance
  #   ~ 1 | location*experiment_year     # Interaction variance
  # )
  #   random_effects <- list(
  #   ~ 1 | id_article,                  # Study-level variance
  #   ~ 1 | location,                    # Location variance
  #   ~ 1 | experiment_year              # Year variance
  # )
  
  # Fit models incrementally using the cabbage approach
  model_results[[response]] <- fit_models_all(
    data_subset = data_subset,
    response_variable = response,
    v_matrix = v_matrix,
    moderators = moderators,
    random_effects = random_effects
  )
}

##########################################################################
# Save All Fitted Models
##########################################################################

output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")

# Save all models in a combined file
saveRDS(model_results, file = file.path(output_dir, "fitted_models_all_new.rds"))

# Save individual model results
for (response in names(model_results)) {
  saveRDS(model_results[[response]], file = file.path(output_dir, paste0("fitted_models_", response, "_new.rds")))
}

cat("\nAll models have been saved successfully!\n")

##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, units(time.taken), "\n")
##########################################################################
# Last go (25/01-2025) 
# Total time taken: 31.73879 secs 

# Last go (01/03-2025) 
# Total time taken: 48.24339 secs

# Processing response variable: Biodiversity 
# Advarsel: Redundant predictors dropped from the model.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.Advarsel: 14 rows with NAs omitted from model fitting.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Greenhouse gas emission 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Product quality 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Crop yield 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Pest and Disease 
# Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Soil quality 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.
# Processing response variable: Water quality 
# Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.Advarsel: Ratio of largest to smallest sampling variance extremely large. May not be able to obtain stable results.Advarsel: Redundant predictors dropped from the model.Advarsel: Single-level factor(s) found in 'random' argument. Corresponding 'sigma2' value(s) fixed to 0.
# All models have been saved successfully!
# 
# Total time taken: 48.24339 secs 
```












############
# STEP 8
##########################################################################################################################################
MODERATOR ANALYSIS - INFLUENCE OF SILVOARABLE AGROFORESTRY CHARACTERISTICS
##########################################################################################################################################

```{r}
# model_results$`Crop yield`$full_model$tau2

# model_results$`Crop yield`$moderator_model$tau2
```

```{r}
# Load the saved v_matrices
output_dir <- here::here("DATA", "OUTPUT_FROM_R", "SAVED_OBJECTS_FROM_R")
v_matrices <- readRDS(file.path(output_dir, "v_matrices_by_response_variable.rds"))
```

```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

# Define response variable
response_variable <- "Crop yield"  # Change accordingly

# Subset data for the response variable
data_subset <- meta_data[meta_data$response_variable == response_variable, ]

# Extract the variance-covariance matrix
v_matrix <- v_matrices[[response_variable]]

# Define the moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Define the random effects structure
random_effects <- list(~ 1 | exp_id)

# Print data checks
print(dim(data_subset))
print(head(data_subset))
```
```{r}
# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

# Fit the Null Model
# null_model <- tryCatch({
#   rma.mv(
#     yi = yi,
#     V = v_matrix,
#     random = random_effects,
#     data = data_subset,
#     method = "REML",
#     control = control_params
#   )
# }, error = function(e) {
#   message("Error in null model: ", e$message)
#   return(NULL)
# })
# 
# # Check if model fitted correctly
# if (!is.null(null_model)) {
#   print(null_model)
#   tau2_null <- sum(null_model$tau2)  # Extract τ²
#   cat("τ² (Null Model):", tau2_null, "\n")
# } else {
#   cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
# }


# Fit the Null Model (NO RANDOM EFFECTS)
null_model <- tryCatch({
  rma(
    yi = yi,
    vi = diag(v_matrix),  # Use diagonal for within-study variance
    data = data_subset,
    method = "REML",
    control = control_params
  )
}, error = function(e) {
  message("Error in null model: ", e$message)
  return(NULL)
})

# Check if model fitted correctly
if (!is.null(null_model)) {
  print(null_model)
  tau2_null <- null_model$tau2  # Extract τ²
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
}
```

```{r}
# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

# Fit the Full Model

# full_model <- tryCatch({
#   rma.mv(
#     yi = yi,
#     V = v_matrix,
#     mods = as.formula(paste("~", paste(moderators, collapse = " + "))),
#     random = random_effects,
#     data = data_subset,
#     method = "REML",
#     control = control_params
#   )
# }, error = function(e) {
#   message("Error in full model: ", e$message)
#   return(NULL)
# })
# 
# # Check if full model fitted correctly
# if (!is.null(full_model)) {
#   print(full_model)
#   tau2_full <- sum(full_model$tau2)  # Extract τ²
#   cat("τ² (Full Model):", tau2_full, "\n")
# } else {
#   cat("⚠ Warning: τ² (Full Model) is 0 or model fitting failed.\n")
# }

# Fit the Full Model (NO RANDOM EFFECTS)
full_model <- tryCatch({
  rma(
    yi = yi,
    vi = diag(v_matrix),
    mods = as.formula(paste("~", paste(moderators, collapse = " + "))),
    data = data_subset,
    method = "REML",
    control = control_params
  )
}, error = function(e) {
  message("Error in full model: ", e$message)
  return(NULL)
})

# Check if full model fitted correctly
if (!is.null(full_model)) {
  print(full_model)
  tau2_full <- full_model$tau2  # Extract τ²
  cat("τ² (Full Model):", tau2_full, "\n")
} else {
  cat("⚠ Warning: τ² (Full Model) is 0 or model fitting failed.\n")
}
```

```{r}
# Calculate proportion of explained heterogeneity

if (!is.null(null_model) && !is.null(full_model)) {
  proportion_explained <- ((tau2_null - tau2_full) / tau2_null) * 100
  proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure within 0-100%
  
  cat("\nProportion of Explained Heterogeneity:", proportion_explained, "%\n")
} else {
  cat("\nCannot compute explained heterogeneity due to missing models.\n")
}
```

Justifying the Use of a Model Without a Random Component

Proportion of Explained Heterogeneity (PEH) is derived by comparing between-study variance (τ²) in models with and without moderators. To estimate PEH, the null model provides a baseline τ² representing total heterogeneity, while models incorporating moderators help partition that heterogeneity and quantify the proportion explained. However, the data structure in this analysis is too sparse to support a hierarchical random-effects model with `~ 1 | id_article/exp_id`, as the number of studies per group is too low for the model to reliably estimate variance components. This led to convergence issues, requiring a modified approach where the models were refitted without the random-effects component.

Refitting was necessary because PEH is calculated as the proportionate reduction in τ² between models, and this requires the ability to fit both a null model and a moderator model under comparable conditions. Without refitting, it would be impossible to determine how much variance each moderator accounts for. The random-effects model structure introduced instability because the hierarchical variance component could not be estimated reliably, making it impractical for PEH analysis.

By removing the random component, the model focuses on within-study variance while still capturing residual heterogeneity (τ²). Although this shifts the approach closer to a fixed-effects model, the key objective here is not to make population-level inferences but rather to partition variance and assess the explanatory power of moderators. The simplified model structure ensures that τ² estimates remain valid, while bootstrapping compensates for potential limitations by providing empirical uncertainty estimates.

Critically, the model still accounts for variability through the study-level variance (`vi = diag(V)`), which ensures appropriate weighting of studies based on precision. While a full random-effects model is generally preferable when estimating overall heterogeneity, its use in this case was not feasible due to sparse data. Alternative approaches, such as Bayesian hierarchical modeling or aggregating studies within `exp_id`, would introduce additional assumptions or reduce statistical power. Removing the random component allowed for a more stable estimation of τ² and thus more reliable PEH calculations.

This approach ensures that the PEH estimates remain interpretable and statistically valid while avoiding model complexity that would otherwise compromise the analysis. The use of bootstrapping further strengthens the robustness of these estimates, providing confidence in the validity of the findings despite the necessary modifications to the modeling approach.


```{r}
# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

# Initialize an empty data frame to store results
heterogeneity_results <- data.frame()

# Define all response variables
response_variables <- unique(meta_data$response_variable)

# Loop through each response variable
for (response_variable in response_variables) {
  
  cat("\n-------------------------\nProcessing:", response_variable, "\n-------------------------\n")
  
  # Subset data for the response variable
  data_subset <- meta_data[meta_data$response_variable == response_variable, ]
  
  # Extract the variance-covariance matrix
  v_matrix <- v_matrices[[response_variable]]
  
  # Skip if variance matrix is missing
  if (is.null(v_matrix)) {
    cat("⚠ Skipping", response_variable, "- No variance matrix found.\n")
    next
  }
  
  # Fit the Null Model (NO RANDOM EFFECTS)
  null_model <- tryCatch({
    rma(
      yi = yi,
      vi = diag(v_matrix),  # Use diagonal for within-study variance
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    message("Error in null model for", response_variable, ":", e$message)
    return(NULL)
  })
  
  # Extract tau² from the null model
  if (!is.null(null_model)) {
    tau2_null <- null_model$tau2
    cat("τ² (Null Model) for", response_variable, ":", tau2_null, "\n")
  } else {
    cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed for", response_variable, "\n")
    next  # Skip to next response variable if null model fails
  }
  
  # Loop through each moderator for this response variable
  for (moderator in moderators) {
    cat("\nFitting model for moderator:", moderator, "on", response_variable, "\n")
    
    mod_model <- tryCatch({
      rma(
        yi = yi,
        vi = diag(v_matrix),
        mods = as.formula(paste("~", moderator)),  # Single moderator model
        data = data_subset,
        method = "REML",
        control = control_params
      )
    }, error = function(e) {
      message("Error in model for", moderator, "on", response_variable, ":", e$message)
      return(NULL)
    })
    
    # Extract tau² from the moderator model
    if (!is.null(mod_model)) {
      tau2_moderated <- mod_model$tau2
      cat("  τ² (Model with", moderator, "on", response_variable, "):", tau2_moderated, "\n")
      
      # Calculate proportion of explained heterogeneity
      proportion_explained <- ifelse(tau2_null > 0, ((tau2_null - tau2_moderated) / tau2_null) * 100, NA)
      proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure between 0-100%
      
      # Store results
      heterogeneity_results <- rbind(heterogeneity_results, 
                                     data.frame(ResponseVariable = response_variable,
                                                Moderator = moderator,
                                                Tau2_Null = tau2_null,
                                                Tau2_Moderated = tau2_moderated,
                                                ProportionExplained = proportion_explained))
    } else {
      cat("⚠ Warning: Model for", moderator, "on", response_variable, "failed.\n")
    }
  }
}

# Print results
heterogeneity_results
```













```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

# Define response variable
response_variable <- "Crop yield"  # Change accordingly

# Subset data for the response variable
data_subset <- meta_data[meta_data$response_variable == response_variable, ]

# Extract the variance-covariance matrix
v_matrix <- v_matrices[[response_variable]]

# Define the moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Define the random effects structure [before "list(~ 1 | exp_id)"]
random_effects <- list(~ 1 | id_article/exp_id) # Random intercept for each study and experiment (location x year)

# Print data checks
# print(dim(data_subset))
# print(head(data_subset))
```

```{r}
# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

##################################################################################################################

# Fit the Null Model (Global Average without Moderators)
null_model <- tryCatch({
  rma(
    yi = yi,                        # Effect size
    vi = diag(v_matrix),            # Use diagonal for within-study variance
    random = random_effects,        # Random effects structure
    data = data_subset,             # Data subset
    method = "REML",                # Restricted Maximum Likelihood Estimation
    control = control_params        # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in null model:", e$message, "\n")
  return(NULL)
})

# Check if the null model fitted correctly
if (!is.null(null_model)) {
  print(null_model)
  tau2_null <- null_model$tau2  # Extract τ² (variance component)
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
}

######################################################

# Fit the Full Moderator Model (All Moderators additive, without Interactions)
full_moderator_model <- tryCatch({
  rma(
    yi = yi,                                      # Effect size
    vi = diag(v_matrix),                          # Use diagonal for within-study variance
    mods = as.formula(paste("~", paste(moderators, collapse = " + "))),  
    random = random_effects,                      # Random effects structure
    data = data_subset,                           # Data subset
    method = "REML",                              # Restricted Maximum Likelihood Estimation
    control = control_params                      # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in interaction model:", e$message, "\n")
  return(NULL)
})

# Check if the interaction model fitted correctly
if (!is.null(full_moderator_model)) {
  print(full_moderator_model)
  tau2_full_moderator <- full_moderator_model$tau2  # Extract τ²
  cat("τ² (Full Moderator Model):", tau2_interaction, "\n")
} else {
  cat("⚠ Warning: τ² (Full Moderator Model) is 0 or model fitting failed.\n")
}
```



```{r}
# Calculate proportion of explained heterogeneity for the single specified response variable (this case it is "Crop yield")

if (!is.null(null_model) && !is.null(full_moderator_model)) {
  proportion_explained <- ((tau2_null - tau2_full_moderator) / tau2_null) * 100
  proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure within 0-100%
  
  cat("\nProportion of Explained Heterogeneity:", proportion_explained, "%\n")
} else {
  cat("\nCannot compute explained heterogeneity due to missing models.\n")
}
```

# Check if the full moderator model fitted correctly
if (!is.null(full_moderator_model)) {
  
  # Extract model summary
  model_summary <- summary(full_moderator_model)
  
  # Extract estimates and relevant statistics
  estimates <- as.numeric(model_summary$b)   # Effect sizes
  se_values <- as.numeric(model_summary$se)  # Standard errors
  z_values <- as.numeric(model_summary$zval) # Z-values
  p_values <- as.numeric(model_summary$pval) # P-values
  ci_lower <- as.numeric(model_summary$ci.lb) # Lower CI
  ci_upper <- as.numeric(model_summary$ci.ub) # Upper CI
  
  # Combine into a data frame
  moderator_results_df <- data.frame(
    Moderator = rownames(model_summary$b), # Get moderator names
    Estimate = estimates,                 # Effect size
    SE = se_values,                        # Standard error
    Z_Value = z_values,                    # Z-score
    P_Value = p_values,                    # P-value
    CI_Lower = ci_lower,                   # Lower confidence interval
    CI_Upper = ci_upper                     # Upper confidence interval
  )
  
  # Print results
  # moderator_results_df
  
} else {
  cat("⚠ Warning: Full Moderator Model fitting failed.\n")
}


# Add significance column based on P-value thresholds
moderator_results_df <- moderator_results_df |> 
  mutate(Significance = case_when(
    P_Value < 0.001 ~ "***",   # Highly significant
    P_Value < 0.01  ~ "**",    # Strong significance
    P_Value < 0.05  ~ "*",     # Moderate significance
    P_Value < 0.1   ~ ".",     # Weak significance
    TRUE            ~ ""       # Not significant
  ))

# Print results with significance levels
print(moderator_results_df)


Updated Interpretation of Moderators' Effects on Model Heterogeneity

1. Significance of Moderators (P-value Interpretation):
- **P-values** reflect whether the moderator has a statistically significant effect on the response variable. 
  - A **P-value < 0.05** suggests strong evidence that the moderator significantly affects the heterogeneity in effect sizes.
  - Higher P-values indicate weaker evidence for significant effects, meaning the moderator’s influence may not be consistent across the response variables.

2. Magnitude of Effects (Estimate and SE):
- The **Estimate** represents the effect size or influence of each moderator term.
  - Positive values suggest that the moderator increases heterogeneity or variability.
  - Negative values suggest it reduces variability or explains heterogeneity in a systematic way.
- The **Standard Error (SE)** indicates the precision of the estimate.
  - Lower SE values imply higher confidence in the estimates, whereas higher SEs indicate less precise estimates.

3. Confidence Intervals (CI.Lower and CI.Upper):
- The confidence intervals provide the range within which the true effect size is likely to fall.
  - Moderators with confidence intervals that **exclude zero** are more likely to have consistent and meaningful effects.

4. Specific Moderators' Contributions:
  1. **Tree Type**:
  - While some terms (e.g., "tree_typeTimber") have moderate P-values (e.g., ~0.19), the confidence intervals often include zero, indicating less consistent influence.
- Moderators under "tree_type" show mixed evidence, with high variability between terms.
2. **Crop Type**:
  - Some subcategories (e.g., "crop_typeLegume") exhibit low P-values (e.g., < 0.001) and confidence intervals excluding zero, suggesting significant effects.
- This moderator likely contributes moderately to explaining heterogeneity in effect sizes.
3. **Age System**:
  - Terms such as "age_systemYoung" consistently show low P-values (<0.01) and confidence intervals excluding zero, highlighting a strong and significant influence on heterogeneity.
- This moderator seems to have one of the most consistent and impactful effects.
4. **Season**:
  - The influence of "season" varies, with subcategories like "seasonWinter" having moderate P-values (~0.13) and wider confidence intervals.
- Its effects are less consistent compared to age system or crop type.
5. **Soil Texture**:
  - Subcategories like "soil_textureSand" often show significant effects (P-values < 0.05), with narrow confidence intervals excluding zero.
- This suggests that soil texture is a critical moderator for explaining heterogeneity.

5. Proportion of Heterogeneity Explained:
  - Combining the above data with the proportion of heterogeneity explained reveals:
  - **Age System** consistently explains the highest proportion of heterogeneity (e.g., 95.47%).
- **Soil Texture** follows closely, explaining ~82.4%.
- **Crop Type** and **Season** contribute moderately (64.5% and 45.2%, respectively).
- **Tree Type** has a lower overall contribution (~50.6%).

Summary:
  - **Key Moderators:** "Age System" and "Soil Texture" are the most impactful, with consistently significant effects and the highest proportions of heterogeneity explained.
- **Moderate Contributors:** "Crop Type" and "Season" show variable but important contributions to explaining heterogeneity.
- **Least Impactful:** "Tree Type" appears to have inconsistent effects, with many subcategories not significantly contributing to heterogeneity.














```{r}
# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)
#########################################################################################################

# Initialize an empty data frame to store results
heterogeneity_results <- data.frame()

# Define all response variables
response_variables <- unique(meta_data$response_variable)

# Loop through each response variable
for (response_variable in response_variables) {
  
  cat("\n-------------------------\nProcessing:", response_variable, "\n-------------------------\n")
  
  # Subset data for the response variable
  data_subset <- meta_data[meta_data$response_variable == response_variable, ]
  
  # Extract the variance-covariance matrix
  v_matrix <- v_matrices[[response_variable]]
  
  # Skip if variance matrix is missing
  if (is.null(v_matrix)) {
    cat("⚠ Skipping", response_variable, "- No variance matrix found.\n")
    next
  }
  
  # Fit the Null Model (NO RANDOM EFFECTS)
  null_model <- tryCatch({
    rma(
      yi = yi,
      vi = diag(v_matrix),  # Use diagonal for within-study variance
      data = data_subset,
      method = "REML",
      control = control_params
    )
  }, error = function(e) {
    message("Error in null model for", response_variable, ":", e$message)
    return(NULL)
  })
  
  # Extract tau² from the null model
  if (!is.null(null_model)) {
    tau2_null <- null_model$tau2
    cat("τ² (Null Model) for", response_variable, ":", tau2_null, "\n")
  } else {
    cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed for", response_variable, "\n")
    next  # Skip to next response variable if null model fails
  }
  
  # Loop through each moderator for this response variable
  for (moderator in moderators) {
    cat("\nFitting model for moderator:", moderator, "on", response_variable, "\n")
    
    mod_model <- tryCatch({
      rma(
        yi = yi,
        vi = diag(v_matrix),
        mods = as.formula(paste("~", moderator)),  # Single moderator model
        data = data_subset,
        method = "REML",
        control = control_params
      )
    }, error = function(e) {
      message("Error in model for", moderator, "on", response_variable, ":", e$message)
      return(NULL)
    })
    
    # Extract tau² from the moderator model
    if (!is.null(mod_model)) {
      tau2_moderated <- mod_model$tau2
      cat("  τ² (Model with", moderator, "on", response_variable, "):", tau2_moderated, "\n")
      
      # Calculate proportion of explained heterogeneity
      proportion_explained <- ifelse(tau2_null > 0, ((tau2_null - tau2_moderated) / tau2_null) * 100, NA)
      proportion_explained <- max(min(proportion_explained, 100), 0)  # Ensure between 0-100%
      
      # Store results
      heterogeneity_results <- rbind(heterogeneity_results, 
                                     data.frame(ResponseVariable = response_variable,
                                                Moderator = moderator,
                                                Tau2_Null = tau2_null,
                                                Tau2_Moderated = tau2_moderated,
                                                ProportionExplained = proportion_explained))
    } else {
      cat("⚠ Warning: Model for", moderator, "on", response_variable, "failed.\n")
    }
  }
}

# Print results
heterogeneity_results
```




```{r}
##########################################################################################################################################
# PUBLICATION BIAS ASSESSMENT - EGGER'S TEST
##########################################################################################################################################


# Refit models using rma
refit_rma_models <- function(model_results) {
  rma_models <- list()
  
  for (response in names(model_results)) {
    tryCatch({
      # Assuming `yi` and `vi` are your effect sizes and variances
      model_data <- model_results[[response]]$data
      if (is.null(model_data)) next
      
      rma_model <- rma(yi = model_data$EffectSize, 
                       vi = model_data$Variance, 
                       data = model_data,
                       method = "REML")
      
      rma_models[[response]] <- rma_model
      cat(paste("Model refitted for", response, "\n"))
    }, error = function(e) {
      message(paste("Error refitting model for", response, ":", e$message))
    })
  }
  
  return(rma_models)
}

# Example usage
rma_models <- refit_rma_models(model_results)


# Run Egger's test on the refitted rma models
run_eggers_test <- function(rma_models) {
  results <- list()
  
  for (response in names(rma_models)) {
    model <- rma_models[[response]]
    if (is.null(model)) next
    
    tryCatch({
      egger_test <- regtest(model, model = "lm", predictor = "sei")
      results[[response]] <- egger_test
      cat(paste("\nEgger's Test for", response, ":\n"))
      print(egger_test)
    }, error = function(e) {
      message(paste("Error running Egger's test for", response, ":", e$message))
    })
  }
  
  return(results)
}

# Run Egger's test on refitted models
eggers_results <- run_eggers_test(rma_models)
eggers_results
```











```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

# Define response variable
response_variable <- "Soil quality"  # Change accordingly, make itaration through all response variables

# Subset data for the response variable
data_subset <- meta_data[meta_data$response_variable == response_variable, ]

# Extract the variance-covariance matrix
v_matrix <- v_matrices[[response_variable]]

# Define the moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Define the random effects structure [before "list(~ 1 | exp_id)"]
random_effects <- list(~ 1 | id_article/exp_id) # Random intercept for each study and experiment (location x year)


# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

##################################################################################################################

# Fit the Null Model (Global Average without Moderators)
null_model <- tryCatch({
  rma(
    yi = yi,                        # Effect size
    vi = diag(v_matrix),            # Use diagonal for within-study variance
    #random = random_effects,        # Random effects structure
    data = data_subset,             # Data subset
    method = "REML",                # Restricted Maximum Likelihood Estimation
    control = control_params        # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in null model:", e$message, "\n")
  return(NULL)
})

# Check if the null model fitted correctly
if (!is.null(null_model)) {
  print(null_model)
  tau2_null <- null_model$tau2  # Extract τ² (variance component)
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
}

######################################################

# Fit the Full Moderator Model (All Moderators additive, without Interactions)
full_moderator_model <- tryCatch({
  rma(
    yi = yi,                                      # Effect size
    vi = diag(v_matrix),                          # Use diagonal for within-study variance
    mods = as.formula(paste("~", paste(moderators, collapse = " + "))),  
    #random = random_effects,                      # Random effects structure
    data = data_subset,                           # Data subset
    method = "REML",                              # Restricted Maximum Likelihood Estimation
    control = control_params                      # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in interaction model:", e$message, "\n")
  return(NULL)
})

# Check if the interaction model fitted correctly
if (!is.null(full_moderator_model)) {
  print(full_moderator_model)
  tau2_full_moderator <- full_moderator_model$tau2  # Extract τ²
  cat("τ² (Full Moderator Model):", tau2_interaction, "\n")
} else {
  cat("⚠ Warning: τ² (Full Moderator Model) is 0 or model fitting failed.\n")
}

# Calculate proportion of explained heterogeneity for the single specified response variable
if (!is.null(null_model) && !is.null(full_moderator_model)) {
  proportion_explained <- abs((tau2_null - tau2_full_moderator) / tau2_null) * 100
  # proportion_explained <- min(proportion_explained, 1000)  # Cap at 1000% if needed
  
  cat("\nProportion of Explained Heterogeneity:", proportion_explained, "%\n")
} else {
  cat("\nCannot compute explained heterogeneity due to missing models.\n")
}
```




```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

##########################################################################
# Start time tracking
start.time <- Sys.time()
##########################################################################
# Set up parallel processing
plan(multisession, workers = parallel::detectCores() - 1)
##########################################################################


# Initialize an empty data frame to store results
heterogeneity_results_clean <- data.frame()

# Define all response variables
response_variables <- unique(meta_data$response_variable)

# Define moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", 
                "soil_texture", "no_tree_per_m", "tree_height", "alley_width")

# Global control parameters for optimization
control_params <- list(
  optimizer = "optim",   
  method = "BFGS",       
  iter.max = 10000,      
  rel.tol = 1e-12        
)

# Loop through each response variable
for (response_variable in response_variables) {
  
  cat("\n-------------------------\nProcessing:", response_variable, "\n-------------------------\n")
  
  # Subset data for the response variable
  data_subset <- meta_data[meta_data$response_variable == response_variable, ]
  
  # Extract the variance-covariance matrix
  v_matrix <- v_matrices[[response_variable]]
  
  # Skip if variance matrix is missing
  if (is.null(v_matrix)) {
    cat("⚠ Skipping", response_variable, "- No variance matrix found.\n")
    next
  }
  
  # Fit the Null Model (Global Average without Moderators)
  null_model <- tryCatch({
    rma(
      yi = yi,                        
      vi = diag(v_matrix),            
      data = data_subset,             
      method = "REML",                
      control = control_params        
    )
  }, error = function(e) {
    cat("Error in null model for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Extract τ² from the null model
  if (!is.null(null_model)) {
    tau2_null <- null_model$tau2
    cat("τ² (Null Model) for", response_variable, ":", tau2_null, "\n")
  } else {
    cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed for", response_variable, "\n")
    next  
  }
  
  # Fit the Full Moderator Model (All Moderators additive, without Interactions)
  full_moderator_model <- tryCatch({
    rma(
      yi = yi,                                     
      vi = diag(v_matrix),                         
      mods = as.formula(paste("~", paste(moderators, collapse = " + "))),  
      data = data_subset,                          
      method = "REML",                             
      control = control_params                     
    )
  }, error = function(e) {
    cat("Error in full moderator model for", response_variable, ":", e$message, "\n")
    return(NULL)
  })
  
  # Extract τ² from the full moderator model
  if (!is.null(full_moderator_model)) {
    tau2_full_moderator <- full_moderator_model$tau2
    cat("τ² (Full Moderator Model) for", response_variable, ":", tau2_full_moderator, "\n")
  } else {
    cat("⚠ Warning: τ² (Full Moderator Model) is 0 or model fitting failed for", response_variable, "\n")
    next  
  }
  
  # Compute proportion of explained heterogeneity
  proportion_explained <- abs((tau2_null - tau2_full_moderator) / tau2_null) * 100
  proportion_explained <- min(proportion_explained, 1000)  
  cat("\nProportion of Explained Heterogeneity:", proportion_explained, "%\n")
  
  # Loop through each moderator individually
  for (moderator in moderators) {
    
    if (!(moderator %in% colnames(data_subset))) {
      cat("\n⚠ Skipping", moderator, "for", response_variable, "- Not present in dataset.\n")
      next
    }
    
    cat("\nFitting model for moderator:", moderator, "on", response_variable, "\n")
    
    mod_model <- tryCatch({
      rma(
        yi = yi,                                 
        vi = diag(v_matrix),                     
        mods = as.formula(paste("~", moderator)),
        data = data_subset,                      
        method = "REML",                         
        control = control_params                 
      )
    }, error = function(e) {
      cat("Error in model for", moderator, "on", response_variable, ":", e$message, "\n")
      return(NULL)
    })
    
    # Extract τ² from the moderator model
    if (!is.null(mod_model)) {
      tau2_moderated <- mod_model$tau2
      cat("  τ² (Model with", moderator, "on", response_variable, "):", tau2_moderated, "\n")
      
      # Compute proportion of explained heterogeneity
      proportion_explained_mod <- abs((tau2_null - tau2_moderated) / tau2_null) * 100
      proportion_explained_mod <- min(proportion_explained_mod, 1000)  
      
      # Extract model summary
      model_summary <- summary(mod_model)
      
      # Extract estimates and statistics
      estimates <- as.numeric(model_summary$b)   # Effect sizes
      se_values <- as.numeric(model_summary$se)  # Standard errors
      z_values <- as.numeric(model_summary$zval) # Z-values
      p_values <- as.numeric(model_summary$pval) # P-values
      ci_lower <- as.numeric(model_summary$ci.lb) # Lower CI
      ci_upper <- as.numeric(model_summary$ci.ub) # Upper CI
      
      # Add significance codes
      significance_levels <- case_when(
        p_values < 0.001 ~ "***",
        p_values < 0.01  ~ "**",
        p_values < 0.05  ~ "*",
        p_values < 0.1   ~ ".",
        TRUE             ~ ""
      )
      
      # Store results
      mod_results_df <- data.frame(
        ResponseVariable = response_variable,
        Moderator = rownames(model_summary$b),
        Tau2_Null = tau2_null,
        Tau2_Moderated = tau2_moderated,
        ProportionExplained = proportion_explained_mod,
        Estimate = estimates,
        SE = se_values,
        Z_Value = z_values,
        P_Value = p_values,
        CI_Lower = ci_lower,
        CI_Upper = ci_upper,
        Significance = significance_levels
      )
      
      # Append results
      heterogeneity_results_clean <- rbind(heterogeneity_results_clean, mod_results_df)
    } else {
      cat("⚠ Warning: Model for", moderator, "on", response_variable, "failed.\n")
    }
  }
}

# Print results
heterogeneity_results_clean


##########################################################################
# End time tracking
end.time <- Sys.time()
time.taken <- end.time - start.time
cat("\nTotal time taken:", time.taken, units(time.taken), "\n")
##########################################################################
# Last go (04/02-2025) 
```

```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

# Define response variable
response_variable <- "Crop yield"  # Change accordingly

# Subset data for the response variable
data_subset <- meta_data[meta_data$response_variable == response_variable, ]

# Extract the variance-covariance matrix
v_matrix <- v_matrices[[response_variable]]

# Define the moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Define the random effects structure [before "list(~ 1 | exp_id)"]
random_effects <- list(~ 1 | id_article/exp_id) # Random intercept for each study and experiment (location x year)

# Print data checks
# print(dim(data_subset))
# print(head(data_subset))

# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

##################################################################################################################

# Fit the Null Model (Global Average without Moderators)
null_model <- tryCatch({
  rma(
    yi = yi,                        # Effect size
    vi = diag(v_matrix),            # Use diagonal for within-study variance
    #random = random_effects,        # Random effects structure
    data = data_subset,             # Data subset
    method = "REML",                # Restricted Maximum Likelihood Estimation
    control = control_params        # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in null model:", e$message, "\n")
  return(NULL)
})

# Check if the null model fitted correctly
if (!is.null(null_model)) {
  print(null_model)
  tau2_null <- null_model$tau2  # Extract τ² (variance component)
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
}

######################################################

# Fit the Full Moderator Model (All Moderators additive, without Interactions)
full_moderator_model <- tryCatch({
  rma(
    yi = yi,                                      # Effect size
    vi = diag(v_matrix),                          # Use diagonal for within-study variance
    mods = as.formula(paste("~ -1 +", paste(moderators, collapse = " + "))),  
    #random = random_effects,                      # Random effects structure
    data = data_subset,                           # Data subset
    method = "REML",                              # Restricted Maximum Likelihood Estimation
    control = control_params                      # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in interaction model:", e$message, "\n")
  return(NULL)
})

# Check if the interaction model fitted correctly
if (!is.null(full_moderator_model)) {
  print(full_moderator_model)
  tau2_full_moderator <- full_moderator_model$tau2  # Extract τ²
  cat("τ² (Full Moderator Model):", tau2_interaction, "\n")
} else {
  cat("⚠ Warning: τ² (Full Moderator Model) is 0 or model fitting failed.\n")
}
```
```{r}

```



```{r}
####################################################################################################
# MODERATOR ANALYSIS - PROPORTION OF EXPLAINED HETEROGENEITY USING RE-FITTED MODELS NO RANDOM INTERCEPT
####################################################################################################

# Define response variable
response_variable <- "Crop yield"  # Change accordingly

# Subset data for the response variable
data_subset <- meta_data[meta_data$response_variable == response_variable, ]

# Extract the variance-covariance matrix
v_matrix <- v_matrices[[response_variable]]

# Define the moderators to include in the model
moderators <- c("tree_type", "crop_type", "age_system", "season", "soil_texture")

# Define the random effects structure [before "list(~ 1 | exp_id)"]
random_effects <- list(~ 1 | id_article/exp_id) # Random intercept for each study and experiment (location x year)

# Print data checks
# print(dim(data_subset))
# print(head(data_subset))

# Global control parameters for optimization
control_params <- list(
  # Specifies the optimization function to use, "optim" is the base R optimizer, allowing for flexible tuning
  optimizer = "optim",
  # Defines the specific optimization algorithm. "BFGS" (Broyden–Fletcher–Goldfarb–Shanno) is a quasi-Newton method
  # This optimization algorithm is often used for unconstrained optimization problems and works well in meta-analytic models with moderate to large datasets
  method = "BFGS",
  # Maximum number of iterations for the optimization routine. If the models does not converge, increasing this value can help.
  # However, very high values may lead to excessive computation time.
  iter.max = 10000,
  # Relative tolerance level for convergence. Determines when the optimization process should stop.
  # Lower values (e.g., 1e-15) enforce stricter convergence, ensuring more precise results but requiring longer run times.
  # Higher values (e.g., 1e-4) allow faster convergence but may reduce accuracy (default in metafor is 1e-10).
  rel.tol = 1e-12
  # Uncomment this line if you want to track optimizer progress for each individual model
  # This will print detailed iteration steps, useful for debugging non-convergence issues.
  # verbose = TRUE   
)

##################################################################################################################

# Fit the Null Model (Global Average without Moderators)
null_model <- tryCatch({
  rma(
    yi = yi,                        # Effect size
    vi = diag(v_matrix),            # Use diagonal for within-study variance
    #random = random_effects,        # Random effects structure
    data = data_subset,             # Data subset
    method = "REML",                # Restricted Maximum Likelihood Estimation
    control = control_params        # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in null model:", e$message, "\n")
  return(NULL)
})

# Check if the null model fitted correctly
if (!is.null(null_model)) {
  print(null_model)
  tau2_null <- null_model$tau2  # Extract τ² (variance component)
  cat("τ² (Null Model):", tau2_null, "\n")
} else {
  cat("⚠ Warning: τ² (Null Model) is 0 or model fitting failed.\n")
}

######################################################

# Fit the Full Moderator Model (All Moderators additive, without Interactions)
full_moderator_model <- tryCatch({
  rma(
    yi = yi,                                      # Effect size
    vi = diag(v_matrix),                          # Use diagonal for within-study variance
    mods = as.formula(paste("~ -1 +", paste(moderators, collapse = " + "))),  
    #random = random_effects,                      # Random effects structure
    data = data_subset,                           # Data subset
    method = "REML",                              # Restricted Maximum Likelihood Estimation
    control = control_params                      # Optimizer control parameters
  )
}, error = function(e) {
  cat("Error in interaction model:", e$message, "\n")
  return(NULL)
})

# Check if the interaction model fitted correctly
if (!is.null(full_moderator_model)) {
  print(full_moderator_model)
  tau2_full_moderator <- full_moderator_model$tau2  # Extract τ²
  cat("τ² (Full Moderator Model):", tau2_interaction, "\n")
} else {
  cat("⚠ Warning: τ² (Full Moderator Model) is 0 or model fitting failed.\n")
}
```
```{r}

```






Interpretation of the back-transformed values (ROM in percentages) for all response variables


### Interpretation of Back-Transformed Values (ROM in Percentages)

The back-transformed Ratio of Means (ROM) values provide an interpretable measure of effect sizes for all response variables. These percentages allow us to assess how the system impacts different response variables compared to controls, including associated uncertainty (confidence intervals).

---
  
  ### **1. Biodiversity**
  - **Null Model:** **4.42%** [CI: -1.25%, 10.42%]  
Biodiversity is estimated to increase by **4.42%**, but the CI includes zero, indicating no significant effect.  
- **Minimal Random Effects Model:** **4.42%** [CI: -1.25%, 10.42%]  
Identical to the null model, suggesting no improvement by accounting for random effects.  
- **Full Model:** **-7.28%** [CI: -57.80%, 103.74%]  
Indicates a **7.28% decrease**, but the very wide CI reflects high uncertainty and potential overfitting.  
- **Interaction Model:** **-2.84%** [CI: -46.59%, 76.75%]  
Shows a **2.84% decrease**, with a similarly wide and uncertain CI.

---
  
  ### **2. Greenhouse Gas Emissions**
  - **Null Model:** **0.94%** [CI: 0.69%, 1.20%]  
A minimal and statistically significant increase in emissions, reflecting consistent results across the CI.  
- **Minimal Random Effects Model:** **0.94%** [CI: 0.69%, 1.20%]  
No improvement over the null model.  
- **Full Model:** **25.73%** [CI: 14.49%, 38.06%]  
A large, statistically significant increase in emissions, highlighting the importance of moderators in explaining variability.  
- **Interaction Model:** **25.58%** [CI: 14.29%, 37.98%]  
Similar to the full model, demonstrating consistency with added interactions.

---
  
  ### **3. Product Quality**
  - **Null Model:** **-2.00%** [CI: -3.66%, -0.32%]  
A small, statistically significant decrease in product quality.  
- **Minimal Random Effects Model:** **-2.00%** [CI: -3.66%, -0.32%]  
Identical to the null model, indicating no improvement.  
- **Full Model:** **-1.71%** [CI: -6.31%, 3.13%]  
A smaller decrease, but with a CI including zero, indicating no significant effect.  
- **Interaction Model:** **-0.80%** [CI: -5.30%, 3.92%]  
The smallest effect with high uncertainty, suggesting minimal influence of moderator interactions.

---
  
  ### **4. Crop Yield**
  - **Null Model:** **-2.28%** [CI: -4.40%, -0.11%]  
A small but statistically significant decrease in yield.  
- **Minimal Random Effects Model:** **-2.28%** [CI: -4.40%, -0.11%]  
Identical to the null model.  
- **Full Model:** **-1.67%** [CI: -6.34%, 3.24%]  
A smaller decrease, but the CI includes zero, showing no significant effect.  
- **Interaction Model:** **-25.28%** [CI: -36.39%, -12.24%]  
A large and significant decrease, highlighting the strong effect of interactions among moderators.

---
  
  ### **5. Pest and Disease**
  - **Null Model:** **-8.67%** [CI: -24.89%, 11.05%]  
A modest decrease, but the CI includes zero, indicating no significant effect.  
- **Minimal Random Effects Model:** **-8.67%** [CI: -24.89%, 11.05%]  
Identical to the null model.  
- **Full Model:** **-40.47%** [CI: -64.04%, -1.46%]  
A large, statistically significant decrease, showing the impact of moderators.  
- **Interaction Model:** **-40.47%** [CI: -64.04%, -1.46%]  
Identical to the full model, suggesting interactions add no further value.

---
  
  ### **6. Soil Quality**
  - **Null Model:** **3.37%** [CI: -1.14%, 8.08%]  
A small increase, but the CI includes zero, indicating no significant effect.  
- **Minimal Random Effects Model:** **3.37%** [CI: -1.14%, 8.08%]  
No improvement over the null model.  
- **Full Model:** **-33.61%** [CI: -54.09%, -3.98%]  
A large and statistically significant decrease, reflecting the critical role of moderators.
- **Interaction Model:**  
  
  ---
  
  ### **7. Water Quality**
  - **Null Model:** **1.56%** [CI: -1.35%, 4.56%]  
A small increase, with the CI including zero, indicating no significant effect.  
- **Minimal Random Effects Model:** **1.56%** [CI: -1.35%, 4.56%]  
Identical to the null model.  
- **Full Model:** **2.69%** [CI: -10.40%, 17.68%]  
A slightly larger increase, but the wide CI reflects high uncertainty.  
- **Interaction Model:** **2.69%** [CI: -10.40%, 17.68%]  
Identical to the full model, suggesting interactions add no value.

---
  
  ### **General Observations**
  1. **Simple Models (Null, Minimal Random Effects):** These provide consistent but limited insights, often failing to capture significant effects.
2. **Full Model:** This generally improves fit, revealing significant effects for variables like `Greenhouse Gas Emissions`, `Pest and Disease`, and `Soil Quality`.
3. **Interaction Model:** Adds minimal value beyond the full model for most variables, except for `Crop Yield`.

---
  
  ### **Conclusions**
  - **Greenhouse Gas Emissions** and **Pest and Disease** see the most substantial improvements with complex models, reflecting the value of moderators.  
- **Biodiversity**, **Product Quality**, and **Water Quality** remain uncertain, with wide CIs indicating potential data limitations or overfitting.  
- Simplified models suffice for some variables, but full models are critical for uncovering nuanced effects in others.





Explanations of the above Multi-Model Fitting 

Descriptions of the 'Hierarchical Complexity Approach' Multi-Model Fitting for Temperate Silvoarable Agroforrestry and Ecosystem Services Meta-Analysis

In this study we decided to employ a hierarchical complexity approach for meta-analysis multi-model fitting aligned with Carrillo-Reche et al. (2023) [https://www.sciencedirect.com/science/article/pii/S0167880923002232], aka the "Cabbage approach" to evaluate the effects of silvoarable agroforestry systems across multiple response variables. Meta-analytic models were constructed using the `rma.mv` function from the **metafor** package, accounting for random effects and systematically including moderators to assess their contribution to heterogeneity reduction and effect size estimation.

Model Construction and Fitting
Models were fitted incrementally for each response variable, starting with a null model to estimate the global average effect size without moderators. A minimal random effects model introduced random variability at the experiment level (`~ 1 | exp_id`) to account for between-study differences. Moderator models were then constructed to examine the independent effects of moderators such as `tree_type`, `crop_type`, `age_system`, `season`, and `soil_texture`. A full model combined all moderators additively, while a full interaction model evaluated interaction terms between all moderators. Convergence criteria were tightened by increasing the maximum iterations (`iter.max = 10000`) and setting the relative tolerance (`rel.tol = 1e-12`) to improve model stability, precision and ensuring convergence.

Error and Warning Analysis
During model fitting, several warnings and errors were encountered, highlighting potential limitations in the dataset or model structure. Redundant predictors were dropped due to collinearity or lack of variability, affecting models for all response variables (see the resulting output messages from the multi-model fitting code chunk above). For instance, in cases like `tree_type` and `crop_type`, collinearity led to the exclusion of certain predictors. Observations with missing values in moderators were omitted, particularly for `Biodiversity`, where 14 rows were removed. High variance ratio warnings indicated extreme variability in sampling variances, notably for `Crop yield`, `Soil quality`, and `Water quality`, leading to unstable parameter estimates. Convergence failures occurred for the full interaction model in `Soil quality` and `Water quality`, likely due to excessive complexity and insufficient data for interaction terms.

Alternatively, we could have fitted the Full model only with selected interaction pairs of moderators?
  
  # Full model with selected moderator interactions
  interaction_pairs <- list(
    c("tree_type", "crop_type"),
    c("age_system", "season"),
    c("season", "soil_texture")
  )

[...]

Model Outcomes
The null and minimal random effects models were generally successful, providing baseline estimates and capturing between-study heterogeneity. However, the interaction models often failed to converge, particularly when all moderators and their interactions were included. This highlights the challenges of modeling complex interactions in datasets with limited observations or high heterogeneity.













Streamlined Interpretation of AIC Values

The relative AIC values provide a comparison of model performance against the Null Model for each response variable. Negative relative AIC values indicate better model performance, while positive values indicate poorer performance. For `Product Quality` and `Water Quality`, the absolute AIC values (which were negative) require special attention, as positive relative differences in these cases reflect better model performance.

**Key Insights by Response Variable:**
  
  1. **Biodiversity:**
  - Both the `Full Model` and `Interaction Model` demonstrate significantly better performance than the Null Model, with **relative AIC values of -656** and **-645**, respectively, underscoring the importance of moderators in explaining biodiversity outcomes.
- The `Minimal Random Effects Model` shows no improvement over the Null Model, indicating that adding random effects alone does not enhance performance.

2. **Crop Yield:**
  - The `Interaction Model` and the `Full Model` show substantial improvements over the Null Model, with **relative AIC values of -1191** and **-894**, respectively. This highlights the critical role of both moderators and their interactions in explaining crop yield variability.
- The `Null Model` and `Minimal Random Effects Model` perform identically, showing no added value from random effects alone.

3. **Greenhouse Gas Emissions:**
  - The `Full Model` and `Interaction Model` exhibit **exceptional improvements** over the Null Model, with **relative AIC values of -3156** and **-3150**, respectively. These results emphasize the necessity of including multiple moderators to capture the complex dynamics of emissions data.
- Similar to crop yield, the `Minimal Random Effects Model` offers no improvement over the Null Model.

4. **Pest and Disease:**
  - Both the `Full Model` and the `Interaction Model` show improved performance, with **relative AIC values of -391** and **-392**, respectively, demonstrating the value of including moderators for pest and disease data.
- However, interactions between moderators (captured in the `Interaction Model`) do not improve the relative AIC further, indicating that the additional complexity is unnecessary.
- The `Minimal Random Effects Model` performs comparably to the Null Model.

5. **Product Quality:**
  - Positive relative AIC values for the `Interaction Model` (**84**) and the `Full Model` (**44**) indicate that these models outperform the Null Model in terms of absolute AIC values, contrary to initial expectations.
- The `Interaction Model` shows the greatest improvement, indicating that capturing interactions between moderators is essential for better explaining product quality outcomes.
- The `Minimal Random Effects Model` performs similarly to the Null Model, offering no substantial benefit.

6. **Soil Quality:**
  - The `Full Model` demonstrates a substantial improvement with a **relative AIC value of -950**, indicating that capturing high complexity significantly enhances model performance for soil quality.
- The absence of data for the `Interaction Model` suggests potential model-fitting issues when including interactions among moderators.
- The `Minimal Random Effects Model` adds no value compared to the Null Model.

7. **Water Quality:**
  - Positive relative AIC values for the `Interaction Model` (**19**) and the `Full Model` (**19**) suggest these models outperform the Null Model in terms of absolute AIC values, similar to product quality.
- The additional complexity of interactions between moderators does not further improve performance, as seen in the similar relative AIC values for the `Full Model` and `Interaction Model`.
- The `Minimal Random Effects Model` again shows no improvement over the Null Model.

**General Observations:**
  - **Complex models like the Full and Interaction Models generally outperform simpler models**, especially for complex response variables such as greenhouse gas emissions, crop yield, and biodiversity.
- **Product Quality and Water Quality require attention to absolute AIC values**, as positive relative AIC values reflect better performance due to negative absolute AICs.
- Moderators play a critical role in improving model performance, but their interactions are selectively important, as shown for product quality and crop yield.

Implications:
  This refined analysis confirms the importance of tailoring model complexity and moderator interactions to specific response variables. While complex models are crucial for variables with significant variability or interactions, simpler models suffice for others, provided they capture the essential dynamics.


The model performance analysis indicates that the **Interaction Model** consistently outperforms all other models, achieving the lowest mean adjusted relative AIC (-913.72) and BIC (-892.45) across all response variables. This result demonstrates that accounting for interactions among moderators is crucial for capturing the complexities of the data and delivering the most explanatory power.

The **Full Model**, which includes all main effects of moderators but excludes interactions, ranks second with a mean adjusted relative AIC of -871.91 and BIC of -861.33. While it provides substantial improvements over simpler models, it falls short of the Interaction Model due to its inability to account for nuanced interactions among variables.

Among individual moderator models, `age_system` shows the most significant improvement in performance, followed by `crop_type`, `tree_type`, and `soil_texture`. These moderators enhance explanatory power to varying degrees but remain less impactful compared to the comprehensive models. The `season` moderator provides only limited improvement, indicating its minimal relevance across the response variables.

Baseline models, including the **Minimal Random Effects Model** and the **Null Model**, perform identically and fail to capture meaningful variability. Their lack of improvement underscores the necessity of incorporating moderators and interactions for effective model performance.

Overall, the results highlight the superiority of the Interaction Model, emphasizing the importance of capturing complex relationships among moderators to explain variations across the response variables effectively. Simpler models, while informative in specific contexts, are insufficient for fully addressing the intricacies of the data.




Crop yield	NA	0	203339.1624	      null_model
Crop yield	NA	0	95862.5010        interaction_model
Crop yield	99.80134	0	134398.4729	full_model

The results of the heterogeneity statistics indicate the following:
  
  1. **I² Values:**
  - The I² values range from 36% (Product quality) to 99.98% (Biodiversity), suggesting varying levels of heterogeneity across response variables.
- High I² values (close to 100%) for most response variables (e.g., Biodiversity, Crop yield, Pest and Disease, Soil quality) indicate that a large proportion of the variability in effect sizes is due to heterogeneity rather than sampling error.
- Moderate I² for Product quality (36.01%) suggests less heterogeneity, with sampling error accounting for most of the variability.

2. **Tau² (Between-Study Variance):**
  - All Tau² values are `0`, indicating no estimated between-study variance in the random-effects models. This could suggest either true homogeneity of effect sizes or insufficient power to detect between-study variance.

3. **QE (Cochran's Q Test):**
   - QE values are very high for most response variables, particularly for Biodiversity, Crop yield, and Soil quality, suggesting significant residual heterogeneity not explained by the model. The associated QE p-values (<0.001 for all) confirm that the observed heterogeneity is statistically significant.

4. **Implications:**
   - The high I² values, combined with significant QE tests and zero Tau², indicate that heterogeneity exists, but it is not being captured by the random-effects variance component (Tau²). This suggests that other moderators or model structures may need to be explored to better explain the variability.

5. **Recommendations:**
   - For variables with very high I² and significant QE (e.g., Biodiversity, Crop yield, Soil quality), consider including additional moderators or interaction terms in the model to account for heterogeneity.
   - For Product quality (lower I²), the model seems to explain a reasonable proportion of variability, though further exploration of moderators could still be beneficial.

In summary, while the models reveal significant heterogeneity across most response variables, the lack of between-study variance (Tau² = 0) suggests that additional moderators or refinements to model specifications may be required to better account for the observed variability.

# 2a: Extract influence of higher-level and sub-level moderators

# 2b: Evaluate the proportion of heterogeneity explained by each moderator


COMPARE WITH THE 



















```{r}
# Function to extract data for the forest plot using `id_study` as labels
prepare_forest_data <- function(model) {
  if (is.null(model)) return(NULL)

  # Extract study-level effect sizes and variances
  study_effects <- model$yi
  study_variances <- model$vi
  
  # Extract study labels from the model's data
         study_labels <- model$data$id_study 
         
         # Create study-level data frame
         study_data <- data.frame(
           Study = as.character(study_labels),  # Ensure labels are character type
           ES = study_effects,
           SE = sqrt(study_variances),
           Type = "Study"
         ) %>%
           mutate(
             Lower_CI = ES - 1.96 * SE,
             Upper_CI = ES + 1.96 * SE
           )
         
         # Extract overall summary estimate from the model
         summary_data <- data.frame(
           Study = "Summary",
           ES = model$b[1],
           SE = model$se,
           Type = "Summary"
         ) %>%
           mutate(
             Lower_CI = ES - 1.96 * SE,
             Upper_CI = ES + 1.96 * SE
           )
         
         # Combine study-level and summary data
         forest_data <- rbind(study_data, summary_data)
         
         # Ensure the ordering of the study factor (so that Summary appears at the bottom)
         forest_data$Study <- factor(forest_data$Study, levels = rev(unique(forest_data$Study)))
         
         return(forest_data)
      }

# Example: Extract data for Biodiversity minimal random effects model
forest_data <- prepare_forest_data(model_results$Biodiversity$minimal_random_effects)

# Check extracted data
print(forest_data)

forest_data |> glimpse()
```

```{r}
# Summarizing the forest data by Study (mean ES and pooled SE)
forest_data_summarized <- forest_data |> 
  group_by(Study) |> 
  summarise(
    Mean_ES = mean(ES, na.rm = TRUE),  # Mean effect size per study
    Pooled_SE = sqrt(sum(SE^2, na.rm = TRUE) / n()),  # Pooled SE (corrected)
    .groups = "drop"
  ) |> 
  mutate(
    Lower_CI = Mean_ES - 1.96 * Pooled_SE,
    Upper_CI = Mean_ES + 1.96 * Pooled_SE,
    Type = "Study"  # Label for ggplot grouping
  )

# Adding the summary estimate from the model
summary_data <- data.frame(
  Study = "Summary",
  Mean_ES = model_results$Biodiversity$minimal_random_effects$b[1],  # Summary ES
  Pooled_SE = model_results$Biodiversity$minimal_random_effects$se,  # Model SE
  Type = "Summary"
) |> 
  mutate(
    Lower_CI = Mean_ES - 1.96 * Pooled_SE,
    Upper_CI = Mean_ES + 1.96 * Pooled_SE
  )

# Combine study-level and overall summary data
forest_data_final <- bind_rows(forest_data_summarized, summary_data)

# Ensure ordering for visualization (Summary last)
forest_data_final$Study <- factor(forest_data_final$Study, levels = rev(unique(forest_data_final$Study)))

# Check summarized dataset
forest_data_final

# Apply back-transformation (convert log response ratios to percent change)
forest_data_final <- forest_data_final |> 
  mutate(
    Mean_ES = (exp(Mean_ES) - 1) * 100,  # Convert log-ROM to percentage
    Lower_CI = (exp(Lower_CI) - 1) * 100,
    Upper_CI = (exp(Upper_CI) - 1) * 100
  )

```

```{r}
# Create Back-Transformed Forest Plot
ggplot(forest_data_final, aes(x = Study, y = Mean_ES, ymin = Lower_CI, ymax = Upper_CI, color = Type)) +
  geom_pointrange(size = 0.7) +  
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +  
  coord_flip() +  
  scale_color_manual(values = c("Study" = "grey50", "Summary" = "black")) +  
  labs(
    title = "Forest Plot: Biodiversity (Minimal Random Effects Model, Back-Transformed)",
    x = "Study",
    y = "Effect Size (% Change from Control)",  # Updated label for clarity
    color = "Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 10),
    axis.title.y = element_text(size = 12, face = "bold"),
    axis.title.x = element_text(size = 12, face = "bold")
  )
```





```{r}
# Preprocessing Step of mv_influence_diagnostics data 

mv_influence_diagnostics |> glimpse()

# Function to clean study names (remove suffixes)
clean_study_name <- function(study) {
  str_replace(study, "\\.\\d+$", "")  # Removes last dot followed by numbers
}

# Apply function to create a new column with cleaned study names
mv_influence_diagnostics <- mv_influence_diagnostics %>%
  mutate(Study_Clean = clean_study_name(Study))  # New column without suffix

# Summarize per unique Response_Variable
mv_influence_diagnostics_summary <- mv_influence_diagnostics %>%
  group_by(Response_Variable) %>%
  summarise(
    Mean_Cooks_Distance = mean(Cooks_Distance, na.rm = TRUE),
    Mean_DFBeta = mean(Max_DFBeta, na.rm = TRUE),
    Mean_Hat = mean(Max_Hat, na.rm = TRUE),
    
    # Compute standard errors for confidence intervals
    SE_Cooks_Distance = sd(Cooks_Distance, na.rm = TRUE) / sqrt(n()),
    SE_DFBeta = sd(Max_DFBeta, na.rm = TRUE) / sqrt(n()),
    SE_Hat = sd(Max_Hat, na.rm = TRUE) / sqrt(n()),
    
    # Compute confidence intervals (CI = Mean ± 1.96 × SE)
    Lower_CI_Cooks = Mean_Cooks_Distance - 1.96 * SE_Cooks_Distance,
    Upper_CI_Cooks = Mean_Cooks_Distance + 1.96 * SE_Cooks_Distance,
    Lower_CI_DFBeta = Mean_DFBeta - 1.96 * SE_DFBeta,
    Upper_CI_DFBeta = Mean_DFBeta + 1.96 * SE_DFBeta,
    Lower_CI_Hat = Mean_Hat - 1.96 * SE_Hat,
    Upper_CI_Hat = Mean_Hat + 1.96 * SE_Hat
  ) %>%
  ungroup()

# View summary to check
# mv_influence_diagnostics_summary |> glimpse()
mv_influence_diagnostics_summary
```


```{r}
# 3. Plot Cook’s Distance
#######################################################################################

# Check for missing values
mv_influence_diagnostics_summary %>%
  filter(is.na(Mean_Cooks_Distance) | is.na(Lower_CI_Cooks) | is.na(Upper_CI_Cooks))

# Check for extreme values outside reasonable range
mv_influence_diagnostics_summary %>%
  filter(Lower_CI_Cooks < 0 | Upper_CI_Cooks > 1)  # Adjust range if needed

# Define Cook's Distance threshold (adjust if needed)
# cooks_threshold <- 0.0037  
# Cook’s Distance helps identify studies that disproportionately impact model results.
# The threshold 4/n is a rule-of-thumb but can be adjusted based on context.
# A threshold of 0.0037 suggests that studies exceeding this value may be influential, potentially distorting the meta-analysis conclusions.
# If the dataset size is large, a lower threshold (e.g., 0.001 - 0.005) is reasonable because each individual study has a smaller impact. 
# If the dataset is small, a higher threshold might be more appropriate.

# Function to calculate Cook's Distance threshold per response variable
calculate_cooks_threshold <- function(data) {
  data %>%
    group_by(Response_Variable) %>%
    summarise(
      n = n(),  # Count the number of observations per response variable
      Cooks_Threshold = 4 / n  # Compute the Cook's Distance threshold
    ) %>%
    ungroup()
}

# Apply function to influence diagnostics dataset
cooks_thresholds <- calculate_cooks_threshold(mv_influence_diagnostics_summary)

# The computed thresholds
cooks_thresholds

#######################################################################################

# Merge the threshold data with the main dataset
mv_influence_diagnostics_summary <- mv_influence_diagnostics_summary %>%
  left_join(cooks_thresholds, by = "Response_Variable")

# Create the plot
mv_influence_diagnostics_summary %>%
  filter(Response_Variable != "Water quality") |>  # Exclude "Water quality"
  ggplot(aes(
    x = reorder(Study_Clean, -Mean_Cooks_Distance), 
    y = Mean_Cooks_Distance
  )) +
  geom_pointrange(aes(ymin = Lower_CI_Cooks, ymax = Upper_CI_Cooks), 
                  color = "black", size = 0.5) + # Confidence intervals
  geom_point(color = "red", size = 2) +  # Mean Cook’s Distance
  geom_hline(aes(yintercept = Cooks_Threshold), 
             linetype = "dashed", color = "blue") +  # Dynamic threshold per response
  coord_flip() +  # Flip axes for readability
  facet_wrap(~Response_Variable, scales = "free_x", nrow = 1) +  # Facet per response variable
  labs(
    title = "Cook’s Distance for Influence Diagnostics (with Confidence Intervals)",
    x = "Study",
    y = "Cook’s Distance",
    caption = "Dashed line = Computed Cook’s Distance Threshold per Response Variable"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 12),  # Highlight facet labels
    axis.text.y = element_text(size = 8),  # Adjust study label size
    panel.grid.minor = element_blank()  # Clean up grid
  )
```

```{r}
# Check for missing values in confidence intervals
# Summary of CI widths per response variable
mv_influence_diagnostics_summary %>%
  mutate(CI_Width = Upper_CI_Cooks - Lower_CI_Cooks) %>%
  group_by(Response_Variable) %>%
  summarise(
    Mean_CI_Width = mean(CI_Width, na.rm = TRUE),
    Max_CI_Width = max(CI_Width, na.rm = TRUE),
    Min_CI_Width = min(CI_Width, na.rm = TRUE)
  ) %>%
  arrange(desc(Mean_CI_Width))

```



Function to perform influence diagnostics

perform_influence_diagnostics <- function(model_results) {
  influence_results <- lapply(names(model_results), function(response) {
    model <- model_results[[response]]$minimal_random_effects
    if (is.null(model)) return(NULL)
    
    # Perform influence diagnostics
    influence <- influence(model, alpha = 0.05, nsim = 1000)
    
    # Extract key influence measures
    cooks_distance <- influence$cooks.distance
    dfbetas <- influence$dfbetas
    dffits <- influence$dffits
    hat_values <- influence$hat
    
    # Combine results into a data frame
    data.frame(
      Response_Variable = response,
      Cooks_Distance = cooks_distance,
      Max_DFBeta = apply(dfbetas, 1, max),
      Max_DFFits = dffits,
      Max_Hat = hat_values,
      stringsAsFactors = FALSE
    )
  })
  
  # Combine results into a single data frame
  do.call(rbind, influence_results)
}

Use the function on selected model
influence_diagnostics <- perform_influence_diagnostics(model_results$Biodiversity$minimal_random_effects$M)



```{r}
# WORING ON THE IMPUTED DATASET
meta_data <- imp_dataset

# Define control parameters for optimization
control_params <- list(optimizer = "optim", 
                       method = "BFGS", 
                       iter.max = 10000, 
                       rel.tol = 1e-12)

# Model 1: Simple Study-Level Random Effects
res_model1 <- rma.mv(
  yi = yi,
  V = vi,
  mods = ~ 1,  # Intercept-only model
  random = ~ 1 | id_article,  # Study-level variance
  data = meta_data,
  method = "REML",
  control = control_params
)

# Model 2: Study + Experiment (Nested Random Effects)
res_model2 <- rma.mv(
  yi = yi,
  V = vi,
  mods = ~ 1,  
  random = ~ 1 | id_article/exp_id,  # Nested structure: study → experiment
  data = meta_data,
  method = "REML",
  control = control_params
)

# Model 3: Study + Location (Crossed Random Effects)
res_model3 <- rma.mv(
  yi = yi,
  V = vi,
  mods = ~ 1,
  random = ~ 1 | id_article/location,  # Crossed structure: study & location
  data = meta_data,
  method = "REML",
  control = control_params
)

# Model 4: Study + Location * Year (Crossed Random Effects with Interaction)
res_model4 <- rma.mv(
  yi = yi,
  V = vi,
  mods = ~ 1,
  random = list(
    ~ 1 | id_article,               # Study-level variance
    ~ 1 | location,                 # Location variance
    ~ 1 | experiment_year,          # Year variance
    ~ 1 | location*experiment_year  # Interaction term (Crossed)
  ),
  data = meta_data,
  method = "REML",
  control = control_params
)
```

```{r}
# Compare Model Performance
AIC(res_model1, res_model2, res_model3, res_model4)
BIC(res_model1, res_model2, res_model3, res_model4)
```


```{r}
# Check Variance Components
summary(res_model4)

# Perform Likelihood Ratio Tests (LRT) to compare models
anova(res_model1, res_model2)
#anova(res_model2, res_model3)
anova(res_model3, res_model4)

```
